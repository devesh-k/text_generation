{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff3de735",
   "metadata": {},
   "source": [
    "## In this notebook, we scale the Cosine with (1-lambda)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac992e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6492f3e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: jupyter in /opt/conda/lib/python3.8/site-packages (1.0.0)\n",
      "Requirement already satisfied: ipywidgets in /opt/conda/lib/python3.8/site-packages (8.1.3)\n",
      "Requirement already satisfied: nbconvert in /opt/conda/lib/python3.8/site-packages (from jupyter) (6.3.0)\n",
      "Requirement already satisfied: qtconsole in /opt/conda/lib/python3.8/site-packages (from jupyter) (5.5.2)\n",
      "Requirement already satisfied: ipykernel in /opt/conda/lib/python3.8/site-packages (from jupyter) (6.29.5)\n",
      "Requirement already satisfied: jupyter-console in /opt/conda/lib/python3.8/site-packages (from jupyter) (6.6.3)\n",
      "Requirement already satisfied: notebook in /opt/conda/lib/python3.8/site-packages (from jupyter) (6.4.1)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.11 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (4.0.11)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.11 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (3.0.11)\n",
      "Requirement already satisfied: comm>=0.1.3 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (7.30.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (5.14.3)\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (4.8.0)\n",
      "Requirement already satisfied: backcall in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: jedi>=0.16 in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.18.1)\n",
      "Requirement already satisfied: pygments in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (2.10.0)\n",
      "Requirement already satisfied: matplotlib-inline in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.3)\n",
      "Requirement already satisfied: pickleshare in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.7.5)\n",
      "Requirement already satisfied: setuptools>=18.5 in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (59.4.0)\n",
      "Requirement already satisfied: decorator in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (5.1.0)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.47)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /opt/conda/lib/python3.8/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.8/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.8/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=6.1.0->ipywidgets) (0.2.5)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.8/site-packages (from ipykernel->jupyter) (5.8.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.8/site-packages (from ipykernel->jupyter) (21.3)\n",
      "Requirement already satisfied: tornado>=6.1 in /opt/conda/lib/python3.8/site-packages (from ipykernel->jupyter) (6.1)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in /opt/conda/lib/python3.8/site-packages (from ipykernel->jupyter) (7.1.0)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /opt/conda/lib/python3.8/site-packages (from ipykernel->jupyter) (5.7.2)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in /opt/conda/lib/python3.8/site-packages (from ipykernel->jupyter) (1.8.2)\n",
      "Requirement already satisfied: pyzmq>=24 in /opt/conda/lib/python3.8/site-packages (from ipykernel->jupyter) (26.0.3)\n",
      "Requirement already satisfied: nest-asyncio in /opt/conda/lib/python3.8/site-packages (from ipykernel->jupyter) (1.5.4)\n",
      "Requirement already satisfied: entrypoints in /opt/conda/lib/python3.8/site-packages (from jupyter-client>=6.1.12->ipykernel->jupyter) (0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /opt/conda/lib/python3.8/site-packages (from jupyter-client>=6.1.12->ipykernel->jupyter) (2.8.2)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /opt/conda/lib/python3.8/site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel->jupyter) (4.2.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.8/site-packages (from python-dateutil>=2.1->jupyter-client>=6.1.12->ipykernel->jupyter) (1.16.0)\n",
      "Requirement already satisfied: nbclient<0.6.0,>=0.5.0 in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (0.5.9)\n",
      "Requirement already satisfied: bleach in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (4.1.0)\n",
      "Requirement already satisfied: testpath in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (0.5.0)\n",
      "Requirement already satisfied: defusedxml in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (0.7.1)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (1.5.0)\n",
      "Requirement already satisfied: jupyterlab-pygments in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (0.1.2)\n",
      "Requirement already satisfied: jinja2>=2.4 in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (3.0.3)\n",
      "Requirement already satisfied: mistune<2,>=0.8.1 in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (0.8.4)\n",
      "Requirement already satisfied: nbformat>=4.4 in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (5.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.8/site-packages (from jinja2>=2.4->nbconvert->jupyter) (2.0.1)\n",
      "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /opt/conda/lib/python3.8/site-packages (from nbformat>=4.4->nbconvert->jupyter) (4.2.1)\n",
      "Requirement already satisfied: ipython-genutils in /opt/conda/lib/python3.8/site-packages (from nbformat>=4.4->nbconvert->jupyter) (0.2.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /opt/conda/lib/python3.8/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.4->nbconvert->jupyter) (21.2.0)\n",
      "Requirement already satisfied: importlib-resources>=1.4.0 in /opt/conda/lib/python3.8/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.4->nbconvert->jupyter) (5.4.0)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /opt/conda/lib/python3.8/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.4->nbconvert->jupyter) (0.18.0)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /opt/conda/lib/python3.8/site-packages (from importlib-resources>=1.4.0->jsonschema!=2.5.0,>=2.4->nbformat>=4.4->nbconvert->jupyter) (3.6.0)\n",
      "Requirement already satisfied: webencodings in /opt/conda/lib/python3.8/site-packages (from bleach->nbconvert->jupyter) (0.5.1)\n",
      "Requirement already satisfied: prometheus-client in /opt/conda/lib/python3.8/site-packages (from notebook->jupyter) (0.12.0)\n",
      "Requirement already satisfied: argon2-cffi in /opt/conda/lib/python3.8/site-packages (from notebook->jupyter) (21.1.0)\n",
      "Requirement already satisfied: terminado>=0.8.3 in /opt/conda/lib/python3.8/site-packages (from notebook->jupyter) (0.12.1)\n",
      "Requirement already satisfied: Send2Trash>=1.5.0 in /opt/conda/lib/python3.8/site-packages (from notebook->jupyter) (1.8.0)\n",
      "Requirement already satisfied: cffi>=1.0.0 in /opt/conda/lib/python3.8/site-packages (from argon2-cffi->notebook->jupyter) (1.15.0)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.8/site-packages (from cffi>=1.0.0->argon2-cffi->notebook->jupyter) (2.21)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging->ipykernel->jupyter) (3.0.6)\n",
      "Requirement already satisfied: qtpy>=2.4.0 in /opt/conda/lib/python3.8/site-packages (from qtconsole->jupyter) (2.4.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#installing some libraries\n",
    "!pip install --upgrade jupyter ipywidgets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b528c179",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch, transformers\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import PreTrainedModel, PretrainedConfig\n",
    "from transformers import AutoModel, AutoConfig,AutoModelForCausalLM, AutoConfig\n",
    "\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "#from datasets import load_dataset\n",
    "import pandas as pd, numpy as np\n",
    "from torch import cuda\n",
    "import datetime\n",
    "import warnings,itertools\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import json,math\n",
    "pre_train = False\n",
    "\n",
    "# Ignore all warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "#pip install transformers bitsandbytes>=0.39.0 -q\n",
    "import zipfile,logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2909a93e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "#global params for training\n",
    "\n",
    "B,T = 32,1024\n",
    "epoch = 50\n",
    "# this controls whether we are using pre-trained wts or not\n",
    "random_init_wts = False\n",
    "nll_learning_steps = int(epoch/3)\n",
    "if cuda.is_available():\n",
    "    device = torch.device('cuda:0')\n",
    "    print(device)\n",
    "else:\n",
    "    device = 'cpu'\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "# these 2 global vars help track the training and val loss\n",
    "#directory to save wts\n",
    "model_path = os.path.join(\"model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "052a7f47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./data/unzip_text_10M'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "directory = os.path.join('.','data','unzip_text_10M')  # Replace with your directory path\n",
    "directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd63f423",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_text(directory):\n",
    "    \"\"\"This function loads the dataset,provided in the zipped format and stores all the text files in list\n",
    "    each file has its contents stored as an item in list and at the end we concatenate all the sub-lists to create a flattened list and return it\"\"\"\n",
    "    directory = os.path.join('.','data','unzip_text_10M',str(directory))  # Replace with your directory path\n",
    "    print(f\"directory :{directory}\")\n",
    "    # List all files in the directory\n",
    "    files = [f for f in os.listdir(directory) if os.path.isfile(os.path.join(directory, f))]\n",
    "    print(f\"files:{files}\")\n",
    "    text_content = []\n",
    "    # Read each file\n",
    "    total_lines = 0\n",
    "    for filenum,filename in enumerate(files):\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            text = file.read()\n",
    "            text_content.append(text)\n",
    "            print(f\"the file:{filename} has been appeneded to the uber list and its length is {len(text_content)} \")\n",
    "            \n",
    "    \n",
    "    flattened_list = ''.join(text_content)\n",
    "    assert (len(flattened_list) == total_lines , f\"Expected {len(flattened_list)} to be equal to {total_lines}\" )\n",
    "    \n",
    "    return flattened_list\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c443c7f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "directory :./data/unzip_text_10M/train_10M\n",
      "files:['switchboard.train', 'simple_wiki.train', 'open_subtitles.train', 'gutenberg.train', 'childes.train', 'bnc_spoken.train']\n",
      "the file:switchboard.train has been appeneded to the uber list and its length is 1 \n",
      "the file:simple_wiki.train has been appeneded to the uber list and its length is 2 \n",
      "the file:open_subtitles.train has been appeneded to the uber list and its length is 3 \n",
      "the file:gutenberg.train has been appeneded to the uber list and its length is 4 \n",
      "the file:childes.train has been appeneded to the uber list and its length is 5 \n",
      "the file:bnc_spoken.train has been appeneded to the uber list and its length is 6 \n"
     ]
    }
   ],
   "source": [
    "#read the dataset and get a list\n",
    "train_list = read_text(\"train_10M\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1e57239",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54215049"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb3c87af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1654\n"
     ]
    }
   ],
   "source": [
    "chunks = len(train_list)//(B*T)\n",
    "print(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1709d2da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17191971 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "#tokeinze the training data\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\",return_tensors = \"pt\" , truncate = True, return_overflowing_tokens=True , padding = False,)\n",
    "enc_train = tokenizer(train_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9c92d8c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.1535097982657136"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comp_ratio = len(train_list)/len(enc_train['input_ids'])\n",
    "comp_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4949c13b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "directory :./data/unzip_text_10M/dev\n",
      "files:['switchboard.dev', 'simple_wiki.dev', 'open_subtitles.dev', 'gutenberg.dev', 'childes.dev', 'bnc_spoken.dev']\n",
      "the file:switchboard.dev has been appeneded to the uber list and its length is 1 \n",
      "the file:simple_wiki.dev has been appeneded to the uber list and its length is 2 \n",
      "the file:open_subtitles.dev has been appeneded to the uber list and its length is 3 \n",
      "the file:gutenberg.dev has been appeneded to the uber list and its length is 4 \n",
      "the file:childes.dev has been appeneded to the uber list and its length is 5 \n",
      "the file:bnc_spoken.dev has been appeneded to the uber list and its length is 6 \n"
     ]
    }
   ],
   "source": [
    "#read validation data and tokenize\n",
    "val_list = read_text(\"dev\")\n",
    "enc_val = tokenizer(val_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c2ec51",
   "metadata": {},
   "source": [
    "### we read the tokenize data and store the input_ids and attention mask as a single long list at run time we reshape the single [1,B*T] tensor into a batched tensor of dimension [B,T].\n",
    "This approach turns out to be more efficient as it removes the need for any extra tokens in the form of padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6bb98b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df_from_list(enc , B= B, T = T, val= False):\n",
    "    \"\"\"This function takes the tokenized list and reated a dataframe where each row contains the input_ids and attention_mask from the tokenizer.\n",
    "    each row of the dataframe is a list with items B*T\"\"\"\n",
    "    chunk_size = B*T\n",
    "    if val:\n",
    "        chunk_size = 2*B*T\n",
    "        \n",
    "    long_list_inp = enc['input_ids']\n",
    "    long_list_attention = enc['attention_mask']\n",
    "\n",
    "    # Step 3: Split the list into chunks and pad the last chunk if necessary\n",
    "    chunks_inp = [long_list_inp[i:i + chunk_size] for i in range(0, len(long_list_inp), chunk_size)]\n",
    "    chunks_att = [long_list_attention[i:i + chunk_size] for i in range(0, len(long_list_attention), chunk_size)]\n",
    "    df = pd.DataFrame({'input_ids': chunks_inp,'attention_mask':chunks_att})\n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4e5d4066",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the dataframe is = 525\n"
     ]
    }
   ],
   "source": [
    "df_train_temp = get_df_from_list(enc_train)\n",
    "# Display the DataFrame\n",
    "df_train_temp.head()\n",
    "print(f\"Length of the dataframe is = {len(df_train_temp)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "17d4195b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the VALDATION dataframe is = 532\n"
     ]
    }
   ],
   "source": [
    "df_val_temp = get_df_from_list(enc_val)\n",
    "# Display the DataFrame\n",
    "df_val_temp.head()\n",
    "print(f\"Length of the VALDATION dataframe is = {len(df_val_temp)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f7f14576",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_df(df,eos_char = tokenizer.eos_token_id,B = B, T = T,val = False):\n",
    "    \"\"\"The 2 functions below makes sure that each row of the dataframe is of equal length and if not it adds the eos token to make it B*T\"\"\"\n",
    "    if val:\n",
    "        B = 2*B\n",
    "    for ind,row in df.iterrows():\n",
    "        if len(row['input_ids']) != B*T :\n",
    "            print(f\"row = {ind} and input_id length = {len(row['input_ids'])}\")\n",
    "            print(f\"row = {ind} and attention length = {len(row['attention_mask'])}\")\n",
    "            pad_len = B*T - len(row['input_ids'])\n",
    "            print(f\"padding the row index {ind} with {pad_len} character\")\n",
    "            row['input_ids'] = row['input_ids']+ [eos_char] * pad_len\n",
    "            #attention mask should be padded to 0\n",
    "            row['attention_mask'] = row['attention_mask']+ [0] * pad_len\n",
    "            print(\"#### POST CONCAT####\")\n",
    "            print(f\"row = {ind} and input_id length = {len(row['input_ids'])}\")\n",
    "            print(f\"row = {ind} and attention length ={len(row['attention_mask'])}\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def verify_len(df,val = False, B= B):\n",
    "    row_ind = []\n",
    "    if val:\n",
    "        B = 2*B\n",
    "    for ind,row in df.iterrows():\n",
    "        if len(row['input_ids']) != B*T :\n",
    "            row_ind.append(ind)\n",
    "        else:\n",
    "            continue\n",
    "    if len(row_ind) !=0:\n",
    "        print(\"CONCATENATION Did not work\")\n",
    "    else:\n",
    "        print(\"CONCATENATION worked\")\n",
    "        \n",
    "            \n",
    "        \n",
    "    \n",
    "        \n",
    "        \n",
    "   \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ce5ab0d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row = 524 and input_id length = 21539\n",
      "row = 524 and attention length = 21539\n",
      "padding the row index 524 with 11229 character\n",
      "#### POST CONCAT####\n",
      "row = 524 and input_id length = 32768\n",
      "row = 524 and attention length =32768\n",
      "CONCATENATION worked\n"
     ]
    }
   ],
   "source": [
    "df_train  = pad_df(df_train_temp)\n",
    "verify_len(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1b0902ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row = 531 and input_id length = 13588\n",
      "row = 531 and attention length = 13588\n",
      "padding the row index 531 with 19180 character\n",
      "#### POST CONCAT####\n",
      "row = 531 and input_id length = 32768\n",
      "row = 531 and attention length =32768\n",
      "CONCATENATION worked\n"
     ]
    }
   ],
   "source": [
    "df_val  = pad_df(df_val_temp)\n",
    "verify_len(df_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "451b06b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_ids</th>\n",
       "      <th>attention_mask</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[32, 25, 197, 40, 1101, 1654, 484, 389, 13, 19...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[6510, 11, 14104, 290, 27913, 13, 198, 32, 25,...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[314, 1612, 11, 340, 338, 1611, 286, 43244, 11...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[318, 407, 922, 329, 262, 1200, 2035, 13, 198,...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[198, 32, 25, 197, 1870, 11, 21480, 11, 314, 4...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           input_ids  \\\n",
       "0  [32, 25, 197, 40, 1101, 1654, 484, 389, 13, 19...   \n",
       "1  [6510, 11, 14104, 290, 27913, 13, 198, 32, 25,...   \n",
       "2  [314, 1612, 11, 340, 338, 1611, 286, 43244, 11...   \n",
       "3  [318, 407, 922, 329, 262, 1200, 2035, 13, 198,...   \n",
       "4  [198, 32, 25, 197, 1870, 11, 21480, 11, 314, 4...   \n",
       "\n",
       "                                      attention_mask  \n",
       "0  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "1  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "2  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "3  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "4  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "000fec0b",
   "metadata": {},
   "source": [
    "## Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2ac3ef6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the tokenizer:\n",
    "model_name = 'distilgpt2'\n",
    "if random_init_wts:\n",
    "    config = AutoConfig.from_pretrained('distilgpt2')\n",
    "    # Initialize the model with random weights\n",
    "    model = AutoModelForCausalLM.from_config(config)\n",
    "else:\n",
    "    #model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "model = torch.compile(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7adc255f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cef3e78d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "525"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f7518abd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# device = \"cuda\"\n",
    "# inp = torch.tensor(df_train.iloc[5]['input_ids']).view(B,T).to(device)\n",
    "# att = torch.tensor(df_train.iloc[5]['attention_mask']).view(B,T).to(device)\n",
    "# lab = inp.clone()\n",
    "# # lab = F.pad(lab[:, 1:], (0, 1), value=-100)  # Shift labels to the left and pad with -100\n",
    "# # lab[:, -1] = -100\n",
    "# model.to(device)\n",
    "# model_out = model(input_ids = inp, attention_mask = att , labels = lab)\n",
    "# assert not torch.isnan(inp).any(), \"Input contains NaN\"\n",
    "# assert not torch.isinf(inp).any(), \"Input contains inf\"\n",
    "# model_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "825fb316",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"Input shape: {inp.shape}\")\n",
    "# print(f\"Attention mask shape: {att.shape}\")\n",
    "# print(f\"Labels shape: {lab.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f532bde6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "93fba05c",
   "metadata": {},
   "source": [
    "### Data loaders and Dataset for batched training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c8559a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataset_pyt_val(Dataset):\n",
    "    \"\"\"Dataset for validation data\"\"\"\n",
    "    def __init__(self, df, B = B, T = T ):\n",
    "        self.df = df\n",
    "        print(f\"Value of B {B}\")\n",
    "                                        \n",
    "    def __getitem__(self, idx):\n",
    "        #print(f\"inside loader...idx ->{idx}\")\n",
    "        input_id_temp = torch.tensor(self.df.iloc[idx]['input_ids'],dtype = torch.long)\n",
    "        att_mask = torch.tensor(self.df.iloc[idx]['attention_mask'],dtype = torch.long)\n",
    "        input_id =   input_id_temp.view(B,T)    \n",
    "        attention_mask = att_mask.view(B,T)\n",
    "        \n",
    "        \n",
    "        return input_id, attention_mask\n",
    "        \n",
    "    def __len__(self):\n",
    "        #return the length of the dataframe\n",
    "        return len(self.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d4f91154",
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataset_pyt_train(Dataset):\n",
    "    def __init__(self, df, B = B, T = T ):\n",
    "        self.df = df\n",
    "                                        \n",
    "    def __getitem__(self, idx):\n",
    "        #print(f\"************* idx for dataloader = {idx}\")\n",
    "        input_id_temp = torch.tensor(self.df.iloc[idx]['input_ids'],dtype = torch.long)\n",
    "        att_mask = torch.tensor(self.df.iloc[idx]['attention_mask'],dtype = torch.long)\n",
    "        input_id =   input_id_temp.view(B,T)    \n",
    "        attention_mask = att_mask.view(B,T)   \n",
    "        \n",
    "        return input_id, attention_mask\n",
    "        \n",
    "    def __len__(self):\n",
    "        #return the length of the dataframe\n",
    "        return len(self.df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390395db",
   "metadata": {},
   "source": [
    "### Note - batch_size is 1 here because we reshape our input data in the shape [B,T] for a batched training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "42d213ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value of B 32\n"
     ]
    }
   ],
   "source": [
    "#train_dataset = dataset_pyt(filtered_df,tokenizer = tokenizer)\n",
    "train_dataset = dataset_pyt_train(df_train)\n",
    "val_dataset = dataset_pyt_val(df_val)\n",
    "#test_dataset = dataset_pyt(df_test,tokenizer = tokenizer)\n",
    "\n",
    "train_loader = DataLoader(train_dataset,batch_size = 1, shuffle = True , num_workers = 0, pin_memory = True)\n",
    "val_loader = DataLoader(val_dataset,batch_size = 1, shuffle = True , num_workers = 0, pin_memory = True)\n",
    "#test_loader = DataLoader(test_dataset,batch_size = batch_size, shuffle = False, collate_fn = custom_collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39a9c20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6c123d7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the train loader is 525\n",
      "Length of the val loader is 532\n",
      "num_tokens= 17203200\n"
     ]
    }
   ],
   "source": [
    "print(f\"Length of the train loader is {len(train_loader)}\")\n",
    "print(f\"Length of the val loader is {len(val_loader)}\")\n",
    "print(f\"num_tokens= {B*T*len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cba95a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_file(log_message, model_name = model_name ,random_init_wts = random_init_wts ):\n",
    "    \"\"\"This function logs the training performance for future reference\"\"\"\n",
    "    current_datetime = datetime.datetime.now()\n",
    "    # Extract date and time components\n",
    "    current_date = str(current_datetime.date())\n",
    "    log_file = model_name +'_Random_init_nll_and_cos_loss'+'random_init_wts'+ '_'+str(random_init_wts)+'_' +current_date+'.log'\n",
    "    print(f\"*****LOGGING INFO IN {log_file}*********\")\n",
    "    filepath = os.path.join(\"model\",log_file)\n",
    "    logging.basicConfig(filename=filepath, \n",
    "                    filemode='a',  # Overwrite the log file each time\n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s', \n",
    "                    level=logging.DEBUG)\n",
    "    logger = logging.getLogger()\n",
    "    logger.info(log_message)\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0611e8a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class check_train_metrics:\n",
    "    def __init__(self, patience=20, min_delta=0 , B = T, T = T,best_loss = torch.inf,early_stop = False):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = best_loss\n",
    "        self.early_stop = early_stop\n",
    "        self.B = B\n",
    "        self.T = T\n",
    "        self.improvement = None\n",
    "\n",
    "    def __call__(self, loss, epoch , epoch_durn, norm , current_lr, num_token):\n",
    "        if self.best_loss - loss > self.min_delta:\n",
    "            \n",
    "            print(f\"training loss has decreased---> reducing the best loss from {self.best_loss:.2f} to {loss:.2f} | throughput = {int(num_token/epoch_durn)} tokens/second | norm = {norm:.4f} | learning rate = {current_lr:.5e}\")\n",
    "            self.best_loss = loss\n",
    "            self.counter = 0\n",
    "            self.improvement = True\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            self.improvement = False\n",
    "            print(f\"No improvement in training  loss-->epoch= {epoch} and best loss is {self.best_loss:.2f}|current_loss = {loss}|counter = {self.counter}\")\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a9d8ff8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class check_val_metrics:\n",
    "    def __init__(self, patience=25, min_delta=0, best_loss = torch.inf,early_stop = False):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = best_loss\n",
    "        self.early_stop = early_stop\n",
    "        \n",
    "\n",
    "    def __call__(self, loss, epoch , model, tokenizer):\n",
    "        if self.best_loss - loss > self.min_delta:\n",
    "            print(f\"Val loss has decreased -->reducing the global validation loss from {self.best_loss:.2f} to {loss:.2f}\")\n",
    "            s1 = (f\"Val loss has decreased -->reducing the global validation loss from {self.best_loss:.2f} to {loss:.2f}\")\n",
    "            print(f\" validation loss for epoch = {epoch} is {loss:.4f}\")\n",
    "            self.best_loss = loss\n",
    "            s2 = f\" validation loss for epoch = {epoch} is {loss:.4f}\"\n",
    "            print(f\" epoch= {epoch} :  val loss is {loss:.4f} \")\n",
    "            s3 = f\" epoch= {epoch} :  val loss is {loss:.4f} \"\n",
    "            #save the model\n",
    "            # Get the current date and time\n",
    "            current_datetime = datetime.datetime.now()\n",
    "            # Extract date and time components\n",
    "            current_date = str(current_datetime.date())\n",
    "            current_time = str(current_datetime.time()).split('.')[0]\n",
    "            file_name = 'model'+ current_date+current_time+'.pth'\n",
    "            path = os.path.join(\"model\",file_name)\n",
    "            print(f\"saving the model {file_name}\")\n",
    "            s4 = f\"saving the model {file_name}\"\n",
    "            #torch.save(model.state_dict(), path)\n",
    "            model.save_pretrained(path)\n",
    "            tokenizer.save_pretrained(path)\n",
    "            log_message = s1+s2+s3+s4\n",
    "            write_file(log_message)\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            print(f\"No improvement in validation loss-->epoch= {epoch} and best val loss is {self.best_loss:.2f}|current_Val loss = {loss}|counter = {self.counter}\")\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f570cbf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad\n",
    "\n",
    "def eval_model(val_loader, model, epoch , device = device,tokenizer = tokenizer,nll_learning_steps = nll_learning_steps):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    e = epoch\n",
    "    embedding_layer = model.transformer.wte\n",
    "    val_loss_accum = 0.0\n",
    "    print(f\"inside validation data for epoch {e}\")\n",
    "    for ind,(input_id,attention_mask) in enumerate(val_loader):\n",
    "        ids = input_id.to(device=device, non_blocking=True)\n",
    "        att_mask = attention_mask.to(device=device, non_blocking=True)\n",
    "        labels = ids.clone()\n",
    "        if e < nll_learning_steps:\n",
    "            with autocast(dtype = torch.bfloat16):\n",
    "                model_output = model(input_ids = ids ,attention_mask = att_mask, labels = labels)\n",
    "                total_loss = model_output.loss \n",
    "                                \n",
    "            \n",
    "        else:\n",
    "            with autocast(dtype = torch.bfloat16):\n",
    "                input_embeddings = embedding_layer(ids)\n",
    "                model_output = model(input_ids = ids ,attention_mask = att_mask, labels = labels)\n",
    "                logits = model_output.logits\n",
    "                predictions = torch.argmax(logits, dim=-1)\n",
    "                #print(f\"predictions = {predictions}\")\n",
    "                prediction_embeddings = embedding_layer(predictions)\n",
    "                cos_sim = F.cosine_similarity(torch.squeeze(prediction_embeddings,dim = 0), torch.squeeze(input_embeddings,dim = 0), dim=1)\n",
    "                cos_loss = 1- cos_sim.mean()\n",
    "                total_loss = cos_loss\n",
    "                del input_embeddings,logits,predictions,cos_sim,cos_loss\n",
    "               \n",
    "        #print(f\"Loss value from val_data = {total_loss}\")\n",
    "        val_loss_accum+=total_loss.detach().item()\n",
    "        del ids,att_mask,labels,model_output\n",
    "    return val_loss_accum\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "58b797c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_loader,val_loader,model,num_epoch = epoch,device = device,tokenizer = tokenizer,nll_learning_steps = nll_learning_steps):\n",
    "    model.train()\n",
    "    device = device\n",
    "    lr_custom = 5e-5\n",
    "    print(f\"inside train model. Device = {device}\")\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.AdamW(params =  model.parameters(), lr= lr_custom,fused = True ,weight_decay = .1)\n",
    "    total_batch_size = 2**19\n",
    "    grad_accum_step = total_batch_size//(B*T)\n",
    "    \n",
    "    extra_train = .1*num_epoch\n",
    "    max_train_steps = int(num_epoch +extra_train )\n",
    "    import time\n",
    "    from transformers import get_linear_schedule_with_warmup\n",
    "    total_steps = len(train_loader) * num_epoch\n",
    "    scheduler_cos = transformers.get_cosine_schedule_with_warmup( optimizer= optimizer, num_warmup_steps =int(total_steps * 0.1) ,num_training_steps= total_steps )\n",
    "        \n",
    "    epoch_train_log = []\n",
    "    epoch_val_log = []\n",
    "    validate_val_metric = check_val_metrics()\n",
    "    validate_train_metric = check_train_metrics(patience=5, min_delta=0 , B = T, T = T)\n",
    "    embedding_layer = model.transformer.wte\n",
    "    for i in range (max_train_steps):\n",
    "        \n",
    "        epoch_start_time = time.time()\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        # we use 2 schedulers - the first LR scheduler uses a cosine decay for 100 epochs the second scheduler takes the last LR from cosine scheduler and then maintains that LR for the next 10 epochs\n",
    "        if i >= num_epoch:\n",
    "            optimizer_reduced_lr = torch.optim.AdamW(params =  model.parameters(), lr= current_lr ,fused = True , weight_decay=.1)\n",
    "            scheduler_constant = transformers.get_constant_schedule_with_warmup( optimizer = optimizer_reduced_lr ,num_warmup_steps = 0, last_epoch = -1 )\n",
    "        \n",
    "        epoch_train_loss = 0       \n",
    "        for ind,(input_id,attention_mask) in enumerate(train_loader):\n",
    "            if ind == int(len(train_loader)/2):\n",
    "                batch_time = time.time()\n",
    "                duration = batch_time - epoch_start_time\n",
    "                print(f\"executing epoch:{i+1}, it took {duration/60} mins from beginning of epoch till batch#{ind}\")\n",
    "            \n",
    "            ids = input_id.to(device=device, non_blocking=True)\n",
    "            att_mask = attention_mask.to(device=device, non_blocking=True)\n",
    "            labels = ids.clone()\n",
    "            assert not torch.isnan(ids).any(), \"Input contains NaN values\"\n",
    "            assert not torch.isinf(ids).any(), \"Input contains infinite values\"\n",
    "            if i < nll_learning_steps:\n",
    "                with autocast(dtype = torch.bfloat16):\n",
    "                    model_output = model(input_ids = ids ,attention_mask = att_mask, labels = labels)\n",
    "                    #print(f\" model_output = {model_output.loss}\")\n",
    "                    if np.isnan(model_output.loss.item()):\n",
    "                        print(\"f nan values encountered..\")\n",
    "                        decoded_texts = [tokenizer.decode(id, skip_special_tokens=True) for id in ids]\n",
    "                        print(f\"*********$$$$$$$$$ decoded_texts = {decoded_texts}*************\")\n",
    "                    total_loss =model_output.loss \n",
    "                                \n",
    "            \n",
    "            else:\n",
    "                with autocast(dtype = torch.bfloat16):\n",
    "                    #print(\"inside cos loss calculations....\")\n",
    "                    input_embeddings = embedding_layer(ids)\n",
    "                    model_output = model(input_ids = ids ,attention_mask = att_mask, labels = labels)\n",
    "                    logits = model_output.logits\n",
    "                    predictions = torch.argmax(logits, dim=-1)\n",
    "                    #print(f\"predictions = {predictions}\")\n",
    "                    prediction_embeddings = embedding_layer(predictions)\n",
    "                    cos_sim = F.cosine_similarity(torch.squeeze(prediction_embeddings,dim = 0), torch.squeeze(input_embeddings,dim = 0), dim=1)\n",
    "                    cos_loss = 1- cos_sim.mean()\n",
    "                    total_loss = cos_loss\n",
    "                del input_embeddings,logits,predictions,cos_sim,cos_loss\n",
    "                     \n",
    "            total_loss.backward()\n",
    "            epoch_train_loss += total_loss.detach().item()\n",
    "            norm = torch.nn.utils.clip_grad_norm(model.parameters() , 1.0)\n",
    "            if i <= num_epoch:\n",
    "                optimizer.step()\n",
    "                scheduler_cos.step()\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "            else:\n",
    "                optimizer_reduced_lr.step()\n",
    "                optimizer_reduced_lr.zero_grad(set_to_none=True)\n",
    "                scheduler_constant.step()\n",
    "                \n",
    "                         \n",
    "            del ids,att_mask,labels,model_output\n",
    "            \n",
    "        #batch processing complete \n",
    "        #print(f\"batch processing complete , lambda = {lambda_val} |total_loss for batch= {total_loss}\")\n",
    "        \n",
    "        if i <= num_epoch:\n",
    "            current_lr = scheduler_cos.get_last_lr()[0]\n",
    "        epoch_end_time = time.time()\n",
    "        epoch_durn = (epoch_end_time - epoch_start_time)\n",
    "        num_token = B*T*len(train_loader)\n",
    "        epoch_train_log.append(epoch_train_loss)\n",
    "        validate_train_metric(epoch_train_loss, i , epoch_durn, norm , current_lr, num_token)\n",
    "        \n",
    "        if validate_train_metric.improvement:\n",
    "            val_loss= eval_model(val_loader, model, epoch = i, device = device,tokenizer = tokenizer)\n",
    "            epoch_val_log.append(val_loss)\n",
    "            validate_val_metric(val_loss, i , model, tokenizer)\n",
    "            if validate_train_metric.early_stop or validate_val_metric.early_stop :\n",
    "                print(f\"early stopping trigerred either from training data or val data | train_counter = {validate_train_metric.counter}|val_counter = {validate_val_metric.counter}\")\n",
    "                break\n",
    "        else:\n",
    "            if validate_val_metric.early_stop:\n",
    "                print(f\"early stopping trigerred from validation data\")\n",
    "                break\n",
    "              \n",
    "    \n",
    "    return model,epoch_train_log,epoch_val_log\n",
    "        \n",
    "            \n",
    "            \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cd7c02ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inside train model. Device = cuda:0\n",
      "executing epoch:1, it took 1.4341915369033813 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the best loss from inf to 1642.30 | throughput = 112630 tokens/second | norm = 5.4803 | learning rate = 1.00000e-05\n",
      "inside validation data for epoch 0\n",
      "Val loss has decreased -->reducing the global validation loss from inf to 1550.01\n",
      " validation loss for epoch = 0 is 1550.0070\n",
      " epoch= 0 :  val loss is 1550.0070 \n",
      "saving the model model2024-07-2102:03:23.pth\n",
      "[2024-07-21 02:03:24,219] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "df: /root/.triton/autotune: No such file or directory\n",
      "df: /root/.triton/autotune: No such file or directory\n",
      "/opt/conda/compiler_compat/ld: cannot find -laio\n",
      "collect2: error: ld returned 1 exit status\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93m [WARNING] \u001b[0m async_io requires the dev libaio .so object and headers but these were not found.\n",
      "\u001b[93m [WARNING] \u001b[0m async_io: please install the libaio-dev package with apt\n",
      "\u001b[93m [WARNING] \u001b[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\n",
      "\u001b[93m [WARNING] \u001b[0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH\n",
      "\u001b[93m [WARNING] \u001b[0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3\n",
      "\u001b[93m [WARNING] \u001b[0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible\n",
      "*****LOGGING INFO IN distilgpt2_Random_init_nll_and_cos_lossrandom_init_wts_False_2024-07-21.log*********\n",
      "executing epoch:2, it took 1.26327699025472 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the best loss from 1642.30 to 1428.19 | throughput = 123831 tokens/second | norm = 2.1480 | learning rate = 2.00000e-05\n",
      "inside validation data for epoch 1\n",
      "Val loss has decreased -->reducing the global validation loss from 1550.01 to 1489.32\n",
      " validation loss for epoch = 1 is 1489.3245\n",
      " epoch= 1 :  val loss is 1489.3245 \n",
      "saving the model model2024-07-2102:06:34.pth\n",
      "*****LOGGING INFO IN distilgpt2_Random_init_nll_and_cos_lossrandom_init_wts_False_2024-07-21.log*********\n",
      "executing epoch:3, it took 1.0521660129229227 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the best loss from 1428.19 to 1391.18 | throughput = 136029 tokens/second | norm = 3.6089 | learning rate = 3.00000e-05\n",
      "inside validation data for epoch 2\n",
      "Val loss has decreased -->reducing the global validation loss from 1489.32 to 1476.89\n",
      " validation loss for epoch = 2 is 1476.8941\n",
      " epoch= 2 :  val loss is 1476.8941 \n",
      "saving the model model2024-07-2102:09:27.pth\n",
      "*****LOGGING INFO IN distilgpt2_Random_init_nll_and_cos_lossrandom_init_wts_False_2024-07-21.log*********\n",
      "executing epoch:4, it took 1.0534012873967489 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the best loss from 1391.18 to 1373.20 | throughput = 135834 tokens/second | norm = 6.7632 | learning rate = 4.00000e-05\n",
      "inside validation data for epoch 3\n",
      "Val loss has decreased -->reducing the global validation loss from 1476.89 to 1463.78\n",
      " validation loss for epoch = 3 is 1463.7755\n",
      " epoch= 3 :  val loss is 1463.7755 \n",
      "saving the model model2024-07-2102:12:20.pth\n",
      "*****LOGGING INFO IN distilgpt2_Random_init_nll_and_cos_lossrandom_init_wts_False_2024-07-21.log*********\n",
      "executing epoch:5, it took 1.0531545241673788 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the best loss from 1373.20 to 1357.33 | throughput = 135912 tokens/second | norm = 3.1010 | learning rate = 5.00000e-05\n",
      "inside validation data for epoch 4\n",
      "Val loss has decreased -->reducing the global validation loss from 1463.78 to 1462.60\n",
      " validation loss for epoch = 4 is 1462.6011\n",
      " epoch= 4 :  val loss is 1462.6011 \n",
      "saving the model model2024-07-2102:15:14.pth\n",
      "*****LOGGING INFO IN distilgpt2_Random_init_nll_and_cos_lossrandom_init_wts_False_2024-07-21.log*********\n",
      "executing epoch:6, it took 1.0519766887029012 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the best loss from 1357.33 to 1341.07 | throughput = 135986 tokens/second | norm = 3.3500 | learning rate = 4.99391e-05\n",
      "inside validation data for epoch 5\n",
      "Val loss has decreased -->reducing the global validation loss from 1462.60 to 1448.75\n",
      " validation loss for epoch = 5 is 1448.7502\n",
      " epoch= 5 :  val loss is 1448.7502 \n",
      "saving the model model2024-07-2102:18:07.pth\n",
      "*****LOGGING INFO IN distilgpt2_Random_init_nll_and_cos_lossrandom_init_wts_False_2024-07-21.log*********\n",
      "executing epoch:7, it took 1.052340575059255 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the best loss from 1341.07 to 1323.37 | throughput = 135989 tokens/second | norm = 3.3541 | learning rate = 4.97567e-05\n",
      "inside validation data for epoch 6\n",
      "Val loss has decreased -->reducing the global validation loss from 1448.75 to 1447.92\n",
      " validation loss for epoch = 6 is 1447.9185\n",
      " epoch= 6 :  val loss is 1447.9185 \n",
      "saving the model model2024-07-2102:21:00.pth\n",
      "*****LOGGING INFO IN distilgpt2_Random_init_nll_and_cos_lossrandom_init_wts_False_2024-07-21.log*********\n",
      "executing epoch:8, it took 1.051716403166453 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the best loss from 1323.37 to 1308.68 | throughput = 136040 tokens/second | norm = 5.9406 | learning rate = 4.94537e-05\n",
      "inside validation data for epoch 7\n",
      "Val loss has decreased -->reducing the global validation loss from 1447.92 to 1441.27\n",
      " validation loss for epoch = 7 is 1441.2716\n",
      " epoch= 7 :  val loss is 1441.2716 \n",
      "saving the model model2024-07-2102:23:53.pth\n",
      "*****LOGGING INFO IN distilgpt2_Random_init_nll_and_cos_lossrandom_init_wts_False_2024-07-21.log*********\n",
      "executing epoch:9, it took 1.052185336748759 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the best loss from 1308.68 to 1296.74 | throughput = 136022 tokens/second | norm = 3.8288 | learning rate = 4.90315e-05\n",
      "inside validation data for epoch 8\n",
      "No improvement in validation loss-->epoch= 8 and best val loss is 1441.27|current_Val loss = 1443.0964550971985|counter = 1\n",
      "executing epoch:10, it took 1.0524561087290445 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the best loss from 1296.74 to 1283.62 | throughput = 135988 tokens/second | norm = 5.2116 | learning rate = 4.84923e-05\n",
      "inside validation data for epoch 9\n",
      "Val loss has decreased -->reducing the global validation loss from 1441.27 to 1439.80\n",
      " validation loss for epoch = 9 is 1439.7957\n",
      " epoch= 9 :  val loss is 1439.7957 \n",
      "saving the model model2024-07-2102:29:38.pth\n",
      "*****LOGGING INFO IN distilgpt2_Random_init_nll_and_cos_lossrandom_init_wts_False_2024-07-21.log*********\n",
      "executing epoch:11, it took 1.0517611185709634 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the best loss from 1283.62 to 1271.68 | throughput = 136034 tokens/second | norm = 6.0068 | learning rate = 4.78386e-05\n",
      "inside validation data for epoch 10\n",
      "No improvement in validation loss-->epoch= 10 and best val loss is 1439.80|current_Val loss = 1440.1872987747192|counter = 1\n",
      "executing epoch:12, it took 1.0522182822227477 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the best loss from 1271.68 to 1261.74 | throughput = 135998 tokens/second | norm = 4.1783 | learning rate = 4.70737e-05\n",
      "inside validation data for epoch 11\n",
      "No improvement in validation loss-->epoch= 11 and best val loss is 1439.80|current_Val loss = 1439.9935483932495|counter = 2\n",
      "executing epoch:13, it took 1.051650849978129 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the best loss from 1261.74 to 1250.33 | throughput = 136078 tokens/second | norm = 6.6234 | learning rate = 4.62012e-05\n",
      "inside validation data for epoch 12\n",
      "No improvement in validation loss-->epoch= 12 and best val loss is 1439.80|current_Val loss = 1443.52831864357|counter = 3\n",
      "executing epoch:14, it took 1.0519029100735982 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the best loss from 1250.33 to 1240.85 | throughput = 136012 tokens/second | norm = 1.8834 | learning rate = 4.52254e-05\n",
      "inside validation data for epoch 13\n",
      "No improvement in validation loss-->epoch= 13 and best val loss is 1439.80|current_Val loss = 1443.664713025093|counter = 4\n",
      "executing epoch:15, it took 1.0497576514879863 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the best loss from 1240.85 to 1230.86 | throughput = 136333 tokens/second | norm = 10.4024 | learning rate = 4.41511e-05\n",
      "inside validation data for epoch 14\n",
      "No improvement in validation loss-->epoch= 14 and best val loss is 1439.80|current_Val loss = 1446.8463852405548|counter = 5\n",
      "executing epoch:16, it took 1.049071458975474 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the best loss from 1230.86 to 1220.81 | throughput = 136406 tokens/second | norm = 2.2430 | learning rate = 4.29835e-05\n",
      "inside validation data for epoch 15\n",
      "No improvement in validation loss-->epoch= 15 and best val loss is 1439.80|current_Val loss = 1446.3425530195236|counter = 6\n",
      "executing epoch:17, it took 0.4653669397036235 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the best loss from 1220.81 to 432.31 | throughput = 309394 tokens/second | norm = 0.0681 | learning rate = 4.17283e-05\n",
      "inside validation data for epoch 16\n",
      "Val loss has decreased -->reducing the global validation loss from 1439.80 to 418.44\n",
      " validation loss for epoch = 16 is 418.4436\n",
      " epoch= 16 :  val loss is 418.4436 \n",
      "saving the model model2024-07-2102:48:32.pth\n",
      "*****LOGGING INFO IN distilgpt2_Random_init_nll_and_cos_lossrandom_init_wts_False_2024-07-21.log*********\n",
      "executing epoch:18, it took 0.4599437514940898 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the best loss from 432.31 to 392.43 | throughput = 311171 tokens/second | norm = 0.0480 | learning rate = 4.03915e-05\n",
      "inside validation data for epoch 17\n",
      "Val loss has decreased -->reducing the global validation loss from 418.44 to 384.87\n",
      " validation loss for epoch = 17 is 384.8705\n",
      " epoch= 17 :  val loss is 384.8705 \n",
      "saving the model model2024-07-2102:50:16.pth\n",
      "*****LOGGING INFO IN distilgpt2_Random_init_nll_and_cos_lossrandom_init_wts_False_2024-07-21.log*********\n",
      "executing epoch:19, it took 0.45999117692311603 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the best loss from 392.43 to 360.10 | throughput = 311077 tokens/second | norm = 0.0378 | learning rate = 3.89798e-05\n",
      "inside validation data for epoch 18\n",
      "Val loss has decreased -->reducing the global validation loss from 384.87 to 352.91\n",
      " validation loss for epoch = 18 is 352.9149\n",
      " epoch= 18 :  val loss is 352.9149 \n",
      "saving the model model2024-07-2102:52:01.pth\n",
      "*****LOGGING INFO IN distilgpt2_Random_init_nll_and_cos_lossrandom_init_wts_False_2024-07-21.log*********\n",
      "executing epoch:20, it took 0.4600063999493917 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the best loss from 360.10 to 328.38 | throughput = 311062 tokens/second | norm = 0.0471 | learning rate = 3.75000e-05\n",
      "inside validation data for epoch 19\n",
      "Val loss has decreased -->reducing the global validation loss from 352.91 to 320.73\n",
      " validation loss for epoch = 19 is 320.7298\n",
      " epoch= 19 :  val loss is 320.7298 \n",
      "saving the model model2024-07-2102:53:45.pth\n",
      "*****LOGGING INFO IN distilgpt2_Random_init_nll_and_cos_lossrandom_init_wts_False_2024-07-21.log*********\n",
      "executing epoch:21, it took 0.45983752806981404 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the best loss from 328.38 to 296.88 | throughput = 311127 tokens/second | norm = 0.0277 | learning rate = 3.59593e-05\n",
      "inside validation data for epoch 20\n",
      "Val loss has decreased -->reducing the global validation loss from 320.73 to 291.29\n",
      " validation loss for epoch = 20 is 291.2945\n",
      " epoch= 20 :  val loss is 291.2945 \n",
      "saving the model model2024-07-2102:55:30.pth\n",
      "*****LOGGING INFO IN distilgpt2_Random_init_nll_and_cos_lossrandom_init_wts_False_2024-07-21.log*********\n",
      "executing epoch:22, it took 0.45989818970362345 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the best loss from 296.88 to 270.58 | throughput = 311130 tokens/second | norm = 0.0316 | learning rate = 3.43652e-05\n",
      "inside validation data for epoch 21\n",
      "Val loss has decreased -->reducing the global validation loss from 291.29 to 266.75\n",
      " validation loss for epoch = 21 is 266.7537\n",
      " epoch= 21 :  val loss is 266.7537 \n",
      "saving the model model2024-07-2102:57:14.pth\n",
      "*****LOGGING INFO IN distilgpt2_Random_init_nll_and_cos_lossrandom_init_wts_False_2024-07-21.log*********\n",
      "executing epoch:23, it took 0.45983296235402427 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the best loss from 270.58 to 248.87 | throughput = 311114 tokens/second | norm = 0.0258 | learning rate = 3.27254e-05\n",
      "inside validation data for epoch 22\n",
      "Val loss has decreased -->reducing the global validation loss from 266.75 to 246.14\n",
      " validation loss for epoch = 22 is 246.1409\n",
      " epoch= 22 :  val loss is 246.1409 \n",
      "saving the model model2024-07-2102:58:58.pth\n",
      "*****LOGGING INFO IN distilgpt2_Random_init_nll_and_cos_lossrandom_init_wts_False_2024-07-21.log*********\n",
      "executing epoch:24, it took 0.4589090267817179 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the best loss from 248.87 to 230.65 | throughput = 311608 tokens/second | norm = 0.0535 | learning rate = 3.10480e-05\n",
      "inside validation data for epoch 23\n",
      "Val loss has decreased -->reducing the global validation loss from 246.14 to 228.26\n",
      " validation loss for epoch = 23 is 228.2583\n",
      " epoch= 23 :  val loss is 228.2583 \n",
      "saving the model model2024-07-2103:00:42.pth\n",
      "*****LOGGING INFO IN distilgpt2_Random_init_nll_and_cos_lossrandom_init_wts_False_2024-07-21.log*********\n",
      "executing epoch:25, it took 0.4595922072728475 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the best loss from 230.65 to 213.23 | throughput = 311395 tokens/second | norm = 0.0451 | learning rate = 2.93412e-05\n",
      "inside validation data for epoch 24\n",
      "Val loss has decreased -->reducing the global validation loss from 228.26 to 209.06\n",
      " validation loss for epoch = 24 is 209.0594\n",
      " epoch= 24 :  val loss is 209.0594 \n",
      "saving the model model2024-07-2103:02:27.pth\n",
      "*****LOGGING INFO IN distilgpt2_Random_init_nll_and_cos_lossrandom_init_wts_False_2024-07-21.log*********\n",
      "executing epoch:26, it took 0.4596808195114136 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the best loss from 213.23 to 197.26 | throughput = 311340 tokens/second | norm = 0.0488 | learning rate = 2.76132e-05\n",
      "inside validation data for epoch 25\n",
      "Val loss has decreased -->reducing the global validation loss from 209.06 to 195.77\n",
      " validation loss for epoch = 25 is 195.7652\n",
      " epoch= 25 :  val loss is 195.7652 \n",
      "saving the model model2024-07-2103:04:11.pth\n",
      "*****LOGGING INFO IN distilgpt2_Random_init_nll_and_cos_lossrandom_init_wts_False_2024-07-21.log*********\n",
      "executing epoch:27, it took 0.4596590240796407 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the best loss from 197.26 to 183.99 | throughput = 311282 tokens/second | norm = 0.0452 | learning rate = 2.58725e-05\n",
      "inside validation data for epoch 26\n",
      "Val loss has decreased -->reducing the global validation loss from 195.77 to 182.43\n",
      " validation loss for epoch = 26 is 182.4283\n",
      " epoch= 26 :  val loss is 182.4283 \n",
      "saving the model model2024-07-2103:05:55.pth\n",
      "*****LOGGING INFO IN distilgpt2_Random_init_nll_and_cos_lossrandom_init_wts_False_2024-07-21.log*********\n",
      "executing epoch:28, it took 0.45939340591430666 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the best loss from 183.99 to 174.04 | throughput = 311435 tokens/second | norm = 0.0387 | learning rate = 2.41275e-05\n",
      "inside validation data for epoch 27\n",
      "Val loss has decreased -->reducing the global validation loss from 182.43 to 173.74\n",
      " validation loss for epoch = 27 is 173.7351\n",
      " epoch= 27 :  val loss is 173.7351 \n",
      "saving the model model2024-07-2103:07:39.pth\n",
      "*****LOGGING INFO IN distilgpt2_Random_init_nll_and_cos_lossrandom_init_wts_False_2024-07-21.log*********\n",
      "executing epoch:29, it took 0.4594215671221415 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the best loss from 174.04 to 166.16 | throughput = 311499 tokens/second | norm = 0.0320 | learning rate = 2.23868e-05\n",
      "inside validation data for epoch 28\n",
      "Val loss has decreased -->reducing the global validation loss from 173.74 to 166.72\n",
      " validation loss for epoch = 28 is 166.7153\n",
      " epoch= 28 :  val loss is 166.7153 \n",
      "saving the model model2024-07-2103:09:24.pth\n",
      "*****LOGGING INFO IN distilgpt2_Random_init_nll_and_cos_lossrandom_init_wts_False_2024-07-21.log*********\n",
      "executing epoch:30, it took 0.4596747716267904 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the best loss from 166.16 to 158.83 | throughput = 311237 tokens/second | norm = 0.0386 | learning rate = 2.06588e-05\n",
      "inside validation data for epoch 29\n",
      "Val loss has decreased -->reducing the global validation loss from 166.72 to 159.94\n",
      " validation loss for epoch = 29 is 159.9387\n",
      " epoch= 29 :  val loss is 159.9387 \n",
      "saving the model model2024-07-2103:11:08.pth\n",
      "*****LOGGING INFO IN distilgpt2_Random_init_nll_and_cos_lossrandom_init_wts_False_2024-07-21.log*********\n",
      "executing epoch:31, it took 0.4592557430267334 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the best loss from 158.83 to 153.80 | throughput = 311526 tokens/second | norm = 0.0202 | learning rate = 1.89520e-05\n",
      "inside validation data for epoch 30\n",
      "Val loss has decreased -->reducing the global validation loss from 159.94 to 152.90\n",
      " validation loss for epoch = 30 is 152.9040\n",
      " epoch= 30 :  val loss is 152.9040 \n",
      "saving the model model2024-07-2103:12:52.pth\n",
      "*****LOGGING INFO IN distilgpt2_Random_init_nll_and_cos_lossrandom_init_wts_False_2024-07-21.log*********\n",
      "executing epoch:32, it took 0.45926709175109864 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the best loss from 153.80 to 149.94 | throughput = 311509 tokens/second | norm = 0.0216 | learning rate = 1.72746e-05\n",
      "inside validation data for epoch 31\n",
      "Val loss has decreased -->reducing the global validation loss from 152.90 to 151.63\n",
      " validation loss for epoch = 31 is 151.6348\n",
      " epoch= 31 :  val loss is 151.6348 \n",
      "saving the model model2024-07-2103:14:36.pth\n",
      "*****LOGGING INFO IN distilgpt2_Random_init_nll_and_cos_lossrandom_init_wts_False_2024-07-21.log*********\n",
      "executing epoch:33, it took 0.4593800822893778 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the best loss from 149.94 to 147.31 | throughput = 311520 tokens/second | norm = 0.0216 | learning rate = 1.56348e-05\n",
      "inside validation data for epoch 32\n",
      "Val loss has decreased -->reducing the global validation loss from 151.63 to 145.95\n",
      " validation loss for epoch = 32 is 145.9526\n",
      " epoch= 32 :  val loss is 145.9526 \n",
      "saving the model model2024-07-2103:16:20.pth\n",
      "*****LOGGING INFO IN distilgpt2_Random_init_nll_and_cos_lossrandom_init_wts_False_2024-07-21.log*********\n",
      "executing epoch:34, it took 0.4592695713043213 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the best loss from 147.31 to 143.56 | throughput = 311572 tokens/second | norm = 0.0356 | learning rate = 1.40407e-05\n",
      "inside validation data for epoch 33\n",
      "Val loss has decreased -->reducing the global validation loss from 145.95 to 144.77\n",
      " validation loss for epoch = 33 is 144.7725\n",
      " epoch= 33 :  val loss is 144.7725 \n",
      "saving the model model2024-07-2103:18:05.pth\n",
      "*****LOGGING INFO IN distilgpt2_Random_init_nll_and_cos_lossrandom_init_wts_False_2024-07-21.log*********\n",
      "executing epoch:35, it took 0.4592286308606466 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the best loss from 143.56 to 142.49 | throughput = 311492 tokens/second | norm = 0.0394 | learning rate = 1.25000e-05\n",
      "inside validation data for epoch 34\n",
      "Val loss has decreased -->reducing the global validation loss from 144.77 to 142.82\n",
      " validation loss for epoch = 34 is 142.8174\n",
      " epoch= 34 :  val loss is 142.8174 \n",
      "saving the model model2024-07-2103:19:49.pth\n",
      "*****LOGGING INFO IN distilgpt2_Random_init_nll_and_cos_lossrandom_init_wts_False_2024-07-21.log*********\n",
      "executing epoch:36, it took 0.45924575328826905 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the best loss from 142.49 to 139.65 | throughput = 311573 tokens/second | norm = 0.0189 | learning rate = 1.10202e-05\n",
      "inside validation data for epoch 35\n",
      "Val loss has decreased -->reducing the global validation loss from 142.82 to 139.81\n",
      " validation loss for epoch = 35 is 139.8096\n",
      " epoch= 35 :  val loss is 139.8096 \n",
      "saving the model model2024-07-2103:21:33.pth\n",
      "*****LOGGING INFO IN distilgpt2_Random_init_nll_and_cos_lossrandom_init_wts_False_2024-07-21.log*********\n",
      "executing epoch:37, it took 0.4593623518943787 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the best loss from 139.65 to 137.39 | throughput = 311475 tokens/second | norm = 0.0185 | learning rate = 9.60846e-06\n",
      "inside validation data for epoch 36\n",
      "Val loss has decreased -->reducing the global validation loss from 139.81 to 137.02\n",
      " validation loss for epoch = 36 is 137.0249\n",
      " epoch= 36 :  val loss is 137.0249 \n",
      "saving the model model2024-07-2103:23:18.pth\n",
      "*****LOGGING INFO IN distilgpt2_Random_init_nll_and_cos_lossrandom_init_wts_False_2024-07-21.log*********\n",
      "executing epoch:38, it took 0.4592719157536825 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the best loss from 137.39 to 134.14 | throughput = 311559 tokens/second | norm = 0.0295 | learning rate = 8.27173e-06\n",
      "inside validation data for epoch 37\n",
      "Val loss has decreased -->reducing the global validation loss from 137.02 to 134.33\n",
      " validation loss for epoch = 37 is 134.3330\n",
      " epoch= 37 :  val loss is 134.3330 \n",
      "saving the model model2024-07-2103:25:02.pth\n",
      "*****LOGGING INFO IN distilgpt2_Random_init_nll_and_cos_lossrandom_init_wts_False_2024-07-21.log*********\n",
      "executing epoch:39, it took 0.4594484011332194 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the best loss from 134.14 to 133.03 | throughput = 311374 tokens/second | norm = 0.0265 | learning rate = 7.01650e-06\n",
      "inside validation data for epoch 38\n",
      "Val loss has decreased -->reducing the global validation loss from 134.33 to 133.73\n",
      " validation loss for epoch = 38 is 133.7307\n",
      " epoch= 38 :  val loss is 133.7307 \n",
      "saving the model model2024-07-2103:26:46.pth\n",
      "*****LOGGING INFO IN distilgpt2_Random_init_nll_and_cos_lossrandom_init_wts_False_2024-07-21.log*********\n",
      "executing epoch:40, it took 0.45952375332514445 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the best loss from 133.03 to 131.27 | throughput = 311423 tokens/second | norm = 0.0458 | learning rate = 5.84889e-06\n",
      "inside validation data for epoch 39\n",
      "Val loss has decreased -->reducing the global validation loss from 133.73 to 131.75\n",
      " validation loss for epoch = 39 is 131.7460\n",
      " epoch= 39 :  val loss is 131.7460 \n",
      "saving the model model2024-07-2103:28:30.pth\n",
      "*****LOGGING INFO IN distilgpt2_Random_init_nll_and_cos_lossrandom_init_wts_False_2024-07-21.log*********\n",
      "executing epoch:41, it took 0.4596784432729085 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the best loss from 131.27 to 129.83 | throughput = 311284 tokens/second | norm = 0.0269 | learning rate = 4.77458e-06\n",
      "inside validation data for epoch 40\n",
      "Val loss has decreased -->reducing the global validation loss from 131.75 to 130.58\n",
      " validation loss for epoch = 40 is 130.5765\n",
      " epoch= 40 :  val loss is 130.5765 \n",
      "saving the model model2024-07-2103:30:15.pth\n",
      "*****LOGGING INFO IN distilgpt2_Random_init_nll_and_cos_lossrandom_init_wts_False_2024-07-21.log*********\n",
      "executing epoch:42, it took 0.4596866965293884 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the best loss from 129.83 to 128.61 | throughput = 311374 tokens/second | norm = 0.0199 | learning rate = 3.79880e-06\n",
      "inside validation data for epoch 41\n",
      "Val loss has decreased -->reducing the global validation loss from 130.58 to 129.59\n",
      " validation loss for epoch = 41 is 129.5881\n",
      " epoch= 41 :  val loss is 129.5881 \n",
      "saving the model model2024-07-2103:31:59.pth\n",
      "*****LOGGING INFO IN distilgpt2_Random_init_nll_and_cos_lossrandom_init_wts_False_2024-07-21.log*********\n",
      "executing epoch:43, it took 0.45961246887842816 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the best loss from 128.61 to 127.71 | throughput = 311151 tokens/second | norm = 0.0518 | learning rate = 2.92631e-06\n",
      "inside validation data for epoch 42\n",
      "Val loss has decreased -->reducing the global validation loss from 129.59 to 128.93\n",
      " validation loss for epoch = 42 is 128.9284\n",
      " epoch= 42 :  val loss is 128.9284 \n",
      "saving the model model2024-07-2103:33:43.pth\n",
      "*****LOGGING INFO IN distilgpt2_Random_init_nll_and_cos_lossrandom_init_wts_False_2024-07-21.log*********\n",
      "executing epoch:44, it took 0.4597652236620585 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the best loss from 127.71 to 126.97 | throughput = 311256 tokens/second | norm = 0.0229 | learning rate = 2.16136e-06\n",
      "inside validation data for epoch 43\n",
      "Val loss has decreased -->reducing the global validation loss from 128.93 to 127.58\n",
      " validation loss for epoch = 43 is 127.5813\n",
      " epoch= 43 :  val loss is 127.5813 \n",
      "saving the model model2024-07-2103:35:28.pth\n",
      "*****LOGGING INFO IN distilgpt2_Random_init_nll_and_cos_lossrandom_init_wts_False_2024-07-21.log*********\n",
      "executing epoch:45, it took 0.45965323050816853 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the best loss from 126.97 to 126.55 | throughput = 311225 tokens/second | norm = 0.0256 | learning rate = 1.50768e-06\n",
      "inside validation data for epoch 44\n",
      "No improvement in validation loss-->epoch= 44 and best val loss is 127.58|current_Val loss = 128.2169730067253|counter = 1\n",
      "executing epoch:46, it took 0.4598056713740031 mins from beginning of epoch till batch#262\n",
      "No improvement in training  loss-->epoch= 45 and best loss is 126.55|current_loss = 126.75480711460114|counter = 1\n",
      "executing epoch:47, it took 0.4598914623260498 mins from beginning of epoch till batch#262\n",
      "No improvement in training  loss-->epoch= 46 and best loss is 126.55|current_loss = 126.63680964708328|counter = 2\n",
      "executing epoch:48, it took 0.45974974234898885 mins from beginning of epoch till batch#262\n",
      "No improvement in training  loss-->epoch= 47 and best loss is 126.55|current_loss = 127.07328510284424|counter = 3\n",
      "executing epoch:49, it took 0.460215957959493 mins from beginning of epoch till batch#262\n",
      "No improvement in training  loss-->epoch= 48 and best loss is 126.55|current_loss = 126.99418556690216|counter = 4\n",
      "executing epoch:50, it took 0.4597586194674174 mins from beginning of epoch till batch#262\n",
      "No improvement in training  loss-->epoch= 49 and best loss is 126.55|current_loss = 126.9630286693573|counter = 5\n",
      "executing epoch:51, it took 0.4595193107922872 mins from beginning of epoch till batch#262\n",
      "No improvement in training  loss-->epoch= 50 and best loss is 126.55|current_loss = 126.95674413442612|counter = 6\n",
      "executing epoch:52, it took 0.45984848737716677 mins from beginning of epoch till batch#262\n",
      "No improvement in training  loss-->epoch= 51 and best loss is 126.55|current_loss = 126.95963376760483|counter = 7\n",
      "executing epoch:53, it took 0.459921928246816 mins from beginning of epoch till batch#262\n",
      "No improvement in training  loss-->epoch= 52 and best loss is 126.55|current_loss = 126.97109115123749|counter = 8\n",
      "executing epoch:54, it took 0.45981680154800414 mins from beginning of epoch till batch#262\n",
      "No improvement in training  loss-->epoch= 53 and best loss is 126.55|current_loss = 126.94453430175781|counter = 9\n",
      "executing epoch:55, it took 0.45970548391342164 mins from beginning of epoch till batch#262\n",
      "No improvement in training  loss-->epoch= 54 and best loss is 126.55|current_loss = 126.96364372968674|counter = 10\n"
     ]
    }
   ],
   "source": [
    "tr_model,epoch_train_log,epoch_val_log = train_model(train_loader, val_loader, model =  model,tokenizer = tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "575702a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json , os\n",
    "path_var_train_log = os.path.join(\".\",\"Tokenizers_and_loss_vals\",\"epoch_train_plain_loss.json\")\n",
    "path_var_val_log = os.path.join(\".\",\"Tokenizers_and_loss_vals\",\"epoch_val_val_loss_.json\")\n",
    "\n",
    "#print(path_var)\n",
    "#Write the list to a JSON file\n",
    "with open(path_var_train_log, \"w\") as file:\n",
    "    json.dump(epoch_train_log, file)\n",
    "\n",
    "with open(path_var_val_log, \"w\") as file:\n",
    "    json.dump(epoch_val_log, file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e9c51713",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path_var_train_log, \"r\") as file:\n",
    "    train_loss = json.load(file)\n",
    "with open(path_var_val_log, \"r\") as file:\n",
    "    val_loss = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "31cd4f5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55\n",
      "45\n"
     ]
    }
   ],
   "source": [
    "print(len(train_loss))\n",
    "print(len(val_loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "75a71162",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fd189871ca0>]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_values = range(len(train_loss))\n",
    "plt.plot(x_values, train_loss, label='Train_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b82d16f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fd15b1a4760>]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_values_val = range(len(val_loss))\n",
    "plt.plot(x_values_val, val_loss, label='val_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bb0abbca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fd1674daf70>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "min_length = min(len(train_loss), len(val_loss))\n",
    "list1 = train_loss[:min_length]\n",
    "list2 = val_loss[:min_length]\n",
    "\n",
    "# Create x-axis values\n",
    "x_values = range(min_length)\n",
    "\n",
    "# Plot the lists\n",
    "plt.plot(x_values, list1, label='Train_loss')\n",
    "plt.plot(x_values, list2, label='Val_loss')\n",
    "\n",
    "# Adding labels and title\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Train Loss and Val_Loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28fc4f7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b879c596",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0210ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f701818a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49731a1e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
