{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f80fe49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## reference https://huggingface.co/learn/nlp-course/en/chapter7/6?fw=pt#training-a-causal-language-model-from-scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07594f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "145fcf62",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: jupyter in /opt/conda/lib/python3.8/site-packages (1.0.0)\n",
      "Requirement already satisfied: ipywidgets in /opt/conda/lib/python3.8/site-packages (8.1.3)\n",
      "Requirement already satisfied: ipykernel in /opt/conda/lib/python3.8/site-packages (from jupyter) (6.29.5)\n",
      "Requirement already satisfied: jupyter-console in /opt/conda/lib/python3.8/site-packages (from jupyter) (6.6.3)\n",
      "Requirement already satisfied: notebook in /opt/conda/lib/python3.8/site-packages (from jupyter) (6.4.1)\n",
      "Requirement already satisfied: qtconsole in /opt/conda/lib/python3.8/site-packages (from jupyter) (5.5.2)\n",
      "Requirement already satisfied: nbconvert in /opt/conda/lib/python3.8/site-packages (from jupyter) (6.3.0)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.11 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (3.0.11)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.11 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (4.0.11)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (7.30.0)\n",
      "Requirement already satisfied: comm>=0.1.3 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (5.14.3)\n",
      "Requirement already satisfied: matplotlib-inline in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.3)\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (4.8.0)\n",
      "Requirement already satisfied: setuptools>=18.5 in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (59.4.0)\n",
      "Requirement already satisfied: pickleshare in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.7.5)\n",
      "Requirement already satisfied: pygments in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (2.10.0)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.47)\n",
      "Requirement already satisfied: backcall in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: decorator in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (5.1.0)\n",
      "Requirement already satisfied: jedi>=0.16 in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.18.1)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /opt/conda/lib/python3.8/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.8/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.8/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=6.1.0->ipywidgets) (0.2.5)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.8/site-packages (from ipykernel->jupyter) (21.3)\n",
      "Requirement already satisfied: tornado>=6.1 in /opt/conda/lib/python3.8/site-packages (from ipykernel->jupyter) (6.1)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.8/site-packages (from ipykernel->jupyter) (5.8.0)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in /opt/conda/lib/python3.8/site-packages (from ipykernel->jupyter) (1.8.2)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in /opt/conda/lib/python3.8/site-packages (from ipykernel->jupyter) (7.1.0)\n",
      "Requirement already satisfied: nest-asyncio in /opt/conda/lib/python3.8/site-packages (from ipykernel->jupyter) (1.5.4)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /opt/conda/lib/python3.8/site-packages (from ipykernel->jupyter) (5.7.2)\n",
      "Requirement already satisfied: pyzmq>=24 in /opt/conda/lib/python3.8/site-packages (from ipykernel->jupyter) (26.0.3)\n",
      "Requirement already satisfied: entrypoints in /opt/conda/lib/python3.8/site-packages (from jupyter-client>=6.1.12->ipykernel->jupyter) (0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /opt/conda/lib/python3.8/site-packages (from jupyter-client>=6.1.12->ipykernel->jupyter) (2.8.2)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /opt/conda/lib/python3.8/site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel->jupyter) (4.2.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.8/site-packages (from python-dateutil>=2.1->jupyter-client>=6.1.12->ipykernel->jupyter) (1.16.0)\n",
      "Requirement already satisfied: jupyterlab-pygments in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (0.1.2)\n",
      "Requirement already satisfied: testpath in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (0.5.0)\n",
      "Requirement already satisfied: bleach in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (4.1.0)\n",
      "Requirement already satisfied: jinja2>=2.4 in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (3.0.3)\n",
      "Requirement already satisfied: mistune<2,>=0.8.1 in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (0.8.4)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (1.5.0)\n",
      "Requirement already satisfied: nbformat>=4.4 in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (5.1.3)\n",
      "Requirement already satisfied: defusedxml in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (0.7.1)\n",
      "Requirement already satisfied: nbclient<0.6.0,>=0.5.0 in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (0.5.9)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.8/site-packages (from jinja2>=2.4->nbconvert->jupyter) (2.0.1)\n",
      "Requirement already satisfied: ipython-genutils in /opt/conda/lib/python3.8/site-packages (from nbformat>=4.4->nbconvert->jupyter) (0.2.0)\n",
      "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /opt/conda/lib/python3.8/site-packages (from nbformat>=4.4->nbconvert->jupyter) (4.2.1)\n",
      "Requirement already satisfied: importlib-resources>=1.4.0 in /opt/conda/lib/python3.8/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.4->nbconvert->jupyter) (5.4.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /opt/conda/lib/python3.8/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.4->nbconvert->jupyter) (21.2.0)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /opt/conda/lib/python3.8/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.4->nbconvert->jupyter) (0.18.0)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /opt/conda/lib/python3.8/site-packages (from importlib-resources>=1.4.0->jsonschema!=2.5.0,>=2.4->nbformat>=4.4->nbconvert->jupyter) (3.6.0)\n",
      "Requirement already satisfied: webencodings in /opt/conda/lib/python3.8/site-packages (from bleach->nbconvert->jupyter) (0.5.1)\n",
      "Requirement already satisfied: Send2Trash>=1.5.0 in /opt/conda/lib/python3.8/site-packages (from notebook->jupyter) (1.8.0)\n",
      "Requirement already satisfied: terminado>=0.8.3 in /opt/conda/lib/python3.8/site-packages (from notebook->jupyter) (0.12.1)\n",
      "Requirement already satisfied: prometheus-client in /opt/conda/lib/python3.8/site-packages (from notebook->jupyter) (0.12.0)\n",
      "Requirement already satisfied: argon2-cffi in /opt/conda/lib/python3.8/site-packages (from notebook->jupyter) (21.1.0)\n",
      "Requirement already satisfied: cffi>=1.0.0 in /opt/conda/lib/python3.8/site-packages (from argon2-cffi->notebook->jupyter) (1.15.0)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.8/site-packages (from cffi>=1.0.0->argon2-cffi->notebook->jupyter) (2.21)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging->ipykernel->jupyter) (3.0.6)\n",
      "Requirement already satisfied: qtpy>=2.4.0 in /opt/conda/lib/python3.8/site-packages (from qtconsole->jupyter) (2.4.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#installing some libraries\n",
    "#!pip install datasets\n",
    "!pip install --upgrade jupyter ipywidgets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "facd84d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch, transformers\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import PreTrainedModel, PretrainedConfig\n",
    "from transformers import AutoModel, AutoConfig,AutoModelForCausalLM, AutoConfig,GPT2Config,GPT2Tokenizer\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "#from datasets import load_dataset\n",
    "import pandas as pd, numpy as np\n",
    "from torch.cuda.amp import autocast\n",
    "from torch import cuda\n",
    "import datetime\n",
    "import warnings,itertools\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "import json\n",
    "# Ignore all warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "#pip install transformers bitsandbytes>=0.39.0 -q\n",
    "import zipfile,logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd32780f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "import random\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2bcb393",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb063159",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "model\n"
     ]
    }
   ],
   "source": [
    "#global params for training\n",
    "\n",
    "B,T = 64,1024\n",
    "epoch = 100\n",
    "random_init_wts = True\n",
    "min_text_len = 0\n",
    "# hard coded com\n",
    "comp_ratio = 3\n",
    "# train_loss_list = []\n",
    "# val_loss_list =[]\n",
    "if cuda.is_available():\n",
    "    device = torch.device('cuda:0')\n",
    "    print(device)\n",
    "else:\n",
    "    device = 'cpu'\n",
    "#print(device)\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "#os.environ[\"MKL_DEBUG_CPU_TYPE\"] = \"5\"\n",
    "\n",
    "#print(global_tr_loss)\n",
    "model_path = os.path.join(\"model\")\n",
    "print(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "edf248c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./data/unzip_text_10M'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "directory = os.path.join('.','data','unzip_text_10M')  # Replace with your directory path\n",
    "directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aff3d0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_text(directory):\n",
    "    directory = os.path.join('.','data','unzip_text_10M',str(directory))  # Replace with your directory path\n",
    "    print(f\"directory :{directory}\")\n",
    "    # List all files in the directory\n",
    "    files = [f for f in os.listdir(directory) if os.path.isfile(os.path.join(directory, f))]\n",
    "    print(f\"files:{files}\")\n",
    "    text_content = []\n",
    "    # Read each file\n",
    "    total_lines = 0\n",
    "    for filenum,filename in enumerate(files):\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            #first_line = file.read()\n",
    "            #print(f\"filename :{filename}->first few lines {first_line}\")\n",
    "            #continue\n",
    "            #lines_list = [line.strip() for line in open(file_path, 'r')]\n",
    "            text = file.read()\n",
    "            text_content.append(text)\n",
    "            print(f\"the file:{filename} has been appeneded to the uber list and its length is {len(text_content)} \")\n",
    "            #total_lines+=len(lines_list)\n",
    "            #text_content.append(lines_list)\n",
    "    \n",
    "    flattened_list = ''.join(text_content)\n",
    "    assert (len(flattened_list) == total_lines , f\"Expected {len(flattened_list)} to be equal to {total_lines}\" )\n",
    "    \n",
    "    return flattened_list\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e8f53fa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "directory :./data/unzip_text_10M/train_10M\n",
      "files:['switchboard.train', 'simple_wiki.train', 'open_subtitles.train', 'gutenberg.train', 'childes.train', 'bnc_spoken.train']\n",
      "the file:switchboard.train has been appeneded to the uber list and its length is 1 \n",
      "the file:simple_wiki.train has been appeneded to the uber list and its length is 2 \n",
      "the file:open_subtitles.train has been appeneded to the uber list and its length is 3 \n",
      "the file:gutenberg.train has been appeneded to the uber list and its length is 4 \n",
      "the file:childes.train has been appeneded to the uber list and its length is 5 \n",
      "the file:bnc_spoken.train has been appeneded to the uber list and its length is 6 \n"
     ]
    }
   ],
   "source": [
    "train_list = read_text(\"train_10M\")\n",
    "#print(train_dict)\n",
    "#val_list = read_text(\"dev\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ff02d13f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54215049"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c7ed7d8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "827\n"
     ]
    }
   ],
   "source": [
    "chunks = len(train_list)//(B*T)\n",
    "print(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "487e9399",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(\"distilgpt2\")\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8386f6c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50257\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.pad_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "735f6fe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "directory :./data/unzip_text_10M/dev\n",
      "files:['switchboard.dev', 'simple_wiki.dev', 'open_subtitles.dev', 'gutenberg.dev', 'childes.dev', 'bnc_spoken.dev']\n",
      "the file:switchboard.dev has been appeneded to the uber list and its length is 1 \n",
      "the file:simple_wiki.dev has been appeneded to the uber list and its length is 2 \n",
      "the file:open_subtitles.dev has been appeneded to the uber list and its length is 3 \n",
      "the file:gutenberg.dev has been appeneded to the uber list and its length is 4 \n",
      "the file:childes.dev has been appeneded to the uber list and its length is 5 \n",
      "the file:bnc_spoken.dev has been appeneded to the uber list and its length is 6 \n"
     ]
    }
   ],
   "source": [
    "val_list = read_text(\"dev\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "25e02cee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50256\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f3cab47e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8408612021052484"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.random()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "556154b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "774ab8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ranked_synonyms(word, pos):\n",
    "    synonyms = []\n",
    "    for syn in wordnet.synsets(word, pos=pos):\n",
    "        for lemma in syn.lemmas():\n",
    "            if lemma.name() != word:\n",
    "                synonyms.append((lemma.name(), syn.wup_similarity(syn)))\n",
    "    \n",
    "    # Sort synonyms by similarity score in descending order\n",
    "    ranked_synonyms = sorted(set(synonyms), key=lambda x: x[1] if x[1] is not None else 0, reverse=True)\n",
    "    return [syn for syn, _ in ranked_synonyms]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "589a5310",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "def replace_adj_with_synonyms(text):\n",
    "    words = word_tokenize(text)\n",
    "    tagged = pos_tag(words)\n",
    "    result = []\n",
    "    for word, pos in tagged:\n",
    "        if pos.startswith('J'):\n",
    "            wordnet_pos = get_wordnet_pos(pos)\n",
    "            lemma = lemmatizer.lemmatize(word, wordnet_pos)\n",
    "            synonyms = get_ranked_synonyms(lemma, wordnet_pos)\n",
    "            if synonyms:\n",
    "                replacement = random.choice(synonyms)  # Choose from top 3 synonyms\n",
    "                #print(f\"word = {word}|replacement = {replacement}\")\n",
    "                result.append(replacement)\n",
    "            else:\n",
    "                result.append(word)\n",
    "        else:\n",
    "            result.append(word)\n",
    "    return \" \".join(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d0dec89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_text = train_list[147888:152221]\n",
    "# repl = replace_verbs_with_synonyms(sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "886dfbd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(repl)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c1e30fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "832c65ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequences(token_ids_list, max_length = B*T, tokenizer = tokenizer):\n",
    "    padded_sequences = tokenizer.pad(\n",
    "        {\"input_ids\": token_ids_list},\n",
    "        padding='max_length',\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    return padded_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "236f2795",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text (text,tokenizer = tokenizer,max_length = B*T):\n",
    "    #print(f\"inside tokenize_text\")\n",
    "    enc = tokenizer(text,padding='max_length',truncation=True,max_length=max_length,return_tensors=\"pt\",return_attention_mask=True)\n",
    "    input_id = enc['input_ids']\n",
    "    att_mask = enc['attention_mask']\n",
    "    \n",
    "    # now concatenate these lists to B*T\n",
    "    input_id = torch.squeeze(input_id, dim = 0).to(dtype = torch.long)\n",
    "    att_mask = torch.squeeze(att_mask, dim = 0).to(dtype = torch.bool)\n",
    "\n",
    "    return input_id,att_mask\n",
    "    \n",
    "    \n",
    "    \n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f268a656",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inp,att = tokenize_text(repl)\n",
    "# #a = inp['input_ids']\n",
    "# print(inp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ac66556b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test the tokenizer:\n",
    "model_name = 'distilgpt2'\n",
    "if random_init_wts:\n",
    "    config = AutoConfig.from_pretrained(model_name, vocab_size = 50304)\n",
    "    # Initialize the model with random weights\n",
    "    model = AutoModelForCausalLM.from_config(config)\n",
    "else:\n",
    "    #model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "    \n",
    "#model = torch.compile(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c8e79c5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50257"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b47108",
   "metadata": {},
   "source": [
    "### Data loaders and Dataset for batched training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "83c6b79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataset_pyt_train(Dataset):\n",
    "    def __init__(self, text_list, B = B, T = T, tokenizer = tokenizer, comp_ratio = comp_ratio,prob = .20):\n",
    "        self.text_list = text_list\n",
    "        #print(f\"Value of B {B}\")\n",
    "        self.prob = prob\n",
    "                                        \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        orig_chunk = self.text_list[idx:idx + B*T*comp_ratio]\n",
    "        random_num = random.random()\n",
    "        if random_num > self.prob:\n",
    "            chunk = replace_adj_with_synonyms(orig_chunk)\n",
    "            inp,att = tokenize_text(chunk, tokenizer)\n",
    "        else:\n",
    "            inp,att = tokenize_text(orig_chunk, tokenizer)\n",
    "        \n",
    "        label_enc,_ =  tokenize_text(orig_chunk, tokenizer)\n",
    "        input_id = inp.view(B,T)\n",
    "        attention_mask = att.view(B,T)\n",
    "        label = label_enc.view(B,T)\n",
    "        #print(f\"shape of input_id = {input_id.shape}| shape of attention = {attention_mask.shape}\")\n",
    "        \n",
    "        return input_id,attention_mask,label\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        #return the length of the dataframe\n",
    "        num_chunks = comp_ratio*B*T\n",
    "        return len(self.text_list)//num_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e4416391",
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataset_pyt_val(Dataset):\n",
    "    def __init__(self, text_list, B = B, T = T, tokenizer = tokenizer, comp_ratio = comp_ratio):\n",
    "        self.text_list = text_list\n",
    "        #print(f\"Value of B {B}\")\n",
    "                                                \n",
    "    def __getitem__(self, idx):\n",
    "        #words_list[start_index:start_index + n]\n",
    "        chunk = self.text_list[idx:idx + B*T*comp_ratio]\n",
    "        #print(f\"length of chunk = {len()}\")\n",
    "        inp,att = tokenize_text(chunk, tokenizer)\n",
    "        input_id = inp.view(B,T)\n",
    "        attention_mask = att.view(B,T)\n",
    "        #print(f\"shape of input_id = {input_id.shape}| shape of attention = {attention_mask.shape}\")\n",
    "        label = input_id.clone()\n",
    "        return input_id,attention_mask,label\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        #return the length of the dataframe\n",
    "        num_chunks = comp_ratio*B*T\n",
    "        return len(self.text_list)//num_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7c83e50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_dataset = dataset_pyt(filtered_df,tokenizer = tokenizer)\n",
    "train_dataset = dataset_pyt_train(train_list)\n",
    "val_dataset = dataset_pyt_val(val_list)\n",
    "\n",
    "train_loader = DataLoader(train_dataset,batch_size = 1, shuffle = True , num_workers = 4, pin_memory = True)\n",
    "val_loader = DataLoader(val_dataset,batch_size = 1, shuffle = True , num_workers = 4, pin_memory = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7e6eb320",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer vocabulary size: 50258\n",
      "EOS token ID: 50256\n",
      "PAD token ID: 50257\n"
     ]
    }
   ],
   "source": [
    "print(f\"Tokenizer vocabulary size: {len(tokenizer)}\")\n",
    "print(f\"EOS token ID: {tokenizer.eos_token_id}\")\n",
    "print(f\"PAD token ID: {tokenizer.pad_token_id}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d32b5406",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "275"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_list)//(3*B*T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "707a4f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_file(log_message, model_name = \"GPT2\" ,random_init_wts = random_init_wts ):\n",
    "    current_datetime = datetime.datetime.now()\n",
    "    # Extract date and time components\n",
    "    current_date = str(current_datetime.date())\n",
    "    log_file = model_name +'_nll_loss_adj_replacement'+'random_init_wts'+ '_'+str(random_init_wts)+'_' +current_date+'.log'\n",
    "    print(f\"*****LOGGING INFO IN {log_file}*********\")\n",
    "    filepath = os.path.join(\"model\",log_file)\n",
    "    logging.basicConfig(filename=filepath, \n",
    "                    filemode='a',  # Overwrite the log file each time\n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s', \n",
    "                    level=logging.DEBUG)\n",
    "    logger = logging.getLogger()\n",
    "    logger.info(log_message)\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e3923e12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the train loader is 275\n",
      "Length of the val loader is 287\n",
      "num_tokens= 18022400\n"
     ]
    }
   ],
   "source": [
    "print(f\"Length of the train loader is {len(train_loader)}\")\n",
    "print(f\"Length of the val loader is {len(val_loader)}\")\n",
    "print(f\"num_tokens= {B*T*len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "91bd1929",
   "metadata": {},
   "outputs": [],
   "source": [
    "#emb,att,inp = train_dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "916ef054",
   "metadata": {},
   "outputs": [],
   "source": [
    "class check_train_metrics:\n",
    "    def __init__(self, patience=25, min_delta=0 , B = T, T = T,best_loss = torch.inf,early_stop = False):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = best_loss\n",
    "        self.early_stop = early_stop\n",
    "        self.B = B\n",
    "        self.T = T\n",
    "        self.improvement = None\n",
    "\n",
    "    def __call__(self, loss, epoch , epoch_durn, norm , current_lr, num_token):\n",
    "        if self.best_loss - loss > self.min_delta:\n",
    "            \n",
    "            print(f\"training loss has decreased---> reducing the best loss from {self.best_loss:.2f} to {loss:.2f} | throughput = {int(num_token/epoch_durn)} tokens/second | norm = {norm:.4f} | learning rate = {current_lr:.5e}\")\n",
    "            self.best_loss = loss\n",
    "            self.counter = 0\n",
    "            self.improvement = True\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            self.improvement = False\n",
    "            print(f\"No improvement in training  loss-->epoch= {epoch} and best loss is {self.best_loss:.2f}|current_loss = {loss}|counter = {self.counter}\")\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bdb2f0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class check_val_metrics:\n",
    "    def __init__(self, patience=25, min_delta=0, best_loss = torch.inf,early_stop = False):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = best_loss\n",
    "        self.early_stop = early_stop\n",
    "        \n",
    "\n",
    "    def __call__(self, loss, epoch , model, tokenizer):\n",
    "        if self.best_loss - loss > self.min_delta:\n",
    "            print(f\"Val loss has decreased -->reducing the global validation loss from {self.best_loss:.2f} to {loss:.2f}\")\n",
    "            s1 = (f\"Val loss has decreased -->reducing the global validation loss from {self.best_loss:.2f} to {loss:.2f}\")\n",
    "            print(f\" validation loss for epoch = {epoch} is {loss:.4f}\")\n",
    "            self.best_loss = loss\n",
    "            s2 = f\" validation loss for epoch = {epoch} is {loss:.4f}\"\n",
    "            print(f\" epoch= {epoch} :  val loss is {loss:.4f} \")\n",
    "            s3 = f\" epoch= {epoch} :  val loss is {loss:.4f} \"\n",
    "            #save the model\n",
    "            # Get the current date and time\n",
    "            current_datetime = datetime.datetime.now()\n",
    "            # Extract date and time components\n",
    "            current_date = str(current_datetime.date())\n",
    "            current_time = str(current_datetime.time()).split('.')[0]\n",
    "            file_name = 'model'+ current_date+current_time+'.pth'\n",
    "            path = os.path.join(\"model\",file_name)\n",
    "            print(f\"saving the model {file_name}\")\n",
    "            s4 = f\"saving the model {file_name}\"\n",
    "            #torch.save(model.state_dict(), path)\n",
    "            model.save_pretrained(path)\n",
    "            tokenizer.save_pretrained(path)\n",
    "            log_message = s1+s2+s3+s4\n",
    "            write_file(log_message)\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            print(f\"No improvement in validation loss-->epoch= {epoch} and best val loss is {self.best_loss:.2f}|current_Val loss = {loss}|counter = {self.counter}\")\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e95114a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss(ids, att , labels, model ,device = device):\n",
    "    embedding_layer = model.transformer.wte \n",
    "    with autocast(dtype = torch.bfloat16):\n",
    "        input_emb = embedding_layer(ids)\n",
    "        model_output = model(input_ids = ids ,attention_mask = att, labels = labels)\n",
    "        logits = model_output.logits\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "        #print(f\"predictions = {predictions}\")\n",
    "        prediction_embeddings = embedding_layer(predictions)\n",
    "        cos_sim = F.cosine_similarity(torch.squeeze(prediction_embeddings,dim = 0), torch.squeeze(input_emb,dim = 0), dim=1)\n",
    "        cos_loss = 1- cos_sim.mean()\n",
    "        total_loss = cos_loss\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "61225312",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad\n",
    "\n",
    "def eval_model(val_loader, model, epoch , device = device,tokenizer = tokenizer):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    e = epoch+1\n",
    "    val_loss_accum = 0.0\n",
    "    print(f\"inside validation data for epoch {e}\")\n",
    "    for ind,(input_id,attention_mask,label) in enumerate(val_loader):\n",
    "        ids = input_id.to(device=device, non_blocking=True)\n",
    "        ids = torch.squeeze(ids, dim = 0)\n",
    "        att_mask = attention_mask.to(device=device, non_blocking=True)\n",
    "        att_mask =  torch.squeeze(att_mask, dim = 0)\n",
    "        labels = label.to(device=device, non_blocking=True)\n",
    "        total_loss = calc_loss(ids,att_mask,labels,model)\n",
    "          \n",
    "    \n",
    "        val_loss_accum+= total_loss.detach().item()\n",
    "        del att_mask,labels,total_loss,ids\n",
    "    return val_loss_accum        \n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e6f36441",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_model(train_loader,val_loader,model,num_epoch = 100,device = device,tokenizer = tokenizer):\n",
    "    #model.train()\n",
    "    device = device\n",
    "    lr_custom = 5e-5\n",
    "    print(f\"inside train model. Device = {device}\")\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.AdamW(params =  model.parameters(), lr= lr_custom,fused = True ,weight_decay = .1)\n",
    "      \n",
    "    extra_train = .1*num_epoch\n",
    "    max_train_steps = int(num_epoch +extra_train )\n",
    "    import time\n",
    "    from transformers import get_linear_schedule_with_warmup\n",
    "    total_steps = len(train_loader) * num_epoch\n",
    "    scheduler_cos = transformers.get_cosine_schedule_with_warmup( optimizer= optimizer, num_warmup_steps =int(total_steps * 0.1) ,num_training_steps= total_steps )\n",
    "        \n",
    "    epoch_train_log = []\n",
    "    epoch_val_log = []\n",
    "    validate_val_metric = check_val_metrics()\n",
    "    validate_train_metric = check_train_metrics()\n",
    "    \n",
    "    for i in range (max_train_steps):\n",
    "        \n",
    "        epoch_start_time = time.time()\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        # we use 2 schedulers - the first LR scheduler uses a cosine decay for 100 epochs the second scheduler takes the last LR from cosine scheduler and then maintains that LR for the next 10 epochs\n",
    "        if i >= num_epoch:\n",
    "            optimizer_reduced_lr = torch.optim.AdamW(params =  model.parameters(), lr= current_lr ,fused = True , weight_decay=.1)\n",
    "            scheduler_constant = transformers.get_constant_schedule_with_warmup( optimizer = optimizer_reduced_lr ,num_warmup_steps = 0, last_epoch = -1 )\n",
    "        \n",
    "        epoch_train_loss = 0       \n",
    "        for ind,(input_id,attention_mask,labels) in enumerate(train_loader):\n",
    "            if ind == int(len(train_loader)/2):\n",
    "                batch_time = time.time()\n",
    "                duration = batch_time - epoch_start_time\n",
    "                print(f\"executing epoch:{i+1}, it took {duration/60} mins from beginning of epoch till batch#{ind}\")\n",
    "            \n",
    "            ids = input_id.to(device=device, non_blocking=True)\n",
    "            ids = torch.squeeze(ids, dim = 0)\n",
    "            att_mask = attention_mask.to(device=device, non_blocking=True)\n",
    "            att_mask =  torch.squeeze(att_mask, dim = 0)\n",
    "            labels = torch.squeeze(labels,dim = 0).to(device)\n",
    "            total_loss = calc_loss(ids,att_mask,labels,model)\n",
    "                                           \n",
    "            total_loss.backward()\n",
    "            epoch_train_loss += total_loss.detach().item()\n",
    "            norm = torch.nn.utils.clip_grad_norm(model.parameters() , 1.0)\n",
    "            if i <= num_epoch:\n",
    "                optimizer.step()\n",
    "                scheduler_cos.step()\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "            else:\n",
    "                optimizer_reduced_lr.step()\n",
    "                optimizer_reduced_lr.zero_grad(set_to_none=True)\n",
    "                scheduler_constant.step()\n",
    "                \n",
    "                         \n",
    "            \n",
    "            \n",
    "        #batch processing complete \n",
    "        #print(f\"batch processing complete , lambda = {lambda_val} |total_loss for batch= {total_loss}\")\n",
    "        \n",
    "        if i <= num_epoch:\n",
    "            current_lr = scheduler_cos.get_last_lr()[0]\n",
    "        epoch_end_time = time.time()\n",
    "        epoch_durn = (epoch_end_time - epoch_start_time)\n",
    "        num_token = B*T*len(train_loader)\n",
    "        epoch_train_log.append(epoch_train_loss)\n",
    "        validate_train_metric(epoch_train_loss, i , epoch_durn, norm , current_lr, num_token)\n",
    "        \n",
    "        if validate_train_metric.improvement:\n",
    "            val_loss= eval_model(val_loader, model, epoch = i, device = device,tokenizer = tokenizer)\n",
    "            epoch_val_log.append(val_loss)\n",
    "            validate_val_metric(val_loss, i , model, tokenizer)\n",
    "            if validate_train_metric.early_stop or validate_val_metric.early_stop :\n",
    "                print(f\"early stopping trigerred either from training data or val data | train_counter = {validate_train_metric.counter}|val_counter = {validate_val_metric.counter}\")\n",
    "                break\n",
    "        else:\n",
    "            if validate_val_metric.early_stop:\n",
    "                print(f\"early stopping trigerred from validation data\")\n",
    "                break\n",
    "              \n",
    "    \n",
    "    return model,epoch_train_log,epoch_val_log\n",
    "        \n",
    "            \n",
    "            \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec64294",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inside train model. Device = cuda:0\n",
      "executing epoch:1, it took 1.0215908924738566 mins from beginning of epoch till batch#137\n",
      "training loss has decreased---> reducing the best loss from inf to 264.74 | throughput = 145859 tokens/second | norm = 0.0551 | learning rate = 5.00000e-06\n",
      "inside validation data for epoch 1\n",
      "Val loss has decreased -->reducing the global validation loss from inf to 264.56\n",
      " validation loss for epoch = 0 is 264.5594\n",
      " epoch= 0 :  val loss is 264.5594 \n",
      "saving the model model2024-08-0100:15:57.pth\n",
      "*****LOGGING INFO IN GPT2_nll_loss_adj_replacementrandom_init_wts_True_2024-08-01.log*********\n",
      "executing epoch:2, it took 1.03871777455012 mins from beginning of epoch till batch#137\n",
      "training loss has decreased---> reducing the best loss from 264.74 to 242.09 | throughput = 145388 tokens/second | norm = 0.1420 | learning rate = 1.00000e-05\n",
      "inside validation data for epoch 2\n",
      "Val loss has decreased -->reducing the global validation loss from 264.56 to 237.82\n",
      " validation loss for epoch = 1 is 237.8222\n",
      " epoch= 1 :  val loss is 237.8222 \n",
      "saving the model model2024-08-0100:19:01.pth\n",
      "*****LOGGING INFO IN GPT2_nll_loss_adj_replacementrandom_init_wts_True_2024-08-01.log*********\n",
      "executing epoch:3, it took 1.0581382830937704 mins from beginning of epoch till batch#137\n",
      "training loss has decreased---> reducing the best loss from 242.09 to 170.32 | throughput = 143578 tokens/second | norm = 0.1759 | learning rate = 1.50000e-05\n",
      "inside validation data for epoch 3\n",
      "Val loss has decreased -->reducing the global validation loss from 237.82 to 187.31\n",
      " validation loss for epoch = 2 is 187.3062\n",
      " epoch= 2 :  val loss is 187.3062 \n",
      "saving the model model2024-08-0100:22:06.pth\n",
      "*****LOGGING INFO IN GPT2_nll_loss_adj_replacementrandom_init_wts_True_2024-08-01.log*********\n",
      "executing epoch:4, it took 1.0400185704231262 mins from beginning of epoch till batch#137\n",
      "training loss has decreased---> reducing the best loss from 170.32 to 114.77 | throughput = 146091 tokens/second | norm = 0.1731 | learning rate = 2.00000e-05\n",
      "inside validation data for epoch 4\n",
      "Val loss has decreased -->reducing the global validation loss from 187.31 to 154.82\n",
      " validation loss for epoch = 3 is 154.8237\n",
      " epoch= 3 :  val loss is 154.8237 \n",
      "saving the model model2024-08-0100:25:10.pth\n",
      "*****LOGGING INFO IN GPT2_nll_loss_adj_replacementrandom_init_wts_True_2024-08-01.log*********\n",
      "executing epoch:5, it took 1.0355684121449789 mins from beginning of epoch till batch#137\n",
      "training loss has decreased---> reducing the best loss from 114.77 to 79.92 | throughput = 146310 tokens/second | norm = 0.1231 | learning rate = 2.50000e-05\n",
      "inside validation data for epoch 5\n",
      "Val loss has decreased -->reducing the global validation loss from 154.82 to 116.62\n",
      " validation loss for epoch = 4 is 116.6155\n",
      " epoch= 4 :  val loss is 116.6155 \n",
      "saving the model model2024-08-0100:28:13.pth\n",
      "*****LOGGING INFO IN GPT2_nll_loss_adj_replacementrandom_init_wts_True_2024-08-01.log*********\n",
      "executing epoch:6, it took 1.0918269753456116 mins from beginning of epoch till batch#137\n",
      "training loss has decreased---> reducing the best loss from 79.92 to 55.95 | throughput = 145390 tokens/second | norm = 0.1917 | learning rate = 3.00000e-05\n",
      "inside validation data for epoch 6\n",
      "Val loss has decreased -->reducing the global validation loss from 116.62 to 82.22\n",
      " validation loss for epoch = 5 is 82.2230\n",
      " epoch= 5 :  val loss is 82.2230 \n",
      "saving the model model2024-08-0100:31:17.pth\n",
      "*****LOGGING INFO IN GPT2_nll_loss_adj_replacementrandom_init_wts_True_2024-08-01.log*********\n",
      "executing epoch:7, it took 1.1129134019215903 mins from beginning of epoch till batch#137\n",
      "training loss has decreased---> reducing the best loss from 55.95 to 42.78 | throughput = 141609 tokens/second | norm = 0.0942 | learning rate = 3.50000e-05\n",
      "inside validation data for epoch 7\n",
      "Val loss has decreased -->reducing the global validation loss from 82.22 to 63.96\n",
      " validation loss for epoch = 6 is 63.9568\n",
      " epoch= 6 :  val loss is 63.9568 \n",
      "saving the model model2024-08-0100:34:24.pth\n",
      "*****LOGGING INFO IN GPT2_nll_loss_adj_replacementrandom_init_wts_True_2024-08-01.log*********\n",
      "executing epoch:8, it took 1.0936204075813294 mins from beginning of epoch till batch#137\n",
      "training loss has decreased---> reducing the best loss from 42.78 to 40.81 | throughput = 140833 tokens/second | norm = 0.1088 | learning rate = 4.00000e-05\n",
      "inside validation data for epoch 8\n",
      "Val loss has decreased -->reducing the global validation loss from 63.96 to 52.24\n",
      " validation loss for epoch = 7 is 52.2404\n",
      " epoch= 7 :  val loss is 52.2404 \n",
      "saving the model model2024-08-0100:37:32.pth\n",
      "*****LOGGING INFO IN GPT2_nll_loss_adj_replacementrandom_init_wts_True_2024-08-01.log*********\n",
      "executing epoch:9, it took 1.0824477076530457 mins from beginning of epoch till batch#137\n",
      "training loss has decreased---> reducing the best loss from 40.81 to 31.87 | throughput = 142080 tokens/second | norm = 0.1327 | learning rate = 4.50000e-05\n",
      "inside validation data for epoch 9\n",
      "Val loss has decreased -->reducing the global validation loss from 52.24 to 38.86\n",
      " validation loss for epoch = 8 is 38.8562\n",
      " epoch= 8 :  val loss is 38.8562 \n",
      "saving the model model2024-08-0100:40:39.pth\n",
      "*****LOGGING INFO IN GPT2_nll_loss_adj_replacementrandom_init_wts_True_2024-08-01.log*********\n",
      "executing epoch:10, it took 1.0711392879486084 mins from beginning of epoch till batch#137\n",
      "training loss has decreased---> reducing the best loss from 31.87 to 16.99 | throughput = 146011 tokens/second | norm = 0.0686 | learning rate = 5.00000e-05\n",
      "inside validation data for epoch 10\n",
      "Val loss has decreased -->reducing the global validation loss from 38.86 to 33.61\n",
      " validation loss for epoch = 9 is 33.6115\n",
      " epoch= 9 :  val loss is 33.6115 \n",
      "saving the model model2024-08-0100:43:42.pth\n",
      "*****LOGGING INFO IN GPT2_nll_loss_adj_replacementrandom_init_wts_True_2024-08-01.log*********\n",
      "executing epoch:11, it took 1.052325145403544 mins from beginning of epoch till batch#137\n",
      "training loss has decreased---> reducing the best loss from 16.99 to 11.87 | throughput = 145782 tokens/second | norm = 0.1095 | learning rate = 4.99848e-05\n",
      "inside validation data for epoch 11\n",
      "Val loss has decreased -->reducing the global validation loss from 33.61 to 27.81\n",
      " validation loss for epoch = 10 is 27.8088\n",
      " epoch= 10 :  val loss is 27.8088 \n",
      "saving the model model2024-08-0100:46:46.pth\n",
      "*****LOGGING INFO IN GPT2_nll_loss_adj_replacementrandom_init_wts_True_2024-08-01.log*********\n",
      "executing epoch:12, it took 1.0626487294832865 mins from beginning of epoch till batch#137\n",
      "training loss has decreased---> reducing the best loss from 11.87 to 8.74 | throughput = 144005 tokens/second | norm = 0.0879 | learning rate = 4.99391e-05\n",
      "inside validation data for epoch 12\n",
      "Val loss has decreased -->reducing the global validation loss from 27.81 to 25.62\n",
      " validation loss for epoch = 11 is 25.6223\n",
      " epoch= 11 :  val loss is 25.6223 \n",
      "saving the model model2024-08-0100:49:51.pth\n",
      "*****LOGGING INFO IN GPT2_nll_loss_adj_replacementrandom_init_wts_True_2024-08-01.log*********\n",
      "executing epoch:13, it took 1.0471571763356526 mins from beginning of epoch till batch#137\n",
      "training loss has decreased---> reducing the best loss from 8.74 to 4.98 | throughput = 145076 tokens/second | norm = 0.1077 | learning rate = 4.98630e-05\n",
      "inside validation data for epoch 13\n",
      "Val loss has decreased -->reducing the global validation loss from 25.62 to 22.56\n",
      " validation loss for epoch = 12 is 22.5573\n",
      " epoch= 12 :  val loss is 22.5573 \n",
      "saving the model model2024-08-0100:52:55.pth\n",
      "*****LOGGING INFO IN GPT2_nll_loss_adj_replacementrandom_init_wts_True_2024-08-01.log*********\n",
      "executing epoch:14, it took 1.1239399909973145 mins from beginning of epoch till batch#137\n",
      "training loss has decreased---> reducing the best loss from 4.98 to 2.87 | throughput = 143508 tokens/second | norm = 0.0121 | learning rate = 4.97567e-05\n",
      "inside validation data for epoch 14\n",
      "Val loss has decreased -->reducing the global validation loss from 22.56 to 21.51\n",
      " validation loss for epoch = 13 is 21.5111\n",
      " epoch= 13 :  val loss is 21.5111 \n",
      "saving the model model2024-08-0100:56:01.pth\n",
      "*****LOGGING INFO IN GPT2_nll_loss_adj_replacementrandom_init_wts_True_2024-08-01.log*********\n",
      "executing epoch:15, it took 1.0302730202674866 mins from beginning of epoch till batch#137\n",
      "training loss has decreased---> reducing the best loss from 2.87 to 2.81 | throughput = 144970 tokens/second | norm = 0.0135 | learning rate = 4.96202e-05\n",
      "inside validation data for epoch 15\n",
      "No improvement in validation loss-->epoch= 14 and best val loss is 21.51|current_Val loss = 21.63894557952881|counter = 1\n",
      "executing epoch:16, it took 1.0751612305641174 mins from beginning of epoch till batch#137\n",
      "No improvement in training  loss-->epoch= 15 and best loss is 2.81|current_loss = 3.354243040084839|counter = 1\n",
      "executing epoch:17, it took 1.125719916820526 mins from beginning of epoch till batch#137\n",
      "training loss has decreased---> reducing the best loss from 2.81 to 1.98 | throughput = 140583 tokens/second | norm = 0.0303 | learning rate = 4.92574e-05\n",
      "inside validation data for epoch 17\n",
      "No improvement in validation loss-->epoch= 16 and best val loss is 21.51|current_Val loss = 21.794517755508423|counter = 2\n",
      "executing epoch:18, it took 1.1061420043309529 mins from beginning of epoch till batch#137\n",
      "training loss has decreased---> reducing the best loss from 1.98 to 1.38 | throughput = 140429 tokens/second | norm = 0.2371 | learning rate = 4.90315e-05\n",
      "inside validation data for epoch 18\n",
      "Val loss has decreased -->reducing the global validation loss from 21.51 to 19.20\n",
      " validation loss for epoch = 17 is 19.2026\n",
      " epoch= 17 :  val loss is 19.2026 \n",
      "saving the model model2024-08-0101:07:23.pth\n",
      "*****LOGGING INFO IN GPT2_nll_loss_adj_replacementrandom_init_wts_True_2024-08-01.log*********\n",
      "executing epoch:19, it took 1.0760054389635723 mins from beginning of epoch till batch#137\n",
      "training loss has decreased---> reducing the best loss from 1.38 to 1.22 | throughput = 143550 tokens/second | norm = 0.0079 | learning rate = 4.87764e-05\n",
      "inside validation data for epoch 19\n",
      "Val loss has decreased -->reducing the global validation loss from 19.20 to 18.06\n",
      " validation loss for epoch = 18 is 18.0551\n",
      " epoch= 18 :  val loss is 18.0551 \n",
      "saving the model model2024-08-0101:10:29.pth\n",
      "*****LOGGING INFO IN GPT2_nll_loss_adj_replacementrandom_init_wts_True_2024-08-01.log*********\n",
      "executing epoch:20, it took 1.0655373016993204 mins from beginning of epoch till batch#137\n",
      "No improvement in training  loss-->epoch= 19 and best loss is 1.22|current_loss = 1.6307060718536377|counter = 1\n",
      "executing epoch:21, it took 1.0292311747868856 mins from beginning of epoch till batch#137\n",
      "No improvement in training  loss-->epoch= 20 and best loss is 1.22|current_loss = 1.4950729012489319|counter = 2\n",
      "executing epoch:22, it took 1.0830345511436463 mins from beginning of epoch till batch#137\n",
      "training loss has decreased---> reducing the best loss from 1.22 to 1.09 | throughput = 143604 tokens/second | norm = 0.0212 | learning rate = 4.78386e-05\n",
      "inside validation data for epoch 22\n",
      "Val loss has decreased -->reducing the global validation loss from 18.06 to 16.09\n",
      " validation loss for epoch = 21 is 16.0879\n",
      " epoch= 21 :  val loss is 16.0879 \n",
      "saving the model model2024-08-0101:17:37.pth\n",
      "*****LOGGING INFO IN GPT2_nll_loss_adj_replacementrandom_init_wts_True_2024-08-01.log*********\n",
      "executing epoch:23, it took 1.0583855907122295 mins from beginning of epoch till batch#137\n",
      "training loss has decreased---> reducing the best loss from 1.09 to 0.95 | throughput = 144158 tokens/second | norm = 0.0067 | learning rate = 4.74699e-05\n",
      "inside validation data for epoch 23\n",
      "Val loss has decreased -->reducing the global validation loss from 16.09 to 15.22\n",
      " validation loss for epoch = 22 is 15.2204\n",
      " epoch= 22 :  val loss is 15.2204 \n",
      "saving the model model2024-08-0101:20:42.pth\n",
      "*****LOGGING INFO IN GPT2_nll_loss_adj_replacementrandom_init_wts_True_2024-08-01.log*********\n",
      "executing epoch:24, it took 1.0631670753161113 mins from beginning of epoch till batch#137\n",
      "No improvement in training  loss-->epoch= 23 and best loss is 0.95|current_loss = 1.0375711917877197|counter = 1\n",
      "executing epoch:25, it took 1.0800194104512533 mins from beginning of epoch till batch#137\n",
      "No improvement in training  loss-->epoch= 24 and best loss is 0.95|current_loss = 3.5767409801483154|counter = 2\n",
      "executing epoch:26, it took 1.065937864780426 mins from beginning of epoch till batch#137\n",
      "No improvement in training  loss-->epoch= 25 and best loss is 0.95|current_loss = 3.5055662989616394|counter = 3\n",
      "executing epoch:27, it took 1.0878718892733257 mins from beginning of epoch till batch#137\n",
      "No improvement in training  loss-->epoch= 26 and best loss is 0.95|current_loss = 1.3508657813072205|counter = 4\n",
      "executing epoch:28, it took 1.11536971728007 mins from beginning of epoch till batch#137\n",
      "training loss has decreased---> reducing the best loss from 0.95 to 0.86 | throughput = 139551 tokens/second | norm = 0.0071 | learning rate = 4.52254e-05\n",
      "inside validation data for epoch 28\n",
      "Val loss has decreased -->reducing the global validation loss from 15.22 to 11.56\n",
      " validation loss for epoch = 27 is 11.5612\n",
      " epoch= 27 :  val loss is 11.5612 \n",
      "saving the model model2024-08-0101:32:18.pth\n",
      "*****LOGGING INFO IN GPT2_nll_loss_adj_replacementrandom_init_wts_True_2024-08-01.log*********\n",
      "executing epoch:29, it took 1.1088540395100912 mins from beginning of epoch till batch#137\n",
      "training loss has decreased---> reducing the best loss from 0.86 to 0.53 | throughput = 140257 tokens/second | norm = 0.0030 | learning rate = 4.47003e-05\n",
      "inside validation data for epoch 29\n",
      "Val loss has decreased -->reducing the global validation loss from 11.56 to 10.71\n",
      " validation loss for epoch = 28 is 10.7138\n",
      " epoch= 28 :  val loss is 10.7138 \n",
      "saving the model model2024-08-0101:35:27.pth\n",
      "*****LOGGING INFO IN GPT2_nll_loss_adj_replacementrandom_init_wts_True_2024-08-01.log*********\n",
      "executing epoch:30, it took 1.1704309900601706 mins from beginning of epoch till batch#137\n",
      "training loss has decreased---> reducing the best loss from 0.53 to 0.35 | throughput = 133879 tokens/second | norm = 0.0069 | learning rate = 4.41511e-05\n",
      "inside validation data for epoch 30\n",
      "Val loss has decreased -->reducing the global validation loss from 10.71 to 9.99\n",
      " validation loss for epoch = 29 is 9.9866\n",
      " epoch= 29 :  val loss is 9.9866 \n",
      "saving the model model2024-08-0101:38:41.pth\n",
      "*****LOGGING INFO IN GPT2_nll_loss_adj_replacementrandom_init_wts_True_2024-08-01.log*********\n",
      "executing epoch:31, it took 1.1127797603607177 mins from beginning of epoch till batch#137\n",
      "training loss has decreased---> reducing the best loss from 0.35 to 0.28 | throughput = 136106 tokens/second | norm = 0.0046 | learning rate = 4.35786e-05\n",
      "inside validation data for epoch 31\n",
      "Val loss has decreased -->reducing the global validation loss from 9.99 to 9.61\n",
      " validation loss for epoch = 30 is 9.6137\n",
      " epoch= 30 :  val loss is 9.6137 \n",
      "saving the model model2024-08-0101:42:11.pth\n",
      "*****LOGGING INFO IN GPT2_nll_loss_adj_replacementrandom_init_wts_True_2024-08-01.log*********\n",
      "executing epoch:32, it took 1.0923573573430378 mins from beginning of epoch till batch#137\n",
      "training loss has decreased---> reducing the best loss from 0.28 to 0.22 | throughput = 137399 tokens/second | norm = 0.0121 | learning rate = 4.29835e-05\n",
      "inside validation data for epoch 32\n",
      "Val loss has decreased -->reducing the global validation loss from 9.61 to 9.12\n",
      " validation loss for epoch = 31 is 9.1179\n",
      " epoch= 31 :  val loss is 9.1179 \n",
      "saving the model model2024-08-0101:45:39.pth\n",
      "*****LOGGING INFO IN GPT2_nll_loss_adj_replacementrandom_init_wts_True_2024-08-01.log*********\n",
      "executing epoch:33, it took 1.0953345894813538 mins from beginning of epoch till batch#137\n",
      "training loss has decreased---> reducing the best loss from 0.22 to 0.18 | throughput = 139338 tokens/second | norm = 0.0087 | learning rate = 4.23665e-05\n",
      "inside validation data for epoch 33\n",
      "Val loss has decreased -->reducing the global validation loss from 9.12 to 8.78\n",
      " validation loss for epoch = 32 is 8.7824\n",
      " epoch= 32 :  val loss is 8.7824 \n",
      "saving the model model2024-08-0101:49:06.pth\n",
      "*****LOGGING INFO IN GPT2_nll_loss_adj_replacementrandom_init_wts_True_2024-08-01.log*********\n",
      "executing epoch:34, it took 1.1582660794258117 mins from beginning of epoch till batch#137\n",
      "training loss has decreased---> reducing the best loss from 0.18 to 0.16 | throughput = 135099 tokens/second | norm = 0.0022 | learning rate = 4.17283e-05\n",
      "inside validation data for epoch 34\n",
      "Val loss has decreased -->reducing the global validation loss from 8.78 to 8.49\n",
      " validation loss for epoch = 33 is 8.4867\n",
      " epoch= 33 :  val loss is 8.4867 \n",
      "saving the model model2024-08-0101:52:36.pth\n",
      "*****LOGGING INFO IN GPT2_nll_loss_adj_replacementrandom_init_wts_True_2024-08-01.log*********\n",
      "executing epoch:35, it took 1.1237343033154805 mins from beginning of epoch till batch#137\n",
      "No improvement in training  loss-->epoch= 34 and best loss is 0.16|current_loss = 0.5107616186141968|counter = 1\n",
      "executing epoch:36, it took 1.131381376584371 mins from beginning of epoch till batch#137\n",
      "No improvement in training  loss-->epoch= 35 and best loss is 0.16|current_loss = 1.197696030139923|counter = 2\n",
      "executing epoch:37, it took 1.1029606540997823 mins from beginning of epoch till batch#137\n",
      "No improvement in training  loss-->epoch= 36 and best loss is 0.16|current_loss = 0.5503268837928772|counter = 3\n",
      "executing epoch:38, it took 1.1377961834271748 mins from beginning of epoch till batch#137\n",
      "No improvement in training  loss-->epoch= 37 and best loss is 0.16|current_loss = 0.2169288992881775|counter = 4\n",
      "executing epoch:39, it took 1.1238062063852945 mins from beginning of epoch till batch#137\n",
      "training loss has decreased---> reducing the best loss from 0.16 to 0.10 | throughput = 135837 tokens/second | norm = 0.0076 | learning rate = 3.82480e-05\n",
      "inside validation data for epoch 39\n",
      "Val loss has decreased -->reducing the global validation loss from 8.49 to 7.67\n",
      " validation loss for epoch = 38 is 7.6739\n",
      " epoch= 38 :  val loss is 7.6739 \n",
      "saving the model model2024-08-0102:04:42.pth\n",
      "*****LOGGING INFO IN GPT2_nll_loss_adj_replacementrandom_init_wts_True_2024-08-01.log*********\n",
      "executing epoch:40, it took 1.1198506474494934 mins from beginning of epoch till batch#137\n",
      "No improvement in training  loss-->epoch= 39 and best loss is 0.10|current_loss = 0.3601536750793457|counter = 1\n",
      "executing epoch:41, it took 1.1432846506436667 mins from beginning of epoch till batch#137\n",
      "No improvement in training  loss-->epoch= 40 and best loss is 0.10|current_loss = 1.9804930686950684|counter = 2\n",
      "executing epoch:42, it took 1.1311447660128275 mins from beginning of epoch till batch#137\n",
      "No improvement in training  loss-->epoch= 41 and best loss is 0.10|current_loss = 1.3844212293624878|counter = 3\n",
      "executing epoch:43, it took 1.1402838985125223 mins from beginning of epoch till batch#137\n",
      "No improvement in training  loss-->epoch= 42 and best loss is 0.10|current_loss = 0.3728964924812317|counter = 4\n",
      "executing epoch:44, it took 1.0794243216514587 mins from beginning of epoch till batch#137\n",
      "No improvement in training  loss-->epoch= 43 and best loss is 0.10|current_loss = 0.2890033721923828|counter = 5\n",
      "executing epoch:45, it took 1.073920436700185 mins from beginning of epoch till batch#137\n",
      "No improvement in training  loss-->epoch= 44 and best loss is 0.10|current_loss = 0.3282167315483093|counter = 6\n",
      "executing epoch:46, it took 1.1286788582801819 mins from beginning of epoch till batch#137\n",
      "No improvement in training  loss-->epoch= 45 and best loss is 0.10|current_loss = 0.3816155791282654|counter = 7\n",
      "executing epoch:47, it took 1.1313746611277262 mins from beginning of epoch till batch#137\n",
      "No improvement in training  loss-->epoch= 46 and best loss is 0.10|current_loss = 0.2877488136291504|counter = 8\n",
      "executing epoch:48, it took 1.14606587489446 mins from beginning of epoch till batch#137\n",
      "No improvement in training  loss-->epoch= 47 and best loss is 0.10|current_loss = 0.251431405544281|counter = 9\n",
      "executing epoch:49, it took 1.1100010514259337 mins from beginning of epoch till batch#137\n",
      "No improvement in training  loss-->epoch= 48 and best loss is 0.10|current_loss = 1.1207783818244934|counter = 10\n",
      "executing epoch:50, it took 1.0748464902242025 mins from beginning of epoch till batch#137\n",
      "No improvement in training  loss-->epoch= 49 and best loss is 0.10|current_loss = 3.045698821544647|counter = 11\n",
      "executing epoch:51, it took 1.0995808243751526 mins from beginning of epoch till batch#137\n",
      "No improvement in training  loss-->epoch= 50 and best loss is 0.10|current_loss = 0.8332093954086304|counter = 12\n",
      "executing epoch:52, it took 1.1124728202819825 mins from beginning of epoch till batch#137\n",
      "No improvement in training  loss-->epoch= 51 and best loss is 0.10|current_loss = 0.46814173460006714|counter = 13\n",
      "executing epoch:53, it took 1.1028700351715088 mins from beginning of epoch till batch#137\n",
      "No improvement in training  loss-->epoch= 52 and best loss is 0.10|current_loss = 0.33316636085510254|counter = 14\n",
      "executing epoch:54, it took 1.106097149848938 mins from beginning of epoch till batch#137\n",
      "No improvement in training  loss-->epoch= 53 and best loss is 0.10|current_loss = 0.24500977993011475|counter = 15\n",
      "executing epoch:55, it took 1.1396735509236653 mins from beginning of epoch till batch#137\n",
      "No improvement in training  loss-->epoch= 54 and best loss is 0.10|current_loss = 0.17929363250732422|counter = 16\n",
      "executing epoch:56, it took 1.023674694697062 mins from beginning of epoch till batch#137\n",
      "No improvement in training  loss-->epoch= 55 and best loss is 0.10|current_loss = 0.16108912229537964|counter = 17\n",
      "executing epoch:57, it took 1.054734714825948 mins from beginning of epoch till batch#137\n",
      "No improvement in training  loss-->epoch= 56 and best loss is 0.10|current_loss = 0.12675869464874268|counter = 18\n",
      "executing epoch:58, it took 1.056213947137197 mins from beginning of epoch till batch#137\n",
      "No improvement in training  loss-->epoch= 57 and best loss is 0.10|current_loss = 0.1282334327697754|counter = 19\n",
      "executing epoch:59, it took 1.0954487005869546 mins from beginning of epoch till batch#137\n",
      "training loss has decreased---> reducing the best loss from 0.10 to 0.09 | throughput = 145009 tokens/second | norm = 0.0053 | learning rate = 2.15207e-05\n",
      "inside validation data for epoch 59\n",
      "Val loss has decreased -->reducing the global validation loss from 7.67 to 6.99\n",
      " validation loss for epoch = 58 is 6.9921\n",
      " epoch= 58 :  val loss is 6.9921 \n",
      "saving the model model2024-08-0102:48:30.pth\n",
      "*****LOGGING INFO IN GPT2_nll_loss_adj_replacementrandom_init_wts_True_2024-08-01.log*********\n",
      "executing epoch:60, it took 1.0472866733868917 mins from beginning of epoch till batch#137\n",
      "training loss has decreased---> reducing the best loss from 0.09 to 0.08 | throughput = 149329 tokens/second | norm = 0.0008 | learning rate = 2.06588e-05\n",
      "inside validation data for epoch 60\n",
      "Val loss has decreased -->reducing the global validation loss from 6.99 to 6.95\n",
      " validation loss for epoch = 59 is 6.9491\n",
      " epoch= 59 :  val loss is 6.9491 \n",
      "saving the model model2024-08-0102:51:31.pth\n",
      "*****LOGGING INFO IN GPT2_nll_loss_adj_replacementrandom_init_wts_True_2024-08-01.log*********\n",
      "executing epoch:61, it took 1.013955330848694 mins from beginning of epoch till batch#137\n",
      "training loss has decreased---> reducing the best loss from 0.08 to 0.07 | throughput = 150264 tokens/second | norm = 0.0007 | learning rate = 1.98022e-05\n",
      "inside validation data for epoch 61\n",
      "Val loss has decreased -->reducing the global validation loss from 6.95 to 6.88\n",
      " validation loss for epoch = 60 is 6.8787\n",
      " epoch= 60 :  val loss is 6.8787 \n",
      "saving the model model2024-08-0102:54:30.pth\n",
      "*****LOGGING INFO IN GPT2_nll_loss_adj_replacementrandom_init_wts_True_2024-08-01.log*********\n",
      "executing epoch:62, it took 1.064368438720703 mins from beginning of epoch till batch#137\n",
      "training loss has decreased---> reducing the best loss from 0.07 to 0.06 | throughput = 148031 tokens/second | norm = 0.0007 | learning rate = 1.89520e-05\n",
      "inside validation data for epoch 62\n",
      "Val loss has decreased -->reducing the global validation loss from 6.88 to 6.86\n",
      " validation loss for epoch = 61 is 6.8634\n",
      " epoch= 61 :  val loss is 6.8634 \n",
      "saving the model model2024-08-0102:57:32.pth\n",
      "*****LOGGING INFO IN GPT2_nll_loss_adj_replacementrandom_init_wts_True_2024-08-01.log*********\n",
      "executing epoch:63, it took 1.0583850741386414 mins from beginning of epoch till batch#137\n",
      "training loss has decreased---> reducing the best loss from 0.06 to 0.05 | throughput = 147568 tokens/second | norm = 0.0006 | learning rate = 1.81091e-05\n",
      "inside validation data for epoch 63\n",
      "Val loss has decreased -->reducing the global validation loss from 6.86 to 6.85\n",
      " validation loss for epoch = 62 is 6.8545\n",
      " epoch= 62 :  val loss is 6.8545 \n",
      "saving the model model2024-08-0103:00:34.pth\n",
      "*****LOGGING INFO IN GPT2_nll_loss_adj_replacementrandom_init_wts_True_2024-08-01.log*********\n",
      "executing epoch:64, it took 0.973853079477946 mins from beginning of epoch till batch#137\n",
      "No improvement in training  loss-->epoch= 63 and best loss is 0.05|current_loss = 0.057772696018218994|counter = 1\n",
      "executing epoch:65, it took 1.074756920337677 mins from beginning of epoch till batch#137\n",
      "training loss has decreased---> reducing the best loss from 0.05 to 0.04 | throughput = 143409 tokens/second | norm = 0.0005 | learning rate = 1.64495e-05\n",
      "inside validation data for epoch 65\n",
      "Val loss has decreased -->reducing the global validation loss from 6.85 to 6.84\n",
      " validation loss for epoch = 64 is 6.8432\n",
      " epoch= 64 :  val loss is 6.8432 \n",
      "saving the model model2024-08-0103:05:33.pth\n",
      "*****LOGGING INFO IN GPT2_nll_loss_adj_replacementrandom_init_wts_True_2024-08-01.log*********\n",
      "executing epoch:66, it took 1.0301697254180908 mins from beginning of epoch till batch#137\n",
      "training loss has decreased---> reducing the best loss from 0.04 to 0.04 | throughput = 154093 tokens/second | norm = 0.0005 | learning rate = 1.56348e-05\n",
      "inside validation data for epoch 66\n",
      "Val loss has decreased -->reducing the global validation loss from 6.84 to 6.84\n",
      " validation loss for epoch = 65 is 6.8374\n",
      " epoch= 65 :  val loss is 6.8374 \n",
      "saving the model model2024-08-0103:08:30.pth\n",
      "*****LOGGING INFO IN GPT2_nll_loss_adj_replacementrandom_init_wts_True_2024-08-01.log*********\n",
      "executing epoch:67, it took 1.0479499657948812 mins from beginning of epoch till batch#137\n",
      "training loss has decreased---> reducing the best loss from 0.04 to 0.04 | throughput = 148318 tokens/second | norm = 0.0019 | learning rate = 1.48316e-05\n",
      "inside validation data for epoch 67\n",
      "No improvement in validation loss-->epoch= 66 and best val loss is 6.84|current_Val loss = 6.841620028018951|counter = 1\n",
      "executing epoch:68, it took 1.0715251604715983 mins from beginning of epoch till batch#137\n",
      "training loss has decreased---> reducing the best loss from 0.04 to 0.03 | throughput = 146532 tokens/second | norm = 0.0018 | learning rate = 1.40407e-05\n",
      "inside validation data for epoch 68\n",
      "No improvement in validation loss-->epoch= 67 and best val loss is 6.84|current_Val loss = 6.849704027175903|counter = 2\n",
      "executing epoch:69, it took 1.0549747149149578 mins from beginning of epoch till batch#137\n",
      "No improvement in training  loss-->epoch= 68 and best loss is 0.03|current_loss = 0.032879650592803955|counter = 1\n",
      "executing epoch:70, it took 0.9902960618336996 mins from beginning of epoch till batch#137\n",
      "training loss has decreased---> reducing the best loss from 0.03 to 0.03 | throughput = 149647 tokens/second | norm = 0.0015 | learning rate = 1.25000e-05\n",
      "inside validation data for epoch 70\n",
      "Val loss has decreased -->reducing the global validation loss from 6.84 to 6.83\n",
      " validation loss for epoch = 69 is 6.8296\n",
      " epoch= 69 :  val loss is 6.8296 \n",
      "saving the model model2024-08-0103:19:31.pth\n",
      "*****LOGGING INFO IN GPT2_nll_loss_adj_replacementrandom_init_wts_True_2024-08-01.log*********\n",
      "executing epoch:71, it took 1.0376566648483276 mins from beginning of epoch till batch#137\n",
      "training loss has decreased---> reducing the best loss from 0.03 to 0.03 | throughput = 149913 tokens/second | norm = 0.0004 | learning rate = 1.17520e-05\n",
      "inside validation data for epoch 71\n",
      "No improvement in validation loss-->epoch= 70 and best val loss is 6.83|current_Val loss = 6.845576465129852|counter = 1\n",
      "executing epoch:72, it took 1.0643625100453695 mins from beginning of epoch till batch#137\n",
      "training loss has decreased---> reducing the best loss from 0.03 to 0.03 | throughput = 145350 tokens/second | norm = 0.0003 | learning rate = 1.10202e-05\n",
      "inside validation data for epoch 72\n",
      "No improvement in validation loss-->epoch= 71 and best val loss is 6.83|current_Val loss = 6.843562424182892|counter = 2\n",
      "executing epoch:73, it took 1.0427385528882345 mins from beginning of epoch till batch#137\n",
      "training loss has decreased---> reducing the best loss from 0.03 to 0.03 | throughput = 143576 tokens/second | norm = 0.0003 | learning rate = 1.03054e-05\n",
      "inside validation data for epoch 73\n",
      "No improvement in validation loss-->epoch= 72 and best val loss is 6.83|current_Val loss = 6.848412036895752|counter = 3\n",
      "executing epoch:74, it took 1.0681734244028727 mins from beginning of epoch till batch#137\n",
      "training loss has decreased---> reducing the best loss from 0.03 to 0.02 | throughput = 148262 tokens/second | norm = 0.0006 | learning rate = 9.60846e-06\n",
      "inside validation data for epoch 74\n",
      "No improvement in validation loss-->epoch= 73 and best val loss is 6.83|current_Val loss = 6.847729802131653|counter = 4\n",
      "executing epoch:75, it took 1.017037578423818 mins from beginning of epoch till batch#137\n",
      "training loss has decreased---> reducing the best loss from 0.02 to 0.02 | throughput = 149607 tokens/second | norm = 0.0003 | learning rate = 8.93031e-06\n",
      "inside validation data for epoch 75\n",
      "No improvement in validation loss-->epoch= 74 and best val loss is 6.83|current_Val loss = 6.855282843112946|counter = 5\n",
      "executing epoch:76, it took 1.0706989407539367 mins from beginning of epoch till batch#137\n",
      "No improvement in training  loss-->epoch= 75 and best loss is 0.02|current_loss = 0.020930588245391846|counter = 1\n",
      "executing epoch:77, it took 1.043361985683441 mins from beginning of epoch till batch#137\n",
      "training loss has decreased---> reducing the best loss from 0.02 to 0.02 | throughput = 148919 tokens/second | norm = 0.0002 | learning rate = 7.63354e-06\n",
      "inside validation data for epoch 77\n",
      "No improvement in validation loss-->epoch= 76 and best val loss is 6.83|current_Val loss = 6.866090416908264|counter = 6\n",
      "executing epoch:78, it took 1.0648706158002217 mins from beginning of epoch till batch#137\n",
      "training loss has decreased---> reducing the best loss from 0.02 to 0.02 | throughput = 148406 tokens/second | norm = 0.0002 | learning rate = 7.01650e-06\n",
      "inside validation data for epoch 78\n",
      "No improvement in validation loss-->epoch= 77 and best val loss is 6.83|current_Val loss = 6.8662039041519165|counter = 7\n",
      "executing epoch:79, it took 1.0253557761510212 mins from beginning of epoch till batch#137\n",
      "training loss has decreased---> reducing the best loss from 0.02 to 0.02 | throughput = 149382 tokens/second | norm = 0.0002 | learning rate = 6.42138e-06\n",
      "inside validation data for epoch 79\n",
      "No improvement in validation loss-->epoch= 78 and best val loss is 6.83|current_Val loss = 6.877269506454468|counter = 8\n",
      "executing epoch:80, it took 1.0410352309544881 mins from beginning of epoch till batch#137\n",
      "training loss has decreased---> reducing the best loss from 0.02 to 0.02 | throughput = 145209 tokens/second | norm = 0.0002 | learning rate = 5.84889e-06\n",
      "inside validation data for epoch 80\n",
      "No improvement in validation loss-->epoch= 79 and best val loss is 6.83|current_Val loss = 6.881027579307556|counter = 9\n",
      "executing epoch:81, it took 1.045584507783254 mins from beginning of epoch till batch#137\n",
      "training loss has decreased---> reducing the best loss from 0.02 to 0.01 | throughput = 146593 tokens/second | norm = 0.0002 | learning rate = 5.29973e-06\n",
      "inside validation data for epoch 81\n",
      "No improvement in validation loss-->epoch= 80 and best val loss is 6.83|current_Val loss = 6.8705673813819885|counter = 10\n",
      "executing epoch:82, it took 1.0402343630790711 mins from beginning of epoch till batch#137\n",
      "training loss has decreased---> reducing the best loss from 0.01 to 0.01 | throughput = 146215 tokens/second | norm = 0.0002 | learning rate = 4.77458e-06\n",
      "inside validation data for epoch 82\n",
      "No improvement in validation loss-->epoch= 81 and best val loss is 6.83|current_Val loss = 6.873588562011719|counter = 11\n",
      "executing epoch:83, it took 1.0707858244578043 mins from beginning of epoch till batch#137\n",
      "No improvement in training  loss-->epoch= 82 and best loss is 0.01|current_loss = 0.01466512680053711|counter = 1\n",
      "executing epoch:84, it took 1.0608545462290446 mins from beginning of epoch till batch#137\n",
      "training loss has decreased---> reducing the best loss from 0.01 to 0.01 | throughput = 142819 tokens/second | norm = 0.0002 | learning rate = 3.79880e-06\n",
      "inside validation data for epoch 84\n",
      "No improvement in validation loss-->epoch= 83 and best val loss is 6.83|current_Val loss = 6.877303719520569|counter = 12\n",
      "executing epoch:85, it took 1.0406396826108297 mins from beginning of epoch till batch#137\n",
      "No improvement in training  loss-->epoch= 84 and best loss is 0.01|current_loss = 0.013182580471038818|counter = 1\n",
      "executing epoch:86, it took 1.0017185886700948 mins from beginning of epoch till batch#137\n",
      "No improvement in training  loss-->epoch= 85 and best loss is 0.01|current_loss = 0.013273775577545166|counter = 2\n",
      "executing epoch:87, it took 1.0189560612042745 mins from beginning of epoch till batch#137\n",
      "No improvement in training  loss-->epoch= 86 and best loss is 0.01|current_loss = 0.013249516487121582|counter = 3\n",
      "executing epoch:88, it took 1.0695671955744426 mins from beginning of epoch till batch#137\n",
      "training loss has decreased---> reducing the best loss from 0.01 to 0.01 | throughput = 147964 tokens/second | norm = 0.0002 | learning rate = 2.16136e-06\n",
      "inside validation data for epoch 88\n",
      "No improvement in validation loss-->epoch= 87 and best val loss is 6.83|current_Val loss = 6.893201589584351|counter = 13\n",
      "executing epoch:89, it took 1.0626347978909811 mins from beginning of epoch till batch#137\n",
      "No improvement in training  loss-->epoch= 88 and best loss is 0.01|current_loss = 0.01240849494934082|counter = 1\n",
      "executing epoch:90, it took 1.098499310016632 mins from beginning of epoch till batch#137\n",
      "No improvement in training  loss-->epoch= 89 and best loss is 0.01|current_loss = 0.012588918209075928|counter = 2\n",
      "executing epoch:91, it took 1.0559856335322062 mins from beginning of epoch till batch#137\n",
      "No improvement in training  loss-->epoch= 90 and best loss is 0.01|current_loss = 0.012717127799987793|counter = 3\n",
      "executing epoch:92, it took 1.032875386873881 mins from beginning of epoch till batch#137\n",
      "No improvement in training  loss-->epoch= 91 and best loss is 0.01|current_loss = 0.01382368803024292|counter = 4\n",
      "executing epoch:93, it took 1.0429709315299989 mins from beginning of epoch till batch#137\n",
      "No improvement in training  loss-->epoch= 92 and best loss is 0.01|current_loss = 0.012353777885437012|counter = 5\n",
      "executing epoch:94, it took 1.0617396394411722 mins from beginning of epoch till batch#137\n",
      "No improvement in training  loss-->epoch= 93 and best loss is 0.01|current_loss = 0.013250410556793213|counter = 6\n",
      "executing epoch:95, it took 1.0443802038828531 mins from beginning of epoch till batch#137\n",
      "No improvement in training  loss-->epoch= 94 and best loss is 0.01|current_loss = 0.012838780879974365|counter = 7\n",
      "executing epoch:96, it took 1.0382126371065776 mins from beginning of epoch till batch#137\n",
      "No improvement in training  loss-->epoch= 95 and best loss is 0.01|current_loss = 0.013253271579742432|counter = 8\n",
      "executing epoch:97, it took 1.0728267947832744 mins from beginning of epoch till batch#137\n",
      "No improvement in training  loss-->epoch= 96 and best loss is 0.01|current_loss = 0.012497305870056152|counter = 9\n",
      "executing epoch:98, it took 1.038892646630605 mins from beginning of epoch till batch#137\n",
      "No improvement in training  loss-->epoch= 97 and best loss is 0.01|current_loss = 0.012734532356262207|counter = 10\n",
      "executing epoch:99, it took 1.0586429516474405 mins from beginning of epoch till batch#137\n",
      "No improvement in training  loss-->epoch= 98 and best loss is 0.01|current_loss = 0.012439489364624023|counter = 11\n",
      "executing epoch:100, it took 1.0652222712834676 mins from beginning of epoch till batch#137\n",
      "No improvement in training  loss-->epoch= 99 and best loss is 0.01|current_loss = 0.01296454668045044|counter = 12\n",
      "executing epoch:101, it took 1.0521101395289103 mins from beginning of epoch till batch#137\n",
      "No improvement in training  loss-->epoch= 100 and best loss is 0.01|current_loss = 0.01256948709487915|counter = 13\n",
      "executing epoch:102, it took 1.1118998964627584 mins from beginning of epoch till batch#137\n",
      "No improvement in training  loss-->epoch= 101 and best loss is 0.01|current_loss = 0.012355625629425049|counter = 14\n",
      "executing epoch:103, it took 1.0675989309946696 mins from beginning of epoch till batch#137\n",
      "No improvement in training  loss-->epoch= 102 and best loss is 0.01|current_loss = 0.012519121170043945|counter = 15\n",
      "executing epoch:104, it took 1.080282485485077 mins from beginning of epoch till batch#137\n",
      "No improvement in training  loss-->epoch= 103 and best loss is 0.01|current_loss = 0.013047575950622559|counter = 16\n",
      "executing epoch:105, it took 1.0700857321421304 mins from beginning of epoch till batch#137\n",
      "No improvement in training  loss-->epoch= 104 and best loss is 0.01|current_loss = 0.01275324821472168|counter = 17\n",
      "executing epoch:106, it took 1.0655474583307902 mins from beginning of epoch till batch#137\n",
      "training loss has decreased---> reducing the best loss from 0.01 to 0.01 | throughput = 144456 tokens/second | norm = 0.0002 | learning rate = 1.52293e-08\n",
      "inside validation data for epoch 106\n",
      "No improvement in validation loss-->epoch= 105 and best val loss is 6.83|current_Val loss = 6.890198290348053|counter = 14\n",
      "executing epoch:107, it took 1.0711094697316488 mins from beginning of epoch till batch#137\n",
      "No improvement in training  loss-->epoch= 106 and best loss is 0.01|current_loss = 0.012054979801177979|counter = 1\n",
      "executing epoch:108, it took 1.050892432530721 mins from beginning of epoch till batch#137\n"
     ]
    }
   ],
   "source": [
    "tr_model,epoch_train_log,epoch_val_log = train_model(train_loader, val_loader,model=model,tokenizer = tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec2bf26",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(epoch_train_log)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2037552",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json , os\n",
    "# path_var_train_log = os.path.join(\".\",\"Tokenizers_and_loss_vals\",\"epoch_train_plain_loss.json\")\n",
    "# path_var_val_log = os.path.join(\".\",\"Tokenizers_and_loss_vals\",\"epoch_val_val_loss_.json\")\n",
    "\n",
    "# #print(path_var)\n",
    "# #Write the list to a JSON file\n",
    "# with open(path_var_train_log, \"w\") as file:\n",
    "#     json.dump(epoch_train_log, file)\n",
    "\n",
    "# with open(path_var_val_log, \"w\") as file:\n",
    "#     json.dump(epoch_val_log, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183b0f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(path_var_train_log, \"r\") as file:\n",
    "#     train_loss = json.load(file)\n",
    "# with open(path_var_val_log, \"r\") as file:\n",
    "#     val_loss = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a99b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_values = range(len(epoch_train_log))\n",
    "plt.plot(x_values, epoch_train_log, label='Train_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8f7a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#epoch_val_log= [t.detach().cpu().numpy() for t in epoch_val_log]\n",
    "x_values_val = range(len(epoch_val_log))\n",
    "plt.plot(x_values_val, epoch_val_log, label='val_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068c8d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_length = min(len(epoch_train_log), len(epoch_val_log))\n",
    "list1 = epoch_train_log[:min_length]\n",
    "list2 = epoch_val_log[:min_length]\n",
    "\n",
    "# Create x-axis values\n",
    "x_values = range(min_length)\n",
    "\n",
    "# Plot the lists\n",
    "plt.plot(x_values, list1, label='Train_loss')\n",
    "plt.plot(x_values, list2, label='Val_loss')\n",
    "\n",
    "# Adding labels and title\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Train Loss and Val_Loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902d20a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218273ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35cbd6ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019218d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af5e0fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8fd928",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
