{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb4fb0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## reference https://huggingface.co/learn/nlp-course/en/chapter7/6?fw=pt#training-a-causal-language-model-from-scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c840cb03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b8da841",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: jupyter in /opt/conda/lib/python3.8/site-packages (1.0.0)\n",
      "Requirement already satisfied: ipywidgets in /opt/conda/lib/python3.8/site-packages (8.1.3)\n",
      "Requirement already satisfied: notebook in /opt/conda/lib/python3.8/site-packages (from jupyter) (6.4.1)\n",
      "Requirement already satisfied: jupyter-console in /opt/conda/lib/python3.8/site-packages (from jupyter) (6.6.3)\n",
      "Requirement already satisfied: ipykernel in /opt/conda/lib/python3.8/site-packages (from jupyter) (6.29.5)\n",
      "Requirement already satisfied: nbconvert in /opt/conda/lib/python3.8/site-packages (from jupyter) (6.3.0)\n",
      "Requirement already satisfied: qtconsole in /opt/conda/lib/python3.8/site-packages (from jupyter) (5.5.2)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.11 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (3.0.11)\n",
      "Requirement already satisfied: comm>=0.1.3 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.11 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (4.0.11)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (7.30.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (5.14.3)\n",
      "Requirement already satisfied: jedi>=0.16 in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.18.1)\n",
      "Requirement already satisfied: backcall in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: matplotlib-inline in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.3)\n",
      "Requirement already satisfied: pygments in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (2.10.0)\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (4.8.0)\n",
      "Requirement already satisfied: decorator in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (5.1.0)\n",
      "Requirement already satisfied: pickleshare in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.7.5)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.47)\n",
      "Requirement already satisfied: setuptools>=18.5 in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (59.4.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /opt/conda/lib/python3.8/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.8/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.8/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=6.1.0->ipywidgets) (0.2.5)\n",
      "Requirement already satisfied: pyzmq>=24 in /opt/conda/lib/python3.8/site-packages (from ipykernel->jupyter) (26.0.3)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.8/site-packages (from ipykernel->jupyter) (21.3)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in /opt/conda/lib/python3.8/site-packages (from ipykernel->jupyter) (7.1.0)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /opt/conda/lib/python3.8/site-packages (from ipykernel->jupyter) (5.7.2)\n",
      "Requirement already satisfied: tornado>=6.1 in /opt/conda/lib/python3.8/site-packages (from ipykernel->jupyter) (6.1)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in /opt/conda/lib/python3.8/site-packages (from ipykernel->jupyter) (1.8.2)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.8/site-packages (from ipykernel->jupyter) (5.8.0)\n",
      "Requirement already satisfied: nest-asyncio in /opt/conda/lib/python3.8/site-packages (from ipykernel->jupyter) (1.5.4)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /opt/conda/lib/python3.8/site-packages (from jupyter-client>=6.1.12->ipykernel->jupyter) (2.8.2)\n",
      "Requirement already satisfied: entrypoints in /opt/conda/lib/python3.8/site-packages (from jupyter-client>=6.1.12->ipykernel->jupyter) (0.3)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /opt/conda/lib/python3.8/site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel->jupyter) (4.2.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.8/site-packages (from python-dateutil>=2.1->jupyter-client>=6.1.12->ipykernel->jupyter) (1.16.0)\n",
      "Requirement already satisfied: testpath in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (0.5.0)\n",
      "Requirement already satisfied: mistune<2,>=0.8.1 in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (0.8.4)\n",
      "Requirement already satisfied: jupyterlab-pygments in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (0.1.2)\n",
      "Requirement already satisfied: jinja2>=2.4 in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (3.0.3)\n",
      "Requirement already satisfied: nbclient<0.6.0,>=0.5.0 in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (0.5.9)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (1.5.0)\n",
      "Requirement already satisfied: nbformat>=4.4 in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (5.1.3)\n",
      "Requirement already satisfied: defusedxml in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (0.7.1)\n",
      "Requirement already satisfied: bleach in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (4.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.8/site-packages (from jinja2>=2.4->nbconvert->jupyter) (2.0.1)\n",
      "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /opt/conda/lib/python3.8/site-packages (from nbformat>=4.4->nbconvert->jupyter) (4.2.1)\n",
      "Requirement already satisfied: ipython-genutils in /opt/conda/lib/python3.8/site-packages (from nbformat>=4.4->nbconvert->jupyter) (0.2.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /opt/conda/lib/python3.8/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.4->nbconvert->jupyter) (21.2.0)\n",
      "Requirement already satisfied: importlib-resources>=1.4.0 in /opt/conda/lib/python3.8/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.4->nbconvert->jupyter) (5.4.0)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /opt/conda/lib/python3.8/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.4->nbconvert->jupyter) (0.18.0)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /opt/conda/lib/python3.8/site-packages (from importlib-resources>=1.4.0->jsonschema!=2.5.0,>=2.4->nbformat>=4.4->nbconvert->jupyter) (3.6.0)\n",
      "Requirement already satisfied: webencodings in /opt/conda/lib/python3.8/site-packages (from bleach->nbconvert->jupyter) (0.5.1)\n",
      "Requirement already satisfied: Send2Trash>=1.5.0 in /opt/conda/lib/python3.8/site-packages (from notebook->jupyter) (1.8.0)\n",
      "Requirement already satisfied: argon2-cffi in /opt/conda/lib/python3.8/site-packages (from notebook->jupyter) (21.1.0)\n",
      "Requirement already satisfied: terminado>=0.8.3 in /opt/conda/lib/python3.8/site-packages (from notebook->jupyter) (0.12.1)\n",
      "Requirement already satisfied: prometheus-client in /opt/conda/lib/python3.8/site-packages (from notebook->jupyter) (0.12.0)\n",
      "Requirement already satisfied: cffi>=1.0.0 in /opt/conda/lib/python3.8/site-packages (from argon2-cffi->notebook->jupyter) (1.15.0)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.8/site-packages (from cffi>=1.0.0->argon2-cffi->notebook->jupyter) (2.21)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging->ipykernel->jupyter) (3.0.6)\n",
      "Requirement already satisfied: qtpy>=2.4.0 in /opt/conda/lib/python3.8/site-packages (from qtconsole->jupyter) (2.4.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#installing some libraries\n",
    "#!pip install datasets\n",
    "!pip install --upgrade jupyter ipywidgets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06af86b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch, transformers\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import PreTrainedModel, PretrainedConfig\n",
    "from transformers import AutoModel, AutoConfig,AutoModelForCausalLM, AutoConfig,GPT2Config,GPT2Tokenizer\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "#from datasets import load_dataset\n",
    "import pandas as pd, numpy as np\n",
    "from torch.cuda.amp import autocast\n",
    "from torch import cuda\n",
    "import datetime\n",
    "import warnings,itertools\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "import json\n",
    "# Ignore all warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "#pip install transformers bitsandbytes>=0.39.0 -q\n",
    "import zipfile,logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c5d0f26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "import random\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f02a7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "414f78aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "model\n"
     ]
    }
   ],
   "source": [
    "#global params for training\n",
    "\n",
    "B,T = 32,1024\n",
    "epoch = 100\n",
    "random_init_wts = True\n",
    "min_text_len = 0\n",
    "# hard coded com\n",
    "comp_ratio = 3\n",
    "# train_loss_list = []\n",
    "# val_loss_list =[]\n",
    "if cuda.is_available():\n",
    "    device = torch.device('cuda:0')\n",
    "    print(device)\n",
    "else:\n",
    "    device = 'cpu'\n",
    "#print(device)\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "#os.environ[\"MKL_DEBUG_CPU_TYPE\"] = \"5\"\n",
    "\n",
    "#print(global_tr_loss)\n",
    "model_path = os.path.join(\"model\")\n",
    "print(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ef15a34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./data/unzip_text_10M'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "directory = os.path.join('.','data','unzip_text_10M')  # Replace with your directory path\n",
    "directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc48e5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_text(directory):\n",
    "    directory = os.path.join('.','data','unzip_text_10M',str(directory))  # Replace with your directory path\n",
    "    print(f\"directory :{directory}\")\n",
    "    # List all files in the directory\n",
    "    files = [f for f in os.listdir(directory) if os.path.isfile(os.path.join(directory, f))]\n",
    "    print(f\"files:{files}\")\n",
    "    text_content = []\n",
    "    # Read each file\n",
    "    total_lines = 0\n",
    "    for filenum,filename in enumerate(files):\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            #first_line = file.read()\n",
    "            #print(f\"filename :{filename}->first few lines {first_line}\")\n",
    "            #continue\n",
    "            #lines_list = [line.strip() for line in open(file_path, 'r')]\n",
    "            text = file.read()\n",
    "            text_content.append(text)\n",
    "            print(f\"the file:{filename} has been appeneded to the uber list and its length is {len(text_content)} \")\n",
    "            #total_lines+=len(lines_list)\n",
    "            #text_content.append(lines_list)\n",
    "    \n",
    "    flattened_list = ''.join(text_content)\n",
    "    assert (len(flattened_list) == total_lines , f\"Expected {len(flattened_list)} to be equal to {total_lines}\" )\n",
    "    \n",
    "    return flattened_list\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cdcca965",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "directory :./data/unzip_text_10M/train_10M\n",
      "files:['switchboard.train', 'simple_wiki.train', 'open_subtitles.train', 'gutenberg.train', 'childes.train', 'bnc_spoken.train']\n",
      "the file:switchboard.train has been appeneded to the uber list and its length is 1 \n",
      "the file:simple_wiki.train has been appeneded to the uber list and its length is 2 \n",
      "the file:open_subtitles.train has been appeneded to the uber list and its length is 3 \n",
      "the file:gutenberg.train has been appeneded to the uber list and its length is 4 \n",
      "the file:childes.train has been appeneded to the uber list and its length is 5 \n",
      "the file:bnc_spoken.train has been appeneded to the uber list and its length is 6 \n"
     ]
    }
   ],
   "source": [
    "train_list = read_text(\"train_10M\")\n",
    "#print(train_dict)\n",
    "#val_list = read_text(\"dev\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eba45cd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54215049"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c315ec95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1654\n"
     ]
    }
   ],
   "source": [
    "chunks = len(train_list)//(B*T)\n",
    "print(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3a8a9eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(\"distilgpt2\")\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3fe63ad6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50257\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.pad_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4c4982af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "directory :./data/unzip_text_10M/dev\n",
      "files:['switchboard.dev', 'simple_wiki.dev', 'open_subtitles.dev', 'gutenberg.dev', 'childes.dev', 'bnc_spoken.dev']\n",
      "the file:switchboard.dev has been appeneded to the uber list and its length is 1 \n",
      "the file:simple_wiki.dev has been appeneded to the uber list and its length is 2 \n",
      "the file:open_subtitles.dev has been appeneded to the uber list and its length is 3 \n",
      "the file:gutenberg.dev has been appeneded to the uber list and its length is 4 \n",
      "the file:childes.dev has been appeneded to the uber list and its length is 5 \n",
      "the file:bnc_spoken.dev has been appeneded to the uber list and its length is 6 \n"
     ]
    }
   ],
   "source": [
    "val_list = read_text(\"dev\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "46eec41c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50256\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "01f0a79f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.014990842220574785"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.random()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "36ebb5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "db8533ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ranked_synonyms(word, pos):\n",
    "    synonyms = []\n",
    "    for syn in wordnet.synsets(word, pos=pos):\n",
    "        for lemma in syn.lemmas():\n",
    "            if lemma.name() != word:\n",
    "                synonyms.append((lemma.name(), syn.wup_similarity(syn)))\n",
    "    \n",
    "    # Sort synonyms by similarity score in descending order\n",
    "    ranked_synonyms = sorted(set(synonyms), key=lambda x: x[1] if x[1] is not None else 0, reverse=True)\n",
    "    return [syn for syn, _ in ranked_synonyms]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "184e3af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def replace_verbs_with_synonyms(text):\n",
    "    words = word_tokenize(text)\n",
    "    tagged = pos_tag(words)\n",
    "    result = []\n",
    "    for word, pos in tagged:\n",
    "        if pos =='VB':\n",
    "            wordnet_pos = get_wordnet_pos(pos)\n",
    "            lemma = lemmatizer.lemmatize(word, wordnet_pos)\n",
    "            synonyms = get_ranked_synonyms(lemma, wordnet_pos)\n",
    "            if synonyms:\n",
    "                replacement = random.choice(synonyms)  # Choose from top 3 synonyms\n",
    "                #print(f\"word = {word}|replacement = {replacement}\")\n",
    "                result.append(replacement)\n",
    "            else:\n",
    "                result.append(word)\n",
    "        else:\n",
    "            result.append(word)\n",
    "    return \" \".join(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "97854c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = train_list[584:1000]\n",
    "repl = replace_verbs_with_synonyms(sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "58476d34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "like Thumper that , boy , I could just go_away out and let all kinds of game B : Did it lick ? A : Yeah , A : except we live in Plano , Texas now B : No , B : right . A : so B : I , um , I had a , for many years I had a dog that was part Springer Spaniel . B : I just bed them . B : Her name was Molly , B : but she is n't alive any more B : We had her for , um , fifteen years , I think , my family did , and just loved her . B : She was the greatest ,\n"
     ]
    }
   ],
   "source": [
    "print(repl)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "143cd4d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "like Thumper that, boy, I could just go out and get all kinds of game\n",
      "B:\tDid it work?\n",
      "A:\tYeah,\n",
      "A:\texcept we live in Plano, Texas now\n",
      "B:\tNo,\n",
      "B:\tright.\n",
      "A:\tso\n",
      "B:\tI, um, I had a, for many years I had a dog that was part Springer Spaniel.\n",
      "B:\tI just love them.\n",
      "B:\tHer name was Molly,\n",
      "B:\tbut she isn't alive any more\n",
      "B:\tWe had her for, um, fifteen years, I think, my family did, and just loved her.\n",
      "B:\tShe was the greatest,\n"
     ]
    }
   ],
   "source": [
    "print(sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2548d167",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b577b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8334f0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequences(token_ids_list, max_length = B*T, tokenizer = tokenizer):\n",
    "    padded_sequences = tokenizer.pad(\n",
    "        {\"input_ids\": token_ids_list},\n",
    "        padding='max_length',\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    return padded_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "067e7676",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text (text,tokenizer = tokenizer,max_length = B*T):\n",
    "    #print(f\"inside tokenize_text\")\n",
    "    enc = tokenizer(text,padding='max_length',truncation=True,max_length=max_length,return_tensors=\"pt\",return_attention_mask=True)\n",
    "    input_id = enc['input_ids']\n",
    "    att_mask = enc['attention_mask']\n",
    "    \n",
    "    # now concatenate these lists to B*T\n",
    "    input_id = torch.squeeze(input_id, dim = 0).to(dtype = torch.long)\n",
    "    att_mask = torch.squeeze(att_mask, dim = 0).to(dtype = torch.bool)\n",
    "    return input_id,att_mask\n",
    "    \n",
    "    \n",
    "    \n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "648517b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test the tokenizer:\n",
    "model_name = 'distilgpt2'\n",
    "if random_init_wts:\n",
    "    config = AutoConfig.from_pretrained(model_name, vocab_size = 50304)\n",
    "    # Initialize the model with random weights\n",
    "    model = AutoModelForCausalLM.from_config(config)\n",
    "else:\n",
    "    #model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "    \n",
    "model = torch.compile(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b65c725",
   "metadata": {},
   "source": [
    "### Data loaders and Dataset for batched training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "62e9ac29",
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataset_pyt_train(Dataset):\n",
    "    def __init__(self, text_list, B = B, T = T, tokenizer = tokenizer, prob = .3,synonym_flag = False):\n",
    "        self.text_list = text_list\n",
    "        #print(f\"Value of B {B}\")\n",
    "        self.prob = prob\n",
    "        self.synonym_flag = synonym_flag\n",
    "        \n",
    "                                        \n",
    "    def __getitem__(self, idx):\n",
    "        #words_list[start_index:start_index + n]\n",
    "        chunk = self.text_list[idx:idx + B*T*comp_ratio]\n",
    "        #print(f\"length of chunk = {len(chunk)}\")\n",
    "        random_num = random.random()\n",
    "        #print(f\"The random number generated = {random_num}\")\n",
    "        if random_num > self.prob:\n",
    "            chunk = replace_verbs_with_synonyms(chunk)\n",
    "            self.synonym_flag = True\n",
    "        inp,att = tokenize_text(chunk, tokenizer)\n",
    "        input_id = inp.view(B,T)\n",
    "        attention_mask = att.view(B,T)\n",
    "        #print(f\"shape of input_id = {input_id.shape}| shape of attention = {attention_mask.shape}\")\n",
    "        \n",
    "        return input_id,attention_mask,self.synonym_flag\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        #return the length of the dataframe\n",
    "        num_chunks = comp_ratio*B*T\n",
    "        return len(self.text_list)//num_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b01dea5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataset_pyt_val(Dataset):\n",
    "    def __init__(self, text_list, B = B, T = T, tokenizer = tokenizer, comp_ratio = comp_ratio):\n",
    "        self.text_list = text_list\n",
    "        #print(f\"Value of B {B}\")\n",
    "                                                \n",
    "    def __getitem__(self, idx):\n",
    "        #words_list[start_index:start_index + n]\n",
    "        chunk = self.text_list[idx:idx + B*T*comp_ratio]\n",
    "        #print(f\"length of chunk = {len()}\")\n",
    "        inp,att = tokenize_text(chunk, tokenizer)\n",
    "        input_id = inp.view(B,T)\n",
    "        attention_mask = att.view(B,T)\n",
    "        #print(f\"shape of input_id = {input_id.shape}| shape of attention = {attention_mask.shape}\")\n",
    "        \n",
    "        return input_id,attention_mask\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        #return the length of the dataframe\n",
    "        num_chunks = comp_ratio*B*T\n",
    "        return len(self.text_list)//num_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cbd75bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_dataset = dataset_pyt(filtered_df,tokenizer = tokenizer)\n",
    "train_dataset = dataset_pyt_train(train_list)\n",
    "val_dataset = dataset_pyt_val(val_list)\n",
    "\n",
    "train_loader = DataLoader(train_dataset,batch_size = 1, shuffle = True , num_workers = 4, pin_memory = True)\n",
    "val_loader = DataLoader(val_dataset,batch_size = 1, shuffle = True , num_workers = 4, pin_memory = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fc051c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x,y,z = train_dataset[0]\n",
    "# print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d7c24f7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer vocabulary size: 50258\n",
      "EOS token ID: 50256\n",
      "PAD token ID: 50257\n"
     ]
    }
   ],
   "source": [
    "print(f\"Tokenizer vocabulary size: {len(tokenizer)}\")\n",
    "print(f\"EOS token ID: {tokenizer.eos_token_id}\")\n",
    "print(f\"PAD token ID: {tokenizer.pad_token_id}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0556eb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_file(log_message, model_name = \"GPT2\" ,random_init_wts = random_init_wts ):\n",
    "    current_datetime = datetime.datetime.now()\n",
    "    # Extract date and time components\n",
    "    current_date = str(current_datetime.date())\n",
    "    log_file = model_name +'_COS_SIM_'+'random_init_wts'+ '_'+str(random_init_wts)+'_' +current_date+'.log'\n",
    "    print(f\"*****LOGGING INFO IN {log_file}*********\")\n",
    "    filepath = os.path.join(\"model\",log_file)\n",
    "    logging.basicConfig(filename=filepath, \n",
    "                    filemode='a',  # Overwrite the log file each time\n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s', \n",
    "                    level=logging.DEBUG)\n",
    "    logger = logging.getLogger()\n",
    "    logger.info(log_message)\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e680347a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the train loader is 551\n",
      "Length of the val loader is 575\n",
      "num_tokens= 18055168\n"
     ]
    }
   ],
   "source": [
    "print(f\"Length of the train loader is {len(train_loader)}\")\n",
    "print(f\"Length of the val loader is {len(val_loader)}\")\n",
    "print(f\"num_tokens= {B*T*len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "99531f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#emb,att,inp = train_dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8956107e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class check_train_metrics:\n",
    "    def __init__(self, patience=20, min_delta=0 , B = T, T = T,best_loss = torch.inf,early_stop = False):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = best_loss\n",
    "        self.early_stop = early_stop\n",
    "        self.B = B\n",
    "        self.T = T\n",
    "        self.improvement = None\n",
    "\n",
    "    def __call__(self, loss, epoch , epoch_durn, norm , current_lr, num_token):\n",
    "        if self.best_loss - loss > self.min_delta:\n",
    "            \n",
    "            print(f\"training loss has decreased---> reducing the best loss from {self.best_loss:.2f} to {loss:.2f} | throughput = {int(num_token/epoch_durn)} tokens/second | norm = {norm:.4f} | learning rate = {current_lr:.5e}\")\n",
    "            self.best_loss = loss\n",
    "            self.counter = 0\n",
    "            self.improvement = True\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            self.improvement = False\n",
    "            print(f\"No improvement in training  loss-->epoch= {epoch} and best loss is {self.best_loss:.2f}|current_loss = {loss}|counter = {self.counter}\")\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b970fbcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class check_val_metrics:\n",
    "    def __init__(self, patience=25, min_delta=0, best_loss = torch.inf,early_stop = False):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = best_loss\n",
    "        self.early_stop = early_stop\n",
    "        \n",
    "\n",
    "    def __call__(self, loss, epoch , model, tokenizer):\n",
    "        if self.best_loss - loss > self.min_delta:\n",
    "            print(f\"Val loss has decreased -->reducing the global validation loss from {self.best_loss:.2f} to {loss:.2f}\")\n",
    "            s1 = (f\"Val loss has decreased -->reducing the global validation loss from {self.best_loss:.2f} to {loss:.2f}\")\n",
    "            print(f\" validation loss for epoch = {epoch} is {loss:.4f}\")\n",
    "            self.best_loss = loss\n",
    "            s2 = f\" validation loss for epoch = {epoch} is {loss:.4f}\"\n",
    "            print(f\" epoch= {epoch} :  val loss is {loss:.4f} \")\n",
    "            s3 = f\" epoch= {epoch} :  val loss is {loss:.4f} \"\n",
    "            #save the model\n",
    "            # Get the current date and time\n",
    "            current_datetime = datetime.datetime.now()\n",
    "            # Extract date and time components\n",
    "            current_date = str(current_datetime.date())\n",
    "            current_time = str(current_datetime.time()).split('.')[0]\n",
    "            file_name = 'model'+ current_date+current_time+'.pth'\n",
    "            path = os.path.join(\"model\",file_name)\n",
    "            print(f\"saving the model {file_name}\")\n",
    "            s4 = f\"saving the model {file_name}\"\n",
    "            #torch.save(model.state_dict(), path)\n",
    "            model.save_pretrained(path)\n",
    "            tokenizer.save_pretrained(path)\n",
    "            log_message = s1+s2+s3+s4\n",
    "            write_file(log_message)\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            print(f\"No improvement in validation loss-->epoch= {epoch} and best val loss is {self.best_loss:.2f}|current_Val loss = {loss}|counter = {self.counter}\")\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "76cb57e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_output = model(input_ids = inp ,attention_mask = att, labels = inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f4bf5434",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad\n",
    "\n",
    "def eval_model(val_loader, model, epoch , device = device,tokenizer = tokenizer):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    e = epoch+1\n",
    "    val_loss_accum = 0.0\n",
    "    embedding_layer = model.transformer.wte\n",
    "    print(f\"inside validation data for epoch {e}\")\n",
    "    for ind,(input_id,attention_mask) in enumerate(val_loader):\n",
    "        ids = input_id.to(device=device, non_blocking=True)\n",
    "        ids = torch.squeeze(ids, dim = 0)\n",
    "        att_mask = attention_mask.to(device=device, non_blocking=True)\n",
    "        att_mask =  torch.squeeze(att_mask, dim = 0)\n",
    "        labels = ids.clone().to(device)\n",
    "        with autocast(dtype = torch.bfloat16):\n",
    "            model_output = model(input_ids = ids ,attention_mask = att_mask, labels = labels)\n",
    "            total_loss = model_output.loss\n",
    "            \n",
    "    \n",
    "    \n",
    "        val_loss_accum+= total_loss.detach().item()\n",
    "        del att_mask,labels,model_output,total_loss,ids\n",
    "    return val_loss_accum        \n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3de8b732",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_model(train_loader,val_loader,model,num_epoch = 100,device = device,tokenizer = tokenizer):\n",
    "    #model.train()\n",
    "    device = device\n",
    "    lr_custom = 1e-5\n",
    "    print(f\"inside train model. Device = {device}\")\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.AdamW(params =  model.parameters(), lr= lr_custom,fused = True ,weight_decay = .1)\n",
    "      \n",
    "    extra_train = .1*num_epoch\n",
    "    max_train_steps = int(num_epoch +extra_train )\n",
    "    import time\n",
    "    from transformers import get_linear_schedule_with_warmup\n",
    "    total_steps = len(train_loader) * num_epoch\n",
    "    scheduler_cos = transformers.get_cosine_schedule_with_warmup( optimizer= optimizer, num_warmup_steps =int(total_steps * 0.1) ,num_training_steps= total_steps )\n",
    "        \n",
    "    epoch_train_log = []\n",
    "    epoch_val_log = []\n",
    "    validate_val_metric = check_val_metrics()\n",
    "    validate_train_metric = check_train_metrics()\n",
    "    embedding_layer = model.transformer.wte\n",
    "    for i in range (max_train_steps):\n",
    "        \n",
    "        epoch_start_time = time.time()\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        # we use 2 schedulers - the first LR scheduler uses a cosine decay for 100 epochs the second scheduler takes the last LR from cosine scheduler and then maintains that LR for the next 10 epochs\n",
    "        if i >= num_epoch:\n",
    "            optimizer_reduced_lr = torch.optim.AdamW(params =  model.parameters(), lr= current_lr ,fused = True , weight_decay=.1)\n",
    "            scheduler_constant = transformers.get_constant_schedule_with_warmup( optimizer = optimizer_reduced_lr ,num_warmup_steps = 0, last_epoch = -1 )\n",
    "        \n",
    "        epoch_train_loss = 0       \n",
    "        for ind,(input_id,attention_mask,synonym_flag) in enumerate(train_loader):\n",
    "            if ind == int(len(train_loader)/2):\n",
    "                batch_time = time.time()\n",
    "                duration = batch_time - epoch_start_time\n",
    "                print(f\"executing epoch:{i+1}, it took {duration/60} mins from beginning of epoch till batch#{ind}\")\n",
    "            \n",
    "            ids = input_id.to(device=device, non_blocking=True)\n",
    "            ids = torch.squeeze(ids, dim = 0)\n",
    "            att_mask = attention_mask.to(device=device, non_blocking=True)\n",
    "            att_mask =  torch.squeeze(att_mask, dim = 0)\n",
    "            labels = ids.clone().to(device)\n",
    "            synonym_flag = synonym_flag\n",
    "            if synonym_flag:\n",
    "                with autocast(dtype = torch.bfloat16):\n",
    "                    model_output = model(input_ids = ids ,attention_mask = att_mask, labels = labels)\n",
    "                    total_loss = model_output.loss\n",
    "            else:\n",
    "                with autocast(dtype = torch.bfloat16):\n",
    "                    input_emb = embedding_layer(ids)\n",
    "                    model_output = model(input_ids = ids ,attention_mask = att_mask, labels = labels)\n",
    "                    logits = model_output.logits\n",
    "                    predictions = torch.argmax(logits, dim=-1)\n",
    "                    #print(f\"predictions = {predictions}\")\n",
    "                    prediction_embeddings = embedding_layer(predictions)\n",
    "                    cos_sim = F.cosine_similarity(torch.squeeze(prediction_embeddings,dim = 0), torch.squeeze(input_emb,dim = 0), dim=1)\n",
    "                    cos_loss = 1- cos_sim.mean()\n",
    "                    total_loss = cos_loss\n",
    "                \n",
    "                del input_emb,logits,predictions,prediction_embeddings,cos_sim,cos_loss\n",
    "            total_loss.backward()\n",
    "            epoch_train_loss += total_loss.detach().item()\n",
    "            norm = torch.nn.utils.clip_grad_norm(model.parameters() , 1.0)\n",
    "            if i <= num_epoch:\n",
    "                optimizer.step()\n",
    "                scheduler_cos.step()\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "            else:\n",
    "                optimizer_reduced_lr.step()\n",
    "                optimizer_reduced_lr.zero_grad(set_to_none=True)\n",
    "                scheduler_constant.step()\n",
    "                \n",
    "                         \n",
    "            del att_mask,labels,model_output,ids\n",
    "            \n",
    "        #batch processing complete \n",
    "        #print(f\"batch processing complete , lambda = {lambda_val} |total_loss for batch= {total_loss}\")\n",
    "        \n",
    "        if i <= num_epoch:\n",
    "            current_lr = scheduler_cos.get_last_lr()[0]\n",
    "        epoch_end_time = time.time()\n",
    "        epoch_durn = (epoch_end_time - epoch_start_time)\n",
    "        num_token = B*T*len(train_loader)\n",
    "        epoch_train_log.append(epoch_train_loss)\n",
    "        validate_train_metric(epoch_train_loss, i , epoch_durn, norm , current_lr, num_token)\n",
    "        \n",
    "        if validate_train_metric.improvement:\n",
    "            val_loss= eval_model(val_loader, model, epoch = i, device = device,tokenizer = tokenizer)\n",
    "            epoch_val_log.append(val_loss)\n",
    "            validate_val_metric(val_loss, i , model, tokenizer)\n",
    "            if validate_train_metric.early_stop or validate_val_metric.early_stop :\n",
    "                print(f\"early stopping trigerred either from training data or val data | train_counter = {validate_train_metric.counter}|val_counter = {validate_val_metric.counter}\")\n",
    "                break\n",
    "        else:\n",
    "            if validate_val_metric.early_stop:\n",
    "                print(f\"early stopping trigerred from validation data\")\n",
    "                break\n",
    "              \n",
    "    \n",
    "    return model,epoch_train_log,epoch_val_log\n",
    "        \n",
    "            \n",
    "            \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b50ef9ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inside train model. Device = cuda:0\n",
      "executing epoch:1, it took 1.5630602757136027 mins from beginning of epoch till batch#275\n",
      "training loss has decreased---> reducing the best loss from inf to 4988.39 | throughput = 107229 tokens/second | norm = 2.8026 | learning rate = 1.00000e-06\n",
      "inside validation data for epoch 1\n",
      "Val loss has decreased -->reducing the global validation loss from inf to 4263.80\n",
      " validation loss for epoch = 0 is 4263.8039\n",
      " epoch= 0 :  val loss is 4263.8039 \n",
      "saving the model model2024-07-2700:44:46.pth\n",
      "*****LOGGING INFO IN GPT2_COS_SIM_random_init_wts_True_2024-07-27.log*********\n",
      "executing epoch:2, it took 1.4534081101417542 mins from beginning of epoch till batch#275\n",
      "training loss has decreased---> reducing the best loss from 4988.39 to 3171.53 | throughput = 112330 tokens/second | norm = 1.7673 | learning rate = 2.00000e-06\n",
      "inside validation data for epoch 2\n",
      "Val loss has decreased -->reducing the global validation loss from 4263.80 to 3001.34\n",
      " validation loss for epoch = 1 is 3001.3440\n",
      " epoch= 1 :  val loss is 3001.3440 \n",
      "saving the model model2024-07-2700:48:05.pth\n",
      "*****LOGGING INFO IN GPT2_COS_SIM_random_init_wts_True_2024-07-27.log*********\n",
      "executing epoch:3, it took 1.230878762404124 mins from beginning of epoch till batch#275\n",
      "training loss has decreased---> reducing the best loss from 3171.53 to 2555.56 | throughput = 124086 tokens/second | norm = 2.0125 | learning rate = 3.00000e-06\n",
      "inside validation data for epoch 3\n",
      "Val loss has decreased -->reducing the global validation loss from 3001.34 to 2641.42\n",
      " validation loss for epoch = 2 is 2641.4165\n",
      " epoch= 2 :  val loss is 2641.4165 \n",
      "saving the model model2024-07-2700:51:08.pth\n",
      "*****LOGGING INFO IN GPT2_COS_SIM_random_init_wts_True_2024-07-27.log*********\n",
      "executing epoch:4, it took 1.266937251885732 mins from beginning of epoch till batch#275\n",
      "training loss has decreased---> reducing the best loss from 2555.56 to 2157.85 | throughput = 121622 tokens/second | norm = 2.0191 | learning rate = 4.00000e-06\n",
      "inside validation data for epoch 4\n",
      "Val loss has decreased -->reducing the global validation loss from 2641.42 to 2398.33\n",
      " validation loss for epoch = 3 is 2398.3287\n",
      " epoch= 3 :  val loss is 2398.3287 \n",
      "saving the model model2024-07-2700:54:15.pth\n",
      "*****LOGGING INFO IN GPT2_COS_SIM_random_init_wts_True_2024-07-27.log*********\n",
      "executing epoch:5, it took 1.2233887910842896 mins from beginning of epoch till batch#275\n",
      "training loss has decreased---> reducing the best loss from 2157.85 to 1860.01 | throughput = 121825 tokens/second | norm = 4.8923 | learning rate = 5.00000e-06\n",
      "inside validation data for epoch 5\n",
      "Val loss has decreased -->reducing the global validation loss from 2398.33 to 2282.05\n",
      " validation loss for epoch = 4 is 2282.0477\n",
      " epoch= 4 :  val loss is 2282.0477 \n",
      "saving the model model2024-07-2700:57:21.pth\n",
      "*****LOGGING INFO IN GPT2_COS_SIM_random_init_wts_True_2024-07-27.log*********\n",
      "executing epoch:6, it took 1.218722931543986 mins from beginning of epoch till batch#275\n",
      "training loss has decreased---> reducing the best loss from 1860.01 to 1619.28 | throughput = 120237 tokens/second | norm = 5.6418 | learning rate = 6.00000e-06\n",
      "inside validation data for epoch 6\n",
      "Val loss has decreased -->reducing the global validation loss from 2282.05 to 2253.39\n",
      " validation loss for epoch = 5 is 2253.3902\n",
      " epoch= 5 :  val loss is 2253.3902 \n",
      "saving the model model2024-07-2701:00:29.pth\n",
      "*****LOGGING INFO IN GPT2_COS_SIM_random_init_wts_True_2024-07-27.log*********\n",
      "executing epoch:7, it took 1.2656551758448282 mins from beginning of epoch till batch#275\n",
      "training loss has decreased---> reducing the best loss from 1619.28 to 1409.50 | throughput = 122395 tokens/second | norm = 10.4200 | learning rate = 7.00000e-06\n",
      "inside validation data for epoch 7\n",
      "Val loss has decreased -->reducing the global validation loss from 2253.39 to 2218.00\n",
      " validation loss for epoch = 6 is 2217.9988\n",
      " epoch= 6 :  val loss is 2217.9988 \n",
      "saving the model model2024-07-2701:03:35.pth\n",
      "*****LOGGING INFO IN GPT2_COS_SIM_random_init_wts_True_2024-07-27.log*********\n",
      "executing epoch:8, it took 1.2483144561449686 mins from beginning of epoch till batch#275\n",
      "training loss has decreased---> reducing the best loss from 1409.50 to 1198.62 | throughput = 119794 tokens/second | norm = 13.5016 | learning rate = 8.00000e-06\n",
      "inside validation data for epoch 8\n",
      "No improvement in validation loss-->epoch= 7 and best val loss is 2218.00|current_Val loss = 2241.6615273952484|counter = 1\n",
      "executing epoch:9, it took 1.2105746428171793 mins from beginning of epoch till batch#275\n",
      "training loss has decreased---> reducing the best loss from 1198.62 to 1002.95 | throughput = 124272 tokens/second | norm = 15.8483 | learning rate = 9.00000e-06\n",
      "inside validation data for epoch 9\n",
      "No improvement in validation loss-->epoch= 8 and best val loss is 2218.00|current_Val loss = 2307.164382457733|counter = 2\n",
      "executing epoch:10, it took 1.2812317252159118 mins from beginning of epoch till batch#275\n",
      "training loss has decreased---> reducing the best loss from 1002.95 to 818.95 | throughput = 119536 tokens/second | norm = 17.9412 | learning rate = 1.00000e-05\n",
      "inside validation data for epoch 10\n",
      "No improvement in validation loss-->epoch= 9 and best val loss is 2218.00|current_Val loss = 2362.7195267677307|counter = 3\n",
      "executing epoch:11, it took 1.2911643902460734 mins from beginning of epoch till batch#275\n",
      "training loss has decreased---> reducing the best loss from 818.95 to 633.02 | throughput = 117537 tokens/second | norm = 38.3524 | learning rate = 9.99695e-06\n",
      "inside validation data for epoch 11\n",
      "No improvement in validation loss-->epoch= 10 and best val loss is 2218.00|current_Val loss = 2451.0088453292847|counter = 4\n",
      "executing epoch:12, it took 1.2438915014266967 mins from beginning of epoch till batch#275\n",
      "training loss has decreased---> reducing the best loss from 633.02 to 452.28 | throughput = 121200 tokens/second | norm = 13.2226 | learning rate = 9.98782e-06\n",
      "inside validation data for epoch 12\n",
      "No improvement in validation loss-->epoch= 11 and best val loss is 2218.00|current_Val loss = 2568.9177346229553|counter = 5\n",
      "executing epoch:13, it took 1.265359063943227 mins from beginning of epoch till batch#275\n",
      "training loss has decreased---> reducing the best loss from 452.28 to 290.95 | throughput = 117639 tokens/second | norm = 8.1104 | learning rate = 9.97261e-06\n",
      "inside validation data for epoch 13\n",
      "No improvement in validation loss-->epoch= 12 and best val loss is 2218.00|current_Val loss = 2688.707935810089|counter = 6\n",
      "executing epoch:14, it took 1.311738673845927 mins from beginning of epoch till batch#275\n",
      "training loss has decreased---> reducing the best loss from 290.95 to 189.25 | throughput = 116909 tokens/second | norm = 5.2881 | learning rate = 9.95134e-06\n",
      "inside validation data for epoch 14\n",
      "No improvement in validation loss-->epoch= 13 and best val loss is 2218.00|current_Val loss = 2803.187023639679|counter = 7\n",
      "executing epoch:15, it took 1.2888063271840413 mins from beginning of epoch till batch#275\n",
      "training loss has decreased---> reducing the best loss from 189.25 to 129.06 | throughput = 119046 tokens/second | norm = 3.8975 | learning rate = 9.92404e-06\n",
      "inside validation data for epoch 15\n",
      "No improvement in validation loss-->epoch= 14 and best val loss is 2218.00|current_Val loss = 2882.4145188331604|counter = 8\n",
      "executing epoch:16, it took 1.265129804611206 mins from beginning of epoch till batch#275\n",
      "training loss has decreased---> reducing the best loss from 129.06 to 90.58 | throughput = 119806 tokens/second | norm = 2.0068 | learning rate = 9.89074e-06\n",
      "inside validation data for epoch 16\n",
      "No improvement in validation loss-->epoch= 15 and best val loss is 2218.00|current_Val loss = 3062.561113357544|counter = 9\n",
      "executing epoch:17, it took 1.2665568987528484 mins from beginning of epoch till batch#275\n",
      "training loss has decreased---> reducing the best loss from 90.58 to 71.93 | throughput = 121426 tokens/second | norm = 3.2404 | learning rate = 9.85148e-06\n",
      "inside validation data for epoch 17\n",
      "No improvement in validation loss-->epoch= 16 and best val loss is 2218.00|current_Val loss = 3015.6360964775085|counter = 10\n",
      "executing epoch:18, it took 1.2576870759328207 mins from beginning of epoch till batch#275\n",
      "training loss has decreased---> reducing the best loss from 71.93 to 56.29 | throughput = 121100 tokens/second | norm = 2.2242 | learning rate = 9.80631e-06\n",
      "inside validation data for epoch 18\n",
      "No improvement in validation loss-->epoch= 17 and best val loss is 2218.00|current_Val loss = 3106.5200510025024|counter = 11\n",
      "executing epoch:19, it took 1.281871251265208 mins from beginning of epoch till batch#275\n",
      "training loss has decreased---> reducing the best loss from 56.29 to 46.68 | throughput = 120986 tokens/second | norm = 2.2733 | learning rate = 9.75528e-06\n",
      "inside validation data for epoch 19\n",
      "No improvement in validation loss-->epoch= 18 and best val loss is 2218.00|current_Val loss = 3156.015001296997|counter = 12\n",
      "executing epoch:20, it took 1.2133894960085552 mins from beginning of epoch till batch#275\n",
      "training loss has decreased---> reducing the best loss from 46.68 to 40.69 | throughput = 123457 tokens/second | norm = 0.9415 | learning rate = 9.69846e-06\n",
      "inside validation data for epoch 20\n",
      "No improvement in validation loss-->epoch= 19 and best val loss is 2218.00|current_Val loss = 3200.6702513694763|counter = 13\n",
      "executing epoch:21, it took 1.2247286438941956 mins from beginning of epoch till batch#275\n",
      "training loss has decreased---> reducing the best loss from 40.69 to 39.74 | throughput = 122988 tokens/second | norm = 0.9327 | learning rate = 9.63592e-06\n",
      "inside validation data for epoch 21\n",
      "No improvement in validation loss-->epoch= 20 and best val loss is 2218.00|current_Val loss = 3209.261106491089|counter = 14\n",
      "executing epoch:22, it took 1.2424745400746664 mins from beginning of epoch till batch#275\n",
      "training loss has decreased---> reducing the best loss from 39.74 to 35.80 | throughput = 122198 tokens/second | norm = 1.2948 | learning rate = 9.56773e-06\n",
      "inside validation data for epoch 22\n",
      "No improvement in validation loss-->epoch= 21 and best val loss is 2218.00|current_Val loss = 3257.5202112197876|counter = 15\n",
      "executing epoch:23, it took 1.2992315530776977 mins from beginning of epoch till batch#275\n",
      "No improvement in training  loss-->epoch= 22 and best loss is 35.80|current_loss = 36.96336744353175|counter = 1\n",
      "executing epoch:24, it took 1.2965899348258971 mins from beginning of epoch till batch#275\n",
      "training loss has decreased---> reducing the best loss from 35.80 to 33.94 | throughput = 114728 tokens/second | norm = 0.8936 | learning rate = 9.41474e-06\n",
      "inside validation data for epoch 24\n",
      "No improvement in validation loss-->epoch= 23 and best val loss is 2218.00|current_Val loss = 3246.3011679649353|counter = 16\n",
      "executing epoch:25, it took 1.3415445327758788 mins from beginning of epoch till batch#275\n",
      "No improvement in training  loss-->epoch= 24 and best loss is 33.94|current_loss = 35.56352287530899|counter = 1\n",
      "executing epoch:26, it took 1.311548618475596 mins from beginning of epoch till batch#275\n",
      "No improvement in training  loss-->epoch= 25 and best loss is 33.94|current_loss = 35.12469167681411|counter = 2\n",
      "executing epoch:27, it took 1.255470577875773 mins from beginning of epoch till batch#275\n",
      "training loss has decreased---> reducing the best loss from 33.94 to 32.40 | throughput = 118082 tokens/second | norm = 0.4957 | learning rate = 9.14519e-06\n",
      "inside validation data for epoch 27\n",
      "No improvement in validation loss-->epoch= 26 and best val loss is 2218.00|current_Val loss = 3271.5960879325867|counter = 17\n",
      "executing epoch:28, it took 1.3001677115758261 mins from beginning of epoch till batch#275\n",
      "training loss has decreased---> reducing the best loss from 32.40 to 31.72 | throughput = 118376 tokens/second | norm = 0.8692 | learning rate = 9.04508e-06\n",
      "inside validation data for epoch 28\n",
      "No improvement in validation loss-->epoch= 27 and best val loss is 2218.00|current_Val loss = 3187.095781326294|counter = 18\n",
      "executing epoch:29, it took 1.226555589834849 mins from beginning of epoch till batch#275\n",
      "No improvement in training  loss-->epoch= 28 and best loss is 31.72|current_loss = 33.26408247416839|counter = 1\n",
      "executing epoch:30, it took 1.2748949646949768 mins from beginning of epoch till batch#275\n",
      "training loss has decreased---> reducing the best loss from 31.72 to 31.37 | throughput = 121231 tokens/second | norm = 0.3531 | learning rate = 8.83022e-06\n",
      "inside validation data for epoch 30\n",
      "No improvement in validation loss-->epoch= 29 and best val loss is 2218.00|current_Val loss = 3268.072723865509|counter = 19\n",
      "executing epoch:31, it took 1.2684168855349223 mins from beginning of epoch till batch#275\n",
      "No improvement in training  loss-->epoch= 30 and best loss is 31.37|current_loss = 33.25984050706029|counter = 1\n",
      "executing epoch:32, it took 1.3161137700080872 mins from beginning of epoch till batch#275\n",
      "No improvement in training  loss-->epoch= 31 and best loss is 31.37|current_loss = 32.1124864066951|counter = 2\n",
      "executing epoch:33, it took 1.2359135309855143 mins from beginning of epoch till batch#275\n",
      "No improvement in training  loss-->epoch= 32 and best loss is 31.37|current_loss = 33.426566801033914|counter = 3\n",
      "executing epoch:34, it took 1.2507107337315877 mins from beginning of epoch till batch#275\n",
      "training loss has decreased---> reducing the best loss from 31.37 to 29.96 | throughput = 119412 tokens/second | norm = 0.3441 | learning rate = 8.34565e-06\n",
      "inside validation data for epoch 34\n",
      "No improvement in validation loss-->epoch= 33 and best val loss is 2218.00|current_Val loss = 3243.9191431999207|counter = 20\n",
      "executing epoch:35, it took 1.3004868904749551 mins from beginning of epoch till batch#275\n",
      "No improvement in training  loss-->epoch= 34 and best loss is 29.96|current_loss = 33.79162294091657|counter = 1\n",
      "executing epoch:36, it took 1.2165682037671408 mins from beginning of epoch till batch#275\n",
      "No improvement in training  loss-->epoch= 35 and best loss is 29.96|current_loss = 31.249129745643586|counter = 2\n",
      "executing epoch:37, it took 1.250717282295227 mins from beginning of epoch till batch#275\n",
      "No improvement in training  loss-->epoch= 36 and best loss is 29.96|current_loss = 34.21608250075951|counter = 3\n",
      "executing epoch:38, it took 1.2587169647216796 mins from beginning of epoch till batch#275\n",
      "training loss has decreased---> reducing the best loss from 29.96 to 29.00 | throughput = 120756 tokens/second | norm = 0.2965 | learning rate = 7.79596e-06\n",
      "inside validation data for epoch 38\n",
      "No improvement in validation loss-->epoch= 37 and best val loss is 2218.00|current_Val loss = 3274.5129132270813|counter = 21\n",
      "executing epoch:39, it took 1.2191296100616456 mins from beginning of epoch till batch#275\n",
      "training loss has decreased---> reducing the best loss from 29.00 to 28.46 | throughput = 122154 tokens/second | norm = 0.5642 | learning rate = 7.64960e-06\n",
      "inside validation data for epoch 39\n",
      "No improvement in validation loss-->epoch= 38 and best val loss is 2218.00|current_Val loss = 3258.9474325180054|counter = 22\n",
      "executing epoch:40, it took 1.2555097619692484 mins from beginning of epoch till batch#275\n",
      "No improvement in training  loss-->epoch= 39 and best loss is 28.46|current_loss = 31.401011534733698|counter = 1\n",
      "executing epoch:41, it took 1.2695618947347005 mins from beginning of epoch till batch#275\n",
      "No improvement in training  loss-->epoch= 40 and best loss is 28.46|current_loss = 31.15573903801851|counter = 2\n",
      "executing epoch:42, it took 1.2361874222755431 mins from beginning of epoch till batch#275\n",
      "No improvement in training  loss-->epoch= 41 and best loss is 28.46|current_loss = 28.908930864185095|counter = 3\n",
      "executing epoch:43, it took 1.2764560262362161 mins from beginning of epoch till batch#275\n",
      "training loss has decreased---> reducing the best loss from 28.46 to 28.03 | throughput = 119456 tokens/second | norm = 0.3571 | learning rate = 7.03368e-06\n",
      "inside validation data for epoch 43\n",
      "No improvement in validation loss-->epoch= 42 and best val loss is 2218.00|current_Val loss = 3275.5166869163513|counter = 23\n",
      "executing epoch:44, it took 1.212318778038025 mins from beginning of epoch till batch#275\n",
      "No improvement in training  loss-->epoch= 43 and best loss is 28.03|current_loss = 30.256124259438366|counter = 1\n",
      "executing epoch:45, it took 1.3171284437179565 mins from beginning of epoch till batch#275\n",
      "No improvement in training  loss-->epoch= 44 and best loss is 28.03|current_loss = 32.424593853764236|counter = 2\n",
      "executing epoch:46, it took 1.2491591135660807 mins from beginning of epoch till batch#275\n",
      "No improvement in training  loss-->epoch= 45 and best loss is 28.03|current_loss = 30.196573526365682|counter = 3\n",
      "executing epoch:47, it took 1.253861169020335 mins from beginning of epoch till batch#275\n",
      "No improvement in training  loss-->epoch= 46 and best loss is 28.03|current_loss = 31.42608020780608|counter = 4\n",
      "executing epoch:48, it took 1.2989646792411804 mins from beginning of epoch till batch#275\n",
      "No improvement in training  loss-->epoch= 47 and best loss is 28.03|current_loss = 31.184444912942126|counter = 5\n",
      "executing epoch:49, it took 1.2441622734069824 mins from beginning of epoch till batch#275\n",
      "No improvement in training  loss-->epoch= 48 and best loss is 28.03|current_loss = 30.769870134536177|counter = 6\n",
      "executing epoch:50, it took 1.2610231479008993 mins from beginning of epoch till batch#275\n",
      "No improvement in training  loss-->epoch= 49 and best loss is 28.03|current_loss = 30.53646521596238|counter = 7\n",
      "executing epoch:51, it took 1.246569530169169 mins from beginning of epoch till batch#275\n",
      "training loss has decreased---> reducing the best loss from 28.03 to 27.78 | throughput = 120397 tokens/second | norm = 0.2764 | learning rate = 5.69587e-06\n",
      "inside validation data for epoch 51\n",
      "No improvement in validation loss-->epoch= 50 and best val loss is 2218.00|current_Val loss = 3316.542459487915|counter = 24\n",
      "executing epoch:52, it took 1.3602126757303874 mins from beginning of epoch till batch#275\n",
      "No improvement in training  loss-->epoch= 51 and best loss is 27.78|current_loss = 28.87038051430136|counter = 1\n",
      "executing epoch:53, it took 1.3101611375808715 mins from beginning of epoch till batch#275\n",
      "No improvement in training  loss-->epoch= 52 and best loss is 27.78|current_loss = 31.622774851275608|counter = 2\n",
      "executing epoch:54, it took 1.3310444235801697 mins from beginning of epoch till batch#275\n",
      "No improvement in training  loss-->epoch= 53 and best loss is 27.78|current_loss = 28.450746352318674|counter = 3\n",
      "executing epoch:55, it took 1.3259597142537436 mins from beginning of epoch till batch#275\n",
      "No improvement in training  loss-->epoch= 54 and best loss is 27.78|current_loss = 30.82706988346763|counter = 4\n",
      "executing epoch:56, it took 1.2907608071962993 mins from beginning of epoch till batch#275\n",
      "No improvement in training  loss-->epoch= 55 and best loss is 27.78|current_loss = 29.140619846293703|counter = 5\n",
      "executing epoch:57, it took 1.3118638515472412 mins from beginning of epoch till batch#275\n",
      "No improvement in training  loss-->epoch= 56 and best loss is 27.78|current_loss = 31.33766636485234|counter = 6\n",
      "executing epoch:58, it took 1.2986884276072184 mins from beginning of epoch till batch#275\n",
      "No improvement in training  loss-->epoch= 57 and best loss is 27.78|current_loss = 30.003560848534107|counter = 7\n",
      "executing epoch:59, it took 1.3356007973353068 mins from beginning of epoch till batch#275\n",
      "No improvement in training  loss-->epoch= 58 and best loss is 27.78|current_loss = 31.439388228580356|counter = 8\n",
      "executing epoch:60, it took 1.29801291624705 mins from beginning of epoch till batch#275\n",
      "No improvement in training  loss-->epoch= 59 and best loss is 27.78|current_loss = 28.85140767064877|counter = 9\n",
      "executing epoch:61, it took 1.3051627675692241 mins from beginning of epoch till batch#275\n",
      "No improvement in training  loss-->epoch= 60 and best loss is 27.78|current_loss = 30.68326682667248|counter = 10\n",
      "executing epoch:62, it took 1.3035013794898986 mins from beginning of epoch till batch#275\n",
      "No improvement in training  loss-->epoch= 61 and best loss is 27.78|current_loss = 28.00038789305836|counter = 11\n",
      "executing epoch:63, it took 1.2469586531321208 mins from beginning of epoch till batch#275\n",
      "No improvement in training  loss-->epoch= 62 and best loss is 27.78|current_loss = 31.356699886731803|counter = 12\n",
      "executing epoch:64, it took 1.28396764198939 mins from beginning of epoch till batch#275\n",
      "No improvement in training  loss-->epoch= 63 and best loss is 27.78|current_loss = 31.43193225422874|counter = 13\n",
      "executing epoch:65, it took 1.2971444129943848 mins from beginning of epoch till batch#275\n",
      "No improvement in training  loss-->epoch= 64 and best loss is 27.78|current_loss = 31.155467274365947|counter = 14\n",
      "executing epoch:66, it took 1.3068797787030537 mins from beginning of epoch till batch#275\n",
      "No improvement in training  loss-->epoch= 65 and best loss is 27.78|current_loss = 30.687450325582176|counter = 15\n",
      "executing epoch:67, it took 1.2772326668103535 mins from beginning of epoch till batch#275\n",
      "No improvement in training  loss-->epoch= 66 and best loss is 27.78|current_loss = 30.616563339252025|counter = 16\n",
      "executing epoch:68, it took 1.2282018303871154 mins from beginning of epoch till batch#275\n",
      "No improvement in training  loss-->epoch= 67 and best loss is 27.78|current_loss = 28.700264463201165|counter = 17\n",
      "executing epoch:69, it took 1.2618796706199646 mins from beginning of epoch till batch#275\n",
      "No improvement in training  loss-->epoch= 68 and best loss is 27.78|current_loss = 31.532246283255517|counter = 18\n",
      "executing epoch:70, it took 1.307148289680481 mins from beginning of epoch till batch#275\n",
      "No improvement in training  loss-->epoch= 69 and best loss is 27.78|current_loss = 30.3859945833683|counter = 19\n",
      "executing epoch:71, it took 1.271272071202596 mins from beginning of epoch till batch#275\n",
      "No improvement in training  loss-->epoch= 70 and best loss is 27.78|current_loss = 30.135522338794544|counter = 20\n",
      "executing epoch:72, it took 1.222565221786499 mins from beginning of epoch till batch#275\n",
      "No improvement in training  loss-->epoch= 71 and best loss is 27.78|current_loss = 29.549389954423532|counter = 21\n",
      "executing epoch:73, it took 1.272204291820526 mins from beginning of epoch till batch#275\n",
      "No improvement in training  loss-->epoch= 72 and best loss is 27.78|current_loss = 29.874021794414148|counter = 22\n",
      "executing epoch:74, it took 1.302896269162496 mins from beginning of epoch till batch#275\n",
      "No improvement in training  loss-->epoch= 73 and best loss is 27.78|current_loss = 28.811056123813614|counter = 23\n",
      "executing epoch:75, it took 1.2650578975677491 mins from beginning of epoch till batch#275\n",
      "No improvement in training  loss-->epoch= 74 and best loss is 27.78|current_loss = 29.14162524836138|counter = 24\n",
      "executing epoch:76, it took 1.257972776889801 mins from beginning of epoch till batch#275\n",
      "No improvement in training  loss-->epoch= 75 and best loss is 27.78|current_loss = 29.00899992370978|counter = 25\n",
      "executing epoch:77, it took 1.2515594244003296 mins from beginning of epoch till batch#275\n",
      "No improvement in training  loss-->epoch= 76 and best loss is 27.78|current_loss = 28.704684557626024|counter = 26\n",
      "executing epoch:78, it took 1.288875436782837 mins from beginning of epoch till batch#275\n",
      "No improvement in training  loss-->epoch= 77 and best loss is 27.78|current_loss = 28.973035505739972|counter = 27\n",
      "executing epoch:79, it took 1.2688249667485556 mins from beginning of epoch till batch#275\n",
      "No improvement in training  loss-->epoch= 78 and best loss is 27.78|current_loss = 29.737637204816565|counter = 28\n",
      "executing epoch:80, it took 1.3132339676221212 mins from beginning of epoch till batch#275\n",
      "No improvement in training  loss-->epoch= 79 and best loss is 27.78|current_loss = 31.158048322191462|counter = 29\n",
      "executing epoch:81, it took 1.278477946917216 mins from beginning of epoch till batch#275\n",
      "No improvement in training  loss-->epoch= 80 and best loss is 27.78|current_loss = 29.316634587943554|counter = 30\n",
      "executing epoch:82, it took 1.2574153621991475 mins from beginning of epoch till batch#275\n",
      "No improvement in training  loss-->epoch= 81 and best loss is 27.78|current_loss = 30.133785978658125|counter = 31\n",
      "executing epoch:83, it took 1.3030096411705017 mins from beginning of epoch till batch#275\n",
      "No improvement in training  loss-->epoch= 82 and best loss is 27.78|current_loss = 27.82589350361377|counter = 32\n",
      "executing epoch:84, it took 1.3228161533673604 mins from beginning of epoch till batch#275\n",
      "No improvement in training  loss-->epoch= 83 and best loss is 27.78|current_loss = 29.218464702600613|counter = 33\n",
      "executing epoch:85, it took 1.2832877119382222 mins from beginning of epoch till batch#275\n",
      "No improvement in training  loss-->epoch= 84 and best loss is 27.78|current_loss = 28.239485308993608|counter = 34\n",
      "executing epoch:86, it took 1.207529330253601 mins from beginning of epoch till batch#275\n",
      "No improvement in training  loss-->epoch= 85 and best loss is 27.78|current_loss = 29.28883857303299|counter = 35\n",
      "executing epoch:87, it took 1.267570924758911 mins from beginning of epoch till batch#275\n",
      "No improvement in training  loss-->epoch= 86 and best loss is 27.78|current_loss = 28.600883117411286|counter = 36\n",
      "executing epoch:88, it took 1.2659888863563538 mins from beginning of epoch till batch#275\n",
      "No improvement in training  loss-->epoch= 87 and best loss is 27.78|current_loss = 30.471964991418645|counter = 37\n",
      "executing epoch:89, it took 1.2736260573069254 mins from beginning of epoch till batch#275\n",
      "No improvement in training  loss-->epoch= 88 and best loss is 27.78|current_loss = 28.040272717596963|counter = 38\n",
      "executing epoch:90, it took 1.2820631663004558 mins from beginning of epoch till batch#275\n",
      "No improvement in training  loss-->epoch= 89 and best loss is 27.78|current_loss = 28.889106342568994|counter = 39\n",
      "executing epoch:91, it took 1.2666392604509988 mins from beginning of epoch till batch#275\n",
      "No improvement in training  loss-->epoch= 90 and best loss is 27.78|current_loss = 31.775400569895282|counter = 40\n",
      "executing epoch:92, it took 1.2930455803871155 mins from beginning of epoch till batch#275\n",
      "No improvement in training  loss-->epoch= 91 and best loss is 27.78|current_loss = 29.50697782356292|counter = 41\n",
      "executing epoch:93, it took 1.2611947218577066 mins from beginning of epoch till batch#275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-129:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.8/threading.py\", line 932, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/ipykernel/ipkernel.py\", line 766, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"/opt/conda/lib/python3.8/threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/torch/utils/data/_utils/pin_memory.py\", line 54, in _pin_memory_loop\n",
      "    do_one_step()\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/torch/utils/data/_utils/pin_memory.py\", line 31, in do_one_step\n",
      "    r = in_queue.get(timeout=MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"/opt/conda/lib/python3.8/multiprocessing/queues.py\", line 116, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/torch/multiprocessing/reductions.py\", line 495, in rebuild_storage_fd\n",
      "    fd = df.detach()\n",
      "  File \"/opt/conda/lib/python3.8/multiprocessing/resource_sharer.py\", line 57, in detach\n",
      "    with _resource_sharer.get_connection(self._id) as conn:\n",
      "  File \"/opt/conda/lib/python3.8/multiprocessing/resource_sharer.py\", line 87, in get_connection\n",
      "    c = Client(address, authkey=process.current_process().authkey)\n",
      "  File \"/opt/conda/lib/python3.8/multiprocessing/connection.py\", line 502, in Client\n",
      "    c = SocketClient(address)\n",
      "  File \"/opt/conda/lib/python3.8/multiprocessing/connection.py\", line 630, in SocketClient\n",
      "    s.connect(address)\n",
      "FileNotFoundError: [Errno 2] No such file or directory\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.8/multiprocessing/queues.py\", line 245, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/opt/conda/lib/python3.8/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/opt/conda/lib/python3.8/multiprocessing/connection.py\", line 411, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/opt/conda/lib/python3.8/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_12220/432417278.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtr_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepoch_train_log\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepoch_val_log\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_12220/783617603.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(train_loader, val_loader, model, num_epoch, device, tokenizer)\u001b[0m\n\u001b[1;32m     59\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0minput_emb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mprediction_embeddings\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcos_sim\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcos_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0mtotal_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m             \u001b[0mepoch_train_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtotal_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m             \u001b[0mnorm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mnum_epoch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tr_model,epoch_train_log,epoch_val_log = train_model(train_loader, val_loader,model=model,tokenizer = tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "65b8f997",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'epoch_train_log' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_12220/3760781422.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_train_log\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'epoch_train_log' is not defined"
     ]
    }
   ],
   "source": [
    "print(epoch_train_log)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "add219e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json , os\n",
    "# path_var_train_log = os.path.join(\".\",\"Tokenizers_and_loss_vals\",\"epoch_train_plain_loss.json\")\n",
    "# path_var_val_log = os.path.join(\".\",\"Tokenizers_and_loss_vals\",\"epoch_val_val_loss_.json\")\n",
    "\n",
    "# #print(path_var)\n",
    "# #Write the list to a JSON file\n",
    "# with open(path_var_train_log, \"w\") as file:\n",
    "#     json.dump(epoch_train_log, file)\n",
    "\n",
    "# with open(path_var_val_log, \"w\") as file:\n",
    "#     json.dump(epoch_val_log, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c592b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(path_var_train_log, \"r\") as file:\n",
    "#     train_loss = json.load(file)\n",
    "# with open(path_var_val_log, \"r\") as file:\n",
    "#     val_loss = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ad011774",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'epoch_train_log' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_12220/672291636.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mx_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_train_log\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_train_log\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Train_loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'epoch_train_log' is not defined"
     ]
    }
   ],
   "source": [
    "x_values = range(len(epoch_train_log))\n",
    "plt.plot(x_values, epoch_train_log, label='Train_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0d909836",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'epoch_val_log' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_12220/709189587.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#epoch_val_log= [t.detach().cpu().numpy() for t in epoch_val_log]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mx_values_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_val_log\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_values_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_val_log\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'epoch_val_log' is not defined"
     ]
    }
   ],
   "source": [
    "#epoch_val_log= [t.detach().cpu().numpy() for t in epoch_val_log]\n",
    "x_values_val = range(len(epoch_val_log))\n",
    "plt.plot(x_values_val, epoch_val_log, label='val_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "09ce1afc",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'epoch_train_log' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_12220/1048180551.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmin_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_train_log\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_val_log\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mlist1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepoch_train_log\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mmin_length\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mlist2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepoch_val_log\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mmin_length\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Create x-axis values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'epoch_train_log' is not defined"
     ]
    }
   ],
   "source": [
    "min_length = min(len(epoch_train_log), len(epoch_val_log))\n",
    "list1 = epoch_train_log[:min_length]\n",
    "list2 = epoch_val_log[:min_length]\n",
    "\n",
    "# Create x-axis values\n",
    "x_values = range(min_length)\n",
    "\n",
    "# Plot the lists\n",
    "plt.plot(x_values, list1, label='Train_loss')\n",
    "plt.plot(x_values, list2, label='Val_loss')\n",
    "\n",
    "# Adding labels and title\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Train Loss and Val_Loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c394764d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd637cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d64a0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91dae994",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e97b019",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53ca40a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
