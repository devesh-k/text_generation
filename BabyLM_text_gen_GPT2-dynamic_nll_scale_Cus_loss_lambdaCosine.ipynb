{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "597aa39b",
   "metadata": {},
   "source": [
    "## In this notebook, we scale the nll and scale the cosine similaity with (1-lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b802e13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f80337e4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: datasets in /opt/conda/lib/python3.8/site-packages (2.20.0)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.8/site-packages (from datasets) (1.3.4)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /opt/conda/lib/python3.8/site-packages (from datasets) (4.66.4)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.8/site-packages (from datasets) (3.9.5)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.8/site-packages (from datasets) (21.3)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.8/site-packages (from datasets) (3.4.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.8/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.8/site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.8/site-packages (from datasets) (6.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.8/site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.8/site-packages (from datasets) (1.21.4)\n",
      "Requirement already satisfied: fsspec[http]<=2024.5.0,>=2023.1.0 in /opt/conda/lib/python3.8/site-packages (from datasets) (2024.5.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.8/site-packages (from datasets) (16.1.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.2 in /opt/conda/lib/python3.8/site-packages (from datasets) (0.23.0)\n",
      "Requirement already satisfied: requests>=2.32.2 in /opt/conda/lib/python3.8/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.8/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets) (21.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.8/site-packages (from huggingface-hub>=0.21.2->datasets) (4.11.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging->datasets) (3.0.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests>=2.32.2->datasets) (1.26.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.8/site-packages (from requests>=2.32.2->datasets) (2.0.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests>=2.32.2->datasets) (3.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests>=2.32.2->datasets) (2021.10.8)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.8/site-packages (from pandas->datasets) (2021.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.8/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: jupyter in /opt/conda/lib/python3.8/site-packages (1.0.0)\n",
      "Requirement already satisfied: ipywidgets in /opt/conda/lib/python3.8/site-packages (8.1.3)\n",
      "Requirement already satisfied: qtconsole in /opt/conda/lib/python3.8/site-packages (from jupyter) (5.5.2)\n",
      "Requirement already satisfied: ipykernel in /opt/conda/lib/python3.8/site-packages (from jupyter) (6.29.5)\n",
      "Requirement already satisfied: jupyter-console in /opt/conda/lib/python3.8/site-packages (from jupyter) (6.6.3)\n",
      "Requirement already satisfied: nbconvert in /opt/conda/lib/python3.8/site-packages (from jupyter) (6.3.0)\n",
      "Requirement already satisfied: notebook in /opt/conda/lib/python3.8/site-packages (from jupyter) (6.4.1)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (7.30.0)\n",
      "Requirement already satisfied: comm>=0.1.3 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.11 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (3.0.11)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (5.14.3)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.11 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (4.0.11)\n",
      "Requirement already satisfied: backcall in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: pygments in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (2.10.0)\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (4.8.0)\n",
      "Requirement already satisfied: pickleshare in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.7.5)\n",
      "Requirement already satisfied: setuptools>=18.5 in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (59.4.0)\n",
      "Requirement already satisfied: jedi>=0.16 in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.18.1)\n",
      "Requirement already satisfied: decorator in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (5.1.0)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.47)\n",
      "Requirement already satisfied: matplotlib-inline in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.3)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /opt/conda/lib/python3.8/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.8/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.8/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=6.1.0->ipywidgets) (0.2.5)\n",
      "Requirement already satisfied: tornado>=6.1 in /opt/conda/lib/python3.8/site-packages (from ipykernel->jupyter) (6.1)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in /opt/conda/lib/python3.8/site-packages (from ipykernel->jupyter) (1.8.2)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in /opt/conda/lib/python3.8/site-packages (from ipykernel->jupyter) (7.1.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.8/site-packages (from ipykernel->jupyter) (21.3)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.8/site-packages (from ipykernel->jupyter) (5.8.0)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /opt/conda/lib/python3.8/site-packages (from ipykernel->jupyter) (5.7.2)\n",
      "Requirement already satisfied: nest-asyncio in /opt/conda/lib/python3.8/site-packages (from ipykernel->jupyter) (1.5.4)\n",
      "Requirement already satisfied: pyzmq>=24 in /opt/conda/lib/python3.8/site-packages (from ipykernel->jupyter) (26.0.3)\n",
      "Requirement already satisfied: entrypoints in /opt/conda/lib/python3.8/site-packages (from jupyter-client>=6.1.12->ipykernel->jupyter) (0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /opt/conda/lib/python3.8/site-packages (from jupyter-client>=6.1.12->ipykernel->jupyter) (2.8.2)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /opt/conda/lib/python3.8/site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel->jupyter) (4.2.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.8/site-packages (from python-dateutil>=2.1->jupyter-client>=6.1.12->ipykernel->jupyter) (1.16.0)\n",
      "Requirement already satisfied: nbformat>=4.4 in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (5.1.3)\n",
      "Requirement already satisfied: defusedxml in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (0.7.1)\n",
      "Requirement already satisfied: jinja2>=2.4 in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (3.0.3)\n",
      "Requirement already satisfied: nbclient<0.6.0,>=0.5.0 in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (0.5.9)\n",
      "Requirement already satisfied: bleach in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (4.1.0)\n",
      "Requirement already satisfied: testpath in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (0.5.0)\n",
      "Requirement already satisfied: jupyterlab-pygments in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (0.1.2)\n",
      "Requirement already satisfied: mistune<2,>=0.8.1 in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (0.8.4)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (1.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.8/site-packages (from jinja2>=2.4->nbconvert->jupyter) (2.0.1)\n",
      "Requirement already satisfied: ipython-genutils in /opt/conda/lib/python3.8/site-packages (from nbformat>=4.4->nbconvert->jupyter) (0.2.0)\n",
      "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /opt/conda/lib/python3.8/site-packages (from nbformat>=4.4->nbconvert->jupyter) (4.2.1)\n",
      "Requirement already satisfied: importlib-resources>=1.4.0 in /opt/conda/lib/python3.8/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.4->nbconvert->jupyter) (5.4.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /opt/conda/lib/python3.8/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.4->nbconvert->jupyter) (21.2.0)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /opt/conda/lib/python3.8/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.4->nbconvert->jupyter) (0.18.0)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /opt/conda/lib/python3.8/site-packages (from importlib-resources>=1.4.0->jsonschema!=2.5.0,>=2.4->nbformat>=4.4->nbconvert->jupyter) (3.6.0)\n",
      "Requirement already satisfied: webencodings in /opt/conda/lib/python3.8/site-packages (from bleach->nbconvert->jupyter) (0.5.1)\n",
      "Requirement already satisfied: prometheus-client in /opt/conda/lib/python3.8/site-packages (from notebook->jupyter) (0.12.0)\n",
      "Requirement already satisfied: argon2-cffi in /opt/conda/lib/python3.8/site-packages (from notebook->jupyter) (21.1.0)\n",
      "Requirement already satisfied: Send2Trash>=1.5.0 in /opt/conda/lib/python3.8/site-packages (from notebook->jupyter) (1.8.0)\n",
      "Requirement already satisfied: terminado>=0.8.3 in /opt/conda/lib/python3.8/site-packages (from notebook->jupyter) (0.12.1)\n",
      "Requirement already satisfied: cffi>=1.0.0 in /opt/conda/lib/python3.8/site-packages (from argon2-cffi->notebook->jupyter) (1.15.0)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.8/site-packages (from cffi>=1.0.0->argon2-cffi->notebook->jupyter) (2.21)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging->ipykernel->jupyter) (3.0.6)\n",
      "Requirement already satisfied: qtpy>=2.4.0 in /opt/conda/lib/python3.8/site-packages (from qtconsole->jupyter) (2.4.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#installing some libraries\n",
    "!pip install datasets\n",
    "!pip install --upgrade jupyter ipywidgets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c14298aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch, transformers\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import PreTrainedModel, PretrainedConfig\n",
    "from transformers import AutoModel, AutoConfig,AutoModelForCausalLM, AutoConfig\n",
    "\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "#from datasets import load_dataset\n",
    "import pandas as pd, numpy as np\n",
    "from torch import cuda\n",
    "import datetime\n",
    "import warnings,itertools\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import json,math\n",
    "pre_train = False\n",
    "\n",
    "# Ignore all warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "#pip install transformers bitsandbytes>=0.39.0 -q\n",
    "import zipfile,logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca10e707",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "#global params for training\n",
    "\n",
    "B,T = 16,1024\n",
    "epoch = 100\n",
    "# this controls whether we are using pre-trained wts or not\n",
    "random_init_wts = False\n",
    "if cuda.is_available():\n",
    "    device = torch.device('cuda:0')\n",
    "    print(device)\n",
    "else:\n",
    "    device = 'cpu'\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "# these 2 global vars help track the training and val loss\n",
    "global_tr_loss = torch.inf\n",
    "global_val_loss = torch.inf\n",
    "#directory to save wts\n",
    "model_path = os.path.join(\"model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "deb33150",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./data/unzip_text_10M'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "directory = os.path.join('.','data','unzip_text_10M')  # Replace with your directory path\n",
    "directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "578118ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_text(directory):\n",
    "    \"\"\"This function loads the dataset,provided in the zipped format and stores all the text files in list\n",
    "    each file has its contents stored as an item in list and at the end we concatenate all the sub-lists to create a flattened list and return it\"\"\"\n",
    "    directory = os.path.join('.','data','unzip_text_10M',str(directory))  # Replace with your directory path\n",
    "    print(f\"directory :{directory}\")\n",
    "    # List all files in the directory\n",
    "    files = [f for f in os.listdir(directory) if os.path.isfile(os.path.join(directory, f))]\n",
    "    print(f\"files:{files}\")\n",
    "    text_content = []\n",
    "    # Read each file\n",
    "    total_lines = 0\n",
    "    for filenum,filename in enumerate(files):\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            text = file.read()\n",
    "            text_content.append(text)\n",
    "            print(f\"the file:{filename} has been appeneded to the uber list and its length is {len(text_content)} \")\n",
    "            \n",
    "    \n",
    "    flattened_list = ''.join(text_content)\n",
    "    assert (len(flattened_list) == total_lines , f\"Expected {len(flattened_list)} to be equal to {total_lines}\" )\n",
    "    \n",
    "    return flattened_list\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30edc266",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "directory :./data/unzip_text_10M/train_10M\n",
      "files:['switchboard.train', 'simple_wiki.train', 'open_subtitles.train', 'gutenberg.train', 'childes.train', 'bnc_spoken.train']\n",
      "the file:switchboard.train has been appeneded to the uber list and its length is 1 \n",
      "the file:simple_wiki.train has been appeneded to the uber list and its length is 2 \n",
      "the file:open_subtitles.train has been appeneded to the uber list and its length is 3 \n",
      "the file:gutenberg.train has been appeneded to the uber list and its length is 4 \n",
      "the file:childes.train has been appeneded to the uber list and its length is 5 \n",
      "the file:bnc_spoken.train has been appeneded to the uber list and its length is 6 \n"
     ]
    }
   ],
   "source": [
    "#read the dataset and get a list\n",
    "train_list = read_text(\"train_10M\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88325940",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54215049"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f5d3f3be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3309\n"
     ]
    }
   ],
   "source": [
    "chunks = len(train_list)//(B*T)\n",
    "print(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "133e1c83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17191971 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "#tokeinze the training data\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\",return_tensors = \"pt\" , truncate = True, return_overflowing_tokens=True , padding = False,)\n",
    "enc_train = tokenizer(train_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "79935b1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.1535097982657136"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comp_ratio = len(train_list)/len(enc_train['input_ids'])\n",
    "comp_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d41a25b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "directory :./data/unzip_text_10M/dev\n",
      "files:['switchboard.dev', 'simple_wiki.dev', 'open_subtitles.dev', 'gutenberg.dev', 'childes.dev', 'bnc_spoken.dev']\n",
      "the file:switchboard.dev has been appeneded to the uber list and its length is 1 \n",
      "the file:simple_wiki.dev has been appeneded to the uber list and its length is 2 \n",
      "the file:open_subtitles.dev has been appeneded to the uber list and its length is 3 \n",
      "the file:gutenberg.dev has been appeneded to the uber list and its length is 4 \n",
      "the file:childes.dev has been appeneded to the uber list and its length is 5 \n",
      "the file:bnc_spoken.dev has been appeneded to the uber list and its length is 6 \n"
     ]
    }
   ],
   "source": [
    "#read validation data and tokenize\n",
    "val_list = read_text(\"dev\")\n",
    "enc_val = tokenizer(val_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6fa861",
   "metadata": {},
   "source": [
    "### we read the tokenize data and store the input_ids and attention mask as a single long list at run time we reshape the single [1,B*T] tensor into a batched tensor of dimension [B,T].\n",
    "This approach turns out to be more efficient as it removes the need for any extra tokens in the form of padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "920facad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df_from_list(enc , B= B, T = T, val= False):\n",
    "    \"\"\"This function takes the tokenized list and reated a dataframe where each row contains the input_ids and attention_mask from the tokenizer.\n",
    "    each row of the dataframe is a list with items B*T\"\"\"\n",
    "    chunk_size = B*T\n",
    "    if val:\n",
    "        chunk_size = 2*B*T\n",
    "        \n",
    "    long_list_inp = enc['input_ids']\n",
    "    long_list_attention = enc['attention_mask']\n",
    "\n",
    "    # Step 3: Split the list into chunks and pad the last chunk if necessary\n",
    "    chunks_inp = [long_list_inp[i:i + chunk_size] for i in range(0, len(long_list_inp), chunk_size)]\n",
    "    chunks_att = [long_list_attention[i:i + chunk_size] for i in range(0, len(long_list_attention), chunk_size)]\n",
    "    df = pd.DataFrame({'input_ids': chunks_inp,'attention_mask':chunks_att})\n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5b1062a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the dataframe is = 1050\n"
     ]
    }
   ],
   "source": [
    "df_train_temp = get_df_from_list(enc_train)\n",
    "# Display the DataFrame\n",
    "df_train_temp.head()\n",
    "print(f\"Length of the dataframe is = {len(df_train_temp)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e77d93ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the VALDATION dataframe is = 1063\n"
     ]
    }
   ],
   "source": [
    "df_val_temp = get_df_from_list(enc_val)\n",
    "# Display the DataFrame\n",
    "df_val_temp.head()\n",
    "print(f\"Length of the VALDATION dataframe is = {len(df_val_temp)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5cb934c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_df(df,eos_char = tokenizer.eos_token_id,B = B, T = T,val = False):\n",
    "    \"\"\"The 2 functions below makes sure that each row of the dataframe is of equal length and if not it adds the eos token to make it B*T\"\"\"\n",
    "    if val:\n",
    "        B = 2*B\n",
    "    for ind,row in df.iterrows():\n",
    "        if len(row['input_ids']) != B*T :\n",
    "            print(f\"row = {ind} and input_id length = {len(row['input_ids'])}\")\n",
    "            print(f\"row = {ind} and attention length = {len(row['attention_mask'])}\")\n",
    "            pad_len = B*T - len(row['input_ids'])\n",
    "            print(f\"padding the row index {ind} with {pad_len} character\")\n",
    "            row['input_ids'] = row['input_ids']+ [eos_char] * pad_len\n",
    "            #attention mask should be padded to 0\n",
    "            row['attention_mask'] = row['attention_mask']+ [0] * pad_len\n",
    "            print(\"#### POST CONCAT####\")\n",
    "            print(f\"row = {ind} and input_id length = {len(row['input_ids'])}\")\n",
    "            print(f\"row = {ind} and attention length ={len(row['attention_mask'])}\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def verify_len(df,val = False, B= B):\n",
    "    row_ind = []\n",
    "    if val:\n",
    "        B = 2*B\n",
    "    for ind,row in df.iterrows():\n",
    "        if len(row['input_ids']) != B*T :\n",
    "            row_ind.append(ind)\n",
    "        else:\n",
    "            continue\n",
    "    if len(row_ind) !=0:\n",
    "        print(\"CONCATENATION Did not work\")\n",
    "    else:\n",
    "        print(\"CONCATENATION worked\")\n",
    "        \n",
    "            \n",
    "        \n",
    "    \n",
    "        \n",
    "        \n",
    "   \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "06e4b932",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row = 1049 and input_id length = 5155\n",
      "row = 1049 and attention length = 5155\n",
      "padding the row index 1049 with 11229 character\n",
      "#### POST CONCAT####\n",
      "row = 1049 and input_id length = 16384\n",
      "row = 1049 and attention length =16384\n",
      "CONCATENATION worked\n"
     ]
    }
   ],
   "source": [
    "df_train  = pad_df(df_train_temp)\n",
    "verify_len(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e5940fe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row = 1062 and input_id length = 13588\n",
      "row = 1062 and attention length = 13588\n",
      "padding the row index 1062 with 2796 character\n",
      "#### POST CONCAT####\n",
      "row = 1062 and input_id length = 16384\n",
      "row = 1062 and attention length =16384\n",
      "CONCATENATION worked\n"
     ]
    }
   ],
   "source": [
    "df_val  = pad_df(df_val_temp)\n",
    "verify_len(df_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f185872b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_ids</th>\n",
       "      <th>attention_mask</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[32, 25, 197, 40, 1101, 1654, 484, 389, 13, 19...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[290, 1243, 588, 326, 198, 33, 25, 197, 392, 1...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[6510, 11, 14104, 290, 27913, 13, 198, 32, 25,...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[314, 892, 340, 6774, 503, 262, 5290, 287, 262...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[314, 1612, 11, 340, 338, 1611, 286, 43244, 11...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           input_ids  \\\n",
       "0  [32, 25, 197, 40, 1101, 1654, 484, 389, 13, 19...   \n",
       "1  [290, 1243, 588, 326, 198, 33, 25, 197, 392, 1...   \n",
       "2  [6510, 11, 14104, 290, 27913, 13, 198, 32, 25,...   \n",
       "3  [314, 892, 340, 6774, 503, 262, 5290, 287, 262...   \n",
       "4  [314, 1612, 11, 340, 338, 1611, 286, 43244, 11...   \n",
       "\n",
       "                                      attention_mask  \n",
       "0  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "1  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "2  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "3  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "4  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24b1d78",
   "metadata": {},
   "source": [
    "## Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d6a326f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLoss(nn.Module):\n",
    "    def __init__(self, num_epochs):\n",
    "        super(CustomLoss, self).__init__()\n",
    "        #self.nll_loss = nn.CrossEntropyLoss()\n",
    "#        self.cosine_similarity = nn.CosineSimilarity(dim=2)\n",
    "        self.num_epochs = num_epochs\n",
    "        self.current_epoch = 0\n",
    "        self.nll_scale = 1.0  # Initialize with a default value\n",
    "\n",
    "    def forward(self, nll,cosine_sim):\n",
    "               \n",
    "        nll_normalized = nll / self.nll_scale         \n",
    "        # Calculate lambda value\n",
    "        lambda_val = self.get_lambda()\n",
    "        # Compute total loss\n",
    "        total_loss = nll_normalized + lambda_val * cosine_sim\n",
    "        #print(f\"inside CustomLoss->nll = {nll}|nll_scale = {self.nll_scale}|nll_normalized= {nll_normalized}|cosine_sim = {cosine_sim} | lambda val = {lambda_val}|total_loss = {total_loss}\")\n",
    "        return total_loss,lambda_val,self.nll_scale\n",
    "\n",
    "    def get_lambda(self):\n",
    "        # Exponential increase from 0 to 1\n",
    "        return 1 - math.exp(-5 * self.current_epoch / self.num_epochs)\n",
    "\n",
    "    def update_epoch(self, epoch):\n",
    "        self.current_epoch = epoch\n",
    "\n",
    "    def update_nll_scale(self, nll_value):\n",
    "        self.nll_scale = max(self.nll_scale, nll_value)\n",
    "    def update_epoch(self, epoch):\n",
    "        self.current_epoch = epoch\n",
    "    def get_nll_scale(self):\n",
    "        #print(self.nll_scale)\n",
    "        return self.nll_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0ba0af3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the tokenizer:\n",
    "model_name = \"gpt2\"\n",
    "if random_init_wts:\n",
    "    config = AutoConfig.from_pretrained(model_name, vocab_size = 50304)\n",
    "    # Initialize the model with random weights\n",
    "    model = AutoModelForCausalLM.from_config(config)\n",
    "else:\n",
    "    #model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "\n",
    "#model = torch.compile(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5d36b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9a96f976",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1050"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "969f9837",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# device = \"cuda\"\n",
    "# inp = torch.tensor(df_train.iloc[5]['input_ids']).view(B,T).to(device)\n",
    "# att = torch.tensor(df_train.iloc[5]['attention_mask']).view(B,T).to(device)\n",
    "# lab = inp.clone()\n",
    "# # lab = F.pad(lab[:, 1:], (0, 1), value=-100)  # Shift labels to the left and pad with -100\n",
    "# # lab[:, -1] = -100\n",
    "# model.to(device)\n",
    "# model_out = model(input_ids = inp, attention_mask = att , labels = lab)\n",
    "# assert not torch.isnan(inp).any(), \"Input contains NaN\"\n",
    "# assert not torch.isinf(inp).any(), \"Input contains inf\"\n",
    "# model_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "433c1070",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"Input shape: {inp.shape}\")\n",
    "# print(f\"Attention mask shape: {att.shape}\")\n",
    "# print(f\"Labels shape: {lab.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1605f777",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "be584e75",
   "metadata": {},
   "source": [
    "### Data loaders and Dataset for batched training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9da418d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataset_pyt_val(Dataset):\n",
    "    \"\"\"Dataset for validation data\"\"\"\n",
    "    def __init__(self, df, B = B, T = T ):\n",
    "        self.df = df\n",
    "        print(f\"Value of B {B}\")\n",
    "                                        \n",
    "    def __getitem__(self, idx):\n",
    "        #print(f\"inside loader...idx ->{idx}\")\n",
    "        input_id_temp = torch.tensor(self.df.iloc[idx]['input_ids'],dtype = torch.long)\n",
    "        att_mask = torch.tensor(self.df.iloc[idx]['attention_mask'],dtype = torch.long)\n",
    "        input_id =   input_id_temp.view(B,T)    \n",
    "        attention_mask = att_mask.view(B,T)\n",
    "        \n",
    "        \n",
    "        return input_id, attention_mask\n",
    "        \n",
    "    def __len__(self):\n",
    "        #return the length of the dataframe\n",
    "        return len(self.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "eeca176d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataset_pyt_train(Dataset):\n",
    "    def __init__(self, df, B = B, T = T ):\n",
    "        self.df = df\n",
    "                                        \n",
    "    def __getitem__(self, idx):\n",
    "        #print(f\"************* idx for dataloader = {idx}\")\n",
    "        input_id_temp = torch.tensor(self.df.iloc[idx]['input_ids'],dtype = torch.long)\n",
    "        att_mask = torch.tensor(self.df.iloc[idx]['attention_mask'],dtype = torch.long)\n",
    "        input_id =   input_id_temp.view(B,T)    \n",
    "        attention_mask = att_mask.view(B,T)   \n",
    "        \n",
    "        return input_id, attention_mask\n",
    "        \n",
    "    def __len__(self):\n",
    "        #return the length of the dataframe\n",
    "        return len(self.df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae737496",
   "metadata": {},
   "source": [
    "### Note - batch_size is 1 here because we reshape our input data in the shape [B,T] for a batched training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "535634b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value of B 16\n"
     ]
    }
   ],
   "source": [
    "#train_dataset = dataset_pyt(filtered_df,tokenizer = tokenizer)\n",
    "train_dataset = dataset_pyt_train(df_train)\n",
    "val_dataset = dataset_pyt_val(df_val)\n",
    "#test_dataset = dataset_pyt(df_test,tokenizer = tokenizer)\n",
    "\n",
    "train_loader = DataLoader(train_dataset,batch_size = 1, shuffle = True , num_workers = 0, pin_memory = True)\n",
    "val_loader = DataLoader(val_dataset,batch_size = 1, shuffle = True , num_workers = 0, pin_memory = True)\n",
    "#test_loader = DataLoader(test_dataset,batch_size = batch_size, shuffle = False, collate_fn = custom_collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533cbc4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "238b1c71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the train loader is 1050\n",
      "Length of the val loader is 1063\n",
      "num_tokens= 17203200\n"
     ]
    }
   ],
   "source": [
    "print(f\"Length of the train loader is {len(train_loader)}\")\n",
    "print(f\"Length of the val loader is {len(val_loader)}\")\n",
    "print(f\"num_tokens= {B*T*len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "41b883d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_file(log_message, model_name = model_name ,random_init_wts = random_init_wts ):\n",
    "    \"\"\"This function logs the training performance for future reference\"\"\"\n",
    "    current_datetime = datetime.datetime.now()\n",
    "    # Extract date and time components\n",
    "    current_date = str(current_datetime.date())\n",
    "    log_file = model_name +'_COS_SIM*lambda_'+'random_init_wts'+ '_'+str(random_init_wts)+'_' +current_date+'.log'\n",
    "    print(f\"*****LOGGING INFO IN {log_file}*********\")\n",
    "    filepath = os.path.join(\"model\",log_file)\n",
    "    logging.basicConfig(filename=filepath, \n",
    "                    filemode='a',  # Overwrite the log file each time\n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s', \n",
    "                    level=logging.DEBUG)\n",
    "    logger = logging.getLogger()\n",
    "    logger.info(log_message)\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0e7ca3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad\n",
    "\n",
    "def eval_model(val_loader, model, lambda_val ,nll_scale,epoch , device = device,tokenizer = tokenizer):\n",
    "    global global_val_loss\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    e = epoch+1\n",
    "    embedding_layer = model.transformer.wte\n",
    "    val_loss_accum = 0.0\n",
    "    print(f\"inside validation data for epoch {e}\")\n",
    "    for ind,(input_id,attention_mask) in enumerate(val_loader):\n",
    "        ids = input_id.to(device=device, non_blocking=True)\n",
    "        att_mask = attention_mask.to(device=device, non_blocking=True)\n",
    "        labels = ids.clone()\n",
    "        \n",
    "        with autocast(dtype = torch.bfloat16):\n",
    "            input_embeddings = embedding_layer(ids)\n",
    "            model_output = model(input_ids = ids ,attention_mask = att_mask, labels = labels)\n",
    "            logits = model_output.logits\n",
    "            predictions = torch.argmax(logits, dim=-1)\n",
    "            #print(f\"predictions = {predictions}\")\n",
    "            prediction_embeddings = embedding_layer(predictions)\n",
    "            cos_sim = F.cosine_similarity(torch.squeeze(prediction_embeddings,dim = 0), torch.squeeze(input_embeddings,dim = 0), dim=1)\n",
    "            cos_loss = 1- cos_sim.mean()\n",
    "            total_loss = cos_loss + model_output.loss.detach().item()\n",
    "        \n",
    "        val_loss_accum+=total_loss.detach().item()\n",
    "        del ids,att_mask,labels,input_embeddings,model_output,logits,predictions,prediction_embeddings,cos_sim,cos_loss,total_loss\n",
    "    \n",
    "    #logging and saving if the validation loss has decreased.\n",
    "    if val_loss_accum < global_val_loss:\n",
    "        print(f\"Val loss has decreased -->reducing the global validation loss from {global_val_loss:.2f} to {val_loss_accum:.2f}\")\n",
    "        s1 = f\"Val loss has decreased -->reducing the global validation loss from {global_val_loss:.2f} to {val_loss_accum:.2f}\"\n",
    "        global_val_loss = val_loss_accum\n",
    "        print(f\" validation loss for epoch = {e} is {val_loss_accum:.4f}\")\n",
    "        s2 = f\" validation loss for epoch = {e} is {val_loss_accum:.4f}\"\n",
    "        print(f\" epoch= {e} :  val loss is {val_loss_accum:.4f} \")\n",
    "        s3 = f\" epoch= {e} :  val loss is {val_loss_accum:.4f} \"\n",
    "        #save the model\n",
    "        \n",
    "        # Get the current date and time\n",
    "        current_datetime = datetime.datetime.now()\n",
    "        # Extract date and time components\n",
    "        current_date = str(current_datetime.date())\n",
    "        current_time = str(current_datetime.time()).split('.')[0]\n",
    "        file_name = 'model'+ current_date+current_time+'.pth'\n",
    "        path = os.path.join(\"model\",file_name)\n",
    "        print(f\"saving the model {file_name}\")\n",
    "        s4 = f\"saving the model {file_name}\"\n",
    "        #torch.save(model.state_dict(), path)\n",
    "        model.save_pretrained(path)\n",
    "        tokenizer.save_pretrained(path)\n",
    "        log_message = s1+s2+s3+s4\n",
    "        write_file(log_message)\n",
    "        \n",
    "        #plot_confusion_matrix(y_val.cpu().numpy(), y_hat_val.cpu().numpy(), labels)\n",
    "    else:\n",
    "        print(f\"No improvement in validation loss-->epoch= {e} and global val loss is {global_val_loss:.2f}|current_Val loss = {val_loss_accum}\")\n",
    "        \n",
    "    return val_loss_accum\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7792b3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_loader,val_loader,model,num_epoch = 100,device = device,tokenizer = tokenizer):\n",
    "    global global_tr_loss\n",
    "    model.train()\n",
    "    device = device\n",
    "    lr_custom = 2e-5\n",
    "    print(f\"inside train model. Device = {device}\")\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.AdamW(params =  model.parameters(), lr= lr_custom,fused = True ,weight_decay = .1)\n",
    "    total_batch_size = 2**19\n",
    "    grad_accum_step = total_batch_size//(B*T)\n",
    "    \n",
    "    extra_train = .1*num_epoch\n",
    "    max_train_steps = int(num_epoch +extra_train )\n",
    "    import time\n",
    "    from transformers import get_linear_schedule_with_warmup\n",
    "    total_steps = len(train_loader) * num_epoch\n",
    "    scheduler_cos = transformers.get_cosine_schedule_with_warmup( optimizer= optimizer, num_warmup_steps =int(total_steps * 0.1) ,num_training_steps= total_steps )\n",
    "    embedding_layer = model.transformer.wte\n",
    "    criterion = CustomLoss(num_epoch)\n",
    "    epoch_loss_log = []\n",
    "    epoch_val_log = []\n",
    "    \n",
    "    for i in range (max_train_steps):\n",
    "        criterion.update_epoch(i)\n",
    "        epoch_start_time = time.time()\n",
    "        epoch_train_loss = 0.0\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        # we use 2 schedulers - the first LR scheduler uses a cosine decay for 100 epochs the second scheduler takes the last LR from cosine scheduler and then maintains that LR for the next 10 epochs\n",
    "        if i >= num_epoch:\n",
    "            optimizer_reduced_lr = torch.optim.AdamW(params =  model.parameters(), lr= current_lr ,fused = True , weight_decay=.1)\n",
    "            scheduler_constant = transformers.get_constant_schedule_with_warmup( optimizer = optimizer_reduced_lr ,num_warmup_steps = 0, last_epoch = -1 )\n",
    "        \n",
    "        lambda_val = None\n",
    "        nll_scale = None\n",
    "        epoch_nll_max = 0\n",
    "        for ind,(input_id,attention_mask) in enumerate(train_loader):\n",
    "            if ind == int(len(train_loader)/2):\n",
    "                batch_time = time.time()\n",
    "                duration = batch_time - epoch_start_time\n",
    "                print(f\"executing epoch:{i+1}, it took {duration/60} mins from beginning of epoch till batch#{ind}\")\n",
    "            \n",
    "            ids = input_id.to(device=device, non_blocking=True)\n",
    "            att_mask = attention_mask.to(device=device, non_blocking=True)\n",
    "            labels = ids.clone()\n",
    "            assert not torch.isnan(ids).any(), \"Input contains NaN values\"\n",
    "            assert not torch.isinf(ids).any(), \"Input contains infinite values\"\n",
    "            \n",
    "            with autocast(dtype = torch.bfloat16):\n",
    "                input_embeddings = embedding_layer(ids)\n",
    "                model_output = model(input_ids = ids ,attention_mask = att_mask, labels = labels)\n",
    "                logits = model_output.logits\n",
    "                predictions = torch.argmax(logits, dim=-1)\n",
    "                prediction_embeddings = embedding_layer(predictions)\n",
    "                #print(f\"model_output.logits = {model_output.logits}|model_output.loss = {model_output.loss}\")\n",
    "                cos_sim = F.cosine_similarity(torch.squeeze(prediction_embeddings,dim = 0), torch.squeeze(input_embeddings,dim = 0), dim=1)\n",
    "                cos_loss = 1- cos_sim.mean()\n",
    "                #print(f\"inside train_vale before calc custom loss|ind = {ind}|model_output.loss = {model_output.loss}|coss_loss = {cos_loss}\")\n",
    "                #print(f\"inside train_vale before calc custom loss|ind = {ind}|model_output.loss = {model_output.loss}|coss_loss = {cos_loss}|learning rate = {scheduler_cos.get_last_lr()[0]:.3e}\")\n",
    "                if np.isnan(model_output.loss.item()):\n",
    "                    print(\"f nan values encountered..\")\n",
    "                    decoded_texts = [tokenizer.decode(ids, skip_special_tokens=True) for ids in ids]\n",
    "                    print(f\"*********$$$$$$$$$ decoded_texts = {decoded_texts}*************\")\n",
    "                total_loss,lambda_val,nll_scale = criterion(nll = model_output.loss.detach().item() , cosine_sim = cos_loss)\n",
    "                #print(f\"Current value of nll max is {epoch_nll_max}| current mini batch loss is = {model_output.loss.item()}\")\n",
    "                epoch_nll_max = max(epoch_nll_max, model_output.loss.detach().item())\n",
    "                #print(f\"batch index = {ind}|epoch_nll_max = {epoch_nll_max}\")\n",
    "\n",
    "\n",
    "            assert not np.isnan(model_output.loss.detach().item()), \"NaN value found\"\n",
    "            #total_loss = cos_loss + model_output.loss.item()\n",
    "            \n",
    "            if ind == (len(train_loader) - 1):\n",
    "                lambda_val = lambda_val\n",
    "                nll_scale = nll_scale\n",
    "                \n",
    "            \n",
    "            total_loss.backward()\n",
    "            epoch_train_loss += total_loss.detach().item()\n",
    "            norm = torch.nn.utils.clip_grad_norm(model.parameters() , 1.0)\n",
    "            if i <= num_epoch:\n",
    "                optimizer.step()\n",
    "                scheduler_cos.step()\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "            else:\n",
    "                optimizer_reduced_lr.step()\n",
    "                optimizer_reduced_lr.zero_grad(set_to_none=True)\n",
    "                scheduler_constant.step()\n",
    "                \n",
    "                                \n",
    "            del ids,att_mask,labels,input_embeddings,model_output,logits,predictions,prediction_embeddings,cos_sim,total_loss\n",
    "            \n",
    "        \n",
    "        \n",
    "        #batch processing complete \n",
    "        #print(f\"batch processing complete , before updating nll_scale,updating the nll_scale from {criterion.get_nll_scale()} to {epoch_nll_max}\")\n",
    "        criterion.update_nll_scale(epoch_nll_max)                           \n",
    "        #print(f\"batch processing complete , after updating nll_scale , value is {epoch_nll_max}\")\n",
    "        if i <= num_epoch:\n",
    "            current_lr = scheduler_cos.get_last_lr()[0]\n",
    "        #This code below compares the training loss during the current epoch with the global loss and if updates the global loss if there is an improvement.\n",
    "        # at every 4 epoch we evluate the model on the validation data or when \n",
    "        epoch_end_time = time.time()\n",
    "        epoch_durn = (epoch_end_time - epoch_start_time)\n",
    "        num_token = B*T*len(train_loader)\n",
    "        epoch_loss_log.append(epoch_train_loss)\n",
    "        if (epoch_train_loss < global_tr_loss) :\n",
    "            improvement_percent = (abs(epoch_train_loss-global_tr_loss)/global_tr_loss)*100\n",
    "            print(f\"training loss has decreased---> reducing the global loss from {global_tr_loss:.2f} to {epoch_train_loss:.2f} | throughput = {int(num_token/epoch_durn)} tokens/second | norm = {norm:.4f} | learning rate = {current_lr:.5e}\")\n",
    "            global_tr_loss = epoch_train_loss\n",
    "            print(f\" epoch= {i+1} and  train loss is {epoch_train_loss:.2f}\")\n",
    "            if (i%4 == 0 or improvement_percent > 5):\n",
    "                val_loss = eval_model(val_loader, model, lambda_val ,nll_scale,epoch = i, device = device,tokenizer = tokenizer)\n",
    "                epoch_val_log.append(val_loss)\n",
    "            \n",
    "        else:\n",
    "            print(f\"No improvement in training loss..the global training loss is -->{global_tr_loss:.2f}|epoch train_loss = {epoch_train_loss:.2f} \")\n",
    "            #print(f\" epoch= {i+1} and mean train loss is {epoch_train_loss:.2f}\")\n",
    "        \n",
    "        \n",
    "    \n",
    "    return model,epoch_loss_log_train,epoch_val_log\n",
    "        \n",
    "            \n",
    "            \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9ca13d07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inside train model. Device = cuda:0\n",
      "executing epoch:1, it took 1.553708521525065 mins from beginning of epoch till batch#525\n",
      "training loss has decreased---> reducing the global loss from inf to 3668.63 | throughput = 92247 tokens/second | norm = 0.0000 | learning rate = 2.00000e-06\n",
      " epoch= 1 and  train loss is 3668.63\n",
      "inside validation data for epoch 1\n",
      "Val loss has decreased -->reducing the global validation loss from inf to 4487.41\n",
      " validation loss for epoch = 1 is 4487.4134\n",
      " epoch= 1 :  val loss is 4487.4134 \n",
      "saving the model model2024-07-1600:46:42.pth\n",
      "*****LOGGING INFO IN gpt2_COS_SIM*lambda_random_init_wts_False_2024-07-16.log*********\n",
      "executing epoch:2, it took 1.4686357577641804 mins from beginning of epoch till batch#525\n",
      "training loss has decreased---> reducing the global loss from 3668.63 to 519.01 | throughput = 97692 tokens/second | norm = 0.0034 | learning rate = 4.00000e-06\n",
      " epoch= 2 and  train loss is 519.01\n",
      "inside validation data for epoch 2\n",
      "No improvement in validation loss-->epoch= 2 and global val loss is 4487.41|current_Val loss = 4510.581533670425\n",
      "executing epoch:3, it took 1.4704185048739116 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->519.01|epoch train_loss = 562.87 \n",
      "executing epoch:4, it took 1.4662803888320923 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->519.01|epoch train_loss = 606.04 \n",
      "executing epoch:5, it took 1.4692366083463033 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->519.01|epoch train_loss = 646.64 \n",
      "executing epoch:6, it took 1.4681453585624695 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->519.01|epoch train_loss = 686.08 \n",
      "executing epoch:7, it took 1.465334657828013 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->519.01|epoch train_loss = 725.94 \n",
      "executing epoch:8, it took 1.4687691728274028 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->519.01|epoch train_loss = 764.22 \n",
      "executing epoch:9, it took 1.4642401258150737 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->519.01|epoch train_loss = 811.67 \n",
      "executing epoch:10, it took 1.469161335627238 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->519.01|epoch train_loss = 844.18 \n",
      "executing epoch:11, it took 1.4659155090649922 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->519.01|epoch train_loss = 875.66 \n",
      "executing epoch:12, it took 1.467343513170878 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->519.01|epoch train_loss = 918.69 \n",
      "executing epoch:13, it took 1.4667426904042562 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->519.01|epoch train_loss = 948.55 \n",
      "executing epoch:14, it took 1.4658312956492106 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->519.01|epoch train_loss = 977.06 \n",
      "executing epoch:15, it took 1.4634329358736673 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->519.01|epoch train_loss = 1014.89 \n",
      "executing epoch:16, it took 1.4631748000780742 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->519.01|epoch train_loss = 1054.14 \n",
      "executing epoch:17, it took 1.4635287563006083 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->519.01|epoch train_loss = 1089.97 \n",
      "executing epoch:18, it took 1.4630460540453594 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->519.01|epoch train_loss = 1114.40 \n",
      "executing epoch:19, it took 1.46242911418279 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->519.01|epoch train_loss = 1119.24 \n",
      "executing epoch:20, it took 1.4629922350247702 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->519.01|epoch train_loss = 1103.43 \n",
      "executing epoch:21, it took 1.4623679757118224 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->519.01|epoch train_loss = 1092.90 \n",
      "executing epoch:22, it took 1.4621051907539369 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->519.01|epoch train_loss = 1073.73 \n",
      "executing epoch:23, it took 1.4617517113685607 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->519.01|epoch train_loss = 1051.17 \n",
      "executing epoch:24, it took 1.4619515736897786 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->519.01|epoch train_loss = 1035.65 \n",
      "executing epoch:25, it took 1.4624217828114827 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->519.01|epoch train_loss = 1033.88 \n",
      "executing epoch:26, it took 1.4621328314145405 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->519.01|epoch train_loss = 1020.12 \n",
      "executing epoch:27, it took 1.461225152015686 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->519.01|epoch train_loss = 1003.74 \n",
      "executing epoch:28, it took 1.4608817934989928 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->519.01|epoch train_loss = 1001.51 \n",
      "executing epoch:29, it took 1.46106059551239 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->519.01|epoch train_loss = 995.01 \n",
      "executing epoch:30, it took 1.4606696089108786 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->519.01|epoch train_loss = 986.58 \n",
      "executing epoch:31, it took 1.460653285185496 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->519.01|epoch train_loss = 984.47 \n",
      "executing epoch:32, it took 1.460073963801066 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->519.01|epoch train_loss = 983.69 \n",
      "executing epoch:33, it took 1.4603896617889405 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->519.01|epoch train_loss = 977.42 \n",
      "executing epoch:34, it took 1.459804860750834 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->519.01|epoch train_loss = 975.70 \n",
      "executing epoch:35, it took 1.45966694355011 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->519.01|epoch train_loss = 972.91 \n",
      "executing epoch:36, it took 1.4588864962259929 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->519.01|epoch train_loss = 967.37 \n",
      "executing epoch:37, it took 1.4593394954999288 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->519.01|epoch train_loss = 965.09 \n",
      "executing epoch:38, it took 1.4588018973668417 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->519.01|epoch train_loss = 962.58 \n",
      "executing epoch:39, it took 1.4588107426961263 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->519.01|epoch train_loss = 961.25 \n",
      "executing epoch:40, it took 1.4586348136266072 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->519.01|epoch train_loss = 956.35 \n",
      "executing epoch:41, it took 1.4583745360374452 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->519.01|epoch train_loss = 948.03 \n",
      "executing epoch:42, it took 1.4583417097727458 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->519.01|epoch train_loss = 946.75 \n",
      "executing epoch:43, it took 1.4585017720858255 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->519.01|epoch train_loss = 942.71 \n",
      "executing epoch:44, it took 1.4583125551541647 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->519.01|epoch train_loss = 941.71 \n",
      "executing epoch:45, it took 1.4579525589942932 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->519.01|epoch train_loss = 944.39 \n",
      "executing epoch:46, it took 1.458359678586324 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->519.01|epoch train_loss = 938.90 \n",
      "executing epoch:47, it took 1.4583746949831644 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->519.01|epoch train_loss = 932.17 \n",
      "executing epoch:48, it took 1.4585057258605958 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->519.01|epoch train_loss = 923.53 \n",
      "executing epoch:49, it took 1.4583709160486857 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->519.01|epoch train_loss = 914.90 \n",
      "executing epoch:50, it took 1.4581358472506205 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->519.01|epoch train_loss = 909.45 \n",
      "executing epoch:51, it took 1.4584091107050579 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->519.01|epoch train_loss = 900.85 \n",
      "executing epoch:52, it took 1.4579567392667134 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->519.01|epoch train_loss = 889.38 \n",
      "executing epoch:53, it took 1.4584623336791993 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->519.01|epoch train_loss = 884.92 \n",
      "executing epoch:54, it took 1.4577773531277975 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->519.01|epoch train_loss = 886.35 \n",
      "executing epoch:55, it took 1.4580267190933227 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->519.01|epoch train_loss = 877.53 \n",
      "executing epoch:56, it took 1.4570859909057616 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->519.01|epoch train_loss = 869.03 \n",
      "executing epoch:57, it took 1.4571410298347474 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->519.01|epoch train_loss = 861.13 \n",
      "executing epoch:58, it took 1.4566763599713644 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->519.01|epoch train_loss = 857.41 \n",
      "executing epoch:59, it took 1.4562716126441955 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->519.01|epoch train_loss = 853.59 \n",
      "executing epoch:60, it took 1.4565303405125936 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->519.01|epoch train_loss = 846.41 \n",
      "executing epoch:61, it took 1.4567203005154927 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->519.01|epoch train_loss = 846.20 \n",
      "executing epoch:62, it took 1.4568459391593933 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->519.01|epoch train_loss = 845.67 \n",
      "executing epoch:63, it took 1.4568289677302042 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->519.01|epoch train_loss = 843.22 \n",
      "executing epoch:64, it took 1.4571085254351297 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->519.01|epoch train_loss = 840.86 \n",
      "executing epoch:65, it took 1.4567652146021526 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->519.01|epoch train_loss = 831.78 \n",
      "executing epoch:66, it took 1.4568059682846068 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->519.01|epoch train_loss = 831.43 \n",
      "executing epoch:67, it took 1.4568636417388916 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->519.01|epoch train_loss = 827.53 \n",
      "executing epoch:68, it took 1.4566975871721903 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->519.01|epoch train_loss = 827.67 \n",
      "executing epoch:69, it took 1.4570240020751952 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->519.01|epoch train_loss = 826.60 \n",
      "executing epoch:70, it took 1.4570603052775064 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->519.01|epoch train_loss = 823.95 \n",
      "executing epoch:71, it took 1.4570352554321289 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->519.01|epoch train_loss = 823.82 \n",
      "executing epoch:72, it took 1.4569685101509093 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->519.01|epoch train_loss = 821.10 \n",
      "executing epoch:73, it took 1.4568955977757772 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->519.01|epoch train_loss = 820.37 \n",
      "executing epoch:74, it took 1.4571388840675354 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->519.01|epoch train_loss = 821.44 \n",
      "executing epoch:75, it took 1.4568614999453227 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->519.01|epoch train_loss = 819.12 \n",
      "executing epoch:76, it took 1.4568881233533224 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->519.01|epoch train_loss = 817.87 \n",
      "executing epoch:77, it took 1.4571699976921082 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->519.01|epoch train_loss = 812.46 \n",
      "executing epoch:78, it took 1.4566478252410888 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->519.01|epoch train_loss = 809.76 \n",
      "executing epoch:79, it took 1.4570146004358928 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->519.01|epoch train_loss = 810.93 \n",
      "executing epoch:80, it took 1.4574122468630473 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->519.01|epoch train_loss = 810.64 \n",
      "executing epoch:81, it took 1.4568771402041116 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->519.01|epoch train_loss = 811.76 \n",
      "executing epoch:82, it took 1.4575833082199097 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->519.01|epoch train_loss = 812.16 \n",
      "executing epoch:83, it took 1.456771695613861 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->519.01|epoch train_loss = 812.82 \n",
      "executing epoch:84, it took 1.4570602695147197 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->519.01|epoch train_loss = 812.61 \n",
      "executing epoch:85, it took 1.4569116830825806 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->519.01|epoch train_loss = 811.04 \n",
      "executing epoch:86, it took 1.4569929877916972 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->519.01|epoch train_loss = 809.20 \n",
      "executing epoch:87, it took 1.4575341860453288 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->519.01|epoch train_loss = 805.84 \n",
      "executing epoch:88, it took 1.4574242154757182 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->519.01|epoch train_loss = 803.38 \n",
      "executing epoch:89, it took 1.457296621799469 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->519.01|epoch train_loss = 803.10 \n",
      "executing epoch:90, it took 1.4569950342178344 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->519.01|epoch train_loss = 803.30 \n",
      "executing epoch:91, it took 1.4572494784990946 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->519.01|epoch train_loss = 803.32 \n",
      "executing epoch:92, it took 1.4573038895924886 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->519.01|epoch train_loss = 803.40 \n",
      "executing epoch:93, it took 1.4571976065635681 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->519.01|epoch train_loss = 803.49 \n",
      "executing epoch:94, it took 1.4577039718627929 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->519.01|epoch train_loss = 803.23 \n",
      "executing epoch:95, it took 1.4567787925402322 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->519.01|epoch train_loss = 803.15 \n",
      "executing epoch:96, it took 1.4567994912465414 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->519.01|epoch train_loss = 803.10 \n",
      "executing epoch:97, it took 1.4567636132240296 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->519.01|epoch train_loss = 802.94 \n",
      "executing epoch:98, it took 1.4571980953216552 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->519.01|epoch train_loss = 802.95 \n",
      "executing epoch:99, it took 1.4569141705830893 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->519.01|epoch train_loss = 802.92 \n",
      "executing epoch:100, it took 1.4568382461865743 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->519.01|epoch train_loss = 802.72 \n",
      "executing epoch:101, it took 1.4568512638409932 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->519.01|epoch train_loss = 802.73 \n",
      "executing epoch:102, it took 1.4568741202354432 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->519.01|epoch train_loss = 802.74 \n",
      "executing epoch:103, it took 1.4575623353322347 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->519.01|epoch train_loss = 802.76 \n",
      "executing epoch:104, it took 1.45665385723114 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->519.01|epoch train_loss = 802.74 \n",
      "executing epoch:105, it took 1.456270690759023 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->519.01|epoch train_loss = 802.76 \n",
      "executing epoch:106, it took 1.456929051876068 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->519.01|epoch train_loss = 802.78 \n",
      "executing epoch:107, it took 1.4568797588348388 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->519.01|epoch train_loss = 802.79 \n",
      "executing epoch:108, it took 1.4567067941029868 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->519.01|epoch train_loss = 802.78 \n",
      "executing epoch:109, it took 1.457015828291575 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->519.01|epoch train_loss = 802.79 \n",
      "executing epoch:110, it took 1.4568046092987061 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->519.01|epoch train_loss = 802.80 \n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'epoch_loss_log_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_19056/3695480596.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtr_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepoch_loss_log_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepoch_val_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_19056/2667146370.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(train_loader, val_loader, model, num_epoch, device, tokenizer)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepoch_loss_log_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepoch_val_log\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'epoch_loss_log_train' is not defined"
     ]
    }
   ],
   "source": [
    "tr_model,epoch_loss_log_train,epoch_val_loss = train_model(train_loader, val_loader, model =  model,tokenizer = tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e84970",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "# Write the list to a JSON file\n",
    "with open(\"epoch_loss_log_train.json\", \"w\") as file:\n",
    "    json.dump(epoch_loss_log_train, file)\n",
    "\n",
    "with open(\"epoch_val_loss.json\", \"w\") as file:\n",
    "    json.dump(epoch_val_loss, file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a954d69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(epoch_loss_log_train,epoch_val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7f4e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ids in inp:\n",
    "    print(tokenizer.decode(ids, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692a4f8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ee84f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8453d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73b92ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8806b02f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c56320",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418ac9c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893a2c01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28b9471",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
