{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72050ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "## reference https://huggingface.co/learn/nlp-course/en/chapter7/6?fw=pt#training-a-causal-language-model-from-scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad8f31d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b52ebc6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting datasets\n",
      "  Downloading datasets-2.19.1-py3-none-any.whl (542 kB)\n",
      "\u001b[K     |████████████████████████████████| 542 kB 12.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.8/site-packages (from datasets) (3.4.0)\n",
      "Collecting pyarrow>=12.0.0\n",
      "  Downloading pyarrow-16.1.0-cp38-cp38-manylinux_2_28_x86_64.whl (40.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 40.9 MB 98.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting aiohttp\n",
      "  Downloading aiohttp-3.9.5-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.3 MB 87.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting xxhash\n",
      "  Downloading xxhash-3.4.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "\u001b[K     |████████████████████████████████| 194 kB 128.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: huggingface-hub>=0.21.2 in /opt/conda/lib/python3.8/site-packages (from datasets) (0.23.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.8/site-packages (from datasets) (6.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.8/site-packages (from datasets) (2.26.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.8/site-packages (from datasets) (4.62.3)\n",
      "Collecting pyarrow-hotfix\n",
      "  Downloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
      "Collecting fsspec[http]<=2024.3.1,>=2023.1.0\n",
      "  Downloading fsspec-2024.3.1-py3-none-any.whl (171 kB)\n",
      "\u001b[K     |████████████████████████████████| 171 kB 135.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting multiprocess\n",
      "  Downloading multiprocess-0.70.16-py38-none-any.whl (132 kB)\n",
      "\u001b[K     |████████████████████████████████| 132 kB 133.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.8/site-packages (from datasets) (1.21.4)\n",
      "Collecting dill<0.3.9,>=0.3.0\n",
      "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "\u001b[K     |████████████████████████████████| 116 kB 133.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: packaging in /opt/conda/lib/python3.8/site-packages (from datasets) (21.3)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.8/site-packages (from datasets) (1.3.4)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets) (21.2.0)\n",
      "Collecting multidict<7.0,>=4.5\n",
      "  Downloading multidict-6.0.5-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
      "\u001b[K     |████████████████████████████████| 129 kB 99.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting async-timeout<5.0,>=4.0\n",
      "  Downloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Downloading frozenlist-1.4.1-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (240 kB)\n",
      "\u001b[K     |████████████████████████████████| 240 kB 136.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting yarl<2.0,>=1.0\n",
      "  Downloading yarl-1.9.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (308 kB)\n",
      "\u001b[K     |████████████████████████████████| 308 kB 118.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting aiosignal>=1.1.2\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.8/site-packages (from huggingface-hub>=0.21.2->datasets) (4.11.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging->datasets) (3.0.6)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (1.26.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (2.0.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (3.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (2021.10.8)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.8/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.8/site-packages (from pandas->datasets) (2021.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.16.0)\n",
      "Installing collected packages: multidict, frozenlist, yarl, async-timeout, aiosignal, fsspec, dill, aiohttp, xxhash, pyarrow-hotfix, pyarrow, multiprocess, datasets\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2024.5.0\n",
      "    Uninstalling fsspec-2024.5.0:\n",
      "      Successfully uninstalled fsspec-2024.5.0\n",
      "  Attempting uninstall: pyarrow\n",
      "    Found existing installation: pyarrow 5.0.0\n",
      "    Uninstalling pyarrow-5.0.0:\n",
      "      Successfully uninstalled pyarrow-5.0.0\n",
      "Successfully installed aiohttp-3.9.5 aiosignal-1.3.1 async-timeout-4.0.3 datasets-2.19.1 dill-0.3.8 frozenlist-1.4.1 fsspec-2024.3.1 multidict-6.0.5 multiprocess-0.70.16 pyarrow-16.1.0 pyarrow-hotfix-0.6 xxhash-3.4.1 yarl-1.9.4\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting jupyter\n",
      "  Downloading jupyter-1.0.0-py2.py3-none-any.whl (2.7 kB)\n",
      "Collecting ipywidgets\n",
      "  Downloading ipywidgets-8.1.3-py3-none-any.whl (139 kB)\n",
      "\u001b[K     |████████████████████████████████| 139 kB 11.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: ipykernel in /opt/conda/lib/python3.8/site-packages (from jupyter) (6.6.0)\n",
      "Collecting qtconsole\n",
      "  Downloading qtconsole-5.5.2-py3-none-any.whl (123 kB)\n",
      "\u001b[K     |████████████████████████████████| 123 kB 107.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: nbconvert in /opt/conda/lib/python3.8/site-packages (from jupyter) (6.3.0)\n",
      "Collecting jupyter-console\n",
      "  Downloading jupyter_console-6.6.3-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: notebook in /opt/conda/lib/python3.8/site-packages (from jupyter) (6.4.1)\n",
      "Collecting widgetsnbextension~=4.0.11\n",
      "  Downloading widgetsnbextension-4.0.11-py3-none-any.whl (2.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.3 MB 105.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting comm>=0.1.3\n",
      "  Downloading comm-0.2.2-py3-none-any.whl (7.2 kB)\n",
      "Collecting jupyterlab-widgets~=3.0.11\n",
      "  Downloading jupyterlab_widgets-3.0.11-py3-none-any.whl (214 kB)\n",
      "\u001b[K     |████████████████████████████████| 214 kB 99.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: traitlets>=4.3.1 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (7.30.0)\n",
      "Requirement already satisfied: backcall in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: jedi>=0.16 in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.18.1)\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (4.8.0)\n",
      "Requirement already satisfied: matplotlib-inline in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.3)\n",
      "Requirement already satisfied: pickleshare in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.7.5)\n",
      "Requirement already satisfied: setuptools>=18.5 in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (59.4.0)\n",
      "Requirement already satisfied: pygments in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (2.10.0)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.22)\n",
      "Requirement already satisfied: decorator in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (5.1.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /opt/conda/lib/python3.8/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.8/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.8/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=6.1.0->ipywidgets) (0.2.5)\n",
      "Requirement already satisfied: jupyter-client<8.0 in /opt/conda/lib/python3.8/site-packages (from ipykernel->jupyter) (7.1.0)\n",
      "Requirement already satisfied: tornado<7.0,>=4.2 in /opt/conda/lib/python3.8/site-packages (from ipykernel->jupyter) (6.1)\n",
      "Requirement already satisfied: debugpy<2.0,>=1.0.0 in /opt/conda/lib/python3.8/site-packages (from ipykernel->jupyter) (1.5.1)\n",
      "Requirement already satisfied: entrypoints in /opt/conda/lib/python3.8/site-packages (from jupyter-client<8.0->ipykernel->jupyter) (0.3)\n",
      "Requirement already satisfied: pyzmq>=13 in /opt/conda/lib/python3.8/site-packages (from jupyter-client<8.0->ipykernel->jupyter) (22.3.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /opt/conda/lib/python3.8/site-packages (from jupyter-client<8.0->ipykernel->jupyter) (2.8.2)\n",
      "Requirement already satisfied: nest-asyncio>=1.5 in /opt/conda/lib/python3.8/site-packages (from jupyter-client<8.0->ipykernel->jupyter) (1.5.4)\n",
      "Requirement already satisfied: jupyter-core>=4.6.0 in /opt/conda/lib/python3.8/site-packages (from jupyter-client<8.0->ipykernel->jupyter) (4.9.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.8/site-packages (from python-dateutil>=2.1->jupyter-client<8.0->ipykernel->jupyter) (1.16.0)\n",
      "Collecting prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0\n",
      "  Downloading prompt_toolkit-3.0.45-py3-none-any.whl (386 kB)\n",
      "\u001b[K     |████████████████████████████████| 386 kB 118.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting jupyter-core>=4.6.0\n",
      "  Downloading jupyter_core-5.7.2-py3-none-any.whl (28 kB)\n",
      "Collecting ipykernel\n",
      "  Downloading ipykernel-6.29.4-py3-none-any.whl (117 kB)\n",
      "\u001b[K     |████████████████████████████████| 117 kB 128.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting traitlets>=4.3.1\n",
      "  Downloading traitlets-5.14.3-py3-none-any.whl (85 kB)\n",
      "\u001b[K     |████████████████████████████████| 85 kB 95.6 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: psutil in /opt/conda/lib/python3.8/site-packages (from ipykernel->jupyter) (5.8.0)\n",
      "Collecting debugpy<2.0,>=1.0.0\n",
      "  Downloading debugpy-1.8.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.1 MB 128.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: packaging in /opt/conda/lib/python3.8/site-packages (from ipykernel->jupyter) (21.3)\n",
      "Collecting pyzmq>=13\n",
      "  Downloading pyzmq-26.0.3-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (912 kB)\n",
      "\u001b[K     |████████████████████████████████| 912 kB 151.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting platformdirs>=2.5\n",
      "  Downloading platformdirs-4.2.2-py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: bleach in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (4.1.0)\n",
      "Requirement already satisfied: testpath in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (0.5.0)\n",
      "Requirement already satisfied: jupyterlab-pygments in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (0.1.2)\n",
      "Requirement already satisfied: mistune<2,>=0.8.1 in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (0.8.4)\n",
      "Requirement already satisfied: defusedxml in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (0.7.1)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (1.5.0)\n",
      "Requirement already satisfied: nbclient<0.6.0,>=0.5.0 in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (0.5.9)\n",
      "Requirement already satisfied: jinja2>=2.4 in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (3.0.3)\n",
      "Requirement already satisfied: nbformat>=4.4 in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (5.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.8/site-packages (from jinja2>=2.4->nbconvert->jupyter) (2.0.1)\n",
      "Requirement already satisfied: ipython-genutils in /opt/conda/lib/python3.8/site-packages (from nbformat>=4.4->nbconvert->jupyter) (0.2.0)\n",
      "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /opt/conda/lib/python3.8/site-packages (from nbformat>=4.4->nbconvert->jupyter) (4.2.1)\n",
      "Requirement already satisfied: importlib-resources>=1.4.0 in /opt/conda/lib/python3.8/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.4->nbconvert->jupyter) (5.4.0)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /opt/conda/lib/python3.8/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.4->nbconvert->jupyter) (0.18.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /opt/conda/lib/python3.8/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.4->nbconvert->jupyter) (21.2.0)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /opt/conda/lib/python3.8/site-packages (from importlib-resources>=1.4.0->jsonschema!=2.5.0,>=2.4->nbformat>=4.4->nbconvert->jupyter) (3.6.0)\n",
      "Requirement already satisfied: webencodings in /opt/conda/lib/python3.8/site-packages (from bleach->nbconvert->jupyter) (0.5.1)\n",
      "Requirement already satisfied: Send2Trash>=1.5.0 in /opt/conda/lib/python3.8/site-packages (from notebook->jupyter) (1.8.0)\n",
      "Requirement already satisfied: prometheus-client in /opt/conda/lib/python3.8/site-packages (from notebook->jupyter) (0.12.0)\n",
      "Requirement already satisfied: argon2-cffi in /opt/conda/lib/python3.8/site-packages (from notebook->jupyter) (21.1.0)\n",
      "Requirement already satisfied: terminado>=0.8.3 in /opt/conda/lib/python3.8/site-packages (from notebook->jupyter) (0.12.1)\n",
      "Requirement already satisfied: cffi>=1.0.0 in /opt/conda/lib/python3.8/site-packages (from argon2-cffi->notebook->jupyter) (1.15.0)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.8/site-packages (from cffi>=1.0.0->argon2-cffi->notebook->jupyter) (2.21)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging->ipykernel->jupyter) (3.0.6)\n",
      "Collecting qtpy>=2.4.0\n",
      "  Downloading QtPy-2.4.1-py3-none-any.whl (93 kB)\n",
      "\u001b[K     |████████████████████████████████| 93 kB 89.1 MB/s  eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: traitlets, platformdirs, pyzmq, jupyter-core, prompt-toolkit, debugpy, comm, widgetsnbextension, qtpy, jupyterlab-widgets, ipykernel, qtconsole, jupyter-console, ipywidgets, jupyter\n",
      "  Attempting uninstall: traitlets\n",
      "    Found existing installation: traitlets 5.1.1\n",
      "    Uninstalling traitlets-5.1.1:\n",
      "      Successfully uninstalled traitlets-5.1.1\n",
      "  Attempting uninstall: pyzmq\n",
      "    Found existing installation: pyzmq 22.3.0\n",
      "    Uninstalling pyzmq-22.3.0:\n",
      "      Successfully uninstalled pyzmq-22.3.0\n",
      "  Attempting uninstall: jupyter-core\n",
      "    Found existing installation: jupyter-core 4.9.1\n",
      "    Uninstalling jupyter-core-4.9.1:\n",
      "      Successfully uninstalled jupyter-core-4.9.1\n",
      "  Attempting uninstall: prompt-toolkit\n",
      "    Found existing installation: prompt-toolkit 3.0.22\n",
      "    Uninstalling prompt-toolkit-3.0.22:\n",
      "      Successfully uninstalled prompt-toolkit-3.0.22\n",
      "  Attempting uninstall: debugpy\n",
      "    Found existing installation: debugpy 1.5.1\n",
      "    Uninstalling debugpy-1.5.1:\n",
      "      Successfully uninstalled debugpy-1.5.1\n",
      "  Attempting uninstall: ipykernel\n",
      "    Found existing installation: ipykernel 6.6.0\n",
      "    Uninstalling ipykernel-6.6.0:\n",
      "      Successfully uninstalled ipykernel-6.6.0\n",
      "Successfully installed comm-0.2.2 debugpy-1.8.1 ipykernel-6.29.4 ipywidgets-8.1.3 jupyter-1.0.0 jupyter-console-6.6.3 jupyter-core-5.7.2 jupyterlab-widgets-3.0.11 platformdirs-4.2.2 prompt-toolkit-3.0.45 pyzmq-26.0.3 qtconsole-5.5.2 qtpy-2.4.1 traitlets-5.14.3 widgetsnbextension-4.0.11\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#installing some libraries\n",
    "!pip install datasets\n",
    "!pip install --upgrade jupyter ipywidgets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6974d135",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch, transformers\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import PreTrainedModel, PretrainedConfig\n",
    "from transformers import AutoModel, AutoConfig,AutoModelForCausalLM\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "#from datasets import load_dataset\n",
    "import pandas as pd, numpy as np\n",
    "from torch import cuda\n",
    "import datetime\n",
    "import warnings,itertools\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "# Ignore all warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "#pip install transformers bitsandbytes>=0.39.0 -q\n",
    "import zipfile\n",
    "import os\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43b19e79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "model\n"
     ]
    }
   ],
   "source": [
    "#global params for training\n",
    "\n",
    "batch_size = 32\n",
    "epoch = 100\n",
    "min_text_len = 0\n",
    "if cuda.is_available():\n",
    "    device = torch.device('cuda:0')\n",
    "    print(device)\n",
    "else:\n",
    "    device = 'cpu'\n",
    "#print(device)\n",
    "\n",
    "#os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "#os.environ[\"MKL_DEBUG_CPU_TYPE\"] = \"5\"\n",
    "context_length = None\n",
    "global_tr_loss = torch.inf\n",
    "global_val_loss = torch.inf\n",
    "#print(global_tr_loss)\n",
    "model_path = os.path.join(\"model\")\n",
    "\n",
    "directory = os.path.join('.','data','unzip_text_10M')  # Replace with your directory path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4df42a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./data/unzip_text_10M'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f8c1550d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataset_pyt(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_length = context_length ):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "                                \n",
    "    def __getitem__(self, idx):\n",
    "        #print(f\"inside loader...idx ->{idx}\")\n",
    "        text = self.df.iloc[idx]['text']\n",
    "        #print(f\"length of text ->{len(text)}\")\n",
    "        #print(f\"text ->{text}\")\n",
    "        #encodings = tokenizer(text, truncation=True, max_length= self.max_length, return_overflowing_tokens=True, padding = 'max_length',return_tensors='pt')\n",
    "        encodings = tokenizer(text, truncation=True, max_length= self.max_length, return_overflowing_tokens=True, padding = False)\n",
    "        # check the length of the encoded list\n",
    "        \n",
    "        #x_dict['input_id'] = input_ids_list\n",
    "        #x_dict['attention_mask'] = input_ids_list\n",
    "                \n",
    "        #print(f\"x_dict = {x_dict}\")             \n",
    "#       print(f\"inside the loader and input_id = {input_ids} and its shape is {input_ids.shape}\")\n",
    "        #labels = input_id_list\n",
    "        #input_ids = torch.tensor(input_id_list)\n",
    "        #attention_mask = torch.tensor(attention_mask_list)\n",
    "        #labels = torch.tensor(labels)\n",
    "        #print(f\"inside the loader and input_id shape= {input_ids.shape} attention_mask_shape is {attention_mask.shape} and label shape is {labels.shape}\")\n",
    "        #print(f\"encoding = {encodings}\")\n",
    "               \n",
    "        return encodings\n",
    "        \n",
    "    def __len__(self):\n",
    "        #return the length of the dataframe\n",
    "        return len(self.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0e12bdb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate_fn(batch, ):\n",
    "    x_dict = {}\n",
    "    #print(\"CUSTOM COllate\")\n",
    "    #print(f\"bacth = {batch}\")\n",
    "    input_ids_list = []\n",
    "    att_mask_list = [] \n",
    "    for elem,item in enumerate(batch):\n",
    "        #print(f\"element {elem}\")\n",
    "        #print(f\"item = {item}\")\n",
    "        #check whether there are any nested lists :\n",
    "        if len(item['input_ids']) > 1:\n",
    "            #print(f\"flattening the list\")\n",
    "            input_id_tensor = torch.tensor(list(itertools.chain(*item['input_ids'])))\n",
    "            #print(f\"shape of the flattended tensor = {input_id_tensor.shape}\")\n",
    "            input_ids_list.append(input_id_tensor[:1024])\n",
    "            att_mask_tensor = torch.tensor(list(itertools.chain(*item['attention_mask'])))\n",
    "            att_mask_list.append(att_mask_tensor[:1024])\n",
    "        else:\n",
    "            input_id_tensor = torch.tensor(item['input_ids']).squeeze(0)\n",
    "            input_ids_list.append(input_id_tensor)\n",
    "            att_mask_tensor = torch.tensor(item['attention_mask']).squeeze(0)\n",
    "            #print(f\"shape of att_mask_tensor tensor = {att_mask_tensor.shape}\")\n",
    "            att_mask_list.append(att_mask_tensor)\n",
    "    #attention_mask = [item['attention_mask'].squeeze(0) for item in batch]\n",
    "\n",
    "    # Pad sequences to the same length\n",
    "    #print(f\"len of input_id_list = {len(input_ids_list)}\")\n",
    "    #print(f\"len of attmask_list = {len(att_mask_list)}\")\n",
    "    #input_id_tensor = torch.tensor(input_ids_list).squeeze()\n",
    "    #att_mask_tensor = torch.tensor(att_mask_list).squeeze()\n",
    "    #print(\"*********************\")\n",
    "    #print(f\"input_ids_list = {input_ids_list}\")\n",
    "    \n",
    "        \n",
    "    \n",
    "    input_ids = torch.nn.utils.rnn.pad_sequence(input_ids_list, batch_first=True, padding_value=tokenizer.eos_token_id)\n",
    "    attention_mask = torch.nn.utils.rnn.pad_sequence(att_mask_list, batch_first=True, padding_value=0)\n",
    "    #print(f\"shape of input_id tensor post padding -{input_ids.shape}\")\n",
    "    #print(f\"shape of attention_masks tensor post padding -{attention_mask.shape}\")\n",
    "    x_dict['input_ids'] = input_ids\n",
    "    x_dict['attention_mask'] = attention_mask\n",
    "    \n",
    "    return x_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "858d2bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad\n",
    "def eval_model(val_loader, model, epoch , device = device,):\n",
    "    global global_val_loss\n",
    "    #m = nn.Softmax()\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    e = epoch+1\n",
    "    val_loss_list = []\n",
    "    #criterion = torch.nn.BCEWithLogitsLoss()\n",
    "    print(f\"inside validation data for epoch {e}\")\n",
    "    #y_hat_val_list = []\n",
    "    #y_val_list = []\n",
    "    \n",
    "    for ind,x_dict  in enumerate(val_loader):\n",
    "        id_list = x_dict['input_ids']\n",
    "        #print(f\"id_list{id_list}\")\n",
    "        ids = id_list.clone().detach().to(device = device)\n",
    "        att_list = x_dict['attention_mask']\n",
    "        att_mask = att_list.clone().detach().to(device= device)\n",
    "        labels = ids.clone().detach().to(device = device)\n",
    "        #predictions\n",
    "        #print(f\"input_ids device = {input_ids.device}\")\n",
    "        with autocast():\n",
    "            model_output = model(input_ids = ids ,attention_mask = att_mask, labels = labels)\n",
    "            act_loss = model_output.loss\n",
    "        \n",
    "        val_loss_list.append(act_loss)\n",
    "                    \n",
    "    mean_val_loss = torch.mean(torch.tensor(val_loss_list))\n",
    "    if mean_val_loss < global_val_loss:\n",
    "        print(f\"Val loss has decreased -->reducing the global validation loss from {global_val_loss:.2f} to {mean_val_loss:.2f}\")\n",
    "        global_val_loss = mean_val_loss\n",
    "        print(f\" validation loss for epoch = {e} is {torch.mean(torch.tensor(val_loss_list)):.4f}\")\n",
    "        #print metrics and save the model\n",
    "        #y_hat_val = torch.cat(y_hat_val_list)\n",
    "        #y_val = torch.cat(y_val_list)\n",
    "        #acc_val = accuracy_score(y_val.cpu().numpy(), y_hat_val.cpu().numpy())\n",
    "        #f1_val = f1_score(y_val.cpu().numpy(), y_hat_val.cpu().numpy(), average='micro')\n",
    "        print(f\" epoch= {e} : mean val loss is {torch.mean(torch.tensor(mean_val_loss)):.4f} \")\n",
    "        #save the model\n",
    "        \n",
    "        # Get the current date and time\n",
    "        current_datetime = datetime.datetime.now()\n",
    "        # Extract date and time components\n",
    "        current_date = str(current_datetime.date())\n",
    "        current_time = str(current_datetime.time()).split('.')[0]\n",
    "        file_name = 'model'+ current_date+current_time+'_'+'.pth'\n",
    "        path = os.path.join(\"model\",file_name)\n",
    "        print(f\"saving the model {file_name}\")\n",
    "        #torch.save(model.state_dict(), path)\n",
    "        model.save_pretrained(path)\n",
    "        \n",
    "        #plot_confusion_matrix(y_val.cpu().numpy(), y_hat_val.cpu().numpy(), labels)\n",
    "    else:\n",
    "        print(f\"No improvement in validation loss-->epoch= {e} and global val loss is {global_val_loss:.2f}\")\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d241cef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_loader,val_loader,model,num_epoch = 10,device = device):\n",
    "    global global_tr_loss\n",
    "    scaler = GradScaler()\n",
    "    model.train()\n",
    "    device = device\n",
    "    print(f\"inside train model. Device = {device}\")\n",
    "    optimizer = torch.optim.AdamW(params =  model.parameters(), lr= 5e-5)\n",
    "    model.to(device)\n",
    "    #m = nn.Softmax()\n",
    "    import time\n",
    "    epoch_start_time = time.time()\n",
    "    from transformers import get_linear_schedule_with_warmup\n",
    "    scheduler = transformers.get_cosine_schedule_with_warmup( optimizer= optimizer, num_warmup_steps =len(train_loader)*num_epoch*.1 ,num_training_steps= len(train_loader)*num_epoch,last_epoch = -1 )\n",
    "    \n",
    "    for i in range (num_epoch):\n",
    "        #y_hat_list =[]\n",
    "        #label_list = []\n",
    "        epoch_train_loss = []\n",
    "        for ind,x_dict in enumerate(train_loader):\n",
    "            #print(f\"x_dict = {x_dict}\")\n",
    "            id_list = x_dict['input_ids']\n",
    "            #print(f\"id_list{id_list}\")\n",
    "            if ind%10000 == 0:\n",
    "                batch_time = time.time()\n",
    "                duration = batch_time - epoch_start_time\n",
    "                print(f\"executing epoch:{i+1}, it took {duration/60} mins from beginning of epoch till batch#{ind}\")\n",
    "            \n",
    "            \n",
    "            ids = id_list.clone().detach().to(device = device)\n",
    "            att_list = x_dict['attention_mask']\n",
    "            att_mask = att_list.clone().detach().to(device= device)\n",
    "            labels = ids.clone().detach().to(device = device)\n",
    "            #predictions\n",
    "            #print(f\"shape of ids = {ids.shape} and shape of att_mask = {att_mask.shape}\")\n",
    "            \n",
    "            with autocast():\n",
    "                model_output = model(input_ids = ids ,attention_mask = att_mask, labels = labels)\n",
    "                act_loss = model_output.loss\n",
    "            \n",
    "            #print(f\"model_output->{model_output}\")\n",
    "            #probs = m(logits)\n",
    "            #y_hat_list.append(torch.argmax(probs , dim = 1))\n",
    "            #label_list.append(torch.argmax(lab, dim = 1))\n",
    "            \n",
    "            #loss calculation                   \n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            scaler.scale(act_loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            epoch_train_loss.append(act_loss)\n",
    "            #print(f\"current LR->{scheduler.get_last_lr()}\")\n",
    "            scheduler.step()\n",
    "            del id_list,ids,att_list,att_mask,labels\n",
    "            \n",
    "        #batch processing complete    \n",
    "        mean_loss = torch.mean(torch.tensor(epoch_train_loss))\n",
    "        \n",
    "        if mean_loss < global_tr_loss:\n",
    "            print(f\"training loss has decreased---> reducing the global loss from {global_tr_loss:.2f} to {mean_loss:.2f}\")\n",
    "            global_tr_loss = mean_loss\n",
    "            print(f\" epoch= {i+1} and mean train loss is {torch.mean(torch.tensor(epoch_train_loss)):.2f}\")\n",
    "            #printing training metrices\n",
    "            #y_hat = torch.cat(y_hat_list)\n",
    "            #y = torch.cat(label_list)\n",
    "            #acc = accuracy_score(y.cpu().numpy(), y_hat.cpu().numpy())\n",
    "            #f1 = f1_score(y.cpu().numpy(), y_hat.cpu().numpy(), average='micro')\n",
    "            print(f\" epoch= {i+1} : mean train loss is {torch.mean(torch.tensor(epoch_train_loss)):.4f} \")\n",
    "            #checking validation metrices\n",
    "            eval_model(val_loader, model, epoch = i , device = device)\n",
    "            \n",
    "        else:\n",
    "            print(f\"No improvement in training loss..the global training loss is -->{global_tr_loss:.2f} \")\n",
    "            print(f\" epoch= {i+1} and mean train loss is {torch.mean(torch.tensor(epoch_train_loss)):.2f}\")\n",
    "        \n",
    "        \n",
    "    \n",
    "    return model\n",
    "        \n",
    "            \n",
    "            \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f548944",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_text(directory):\n",
    "    directory = os.path.join('.','data','unzip_text_10M',str(directory))  # Replace with your directory path\n",
    "    print(f\"directory :{directory}\")\n",
    "    # List all files in the directory\n",
    "    files = [f for f in os.listdir(directory) if os.path.isfile(os.path.join(directory, f))]\n",
    "    print(f\"files:{files}\")\n",
    "    text_content = []\n",
    "    # Read each file\n",
    "    total_lines = 0\n",
    "    for filenum,filename in enumerate(files):\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            #first_line = file.read()\n",
    "            #print(f\"filename :{filename}->first few lines {first_line}\")\n",
    "            #continue\n",
    "            lines_list = [line.strip() for line in open(file_path, 'r')]\n",
    "            print(f\"the file:{filename} added {len(lines_list)} rows to the list\")\n",
    "            total_lines+=len(lines_list)\n",
    "            text_content.append(lines_list)\n",
    "    \n",
    "    flattened_list = list(itertools.chain(*text_content))\n",
    "    assert (len(flattened_list) == total_lines , f\"Expected {len(flattened_list)} to be equal to {total_lines}\" )\n",
    "    \n",
    "    return flattened_list\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "abcf1ce1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_75/2839251141.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mread_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"train_10M\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mread_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"dev\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#let's create a new column called 'length' on our dataframe to analyze the text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "df_train = pd.DataFrame(read_text(\"train_10M\"), columns=['text'])\n",
    "df_val = pd.DataFrame(read_text(\"dev\"), columns=['text'])\n",
    "\n",
    "#let's create a new column called 'length' on our dataframe to analyze the text\n",
    "\n",
    "df_train['length'] = df_train['text'].apply(lambda x: len(x))\n",
    "df_val['length'] = df_val['text'].apply(lambda x: len(x))\n",
    "df_train.head()\n",
    "\n",
    "print(f\"the range of length in the train set is {max(df_train.length)} down to {min(df_train.length)}\")\n",
    "\n",
    "## Lets check the the distributions\n",
    "\n",
    "sns.histplot(df_train['length'], bins='auto')\n",
    "plt.title('Histogram of text lengths')\n",
    "plt.show()\n",
    "array = np.arange(0, 2000, step=100)\n",
    "\n",
    "sns.histplot(df_train['length'], bins= array)\n",
    "plt.title('Histogram with length of text from 0 to  Model max capacity')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Let us check the number of rows whose length > 1024(the defualt length that the tokenizer can process)\n",
    "exceed_tok_len = sum(df_train['length']> 1024)/len(df_train)*100\n",
    "print(f\"fewer than {exceed_tok_len}% of rows have text more than the length of model\")\n",
    "\n",
    "#Lets check for the number of rows where length is 0\n",
    "sum(df_train['length'] == min_text_len)\n",
    "print(f\"training dataframe has {sum(df_train['length'] == min_text_len)} rows with {min_text_len} text length\")\n",
    "\n",
    "#Remoing the rows with min_len text\n",
    "\n",
    "df_train = df_train[df_train['length'].astype(bool)]\n",
    "df_val = df_val[df_val['length'].astype(bool)]\n",
    "\n",
    "#assert statement here to check if there are still any rows still with 0 text\n",
    "assert sum(df_train['length'] == min_text_len) , 0 , f\" there are still rows with {min_text_len} left\"\n",
    "\n",
    "\n",
    "# resetting the index:\n",
    "df_train.reset_index(drop=True, inplace=True)\n",
    "df_val.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# calculate the mean text length so that you can define the context length for the tokenizer\n",
    "#calc the average len of the text:\n",
    "mean_len = int(df_train.length.mean())\n",
    "print(f\"The average length of the text is {mean_len}\")\n",
    "power = math.ceil(math.log2(mean_len))\n",
    "print(power)\n",
    "context_length = 2**power\n",
    "context_length\n",
    "print(f\"The context length is {context_length} \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1fe3c486",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.010310888290405273,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "tokenizer_config.json",
       "rate": null,
       "total": 26,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0fb12a22d63431e83087ff48309ae88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.00988316535949707,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "config.json",
       "rate": null,
       "total": 762,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "851f95909a45476ba246ade538789b42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/762 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.00979161262512207,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "vocab.json",
       "rate": null,
       "total": 1042301,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3391e6fd7aed4e8fb8d761135bfd8d3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.010237932205200195,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "merges.txt",
       "rate": null,
       "total": 456318,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c0d43c222ac48bab5d1511f3f80e4e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.009854555130004883,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "tokenizer.json",
       "rate": null,
       "total": 1355256,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c56442dd72b544b881725024f425f047",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.01070713996887207,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "config.json",
       "rate": null,
       "total": 762,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6852be3e65fa4394af223b9dc684d702",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/762 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.010078907012939453,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "model.safetensors",
       "rate": null,
       "total": 352824413,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f2082624d1a4b43b2b4297b3e8119b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/353M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.011424064636230469,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "generation_config.json",
       "rate": null,
       "total": 124,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11ec94c78c634d52b67ca607ddf66cf5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test the tokenizer:\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilgpt2\",return_tensors = \"pt\" , truncate = True, max_length  = context_length ,return_overflowing_tokens=True , padding = False)\n",
    "model = AutoModelForCausalLM.from_pretrained(\"distilgpt2\")\n",
    "tokenizer.save_pretrained(\"./model\")\n",
    "train_dataset = dataset_pyt(df_train,tokenizer = tokenizer)\n",
    "val_dataset = dataset_pyt(df_val,tokenizer = tokenizer)\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset,batch_size = batch_size, shuffle = True , num_workers = 4, pin_memory = True, collate_fn = custom_collate_fn)\n",
    "val_loader = DataLoader(val_dataset,batch_size = batch_size, shuffle = True , collate_fn = custom_collate_fn)\n",
    "\n",
    "tr_model = train_model(train_loader, val_loader, model =  model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b7e10801",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff69ccc1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e4144f62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inside train model. Device = cuda:0\n",
      "executing epoch:1, it took 0.0028143723805745444 mins from beginning of epoch till batch#0\n",
      "executing epoch:1, it took 6.751577790578207 mins from beginning of epoch till batch#10000\n",
      "executing epoch:1, it took 13.414709762732189 mins from beginning of epoch till batch#20000\n",
      "executing epoch:1, it took 20.118542404969535 mins from beginning of epoch till batch#30000\n",
      "training loss has decreased---> reducing the global loss from inf to 0.77\n",
      " epoch= 1 and mean train loss is 0.77\n",
      " epoch= 1 : mean train loss is 0.7665 \n",
      "inside validation data for epoch 1\n",
      "Val loss has decreased -->reducing the global validation loss from inf to 0.65\n",
      " validation loss for epoch = 1 is 0.6494\n",
      " epoch= 1 : mean val loss is 0.6494 \n",
      "saving the model model2024-05-2922:55:57_.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "executing epoch:2, it took 33.867545727888746 mins from beginning of epoch till batch#0\n",
      "executing epoch:2, it took 40.51047882239024 mins from beginning of epoch till batch#10000\n",
      "executing epoch:2, it took 47.1551540851593 mins from beginning of epoch till batch#20000\n",
      "executing epoch:2, it took 53.853352069854736 mins from beginning of epoch till batch#30000\n",
      "training loss has decreased---> reducing the global loss from 0.77 to 0.60\n",
      " epoch= 2 and mean train loss is 0.60\n",
      " epoch= 2 : mean train loss is 0.6044 \n",
      "inside validation data for epoch 2\n",
      "Val loss has decreased -->reducing the global validation loss from 0.65 to 0.64\n",
      " validation loss for epoch = 2 is 0.6427\n",
      " epoch= 2 : mean val loss is 0.6427 \n",
      "saving the model model2024-05-2923:30:09_.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "executing epoch:3, it took 68.10993497371673 mins from beginning of epoch till batch#0\n",
      "executing epoch:3, it took 74.75176033178965 mins from beginning of epoch till batch#10000\n",
      "executing epoch:3, it took 81.40756795008977 mins from beginning of epoch till batch#20000\n",
      "executing epoch:3, it took 88.12616647879283 mins from beginning of epoch till batch#30000\n",
      "training loss has decreased---> reducing the global loss from 0.60 to 0.58\n",
      " epoch= 3 and mean train loss is 0.58\n",
      " epoch= 3 : mean train loss is 0.5785 \n",
      "inside validation data for epoch 3\n",
      "No improvement in validation loss-->epoch= 3 and global val loss is 0.64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "executing epoch:4, it took 102.29235746860505 mins from beginning of epoch till batch#0\n",
      "executing epoch:4, it took 108.97194888989131 mins from beginning of epoch till batch#10000\n",
      "executing epoch:4, it took 115.68814454476039 mins from beginning of epoch till batch#20000\n",
      "executing epoch:4, it took 122.3911456545194 mins from beginning of epoch till batch#30000\n",
      "training loss has decreased---> reducing the global loss from 0.58 to 0.55\n",
      " epoch= 4 and mean train loss is 0.55\n",
      " epoch= 4 : mean train loss is 0.5546 \n",
      "inside validation data for epoch 4\n",
      "No improvement in validation loss-->epoch= 4 and global val loss is 0.64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "executing epoch:5, it took 136.59514906803767 mins from beginning of epoch till batch#0\n",
      "executing epoch:5, it took 143.27317264080048 mins from beginning of epoch till batch#10000\n",
      "executing epoch:5, it took 149.99504707256952 mins from beginning of epoch till batch#20000\n",
      "executing epoch:5, it took 156.67467699050903 mins from beginning of epoch till batch#30000\n",
      "training loss has decreased---> reducing the global loss from 0.55 to 0.54\n",
      " epoch= 5 and mean train loss is 0.54\n",
      " epoch= 5 : mean train loss is 0.5356 \n",
      "inside validation data for epoch 5\n",
      "No improvement in validation loss-->epoch= 5 and global val loss is 0.64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "executing epoch:6, it took 170.91215680440266 mins from beginning of epoch till batch#0\n",
      "executing epoch:6, it took 177.62393314441044 mins from beginning of epoch till batch#10000\n",
      "executing epoch:6, it took 184.3071643948555 mins from beginning of epoch till batch#20000\n",
      "executing epoch:6, it took 190.91964925527571 mins from beginning of epoch till batch#30000\n",
      "training loss has decreased---> reducing the global loss from 0.54 to 0.52\n",
      " epoch= 6 and mean train loss is 0.52\n",
      " epoch= 6 : mean train loss is 0.5164 \n",
      "inside validation data for epoch 6\n",
      "No improvement in validation loss-->epoch= 6 and global val loss is 0.64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "executing epoch:7, it took 205.0955301562945 mins from beginning of epoch till batch#0\n",
      "executing epoch:7, it took 211.7667381922404 mins from beginning of epoch till batch#10000\n",
      "executing epoch:7, it took 218.44795283873876 mins from beginning of epoch till batch#20000\n",
      "executing epoch:7, it took 225.14807022412617 mins from beginning of epoch till batch#30000\n",
      "training loss has decreased---> reducing the global loss from 0.52 to 0.50\n",
      " epoch= 7 and mean train loss is 0.50\n",
      " epoch= 7 : mean train loss is 0.4993 \n",
      "inside validation data for epoch 7\n",
      "No improvement in validation loss-->epoch= 7 and global val loss is 0.64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "executing epoch:8, it took 239.36520983775458 mins from beginning of epoch till batch#0\n",
      "executing epoch:8, it took 246.0550956606865 mins from beginning of epoch till batch#10000\n",
      "executing epoch:8, it took 252.6642674724261 mins from beginning of epoch till batch#20000\n",
      "executing epoch:8, it took 259.3894014596939 mins from beginning of epoch till batch#30000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss has decreased---> reducing the global loss from 0.50 to 0.48\n",
      " epoch= 8 and mean train loss is 0.48\n",
      " epoch= 8 : mean train loss is 0.4842 \n",
      "inside validation data for epoch 8\n",
      "No improvement in validation loss-->epoch= 8 and global val loss is 0.64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "executing epoch:9, it took 273.5850135445595 mins from beginning of epoch till batch#0\n",
      "executing epoch:9, it took 280.2811160683632 mins from beginning of epoch till batch#10000\n",
      "executing epoch:9, it took 286.92080185413363 mins from beginning of epoch till batch#20000\n",
      "executing epoch:9, it took 293.5819669405619 mins from beginning of epoch till batch#30000\n",
      "training loss has decreased---> reducing the global loss from 0.48 to 0.47\n",
      " epoch= 9 and mean train loss is 0.47\n",
      " epoch= 9 : mean train loss is 0.4723 \n",
      "inside validation data for epoch 9\n",
      "No improvement in validation loss-->epoch= 9 and global val loss is 0.64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "executing epoch:10, it took 307.82337380250294 mins from beginning of epoch till batch#0\n",
      "executing epoch:10, it took 314.5148380359014 mins from beginning of epoch till batch#10000\n",
      "executing epoch:10, it took 321.20669003327686 mins from beginning of epoch till batch#20000\n",
      "executing epoch:10, it took 327.88360801140465 mins from beginning of epoch till batch#30000\n",
      "training loss has decreased---> reducing the global loss from 0.47 to 0.47\n",
      " epoch= 10 and mean train loss is 0.47\n",
      " epoch= 10 : mean train loss is 0.4651 \n",
      "inside validation data for epoch 10\n",
      "No improvement in validation loss-->epoch= 10 and global val loss is 0.64\n"
     ]
    }
   ],
   "source": [
    "# config = MyConfig(42)\n",
    "# model = MyModel(config)\n",
    "\n",
    "#train_loader,optimizer,val_loader ,num_epoch = 100, model = clf_model()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0074ab0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3b6e45d7",
   "metadata": {},
   "source": [
    "### Evaluating the models on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "772c2882",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'raw_datasets' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1444/1072214876.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m tokenized_datasets = raw_datasets.map(\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mtokenize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatched\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremove_columns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mraw_datasets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumn_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'raw_datasets' is not defined"
     ]
    }
   ],
   "source": [
    "def tokenize(element):\n",
    "    outputs = tokenizer(\n",
    "        element[\"content\"],\n",
    "        truncation=True,\n",
    "        max_length=context_length,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_length=True,\n",
    "    )\n",
    "    input_batch = []\n",
    "    for length, input_ids in zip(outputs[\"length\"], outputs[\"input_ids\"]):\n",
    "        if length == context_length:\n",
    "            input_batch.append(input_ids)\n",
    "    return {\"input_ids\": input_batch}\n",
    "\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(\n",
    "    tokenize, batched=True, remove_columns=raw_datasets[\"train\"].column_names\n",
    ")\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74abcd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = df_train.iloc[400327]['text']\n",
    "print (t)\n",
    "print(len(t))\n",
    "encoded = tokenizer(t, truncation=True, max_length=context_length, return_overflowing_tokens=True, padding = 'max_length',return_length= True)\n",
    "print(encoded)\n",
    "input_ids = torch.tensor(encoded['input_ids'])\n",
    "print(f\"inside the loader and input_id = {input_ids} and its shape is {input_ids.shape}\")\n",
    "\n",
    "attention_mask = torch.tensor(encoded['attention_mask'])\n",
    "#       print(f\"inside the loader and input_id = {input_ids} and its shape is {input_ids.shape}\")\n",
    "labels = input_ids.clone()\n",
    "print(f\"inside the loader and input_id shape= {input_ids.shape} attention_mask_shape is {attention_mask.shape} and label shape is {labels.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f8e8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_pred(loader, model, device = device,):\n",
    "    m = nn.Softmax()\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    test_loss_list = []\n",
    "    criterion = torch.nn.BCEWithLogitsLoss()\n",
    "    y_hat_test_list = []\n",
    "    y_test_list = []\n",
    "    for ind,(x_dict, label_vec) in enumerate(loader):\n",
    "        model.to(device)\n",
    "        id_list = x_dict['id']\n",
    "        ids = torch.squeeze(torch.tensor(id_list, device = device),dim = 1).clone().detach()\n",
    "        tok_type_list = x_dict['token_type']\n",
    "        token_type = torch.squeeze(torch.tensor(tok_type_list, device = device),dim = 1).clone().detach()\n",
    "        att_list = x_dict['attention_mask']\n",
    "        att_mask = torch.squeeze(torch.tensor(att_list, device = device),dim = 1).clone().detach()\n",
    "        lab = label_vec.to(torch.device('cuda:0'))\n",
    "        logits = model(ids ,token_type,att_mask)\n",
    "        \n",
    "        probs = m(logits)\n",
    "        y_hat_test_list.append(torch.argmax(probs , dim = 1))\n",
    "        y_test_list.append(torch.argmax(lab , dim = 1))\n",
    "        \n",
    "        act_loss = criterion(logits, lab)\n",
    "        test_loss_list.append(act_loss.item())\n",
    "            \n",
    "    mean_test_loss = torch.mean(torch.tensor(test_loss_list))\n",
    "    print(f\" Test loss is {torch.mean(torch.tensor(test_loss_list)):.4f}\")\n",
    "    #print metrics and save the model\n",
    "    y_hat_test = torch.cat(y_hat_test_list)\n",
    "    y_test = torch.cat(y_test_list)\n",
    "    acc_test = accuracy_score(y_test.cpu().numpy(), y_hat_test.cpu().numpy())\n",
    "    f1_test = f1_score(y_test.cpu().numpy(), y_hat_test.cpu().numpy(), average='micro')\n",
    "    print(f\"mean loss is {torch.mean(torch.tensor(mean_test_loss)):.4f} -> the accuracy is {acc_test:.2f} ->the f1 is {f1_test:.2f} \")\n",
    "    #save the model\n",
    "    plot_confusion_matrix(y_test.cpu().numpy(), y_hat_test.cpu().numpy(), labels)\n",
    "\n",
    "   \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0a5a43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc30355",
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model = MyModel.from_pretrained('./model/model2024-05-1712:14:28_mrpc.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dbf228e",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_pred(test_loader , saved_model )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5908da44",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_new = AutoModelForCausalLM.from_pretrained('./model/model2024-05-2923:30:09_.pth', from_flax=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dff94827",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_new.save_pretrained('./model', safe_serialization=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8234084",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
