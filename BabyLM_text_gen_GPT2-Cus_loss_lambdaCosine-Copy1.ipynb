{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e54d31d3",
   "metadata": {},
   "source": [
    "## In this notebook, we don't scale the nll and scale the cosine similaity with (1-lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866e2382",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e07fd768",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: datasets in /opt/conda/lib/python3.8/site-packages (2.20.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.8/site-packages (from datasets) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.8/site-packages (from datasets) (6.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.8/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.8/site-packages (from datasets) (1.21.4)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.8/site-packages (from datasets) (1.3.4)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.8/site-packages (from datasets) (3.9.5)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.8/site-packages (from datasets) (16.1.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.8/site-packages (from datasets) (3.4.0)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.8/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: requests>=2.32.2 in /opt/conda/lib/python3.8/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: fsspec[http]<=2024.5.0,>=2023.1.0 in /opt/conda/lib/python3.8/site-packages (from datasets) (2024.5.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.8/site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.8/site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /opt/conda/lib/python3.8/site-packages (from datasets) (4.66.4)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.2 in /opt/conda/lib/python3.8/site-packages (from datasets) (0.23.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets) (21.2.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.8/site-packages (from huggingface-hub>=0.21.2->datasets) (4.12.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging->datasets) (3.0.6)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests>=2.32.2->datasets) (3.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests>=2.32.2->datasets) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.8/site-packages (from requests>=2.32.2->datasets) (2.0.8)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests>=2.32.2->datasets) (1.26.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.8/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.8/site-packages (from pandas->datasets) (2021.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: jupyter in /opt/conda/lib/python3.8/site-packages (1.0.0)\n",
      "Requirement already satisfied: ipywidgets in /opt/conda/lib/python3.8/site-packages (8.1.3)\n",
      "Requirement already satisfied: notebook in /opt/conda/lib/python3.8/site-packages (from jupyter) (6.4.1)\n",
      "Requirement already satisfied: ipykernel in /opt/conda/lib/python3.8/site-packages (from jupyter) (6.29.5)\n",
      "Requirement already satisfied: nbconvert in /opt/conda/lib/python3.8/site-packages (from jupyter) (6.3.0)\n",
      "Requirement already satisfied: qtconsole in /opt/conda/lib/python3.8/site-packages (from jupyter) (5.5.2)\n",
      "Requirement already satisfied: jupyter-console in /opt/conda/lib/python3.8/site-packages (from jupyter) (6.6.3)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.11 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (3.0.11)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (5.14.3)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.11 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (4.0.11)\n",
      "Requirement already satisfied: comm>=0.1.3 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (7.30.0)\n",
      "Requirement already satisfied: setuptools>=18.5 in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (59.4.0)\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (4.8.0)\n",
      "Requirement already satisfied: pickleshare in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.7.5)\n",
      "Requirement already satisfied: decorator in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (5.1.0)\n",
      "Requirement already satisfied: pygments in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (2.10.0)\n",
      "Requirement already satisfied: jedi>=0.16 in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.18.1)\n",
      "Requirement already satisfied: matplotlib-inline in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.3)\n",
      "Requirement already satisfied: backcall in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.47)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /opt/conda/lib/python3.8/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.8/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.8/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=6.1.0->ipywidgets) (0.2.5)\n",
      "Requirement already satisfied: tornado>=6.1 in /opt/conda/lib/python3.8/site-packages (from ipykernel->jupyter) (6.1)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /opt/conda/lib/python3.8/site-packages (from ipykernel->jupyter) (5.7.2)\n",
      "Requirement already satisfied: nest-asyncio in /opt/conda/lib/python3.8/site-packages (from ipykernel->jupyter) (1.5.4)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in /opt/conda/lib/python3.8/site-packages (from ipykernel->jupyter) (7.1.0)\n",
      "Requirement already satisfied: pyzmq>=24 in /opt/conda/lib/python3.8/site-packages (from ipykernel->jupyter) (26.0.3)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.8/site-packages (from ipykernel->jupyter) (21.3)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in /opt/conda/lib/python3.8/site-packages (from ipykernel->jupyter) (1.8.2)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.8/site-packages (from ipykernel->jupyter) (5.8.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /opt/conda/lib/python3.8/site-packages (from jupyter-client>=6.1.12->ipykernel->jupyter) (2.8.2)\n",
      "Requirement already satisfied: entrypoints in /opt/conda/lib/python3.8/site-packages (from jupyter-client>=6.1.12->ipykernel->jupyter) (0.3)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /opt/conda/lib/python3.8/site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel->jupyter) (4.2.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.8/site-packages (from python-dateutil>=2.1->jupyter-client>=6.1.12->ipykernel->jupyter) (1.16.0)\n",
      "Requirement already satisfied: mistune<2,>=0.8.1 in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (0.8.4)\n",
      "Requirement already satisfied: jinja2>=2.4 in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (3.0.3)\n",
      "Requirement already satisfied: nbclient<0.6.0,>=0.5.0 in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (0.5.9)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (1.5.0)\n",
      "Requirement already satisfied: defusedxml in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (0.7.1)\n",
      "Requirement already satisfied: nbformat>=4.4 in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (5.1.3)\n",
      "Requirement already satisfied: jupyterlab-pygments in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (0.1.2)\n",
      "Requirement already satisfied: testpath in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (0.5.0)\n",
      "Requirement already satisfied: bleach in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (4.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.8/site-packages (from jinja2>=2.4->nbconvert->jupyter) (2.0.1)\n",
      "Requirement already satisfied: ipython-genutils in /opt/conda/lib/python3.8/site-packages (from nbformat>=4.4->nbconvert->jupyter) (0.2.0)\n",
      "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /opt/conda/lib/python3.8/site-packages (from nbformat>=4.4->nbconvert->jupyter) (4.2.1)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /opt/conda/lib/python3.8/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.4->nbconvert->jupyter) (0.18.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /opt/conda/lib/python3.8/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.4->nbconvert->jupyter) (21.2.0)\n",
      "Requirement already satisfied: importlib-resources>=1.4.0 in /opt/conda/lib/python3.8/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.4->nbconvert->jupyter) (5.4.0)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /opt/conda/lib/python3.8/site-packages (from importlib-resources>=1.4.0->jsonschema!=2.5.0,>=2.4->nbformat>=4.4->nbconvert->jupyter) (3.6.0)\n",
      "Requirement already satisfied: webencodings in /opt/conda/lib/python3.8/site-packages (from bleach->nbconvert->jupyter) (0.5.1)\n",
      "Requirement already satisfied: terminado>=0.8.3 in /opt/conda/lib/python3.8/site-packages (from notebook->jupyter) (0.12.1)\n",
      "Requirement already satisfied: prometheus-client in /opt/conda/lib/python3.8/site-packages (from notebook->jupyter) (0.12.0)\n",
      "Requirement already satisfied: argon2-cffi in /opt/conda/lib/python3.8/site-packages (from notebook->jupyter) (21.1.0)\n",
      "Requirement already satisfied: Send2Trash>=1.5.0 in /opt/conda/lib/python3.8/site-packages (from notebook->jupyter) (1.8.0)\n",
      "Requirement already satisfied: cffi>=1.0.0 in /opt/conda/lib/python3.8/site-packages (from argon2-cffi->notebook->jupyter) (1.15.0)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.8/site-packages (from cffi>=1.0.0->argon2-cffi->notebook->jupyter) (2.21)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging->ipykernel->jupyter) (3.0.6)\n",
      "Requirement already satisfied: qtpy>=2.4.0 in /opt/conda/lib/python3.8/site-packages (from qtconsole->jupyter) (2.4.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#installing some libraries\n",
    "!pip install datasets\n",
    "!pip install --upgrade jupyter ipywidgets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e77a0149",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch, transformers\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import PreTrainedModel, PretrainedConfig\n",
    "from transformers import AutoModel, AutoConfig,AutoModelForCausalLM, AutoConfig\n",
    "\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "#from datasets import load_dataset\n",
    "import pandas as pd, numpy as np\n",
    "from torch import cuda\n",
    "import datetime\n",
    "import warnings,itertools\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import json,math\n",
    "pre_train = False\n",
    "\n",
    "# Ignore all warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "#pip install transformers bitsandbytes>=0.39.0 -q\n",
    "import zipfile,logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e53f4182",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "#global params for training\n",
    "\n",
    "B,T = 32,1024\n",
    "epoch = 100\n",
    "# this controls whether we are using pre-trained wts or not\n",
    "random_init_wts = False\n",
    "if cuda.is_available():\n",
    "    device = torch.device('cuda:0')\n",
    "    print(device)\n",
    "else:\n",
    "    device = 'cpu'\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "# these 2 global vars help track the training and val loss\n",
    "global_tr_loss = torch.inf\n",
    "global_val_loss = torch.inf\n",
    "#directory to save wts\n",
    "model_path = os.path.join(\"model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2be36afa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./data/unzip_text_10M'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "directory = os.path.join('.','data','unzip_text_10M')  # Replace with your directory path\n",
    "directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6e618bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_text(directory):\n",
    "    \"\"\"This function loads the dataset,provided in the zipped format and stores all the text files in list\n",
    "    each file has its contents stored as an item in list and at the end we concatenate all the sub-lists to create a flattened list and return it\"\"\"\n",
    "    directory = os.path.join('.','data','unzip_text_10M',str(directory))  # Replace with your directory path\n",
    "    print(f\"directory :{directory}\")\n",
    "    # List all files in the directory\n",
    "    files = [f for f in os.listdir(directory) if os.path.isfile(os.path.join(directory, f))]\n",
    "    print(f\"files:{files}\")\n",
    "    text_content = []\n",
    "    # Read each file\n",
    "    total_lines = 0\n",
    "    for filenum,filename in enumerate(files):\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            text = file.read()\n",
    "            text_content.append(text)\n",
    "            print(f\"the file:{filename} has been appeneded to the uber list and its length is {len(text_content)} \")\n",
    "            \n",
    "    \n",
    "    flattened_list = ''.join(text_content)\n",
    "    assert (len(flattened_list) == total_lines , f\"Expected {len(flattened_list)} to be equal to {total_lines}\" )\n",
    "    \n",
    "    return flattened_list\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc0daad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "directory :./data/unzip_text_10M/train_10M\n",
      "files:['switchboard.train', 'simple_wiki.train', 'open_subtitles.train', 'gutenberg.train', 'childes.train', 'bnc_spoken.train']\n",
      "the file:switchboard.train has been appeneded to the uber list and its length is 1 \n",
      "the file:simple_wiki.train has been appeneded to the uber list and its length is 2 \n",
      "the file:open_subtitles.train has been appeneded to the uber list and its length is 3 \n",
      "the file:gutenberg.train has been appeneded to the uber list and its length is 4 \n",
      "the file:childes.train has been appeneded to the uber list and its length is 5 \n",
      "the file:bnc_spoken.train has been appeneded to the uber list and its length is 6 \n"
     ]
    }
   ],
   "source": [
    "#read the dataset and get a list\n",
    "train_list = read_text(\"train_10M\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c57c281",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54215049"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a114866c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1654\n"
     ]
    }
   ],
   "source": [
    "chunks = len(train_list)//(B*T)\n",
    "print(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5c12e41b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17191971 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "#tokeinze the training data\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\",return_tensors = \"pt\" , truncate = True, return_overflowing_tokens=True , padding = False,)\n",
    "enc_train = tokenizer(train_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e0ce901b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.1535097982657136"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comp_ratio = len(train_list)/len(enc_train['input_ids'])\n",
    "comp_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "44dde6b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "directory :./data/unzip_text_10M/dev\n",
      "files:['switchboard.dev', 'simple_wiki.dev', 'open_subtitles.dev', 'gutenberg.dev', 'childes.dev', 'bnc_spoken.dev']\n",
      "the file:switchboard.dev has been appeneded to the uber list and its length is 1 \n",
      "the file:simple_wiki.dev has been appeneded to the uber list and its length is 2 \n",
      "the file:open_subtitles.dev has been appeneded to the uber list and its length is 3 \n",
      "the file:gutenberg.dev has been appeneded to the uber list and its length is 4 \n",
      "the file:childes.dev has been appeneded to the uber list and its length is 5 \n",
      "the file:bnc_spoken.dev has been appeneded to the uber list and its length is 6 \n"
     ]
    }
   ],
   "source": [
    "#read validation data and tokenize\n",
    "val_list = read_text(\"dev\")\n",
    "enc_val = tokenizer(val_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd187e77",
   "metadata": {},
   "source": [
    "### we read the tokenize data and store the input_ids and attention mask as a single long list at run time we reshape the single [1,B*T] tensor into a batched tensor of dimension [B,T].\n",
    "This approach turns out to be more efficient as it removes the need for any extra tokens in the form of padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5f095bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df_from_list(enc , B= B, T = T, val= False):\n",
    "    \"\"\"This function takes the tokenized list and reated a dataframe where each row contains the input_ids and attention_mask from the tokenizer.\n",
    "    each row of the dataframe is a list with items B*T\"\"\"\n",
    "    chunk_size = B*T\n",
    "    if val:\n",
    "        chunk_size = 2*B*T\n",
    "        \n",
    "    long_list_inp = enc['input_ids']\n",
    "    long_list_attention = enc['attention_mask']\n",
    "\n",
    "    # Step 3: Split the list into chunks and pad the last chunk if necessary\n",
    "    chunks_inp = [long_list_inp[i:i + chunk_size] for i in range(0, len(long_list_inp), chunk_size)]\n",
    "    chunks_att = [long_list_attention[i:i + chunk_size] for i in range(0, len(long_list_attention), chunk_size)]\n",
    "    df = pd.DataFrame({'input_ids': chunks_inp,'attention_mask':chunks_att})\n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cf505e03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the dataframe is = 525\n"
     ]
    }
   ],
   "source": [
    "df_train_temp = get_df_from_list(enc_train)\n",
    "# Display the DataFrame\n",
    "df_train_temp.head()\n",
    "print(f\"Length of the dataframe is = {len(df_train_temp)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4fcb18ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the VALDATION dataframe is = 532\n"
     ]
    }
   ],
   "source": [
    "df_val_temp = get_df_from_list(enc_val)\n",
    "# Display the DataFrame\n",
    "df_val_temp.head()\n",
    "print(f\"Length of the VALDATION dataframe is = {len(df_val_temp)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "62a11152",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_df(df,eos_char = tokenizer.eos_token_id,B = B, T = T,val = False):\n",
    "    \"\"\"The 2 functions below makes sure that each row of the dataframe is of equal length and if not it adds the eos token to make it B*T\"\"\"\n",
    "    if val:\n",
    "        B = 2*B\n",
    "    for ind,row in df.iterrows():\n",
    "        if len(row['input_ids']) != B*T :\n",
    "            print(f\"row = {ind} and input_id length = {len(row['input_ids'])}\")\n",
    "            print(f\"row = {ind} and attention length = {len(row['attention_mask'])}\")\n",
    "            pad_len = B*T - len(row['input_ids'])\n",
    "            print(f\"padding the row index {ind} with {pad_len} character\")\n",
    "            row['input_ids'] = row['input_ids']+ [eos_char] * pad_len\n",
    "            #attention mask should be padded to 0\n",
    "            row['attention_mask'] = row['attention_mask']+ [0] * pad_len\n",
    "            print(\"#### POST CONCAT####\")\n",
    "            print(f\"row = {ind} and input_id length = {len(row['input_ids'])}\")\n",
    "            print(f\"row = {ind} and attention length ={len(row['attention_mask'])}\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def verify_len(df,val = False, B= B):\n",
    "    row_ind = []\n",
    "    if val:\n",
    "        B = 2*B\n",
    "    for ind,row in df.iterrows():\n",
    "        if len(row['input_ids']) != B*T :\n",
    "            row_ind.append(ind)\n",
    "        else:\n",
    "            continue\n",
    "    if len(row_ind) !=0:\n",
    "        print(\"CONCATENATION Did not work\")\n",
    "    else:\n",
    "        print(\"CONCATENATION worked\")\n",
    "        \n",
    "            \n",
    "        \n",
    "    \n",
    "        \n",
    "        \n",
    "   \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f0247765",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row = 524 and input_id length = 21539\n",
      "row = 524 and attention length = 21539\n",
      "padding the row index 524 with 11229 character\n",
      "#### POST CONCAT####\n",
      "row = 524 and input_id length = 32768\n",
      "row = 524 and attention length =32768\n",
      "CONCATENATION worked\n"
     ]
    }
   ],
   "source": [
    "df_train  = pad_df(df_train_temp)\n",
    "verify_len(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "862dcc3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row = 531 and input_id length = 13588\n",
      "row = 531 and attention length = 13588\n",
      "padding the row index 531 with 19180 character\n",
      "#### POST CONCAT####\n",
      "row = 531 and input_id length = 32768\n",
      "row = 531 and attention length =32768\n",
      "CONCATENATION worked\n"
     ]
    }
   ],
   "source": [
    "df_val  = pad_df(df_val_temp)\n",
    "verify_len(df_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b02be8c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_ids</th>\n",
       "      <th>attention_mask</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[32, 25, 197, 40, 1101, 1654, 484, 389, 13, 19...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[6510, 11, 14104, 290, 27913, 13, 198, 32, 25,...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[314, 1612, 11, 340, 338, 1611, 286, 43244, 11...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[318, 407, 922, 329, 262, 1200, 2035, 13, 198,...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[198, 32, 25, 197, 1870, 11, 21480, 11, 314, 4...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           input_ids  \\\n",
       "0  [32, 25, 197, 40, 1101, 1654, 484, 389, 13, 19...   \n",
       "1  [6510, 11, 14104, 290, 27913, 13, 198, 32, 25,...   \n",
       "2  [314, 1612, 11, 340, 338, 1611, 286, 43244, 11...   \n",
       "3  [318, 407, 922, 329, 262, 1200, 2035, 13, 198,...   \n",
       "4  [198, 32, 25, 197, 1870, 11, 21480, 11, 314, 4...   \n",
       "\n",
       "                                      attention_mask  \n",
       "0  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "1  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "2  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "3  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "4  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3d08dd",
   "metadata": {},
   "source": [
    "## Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "26a78011",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLoss(nn.Module):\n",
    "    def __init__(self, num_epochs):\n",
    "        super(CustomLoss, self).__init__()\n",
    "        #self.nll_loss = nn.CrossEntropyLoss()\n",
    "#        self.cosine_similarity = nn.CosineSimilarity(dim=2)\n",
    "        self.num_epochs = num_epochs\n",
    "        self.current_epoch = 0\n",
    "        self.nll_scale = 1.0  # Initialize with a default value\n",
    "\n",
    "    def forward(self, nll,cosine_sim):\n",
    "               \n",
    "        nll_normalized = nll / self.nll_scale         \n",
    "        # Calculate lambda value\n",
    "        lambda_val = self.get_lambda()\n",
    "        # Compute total loss\n",
    "        total_loss = nll_normalized + lambda_val * cosine_sim\n",
    "        #print(f\"inside CustomLoss->nll = {nll}|nll_scale = {self.nll_scale}|nll_normalized= {nll_normalized}|cosine_sim = {cosine_sim} | lambda val = {lambda_val}|total_loss = {total_loss}\")\n",
    "        return total_loss,lambda_val,self.nll_scale\n",
    "\n",
    "    def get_lambda(self):\n",
    "        # Exponential increase from 0 to 1\n",
    "        return 1 - math.exp(-5 * self.current_epoch / self.num_epochs)\n",
    "\n",
    "    def update_epoch(self, epoch):\n",
    "        self.current_epoch = epoch\n",
    "\n",
    "    def update_nll_scale(self, nll_value):\n",
    "        self.nll_scale = max(self.nll_scale, nll_value)\n",
    "    def update_epoch(self, epoch):\n",
    "        self.current_epoch = epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f8f7ba1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the tokenizer:\n",
    "model_name = \"gpt2\"\n",
    "if random_init_wts:\n",
    "    config = AutoConfig.from_pretrained(model_name, vocab_size = 50304)\n",
    "    # Initialize the model with random weights\n",
    "    model = AutoModelForCausalLM.from_config(config)\n",
    "else:\n",
    "    #model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "\n",
    "#model = torch.compile(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5b8aaa0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "525"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e9c1be32",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# device = \"cuda\"\n",
    "# inp = torch.tensor(df_train.iloc[5]['input_ids']).view(B,T).to(device)\n",
    "# att = torch.tensor(df_train.iloc[5]['attention_mask']).view(B,T).to(device)\n",
    "# lab = inp.clone()\n",
    "# # lab = F.pad(lab[:, 1:], (0, 1), value=-100)  # Shift labels to the left and pad with -100\n",
    "# # lab[:, -1] = -100\n",
    "# model.to(device)\n",
    "# model_out = model(input_ids = inp, attention_mask = att , labels = lab)\n",
    "# assert not torch.isnan(inp).any(), \"Input contains NaN\"\n",
    "# assert not torch.isinf(inp).any(), \"Input contains inf\"\n",
    "# model_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f8a1e922",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"Input shape: {inp.shape}\")\n",
    "# print(f\"Attention mask shape: {att.shape}\")\n",
    "# print(f\"Labels shape: {lab.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d86c4f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a59c72d2",
   "metadata": {},
   "source": [
    "### Data loaders and Dataset for batched training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2149bacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataset_pyt_val(Dataset):\n",
    "    \"\"\"Dataset for validation data\"\"\"\n",
    "    def __init__(self, df, B = B, T = T ):\n",
    "        self.df = df\n",
    "        print(f\"Value of B {B}\")\n",
    "                                        \n",
    "    def __getitem__(self, idx):\n",
    "        #print(f\"inside loader...idx ->{idx}\")\n",
    "        input_id_temp = torch.tensor(self.df.iloc[idx]['input_ids'],dtype = torch.long)\n",
    "        att_mask = torch.tensor(self.df.iloc[idx]['attention_mask'],dtype = torch.long)\n",
    "        input_id =   input_id_temp.view(B,T)    \n",
    "        attention_mask = att_mask.view(B,T)\n",
    "        \n",
    "        \n",
    "        return input_id, attention_mask\n",
    "        \n",
    "    def __len__(self):\n",
    "        #return the length of the dataframe\n",
    "        return len(self.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "411e10a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataset_pyt_train(Dataset):\n",
    "    def __init__(self, df, B = B, T = T ):\n",
    "        self.df = df\n",
    "                                        \n",
    "    def __getitem__(self, idx):\n",
    "        #print(f\"************* idx for dataloader = {idx}\")\n",
    "        input_id_temp = torch.tensor(self.df.iloc[idx]['input_ids'],dtype = torch.long)\n",
    "        att_mask = torch.tensor(self.df.iloc[idx]['attention_mask'],dtype = torch.long)\n",
    "        input_id =   input_id_temp.view(B,T)    \n",
    "        attention_mask = att_mask.view(B,T)   \n",
    "        \n",
    "        return input_id, attention_mask\n",
    "        \n",
    "    def __len__(self):\n",
    "        #return the length of the dataframe\n",
    "        return len(self.df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28a9226",
   "metadata": {},
   "source": [
    "### Note - batch_size is 1 here because we reshape our input data in the shape [B,T] for a batched training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "28ae5610",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value of B 32\n"
     ]
    }
   ],
   "source": [
    "#train_dataset = dataset_pyt(filtered_df,tokenizer = tokenizer)\n",
    "train_dataset = dataset_pyt_train(df_train)\n",
    "val_dataset = dataset_pyt_val(df_val)\n",
    "#test_dataset = dataset_pyt(df_test,tokenizer = tokenizer)\n",
    "\n",
    "train_loader = DataLoader(train_dataset,batch_size = 1, shuffle = True , num_workers = 0, pin_memory = True)\n",
    "val_loader = DataLoader(val_dataset,batch_size = 1, shuffle = True , num_workers = 0, pin_memory = True)\n",
    "#test_loader = DataLoader(test_dataset,batch_size = batch_size, shuffle = False, collate_fn = custom_collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ca2077",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0e8f9200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the train loader is 525\n",
      "Length of the val loader is 532\n",
      "num_tokens= 17203200\n"
     ]
    }
   ],
   "source": [
    "print(f\"Length of the train loader is {len(train_loader)}\")\n",
    "print(f\"Length of the val loader is {len(val_loader)}\")\n",
    "print(f\"num_tokens= {B*T*len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e0870a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_file(log_message, model_name = model_name ,random_init_wts = random_init_wts ):\n",
    "    \"\"\"This function logs the training performance for future reference\"\"\"\n",
    "    current_datetime = datetime.datetime.now()\n",
    "    # Extract date and time components\n",
    "    current_date = str(current_datetime.date())\n",
    "    log_file = model_name +'_COS_SIM*lambda_'+'random_init_wts'+ '_'+str(random_init_wts)+'_' +current_date+'.log'\n",
    "    print(f\"*****LOGGING INFO IN {log_file}*********\")\n",
    "    filepath = os.path.join(\"model\",log_file)\n",
    "    logging.basicConfig(filename=filepath, \n",
    "                    filemode='a',  # Overwrite the log file each time\n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s', \n",
    "                    level=logging.DEBUG)\n",
    "    logger = logging.getLogger()\n",
    "    logger.info(log_message)\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f0e3b235",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad\n",
    "\n",
    "def eval_model(val_loader, model, lambda_val ,nll_scale,epoch , device = device,tokenizer = tokenizer):\n",
    "    global global_val_loss\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    e = epoch+1\n",
    "    embedding_layer = model.transformer.wte\n",
    "    val_loss_accum = 0.0\n",
    "    print(f\"inside validation data for epoch {e}\")\n",
    "    for ind,(input_id,attention_mask) in enumerate(val_loader):\n",
    "        ids = input_id.to(device=device, non_blocking=True)\n",
    "        att_mask = attention_mask.to(device=device, non_blocking=True)\n",
    "        labels = ids.clone()\n",
    "        \n",
    "        with autocast(dtype = torch.bfloat16):\n",
    "            input_embeddings = embedding_layer(ids)\n",
    "            model_output = model(input_ids = ids ,attention_mask = att_mask, labels = labels)\n",
    "            logits = model_output.logits\n",
    "            predictions = torch.argmax(logits, dim=-1)\n",
    "            #print(f\"predictions = {predictions}\")\n",
    "            prediction_embeddings = embedding_layer(predictions)\n",
    "            cos_sim = F.cosine_similarity(torch.squeeze(prediction_embeddings,dim = 0), torch.squeeze(input_embeddings,dim = 0), dim=1)\n",
    "            cos_loss = 1- cos_sim.mean()\n",
    "            total_loss = cos_loss + model_output.loss\n",
    "        \n",
    "        val_loss_accum+=total_loss.detach().item()\n",
    "        del ids,att_mask,labels,input_embeddings,model_output,logits,predictions,prediction_embeddings,cos_sim,cos_loss\n",
    "    \n",
    "    #logging and saving if the validation loss has decreased.\n",
    "    if val_loss_accum < global_val_loss:\n",
    "        print(f\"Val loss has decreased -->reducing the global validation loss from {global_val_loss:.2f} to {val_loss_accum:.2f}\")\n",
    "        s1 = f\"Val loss has decreased -->reducing the global validation loss from {global_val_loss:.2f} to {val_loss_accum:.2f}\"\n",
    "        global_val_loss = val_loss_accum\n",
    "        print(f\" validation loss for epoch = {e} is {val_loss_accum:.4f}\")\n",
    "        s2 = f\" validation loss for epoch = {e} is {val_loss_accum:.4f}\"\n",
    "        print(f\" epoch= {e} :  val loss is {val_loss_accum:.4f} \")\n",
    "        s3 = f\" epoch= {e} :  val loss is {val_loss_accum:.4f} \"\n",
    "        #save the model\n",
    "        \n",
    "        # Get the current date and time\n",
    "        current_datetime = datetime.datetime.now()\n",
    "        # Extract date and time components\n",
    "        current_date = str(current_datetime.date())\n",
    "        current_time = str(current_datetime.time()).split('.')[0]\n",
    "        file_name = 'model'+ current_date+current_time+'.pth'\n",
    "        path = os.path.join(\"model\",file_name)\n",
    "        print(f\"saving the model {file_name}\")\n",
    "        s4 = f\"saving the model {file_name}\"\n",
    "        #torch.save(model.state_dict(), path)\n",
    "        model.save_pretrained(path)\n",
    "        tokenizer.save_pretrained(path)\n",
    "        log_message = s1+s2+s3+s4\n",
    "        write_file(log_message)\n",
    "        \n",
    "        #plot_confusion_matrix(y_val.cpu().numpy(), y_hat_val.cpu().numpy(), labels)\n",
    "    else:\n",
    "        print(f\"No improvement in validation loss-->epoch= {e} and global val loss is {global_val_loss:.2f}|current_Val loss = {val_loss_accum}\")\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "57ae027e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_loader,val_loader,model,num_epoch = 100,device = device,tokenizer = tokenizer):\n",
    "    global global_tr_loss\n",
    "    model.train()\n",
    "    device = device\n",
    "    lr_custom = 2e-5\n",
    "    print(f\"inside train model. Device = {device}\")\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.AdamW(params =  model.parameters(), lr= lr_custom,fused = True ,weight_decay = .1)\n",
    "    total_batch_size = 2**19\n",
    "    grad_accum_step = total_batch_size//(B*T)\n",
    "    \n",
    "    extra_train = .1*num_epoch\n",
    "    max_train_steps = int(num_epoch +extra_train )\n",
    "    import time\n",
    "    from transformers import get_linear_schedule_with_warmup\n",
    "    total_steps = len(train_loader) * num_epoch\n",
    "    scheduler_cos = transformers.get_cosine_schedule_with_warmup( optimizer= optimizer, num_warmup_steps =int(total_steps * 0.1) ,num_training_steps= total_steps )\n",
    "    embedding_layer = model.transformer.wte\n",
    "    criterion = CustomLoss(num_epoch)\n",
    "        \n",
    "    for i in range (max_train_steps):\n",
    "        criterion.update_epoch(i)\n",
    "        epoch_start_time = time.time()\n",
    "        epoch_train_loss = 0.0\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        # we use 2 schedulers - the first LR scheduler uses a cosine decay for 100 epochs the second scheduler takes the last LR from cosine scheduler and then maintains that LR for the next 10 epochs\n",
    "        if i >= num_epoch:\n",
    "            optimizer_reduced_lr = torch.optim.AdamW(params =  model.parameters(), lr= current_lr ,fused = True , weight_decay=.1)\n",
    "            scheduler_constant = transformers.get_constant_schedule_with_warmup( optimizer = optimizer_reduced_lr ,num_warmup_steps = 0, last_epoch = -1 )\n",
    "        \n",
    "        lambda_val = None\n",
    "        nll_scale = None\n",
    "        for ind,(input_id,attention_mask) in enumerate(train_loader):\n",
    "            if ind == int(len(train_loader)/2):\n",
    "                batch_time = time.time()\n",
    "                duration = batch_time - epoch_start_time\n",
    "                print(f\"executing epoch:{i+1}, it took {duration/60} mins from beginning of epoch till batch#{ind}\")\n",
    "            \n",
    "            ids = input_id.to(device=device, non_blocking=True)\n",
    "            att_mask = attention_mask.to(device=device, non_blocking=True)\n",
    "            labels = ids.clone()\n",
    "            assert not torch.isnan(ids).any(), \"Input contains NaN values\"\n",
    "            assert not torch.isinf(ids).any(), \"Input contains infinite values\"\n",
    "            \n",
    "            with autocast(dtype = torch.bfloat16):\n",
    "                input_embeddings = embedding_layer(ids)\n",
    "                model_output = model(input_ids = ids ,attention_mask = att_mask, labels = labels)\n",
    "                logits = model_output.logits\n",
    "                predictions = torch.argmax(logits, dim=-1)\n",
    "                prediction_embeddings = embedding_layer(predictions)\n",
    "                #print(f\"model_output.logits = {model_output.logits}|model_output.loss = {model_output.loss}\")\n",
    "                cos_sim = F.cosine_similarity(torch.squeeze(prediction_embeddings,dim = 0), torch.squeeze(input_embeddings,dim = 0), dim=1)\n",
    "                cos_loss = 1- cos_sim.mean()\n",
    "                #print(f\"inside train_vale before calc custom loss|ind = {ind}|model_output.loss = {model_output.loss}|coss_loss = {cos_loss}\")\n",
    "                #print(f\"inside train_vale before calc custom loss|ind = {ind}|model_output.loss = {model_output.loss}|coss_loss = {cos_loss}|learning rate = {scheduler_cos.get_last_lr()[0]:.3e}\")\n",
    "                if np.isnan(model_output.loss.item()):\n",
    "                    print(\"f nan values encountered..\")\n",
    "                    decoded_texts = [tokenizer.decode(ids, skip_special_tokens=True) for ids in ids]\n",
    "                    print(f\"*********$$$$$$$$$ decoded_texts = {decoded_texts}*************\")\n",
    "\n",
    "\n",
    "            assert not np.isnan(model_output.loss.item()), \"NaN value found\"\n",
    "            #total_loss = cos_loss + model_output.loss.item()\n",
    "            \n",
    "            total_loss,lambda_val,nll_scale = criterion(nll = model_output.loss.item() , cosine_sim = cos_loss)\n",
    "            if ind == (len(train_loader) - 1):\n",
    "                lambda_val = lambda_val\n",
    "                nll_scale = nll_scale\n",
    "                \n",
    "            #criterion.update_nll_scale(model_output.loss.item())                           \n",
    "            total_loss.backward()\n",
    "            epoch_train_loss += total_loss.detach().item()\n",
    "            norm = torch.nn.utils.clip_grad_norm(model.parameters() , 1.0)\n",
    "            if i <= num_epoch:\n",
    "                optimizer.step()\n",
    "                scheduler_cos.step()\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "            else:\n",
    "                optimizer_reduced_lr.step()\n",
    "                optimizer_reduced_lr.zero_grad(set_to_none=True)\n",
    "                scheduler_constant.step()\n",
    "                \n",
    "                                \n",
    "            del ids,att_mask,labels,input_embeddings,model_output,logits,predictions,prediction_embeddings,cos_sim\n",
    "            \n",
    "        \n",
    "        \n",
    "        #batch processing complete \n",
    "        if i <= num_epoch:\n",
    "            current_lr = scheduler_cos.get_last_lr()[0]\n",
    "        #This code below compares the training loss during the current epoch with the global loss and if updates the global loss if there is an improvement.\n",
    "        # at every 4 epoch we evluate the model on the validation data or when \n",
    "        epoch_end_time = time.time()\n",
    "        epoch_durn = (epoch_end_time - epoch_start_time)\n",
    "        num_token = B*T*len(train_loader)\n",
    "        if (epoch_train_loss < global_tr_loss) :\n",
    "            improvement_percent = (abs(epoch_train_loss-global_tr_loss)/global_tr_loss)*100\n",
    "            print(f\"training loss has decreased---> reducing the global loss from {global_tr_loss:.2f} to {epoch_train_loss:.2f} | throughput = {int(num_token/epoch_durn)} tokens/second | norm = {norm:.4f} | learning rate = {current_lr:.5e}\")\n",
    "            global_tr_loss = epoch_train_loss\n",
    "            print(f\" epoch= {i+1} and  train loss is {epoch_train_loss:.2f}\")\n",
    "            if (i%4 == 0 or improvement_percent > 5):\n",
    "                eval_model(val_loader, model, lambda_val ,nll_scale,epoch = i, device = device,tokenizer = tokenizer)\n",
    "            \n",
    "        else:\n",
    "            print(f\"No improvement in training loss..the global training loss is -->{global_tr_loss:.2f}|epoch train_loss = {epoch_train_loss:.2f} \")\n",
    "            #print(f\" epoch= {i+1} and mean train loss is {epoch_train_loss:.2f}\")\n",
    "        \n",
    "        \n",
    "    \n",
    "    return model\n",
    "        \n",
    "            \n",
    "            \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c24e2af8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inside train model. Device = cuda:0\n",
      "executing epoch:1, it took 0.8266011754671733 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the global loss from inf to 1836.96 | throughput = 173604 tokens/second | norm = 0.0000 | learning rate = 2.00000e-06\n",
      " epoch= 1 and  train loss is 1836.96\n",
      "inside validation data for epoch 1\n",
      "Val loss has decreased -->reducing the global validation loss from inf to 2249.98\n",
      " validation loss for epoch = 1 is 2249.9774\n",
      " epoch= 1 :  val loss is 2249.9774 \n",
      "saving the model model2024-07-1405:05:00.pth\n",
      "*****LOGGING INFO IN gpt2_COS_SIM*lambda_random_init_wts_False_2024-07-14.log*********\n",
      "executing epoch:2, it took 0.8099090973536174 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the global loss from 1836.96 to 1787.80 | throughput = 176612 tokens/second | norm = 0.0013 | learning rate = 4.00000e-06\n",
      " epoch= 2 and  train loss is 1787.80\n",
      "executing epoch:3, it took 0.8096546411514283 mins from beginning of epoch till batch#262\n",
      "No improvement in training loss..the global training loss is -->1787.80|epoch train_loss = 1813.20 \n",
      "executing epoch:4, it took 0.8103258967399597 mins from beginning of epoch till batch#262\n",
      "No improvement in training loss..the global training loss is -->1787.80|epoch train_loss = 1844.42 \n",
      "executing epoch:5, it took 0.8098694682121277 mins from beginning of epoch till batch#262\n",
      "No improvement in training loss..the global training loss is -->1787.80|epoch train_loss = 1882.70 \n",
      "executing epoch:6, it took 0.8102260828018188 mins from beginning of epoch till batch#262\n",
      "No improvement in training loss..the global training loss is -->1787.80|epoch train_loss = 1921.26 \n",
      "executing epoch:7, it took 0.8102927684783936 mins from beginning of epoch till batch#262\n",
      "No improvement in training loss..the global training loss is -->1787.80|epoch train_loss = 1963.88 \n",
      "executing epoch:8, it took 0.8104153871536255 mins from beginning of epoch till batch#262\n",
      "No improvement in training loss..the global training loss is -->1787.80|epoch train_loss = 2009.52 \n",
      "executing epoch:9, it took 0.8104046940803528 mins from beginning of epoch till batch#262\n",
      "No improvement in training loss..the global training loss is -->1787.80|epoch train_loss = 2065.21 \n",
      "executing epoch:10, it took 0.8094756046930949 mins from beginning of epoch till batch#262\n",
      "No improvement in training loss..the global training loss is -->1787.80|epoch train_loss = 2131.09 \n",
      "executing epoch:11, it took 0.8104203701019287 mins from beginning of epoch till batch#262\n",
      "No improvement in training loss..the global training loss is -->1787.80|epoch train_loss = 2191.95 \n",
      "executing epoch:12, it took 0.8095318714777628 mins from beginning of epoch till batch#262\n",
      "No improvement in training loss..the global training loss is -->1787.80|epoch train_loss = 2286.80 \n",
      "executing epoch:13, it took 0.8086872895558676 mins from beginning of epoch till batch#262\n",
      "No improvement in training loss..the global training loss is -->1787.80|epoch train_loss = 2374.66 \n",
      "executing epoch:14, it took 0.8076460321744283 mins from beginning of epoch till batch#262\n",
      "No improvement in training loss..the global training loss is -->1787.80|epoch train_loss = 2446.44 \n",
      "executing epoch:15, it took 0.8070953011512756 mins from beginning of epoch till batch#262\n",
      "No improvement in training loss..the global training loss is -->1787.80|epoch train_loss = 2551.96 \n",
      "executing epoch:16, it took 0.8067103226979574 mins from beginning of epoch till batch#262\n",
      "No improvement in training loss..the global training loss is -->1787.80|epoch train_loss = 2650.94 \n",
      "executing epoch:17, it took 0.8070749282836914 mins from beginning of epoch till batch#262\n",
      "No improvement in training loss..the global training loss is -->1787.80|epoch train_loss = 2767.63 \n",
      "executing epoch:18, it took 0.8066087245941163 mins from beginning of epoch till batch#262\n",
      "No improvement in training loss..the global training loss is -->1787.80|epoch train_loss = 2855.47 \n",
      "executing epoch:19, it took 0.8069218317667644 mins from beginning of epoch till batch#262\n",
      "No improvement in training loss..the global training loss is -->1787.80|epoch train_loss = 2914.17 \n",
      "executing epoch:20, it took 0.8070578614870707 mins from beginning of epoch till batch#262\n",
      "No improvement in training loss..the global training loss is -->1787.80|epoch train_loss = 2991.77 \n",
      "executing epoch:21, it took 0.8075159947077434 mins from beginning of epoch till batch#262\n",
      "No improvement in training loss..the global training loss is -->1787.80|epoch train_loss = 3056.74 \n",
      "executing epoch:22, it took 0.8071791330973307 mins from beginning of epoch till batch#262\n",
      "No improvement in training loss..the global training loss is -->1787.80|epoch train_loss = 3127.09 \n",
      "executing epoch:23, it took 0.8075563391049703 mins from beginning of epoch till batch#262\n",
      "No improvement in training loss..the global training loss is -->1787.80|epoch train_loss = 3209.02 \n",
      "executing epoch:24, it took 0.8075781623522441 mins from beginning of epoch till batch#262\n",
      "No improvement in training loss..the global training loss is -->1787.80|epoch train_loss = 3298.66 \n",
      "executing epoch:25, it took 0.8074515779813131 mins from beginning of epoch till batch#262\n",
      "No improvement in training loss..the global training loss is -->1787.80|epoch train_loss = 3391.08 \n",
      "executing epoch:26, it took 0.8077240109443664 mins from beginning of epoch till batch#262\n",
      "No improvement in training loss..the global training loss is -->1787.80|epoch train_loss = 3477.44 \n",
      "executing epoch:27, it took 0.8077586015065511 mins from beginning of epoch till batch#262\n",
      "No improvement in training loss..the global training loss is -->1787.80|epoch train_loss = 3556.68 \n",
      "executing epoch:28, it took 0.8076532522837321 mins from beginning of epoch till batch#262\n",
      "No improvement in training loss..the global training loss is -->1787.80|epoch train_loss = 3623.51 \n",
      "executing epoch:29, it took 0.8069126963615417 mins from beginning of epoch till batch#262\n",
      "No improvement in training loss..the global training loss is -->1787.80|epoch train_loss = 3678.77 \n",
      "executing epoch:30, it took 0.8076306462287903 mins from beginning of epoch till batch#262\n",
      "No improvement in training loss..the global training loss is -->1787.80|epoch train_loss = 3727.59 \n",
      "executing epoch:31, it took 0.8072611053784688 mins from beginning of epoch till batch#262\n",
      "No improvement in training loss..the global training loss is -->1787.80|epoch train_loss = 3769.26 \n",
      "executing epoch:32, it took 0.8072601358095804 mins from beginning of epoch till batch#262\n",
      "No improvement in training loss..the global training loss is -->1787.80|epoch train_loss = 3805.77 \n",
      "executing epoch:33, it took 0.8073199470837911 mins from beginning of epoch till batch#262\n",
      "No improvement in training loss..the global training loss is -->1787.80|epoch train_loss = 3839.08 \n",
      "executing epoch:34, it took 0.8067416270573934 mins from beginning of epoch till batch#262\n",
      "No improvement in training loss..the global training loss is -->1787.80|epoch train_loss = 3869.55 \n",
      "executing epoch:35, it took 0.8062975605328878 mins from beginning of epoch till batch#262\n",
      "No improvement in training loss..the global training loss is -->1787.80|epoch train_loss = 3897.17 \n",
      "executing epoch:36, it took 0.8065986474355061 mins from beginning of epoch till batch#262\n",
      "No improvement in training loss..the global training loss is -->1787.80|epoch train_loss = 3922.47 \n",
      "executing epoch:37, it took 0.8065827369689942 mins from beginning of epoch till batch#262\n",
      "No improvement in training loss..the global training loss is -->1787.80|epoch train_loss = 3947.06 \n",
      "executing epoch:38, it took 0.8058343092600505 mins from beginning of epoch till batch#262\n",
      "No improvement in training loss..the global training loss is -->1787.80|epoch train_loss = 3970.64 \n",
      "executing epoch:39, it took 0.8061428984006246 mins from beginning of epoch till batch#262\n",
      "No improvement in training loss..the global training loss is -->1787.80|epoch train_loss = 3991.62 \n",
      "executing epoch:40, it took 0.8061346054077149 mins from beginning of epoch till batch#262\n",
      "No improvement in training loss..the global training loss is -->1787.80|epoch train_loss = 4011.35 \n",
      "executing epoch:41, it took 0.8066771467526753 mins from beginning of epoch till batch#262\n",
      "No improvement in training loss..the global training loss is -->1787.80|epoch train_loss = 4028.29 \n",
      "executing epoch:42, it took 0.8057797471682231 mins from beginning of epoch till batch#262\n",
      "No improvement in training loss..the global training loss is -->1787.80|epoch train_loss = 4045.88 \n",
      "executing epoch:43, it took 0.805240531762441 mins from beginning of epoch till batch#262\n",
      "No improvement in training loss..the global training loss is -->1787.80|epoch train_loss = 4061.05 \n",
      "executing epoch:44, it took 0.8057466467221578 mins from beginning of epoch till batch#262\n",
      "No improvement in training loss..the global training loss is -->1787.80|epoch train_loss = 4076.79 \n",
      "executing epoch:45, it took 0.8051245252291361 mins from beginning of epoch till batch#262\n",
      "No improvement in training loss..the global training loss is -->1787.80|epoch train_loss = 4090.87 \n",
      "executing epoch:46, it took 0.8055732409159343 mins from beginning of epoch till batch#262\n",
      "No improvement in training loss..the global training loss is -->1787.80|epoch train_loss = 4105.53 \n",
      "executing epoch:47, it took 0.8048671404520671 mins from beginning of epoch till batch#262\n",
      "No improvement in training loss..the global training loss is -->1787.80|epoch train_loss = 4117.14 \n",
      "executing epoch:48, it took 0.8053542494773864 mins from beginning of epoch till batch#262\n",
      "No improvement in training loss..the global training loss is -->1787.80|epoch train_loss = 4128.85 \n",
      "executing epoch:49, it took 0.8047973195711772 mins from beginning of epoch till batch#262\n",
      "No improvement in training loss..the global training loss is -->1787.80|epoch train_loss = 4140.22 \n",
      "executing epoch:50, it took 0.8047478953997295 mins from beginning of epoch till batch#262\n",
      "No improvement in training loss..the global training loss is -->1787.80|epoch train_loss = 4150.82 \n",
      "executing epoch:51, it took 0.8047152400016785 mins from beginning of epoch till batch#262\n",
      "No improvement in training loss..the global training loss is -->1787.80|epoch train_loss = 4161.01 \n",
      "executing epoch:52, it took 0.8040628631909689 mins from beginning of epoch till batch#262\n",
      "No improvement in training loss..the global training loss is -->1787.80|epoch train_loss = 4172.78 \n",
      "executing epoch:53, it took 0.8047727862993876 mins from beginning of epoch till batch#262\n",
      "No improvement in training loss..the global training loss is -->1787.80|epoch train_loss = 4179.09 \n",
      "executing epoch:54, it took 0.8046316464742025 mins from beginning of epoch till batch#262\n",
      "No improvement in training loss..the global training loss is -->1787.80|epoch train_loss = 4186.50 \n",
      "executing epoch:55, it took 0.804907222588857 mins from beginning of epoch till batch#262\n",
      "No improvement in training loss..the global training loss is -->1787.80|epoch train_loss = 4193.38 \n",
      "executing epoch:56, it took 0.8048475503921508 mins from beginning of epoch till batch#262\n",
      "No improvement in training loss..the global training loss is -->1787.80|epoch train_loss = 4203.79 \n",
      "executing epoch:57, it took 0.8043764432271322 mins from beginning of epoch till batch#262\n",
      "No improvement in training loss..the global training loss is -->1787.80|epoch train_loss = 4212.94 \n",
      "executing epoch:58, it took 0.8040909806887309 mins from beginning of epoch till batch#262\n",
      "No improvement in training loss..the global training loss is -->1787.80|epoch train_loss = 4218.40 \n",
      "executing epoch:59, it took 0.8039709210395813 mins from beginning of epoch till batch#262\n",
      "No improvement in training loss..the global training loss is -->1787.80|epoch train_loss = 4225.49 \n",
      "executing epoch:60, it took 0.8038828452428182 mins from beginning of epoch till batch#262\n",
      "No improvement in training loss..the global training loss is -->1787.80|epoch train_loss = 4233.02 \n",
      "executing epoch:61, it took 0.8033395767211914 mins from beginning of epoch till batch#262\n",
      "No improvement in training loss..the global training loss is -->1787.80|epoch train_loss = 4241.80 \n",
      "executing epoch:62, it took 0.8041192531585694 mins from beginning of epoch till batch#262\n",
      "No improvement in training loss..the global training loss is -->1787.80|epoch train_loss = 4246.16 \n",
      "executing epoch:63, it took 0.8037877400716146 mins from beginning of epoch till batch#262\n",
      "No improvement in training loss..the global training loss is -->1787.80|epoch train_loss = 4252.10 \n",
      "executing epoch:64, it took 0.8037614504496257 mins from beginning of epoch till batch#262\n",
      "No improvement in training loss..the global training loss is -->1787.80|epoch train_loss = 4260.50 \n",
      "executing epoch:65, it took 0.8035970012346904 mins from beginning of epoch till batch#262\n",
      "No improvement in training loss..the global training loss is -->1787.80|epoch train_loss = 4264.26 \n",
      "executing epoch:66, it took 0.8035134832064311 mins from beginning of epoch till batch#262\n",
      "No improvement in training loss..the global training loss is -->1787.80|epoch train_loss = 4270.18 \n",
      "executing epoch:67, it took 0.8040265679359436 mins from beginning of epoch till batch#262\n",
      "No improvement in training loss..the global training loss is -->1787.80|epoch train_loss = 4276.94 \n",
      "executing epoch:68, it took 0.8035696784655253 mins from beginning of epoch till batch#262\n",
      "No improvement in training loss..the global training loss is -->1787.80|epoch train_loss = 4274.80 \n",
      "executing epoch:69, it took 0.8036893526713054 mins from beginning of epoch till batch#262\n",
      "No improvement in training loss..the global training loss is -->1787.80|epoch train_loss = 4280.31 \n",
      "executing epoch:70, it took 0.8040216724077861 mins from beginning of epoch till batch#262\n",
      "No improvement in training loss..the global training loss is -->1787.80|epoch train_loss = 4287.36 \n",
      "executing epoch:71, it took 0.8032957116762797 mins from beginning of epoch till batch#262\n",
      "No improvement in training loss..the global training loss is -->1787.80|epoch train_loss = 4293.50 \n",
      "executing epoch:72, it took 0.8034097194671631 mins from beginning of epoch till batch#262\n",
      "No improvement in training loss..the global training loss is -->1787.80|epoch train_loss = 4297.96 \n",
      "executing epoch:73, it took 0.8035100261370342 mins from beginning of epoch till batch#262\n",
      "No improvement in training loss..the global training loss is -->1787.80|epoch train_loss = 4299.71 \n",
      "executing epoch:74, it took 0.8029280662536621 mins from beginning of epoch till batch#262\n",
      "No improvement in training loss..the global training loss is -->1787.80|epoch train_loss = 4304.25 \n",
      "executing epoch:75, it took 0.8034516533215841 mins from beginning of epoch till batch#262\n",
      "No improvement in training loss..the global training loss is -->1787.80|epoch train_loss = 4309.00 \n",
      "executing epoch:76, it took 0.8038477738698323 mins from beginning of epoch till batch#262\n",
      "No improvement in training loss..the global training loss is -->1787.80|epoch train_loss = 4305.77 \n",
      "executing epoch:77, it took 0.8041961908340454 mins from beginning of epoch till batch#262\n",
      "No improvement in training loss..the global training loss is -->1787.80|epoch train_loss = 4307.60 \n",
      "executing epoch:78, it took 0.8042135953903198 mins from beginning of epoch till batch#262\n",
      "No improvement in training loss..the global training loss is -->1787.80|epoch train_loss = 4308.96 \n",
      "executing epoch:79, it took 0.8039506594340007 mins from beginning of epoch till batch#262\n",
      "No improvement in training loss..the global training loss is -->1787.80|epoch train_loss = 4311.16 \n",
      "executing epoch:80, it took 0.8037167469660441 mins from beginning of epoch till batch#262\n",
      "No improvement in training loss..the global training loss is -->1787.80|epoch train_loss = 4313.59 \n",
      "executing epoch:81, it took 0.8039737224578858 mins from beginning of epoch till batch#262\n",
      "No improvement in training loss..the global training loss is -->1787.80|epoch train_loss = 4316.47 \n",
      "executing epoch:82, it took 0.8037711302439372 mins from beginning of epoch till batch#262\n",
      "No improvement in training loss..the global training loss is -->1787.80|epoch train_loss = 4319.42 \n",
      "executing epoch:83, it took 0.8032791296641032 mins from beginning of epoch till batch#262\n",
      "No improvement in training loss..the global training loss is -->1787.80|epoch train_loss = 4321.93 \n",
      "executing epoch:84, it took 0.8034054597218832 mins from beginning of epoch till batch#262\n",
      "No improvement in training loss..the global training loss is -->1787.80|epoch train_loss = 4324.08 \n",
      "executing epoch:85, it took 0.8035027384757996 mins from beginning of epoch till batch#262\n",
      "No improvement in training loss..the global training loss is -->1787.80|epoch train_loss = 4326.27 \n",
      "executing epoch:86, it took 0.803214156627655 mins from beginning of epoch till batch#262\n",
      "No improvement in training loss..the global training loss is -->1787.80|epoch train_loss = 4327.62 \n",
      "executing epoch:87, it took 0.8032987197240193 mins from beginning of epoch till batch#262\n",
      "No improvement in training loss..the global training loss is -->1787.80|epoch train_loss = 4328.85 \n",
      "executing epoch:88, it took 0.8030939539273579 mins from beginning of epoch till batch#262\n",
      "No improvement in training loss..the global training loss is -->1787.80|epoch train_loss = 4330.01 \n",
      "executing epoch:89, it took 0.8035327911376953 mins from beginning of epoch till batch#262\n",
      "No improvement in training loss..the global training loss is -->1787.80|epoch train_loss = 4330.57 \n",
      "executing epoch:90, it took 0.8036267638206482 mins from beginning of epoch till batch#262\n",
      "No improvement in training loss..the global training loss is -->1787.80|epoch train_loss = 4331.02 \n",
      "executing epoch:91, it took 0.8035284956296285 mins from beginning of epoch till batch#262\n",
      "No improvement in training loss..the global training loss is -->1787.80|epoch train_loss = 4331.86 \n",
      "executing epoch:92, it took 0.8031643231709799 mins from beginning of epoch till batch#262\n",
      "No improvement in training loss..the global training loss is -->1787.80|epoch train_loss = 4332.65 \n",
      "executing epoch:93, it took 0.8033183296521504 mins from beginning of epoch till batch#262\n",
      "No improvement in training loss..the global training loss is -->1787.80|epoch train_loss = 4333.08 \n",
      "executing epoch:94, it took 0.8032402356465658 mins from beginning of epoch till batch#262\n",
      "No improvement in training loss..the global training loss is -->1787.80|epoch train_loss = 4333.43 \n",
      "executing epoch:95, it took 0.8029611229896545 mins from beginning of epoch till batch#262\n",
      "No improvement in training loss..the global training loss is -->1787.80|epoch train_loss = 4333.61 \n",
      "executing epoch:96, it took 0.8034756620724995 mins from beginning of epoch till batch#262\n",
      "No improvement in training loss..the global training loss is -->1787.80|epoch train_loss = 4333.84 \n",
      "executing epoch:97, it took 0.8033622900644938 mins from beginning of epoch till batch#262\n",
      "No improvement in training loss..the global training loss is -->1787.80|epoch train_loss = 4333.99 \n",
      "executing epoch:98, it took 0.8034892598787944 mins from beginning of epoch till batch#262\n",
      "No improvement in training loss..the global training loss is -->1787.80|epoch train_loss = 4333.95 \n",
      "executing epoch:99, it took 0.8034278353055319 mins from beginning of epoch till batch#262\n",
      "No improvement in training loss..the global training loss is -->1787.80|epoch train_loss = 4333.99 \n",
      "executing epoch:100, it took 0.803728969891866 mins from beginning of epoch till batch#262\n",
      "No improvement in training loss..the global training loss is -->1787.80|epoch train_loss = 4334.02 \n",
      "executing epoch:101, it took 0.8034112731615702 mins from beginning of epoch till batch#262\n",
      "No improvement in training loss..the global training loss is -->1787.80|epoch train_loss = 4333.99 \n",
      "executing epoch:102, it took 0.803443189462026 mins from beginning of epoch till batch#262\n",
      "No improvement in training loss..the global training loss is -->1787.80|epoch train_loss = 4334.06 \n",
      "executing epoch:103, it took 0.80372074842453 mins from beginning of epoch till batch#262\n",
      "No improvement in training loss..the global training loss is -->1787.80|epoch train_loss = 4334.00 \n",
      "executing epoch:104, it took 0.8039199908574423 mins from beginning of epoch till batch#262\n",
      "No improvement in training loss..the global training loss is -->1787.80|epoch train_loss = 4334.01 \n",
      "executing epoch:105, it took 0.8033082167307536 mins from beginning of epoch till batch#262\n",
      "No improvement in training loss..the global training loss is -->1787.80|epoch train_loss = 4333.99 \n",
      "executing epoch:106, it took 0.803125258286794 mins from beginning of epoch till batch#262\n",
      "No improvement in training loss..the global training loss is -->1787.80|epoch train_loss = 4334.08 \n",
      "executing epoch:107, it took 0.8038861234982808 mins from beginning of epoch till batch#262\n",
      "No improvement in training loss..the global training loss is -->1787.80|epoch train_loss = 4334.09 \n",
      "executing epoch:108, it took 0.8036835471789042 mins from beginning of epoch till batch#262\n",
      "No improvement in training loss..the global training loss is -->1787.80|epoch train_loss = 4334.07 \n",
      "executing epoch:109, it took 0.8034327546755473 mins from beginning of epoch till batch#262\n",
      "No improvement in training loss..the global training loss is -->1787.80|epoch train_loss = 4334.13 \n",
      "executing epoch:110, it took 0.8038945237795512 mins from beginning of epoch till batch#262\n",
      "No improvement in training loss..the global training loss is -->1787.80|epoch train_loss = 4334.08 \n"
     ]
    }
   ],
   "source": [
    "tr_model = train_model(train_loader, val_loader, model =  model,tokenizer = tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aafaf287",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c6d995dd",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'inp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_98451/3314877709.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mids\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'inp' is not defined"
     ]
    }
   ],
   "source": [
    "for ids in inp:\n",
    "    print(tokenizer.decode(ids, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caeae64d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d85c762",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09fe54b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e98705d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7843a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13bbc56a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a194f09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92dbe8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10399441",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
