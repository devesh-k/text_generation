{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6685ada5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## reference https://huggingface.co/learn/nlp-course/en/chapter7/6?fw=pt#training-a-causal-language-model-from-scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900345c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f6f7200",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: datasets in /opt/conda/lib/python3.8/site-packages (2.18.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.8/site-packages (from datasets) (3.4.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.8/site-packages (from datasets) (6.0)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.8/site-packages (from datasets) (1.3.4)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.8/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.8/site-packages (from datasets) (16.1.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.8/site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.8/site-packages (from datasets) (4.66.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.8/site-packages (from datasets) (1.21.4)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.8/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.8/site-packages (from datasets) (3.9.5)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.8/site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.8/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec[http]<=2024.2.0,>=2023.1.0 in /opt/conda/lib/python3.8/site-packages (from datasets) (2024.2.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.8/site-packages (from datasets) (21.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.4 in /opt/conda/lib/python3.8/site-packages (from datasets) (0.23.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets) (21.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.8/site-packages (from huggingface-hub>=0.19.4->datasets) (4.11.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging->datasets) (3.0.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (1.26.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (2.0.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.8/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.8/site-packages (from pandas->datasets) (2021.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: jupyter in /opt/conda/lib/python3.8/site-packages (1.0.0)\n",
      "Requirement already satisfied: ipywidgets in /opt/conda/lib/python3.8/site-packages (8.1.3)\n",
      "Requirement already satisfied: qtconsole in /opt/conda/lib/python3.8/site-packages (from jupyter) (5.5.2)\n",
      "Requirement already satisfied: notebook in /opt/conda/lib/python3.8/site-packages (from jupyter) (6.4.1)\n",
      "Requirement already satisfied: nbconvert in /opt/conda/lib/python3.8/site-packages (from jupyter) (6.3.0)\n",
      "Requirement already satisfied: jupyter-console in /opt/conda/lib/python3.8/site-packages (from jupyter) (6.6.3)\n",
      "Requirement already satisfied: ipykernel in /opt/conda/lib/python3.8/site-packages (from jupyter) (6.29.4)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (5.14.3)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (7.30.0)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.11 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (3.0.11)\n",
      "Requirement already satisfied: comm>=0.1.3 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.11 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (4.0.11)\n",
      "Requirement already satisfied: jedi>=0.16 in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.18.1)\n",
      "Requirement already satisfied: pygments in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (2.10.0)\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (4.8.0)\n",
      "Requirement already satisfied: decorator in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (5.1.0)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.47)\n",
      "Requirement already satisfied: setuptools>=18.5 in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (59.4.0)\n",
      "Requirement already satisfied: backcall in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: matplotlib-inline in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.3)\n",
      "Requirement already satisfied: pickleshare in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.7.5)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /opt/conda/lib/python3.8/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.8/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.8/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=6.1.0->ipywidgets) (0.2.5)\n",
      "Requirement already satisfied: nest-asyncio in /opt/conda/lib/python3.8/site-packages (from ipykernel->jupyter) (1.5.4)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.8/site-packages (from ipykernel->jupyter) (5.8.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.8/site-packages (from ipykernel->jupyter) (21.3)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in /opt/conda/lib/python3.8/site-packages (from ipykernel->jupyter) (1.8.1)\n",
      "Requirement already satisfied: pyzmq>=24 in /opt/conda/lib/python3.8/site-packages (from ipykernel->jupyter) (26.0.3)\n",
      "Requirement already satisfied: tornado>=6.1 in /opt/conda/lib/python3.8/site-packages (from ipykernel->jupyter) (6.1)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in /opt/conda/lib/python3.8/site-packages (from ipykernel->jupyter) (7.1.0)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /opt/conda/lib/python3.8/site-packages (from ipykernel->jupyter) (5.7.2)\n",
      "Requirement already satisfied: entrypoints in /opt/conda/lib/python3.8/site-packages (from jupyter-client>=6.1.12->ipykernel->jupyter) (0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /opt/conda/lib/python3.8/site-packages (from jupyter-client>=6.1.12->ipykernel->jupyter) (2.8.2)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /opt/conda/lib/python3.8/site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel->jupyter) (4.2.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.8/site-packages (from python-dateutil>=2.1->jupyter-client>=6.1.12->ipykernel->jupyter) (1.16.0)\n",
      "Requirement already satisfied: defusedxml in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (0.7.1)\n",
      "Requirement already satisfied: mistune<2,>=0.8.1 in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (0.8.4)\n",
      "Requirement already satisfied: bleach in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (4.1.0)\n",
      "Requirement already satisfied: testpath in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (0.5.0)\n",
      "Requirement already satisfied: jupyterlab-pygments in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (0.1.2)\n",
      "Requirement already satisfied: nbclient<0.6.0,>=0.5.0 in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (0.5.9)\n",
      "Requirement already satisfied: nbformat>=4.4 in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (5.1.3)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (1.5.0)\n",
      "Requirement already satisfied: jinja2>=2.4 in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (3.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.8/site-packages (from jinja2>=2.4->nbconvert->jupyter) (2.0.1)\n",
      "Requirement already satisfied: ipython-genutils in /opt/conda/lib/python3.8/site-packages (from nbformat>=4.4->nbconvert->jupyter) (0.2.0)\n",
      "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /opt/conda/lib/python3.8/site-packages (from nbformat>=4.4->nbconvert->jupyter) (4.2.1)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /opt/conda/lib/python3.8/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.4->nbconvert->jupyter) (0.18.0)\n",
      "Requirement already satisfied: importlib-resources>=1.4.0 in /opt/conda/lib/python3.8/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.4->nbconvert->jupyter) (5.4.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /opt/conda/lib/python3.8/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.4->nbconvert->jupyter) (21.2.0)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /opt/conda/lib/python3.8/site-packages (from importlib-resources>=1.4.0->jsonschema!=2.5.0,>=2.4->nbformat>=4.4->nbconvert->jupyter) (3.6.0)\n",
      "Requirement already satisfied: webencodings in /opt/conda/lib/python3.8/site-packages (from bleach->nbconvert->jupyter) (0.5.1)\n",
      "Requirement already satisfied: prometheus-client in /opt/conda/lib/python3.8/site-packages (from notebook->jupyter) (0.12.0)\n",
      "Requirement already satisfied: terminado>=0.8.3 in /opt/conda/lib/python3.8/site-packages (from notebook->jupyter) (0.12.1)\n",
      "Requirement already satisfied: Send2Trash>=1.5.0 in /opt/conda/lib/python3.8/site-packages (from notebook->jupyter) (1.8.0)\n",
      "Requirement already satisfied: argon2-cffi in /opt/conda/lib/python3.8/site-packages (from notebook->jupyter) (21.1.0)\n",
      "Requirement already satisfied: cffi>=1.0.0 in /opt/conda/lib/python3.8/site-packages (from argon2-cffi->notebook->jupyter) (1.15.0)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.8/site-packages (from cffi>=1.0.0->argon2-cffi->notebook->jupyter) (2.21)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging->ipykernel->jupyter) (3.0.6)\n",
      "Requirement already satisfied: qtpy>=2.4.0 in /opt/conda/lib/python3.8/site-packages (from qtconsole->jupyter) (2.4.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#installing some libraries\n",
    "!pip install datasets\n",
    "!pip install --upgrade jupyter ipywidgets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4819d7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch, transformers\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import PreTrainedModel, PretrainedConfig\n",
    "from transformers import AutoModel, AutoConfig,AutoModelForCausalLM, AutoConfig\n",
    "\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "#from datasets import load_dataset\n",
    "import pandas as pd, numpy as np\n",
    "from torch import cuda\n",
    "import datetime\n",
    "import warnings,itertools\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "# Ignore all warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "#pip install transformers bitsandbytes>=0.39.0 -q\n",
    "import zipfile,logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aafa5a3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "model\n"
     ]
    }
   ],
   "source": [
    "#global params for training\n",
    "\n",
    "B,T = 32,1024\n",
    "epoch = 100\n",
    "random_init_wts = True\n",
    "min_text_len = 0\n",
    "# train_loss_list = []\n",
    "# val_loss_list =[]\n",
    "if cuda.is_available():\n",
    "    device = torch.device('cuda:0')\n",
    "    print(device)\n",
    "else:\n",
    "    device = 'cpu'\n",
    "#print(device)\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "#os.environ[\"MKL_DEBUG_CPU_TYPE\"] = \"5\"\n",
    "context_length = None\n",
    "global_tr_loss = torch.inf\n",
    "global_val_loss = torch.inf\n",
    "#print(global_tr_loss)\n",
    "model_path = os.path.join(\"model\")\n",
    "print(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0be34f3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./data/unzip_text_10M'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "directory = os.path.join('.','data','unzip_text_10M')  # Replace with your directory path\n",
    "directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6244239",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_text(directory):\n",
    "    directory = os.path.join('.','data','unzip_text_10M',str(directory))  # Replace with your directory path\n",
    "    print(f\"directory :{directory}\")\n",
    "    # List all files in the directory\n",
    "    files = [f for f in os.listdir(directory) if os.path.isfile(os.path.join(directory, f))]\n",
    "    print(f\"files:{files}\")\n",
    "    text_content = []\n",
    "    # Read each file\n",
    "    total_lines = 0\n",
    "    for filenum,filename in enumerate(files):\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            #first_line = file.read()\n",
    "            #print(f\"filename :{filename}->first few lines {first_line}\")\n",
    "            #continue\n",
    "            #lines_list = [line.strip() for line in open(file_path, 'r')]\n",
    "            text = file.read()\n",
    "            text_content.append(text)\n",
    "            print(f\"the file:{filename} has been appeneded to the uber list and its length is {len(text_content)} \")\n",
    "            #total_lines+=len(lines_list)\n",
    "            #text_content.append(lines_list)\n",
    "    \n",
    "    flattened_list = ''.join(text_content)\n",
    "    assert (len(flattened_list) == total_lines , f\"Expected {len(flattened_list)} to be equal to {total_lines}\" )\n",
    "    \n",
    "    return flattened_list\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae33a46c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "directory :./data/unzip_text_10M/train_10M\n",
      "files:['switchboard.train', 'simple_wiki.train', 'open_subtitles.train', 'gutenberg.train', 'childes.train', 'bnc_spoken.train']\n",
      "the file:switchboard.train has been appeneded to the uber list and its length is 1 \n",
      "the file:simple_wiki.train has been appeneded to the uber list and its length is 2 \n",
      "the file:open_subtitles.train has been appeneded to the uber list and its length is 3 \n",
      "the file:gutenberg.train has been appeneded to the uber list and its length is 4 \n",
      "the file:childes.train has been appeneded to the uber list and its length is 5 \n",
      "the file:bnc_spoken.train has been appeneded to the uber list and its length is 6 \n"
     ]
    }
   ],
   "source": [
    "train_list = read_text(\"train_10M\")\n",
    "#print(train_dict)\n",
    "#val_list = read_text(\"dev\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f7c22e9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54215049"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5b96830f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1654\n"
     ]
    }
   ],
   "source": [
    "chunks = len(train_list)//(B*T)\n",
    "print(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d5e4e866",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17191971 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilgpt2\",return_tensors = \"pt\" , truncate = True, max_length  = context_length ,return_overflowing_tokens=True , padding = False,)\n",
    "enc_train = tokenizer(train_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2f4c964e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.1535097982657136"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comp_ratio = len(train_list)/len(enc_train['input_ids'])\n",
    "comp_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eedb8589",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "directory :./data/unzip_text_10M/dev\n",
      "files:['switchboard.dev', 'simple_wiki.dev', 'open_subtitles.dev', 'gutenberg.dev', 'childes.dev', 'bnc_spoken.dev']\n",
      "the file:switchboard.dev has been appeneded to the uber list and its length is 1 \n",
      "the file:simple_wiki.dev has been appeneded to the uber list and its length is 2 \n",
      "the file:open_subtitles.dev has been appeneded to the uber list and its length is 3 \n",
      "the file:gutenberg.dev has been appeneded to the uber list and its length is 4 \n",
      "the file:childes.dev has been appeneded to the uber list and its length is 5 \n",
      "the file:bnc_spoken.dev has been appeneded to the uber list and its length is 6 \n"
     ]
    }
   ],
   "source": [
    "val_list = read_text(\"dev\")\n",
    "enc_val = tokenizer(val_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc56620",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d6dace72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df_from_list(enc , B= B, T = T, val= False):\n",
    "    chunk_size = B*T\n",
    "    if val:\n",
    "        chunk_size = 2*B*T\n",
    "        \n",
    "    long_list_inp = enc['input_ids']\n",
    "    long_list_attention = enc['attention_mask']\n",
    "\n",
    "    # Step 3: Split the list into chunks and pad the last chunk if necessary\n",
    "    chunks_inp = [long_list_inp[i:i + chunk_size] for i in range(0, len(long_list_inp), chunk_size)]\n",
    "    chunks_att = [long_list_attention[i:i + chunk_size] for i in range(0, len(long_list_attention), chunk_size)]\n",
    "    df = pd.DataFrame({'input_ids': chunks_inp,'attention_mask':chunks_att})\n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "713e228d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the dataframe is = 525\n"
     ]
    }
   ],
   "source": [
    "df_train_temp = get_df_from_list(enc_train)\n",
    "# Display the DataFrame\n",
    "df_train_temp.head()\n",
    "print(f\"Length of the dataframe is = {len(df_train_temp)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a6d4c4eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the VALDATION dataframe is = 266\n"
     ]
    }
   ],
   "source": [
    "df_val_temp = get_df_from_list(enc_val,val = True)\n",
    "# Display the DataFrame\n",
    "df_val_temp.head()\n",
    "print(f\"Length of the VALDATION dataframe is = {len(df_val_temp)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b061219d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_df(df,eos_char = tokenizer.eos_token_id,B = B, T = T,val = False):\n",
    "    if val:\n",
    "        B = 2*B\n",
    "    for ind,row in df.iterrows():\n",
    "        if len(row['input_ids']) != B*T :\n",
    "            print(f\"row = {ind} and input_id length = {len(row['input_ids'])}\")\n",
    "            print(f\"row = {ind} and attention length = {len(row['attention_mask'])}\")\n",
    "            pad_len = B*T - len(row['input_ids'])\n",
    "            print(f\"padding the row index {ind} with {pad_len} character\")\n",
    "            row['input_ids'] = row['input_ids']+ [eos_char] * pad_len\n",
    "            #attention mask should be padded to 0\n",
    "            row['attention_mask'] = row['attention_mask']+ [0] * pad_len\n",
    "            print(\"#### POST CONCAT####\")\n",
    "            print(f\"row = {ind} and input_id length = {len(row['input_ids'])}\")\n",
    "            print(f\"row = {ind} and attention length ={len(row['attention_mask'])}\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def verify_len(df,val = False, B= B):\n",
    "    row_ind = []\n",
    "    if val:\n",
    "        B = 2*B\n",
    "    for ind,row in df.iterrows():\n",
    "        if len(row['input_ids']) != B*T :\n",
    "            row_ind.append(ind)\n",
    "        else:\n",
    "            continue\n",
    "    if len(row_ind) !=0:\n",
    "        print(\"CONCATENATION Did not work\")\n",
    "    else:\n",
    "        print(\"CONCATENATION worked\")\n",
    "        \n",
    "            \n",
    "        \n",
    "    \n",
    "        \n",
    "        \n",
    "   \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b7d3fd69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row = 524 and input_id length = 21539\n",
      "row = 524 and attention length = 21539\n",
      "padding the row index 524 with 11229 character\n",
      "#### POST CONCAT####\n",
      "row = 524 and input_id length = 32768\n",
      "row = 524 and attention length =32768\n",
      "CONCATENATION worked\n"
     ]
    }
   ],
   "source": [
    "df_train  = pad_df(df_train_temp)\n",
    "verify_len(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "651e4657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row = 265 and input_id length = 46356\n",
      "row = 265 and attention length = 46356\n",
      "padding the row index 265 with 19180 character\n",
      "#### POST CONCAT####\n",
      "row = 265 and input_id length = 65536\n",
      "row = 265 and attention length =65536\n",
      "CONCATENATION worked\n"
     ]
    }
   ],
   "source": [
    "df_val  = pad_df(df_val_temp,val = True)\n",
    "verify_len(df_val,val = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d21ef277",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_ids</th>\n",
       "      <th>attention_mask</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[32, 25, 197, 40, 1101, 1654, 484, 389, 13, 19...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[6510, 11, 14104, 290, 27913, 13, 198, 32, 25,...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[314, 1612, 11, 340, 338, 1611, 286, 43244, 11...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[318, 407, 922, 329, 262, 1200, 2035, 13, 198,...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[198, 32, 25, 197, 1870, 11, 21480, 11, 314, 4...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           input_ids  \\\n",
       "0  [32, 25, 197, 40, 1101, 1654, 484, 389, 13, 19...   \n",
       "1  [6510, 11, 14104, 290, 27913, 13, 198, 32, 25,...   \n",
       "2  [314, 1612, 11, 340, 338, 1611, 286, 43244, 11...   \n",
       "3  [318, 407, 922, 329, 262, 1200, 2035, 13, 198,...   \n",
       "4  [198, 32, 25, 197, 1870, 11, 21480, 11, 314, 4...   \n",
       "\n",
       "                                      attention_mask  \n",
       "0  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "1  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "2  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "3  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "4  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0f2570",
   "metadata": {},
   "source": [
    "## Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "764f3c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the tokenizer:\n",
    "model_name = \"distilgpt2\"\n",
    "if random_init_wts:\n",
    "    config = AutoConfig.from_pretrained(model_name, vocab_size = 50304)\n",
    "    # Initialize the model with random weights\n",
    "    model = AutoModelForCausalLM.from_config(config)\n",
    "else:\n",
    "    #model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\"distilgpt2\")\n",
    "\n",
    "model = torch.compile(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84477631",
   "metadata": {},
   "source": [
    "### Data loaders and Dataset for batched training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a8549f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataset_pyt_val(Dataset):\n",
    "    def __init__(self, df, B = 2*B, T = T ):\n",
    "        self.df = df\n",
    "        print(f\"Value of B {B}\")\n",
    "                                        \n",
    "    def __getitem__(self, idx):\n",
    "        #print(f\"inside loader...idx ->{idx}\")\n",
    "        input_id_temp = torch.tensor(self.df.iloc[idx]['input_ids'],dtype = torch.long)\n",
    "        #print(f\"length of text ->{len(text)}\")\n",
    "        #print(f\"text ->{text}\")\n",
    "        #encodings = tokenizer(text, truncation=True, max_length= self.max_length, return_overflowing_tokens=True, padding = 'max_length',return_tensors='pt')\n",
    "        att_mask = torch.tensor(self.df.iloc[idx]['attention_mask'],dtype = torch.long)\n",
    "        # check the length of the encoded list\n",
    "        \n",
    "        #x_dict['input_id'] = input_ids_list\n",
    "        #x_dict['attention_mask'] = input_ids_list\n",
    "                \n",
    "        #print(f\"x_dict = {x_dict}\")             \n",
    "#       print(f\"inside the loader and input_id = {input_ids} and its shape is {input_ids.shape}\")\n",
    "        #labels = input_id_list\n",
    "        #input_ids = torch.tensor(input_id_list)\n",
    "        #attention_mask = torch.tensor(attention_mask_list)\n",
    "        #labels = torch.tensor(labels)\n",
    "        #print(f\"inside the loader and input_id shape= {input_ids.shape} attention_mask_shape is {attention_mask.shape} and label shape is {labels.shape}\")\n",
    "        #print(f\"encoding = {encodings}\")\n",
    "        input_id =   input_id_temp.view(2*B,T)    \n",
    "        attention_mask = att_mask.view(2*B,T)   \n",
    "        \n",
    "        return input_id, attention_mask\n",
    "        \n",
    "    def __len__(self):\n",
    "        #return the length of the dataframe\n",
    "        return len(self.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3a742340",
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataset_pyt_train(Dataset):\n",
    "    def __init__(self, df, B = B, T = T ):\n",
    "        self.df = df\n",
    "                                        \n",
    "    def __getitem__(self, idx):\n",
    "        #print(f\"inside loader...idx ->{idx}\")\n",
    "        input_id_temp = torch.tensor(self.df.iloc[idx]['input_ids'],dtype = torch.long)\n",
    "        #print(f\"length of text ->{len(text)}\")\n",
    "        #print(f\"text ->{text}\")\n",
    "        #encodings = tokenizer(text, truncation=True, max_length= self.max_length, return_overflowing_tokens=True, padding = 'max_length',return_tensors='pt')\n",
    "        att_mask = torch.tensor(self.df.iloc[idx]['attention_mask'],dtype = torch.long)\n",
    "        # check the length of the encoded list\n",
    "        \n",
    "        #x_dict['input_id'] = input_ids_list\n",
    "        #x_dict['attention_mask'] = input_ids_list\n",
    "                \n",
    "        #print(f\"x_dict = {x_dict}\")             \n",
    "#       print(f\"inside the loader and input_id = {input_ids} and its shape is {input_ids.shape}\")\n",
    "        #labels = input_id_list\n",
    "        #input_ids = torch.tensor(input_id_list)\n",
    "        #attention_mask = torch.tensor(attention_mask_list)\n",
    "        #labels = torch.tensor(labels)\n",
    "        #print(f\"inside the loader and input_id shape= {input_ids.shape} attention_mask_shape is {attention_mask.shape} and label shape is {labels.shape}\")\n",
    "        #print(f\"encoding = {encodings}\")\n",
    "        input_id =   input_id_temp.view(B,T)    \n",
    "        attention_mask = att_mask.view(B,T)   \n",
    "        \n",
    "        return input_id, attention_mask\n",
    "        \n",
    "    def __len__(self):\n",
    "        #return the length of the dataframe\n",
    "        return len(self.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e54c76aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value of B 64\n"
     ]
    }
   ],
   "source": [
    "#train_dataset = dataset_pyt(filtered_df,tokenizer = tokenizer)\n",
    "train_dataset = dataset_pyt_train(df_train)\n",
    "val_dataset = dataset_pyt_val(df_val)\n",
    "#test_dataset = dataset_pyt(df_test,tokenizer = tokenizer)\n",
    "\n",
    "train_loader = DataLoader(train_dataset,batch_size = 1, shuffle = True , num_workers = 4, pin_memory = True)\n",
    "val_loader = DataLoader(val_dataset,batch_size = 1, shuffle = True , num_workers = 4, pin_memory = True)\n",
    "#test_loader = DataLoader(test_dataset,batch_size = batch_size, shuffle = False, collate_fn = custom_collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e266e7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7c5dc1cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the train loader is 525\n",
      "Length of the val loader is 266\n",
      "num_tokens= 17203200\n"
     ]
    }
   ],
   "source": [
    "print(f\"Length of the train loader is {len(train_loader)}\")\n",
    "print(f\"Length of the val loader is {len(val_loader)}\")\n",
    "print(f\"num_tokens= {B*T*len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "68012ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_output = model(input_ids = inp ,attention_mask = att, labels = inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "18f91a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_file(log_message, model_name = model_name ,random_init_wts = random_init_wts ):\n",
    "    current_datetime = datetime.datetime.now()\n",
    "    # Extract date and time components\n",
    "    current_date = str(current_datetime.date())\n",
    "    log_file = model_name +'random_init_wts'+ '_'+random_init_wts+'_' +current_date+'.log'\n",
    "    print(f\"*****LOGGING INFO IN {log_file}*********\")\n",
    "    filepath = os.path.join(\"model\",log_file)\n",
    "    logging.basicConfig(filename=filepath, \n",
    "                    filemode='a',  # Overwrite the log file each time\n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s', \n",
    "                    level=logging.DEBUG)\n",
    "    logger = logging.getLogger()\n",
    "    logger.info(log_message)\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f51b2b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad\n",
    "def eval_model(val_loader, model, epoch , device = device,tokenizer = tokenizer):\n",
    "    global global_val_loss\n",
    "    #m = nn.Softmax()\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    e = epoch+1\n",
    "    val_loss_accum = 0.0\n",
    "    #criterion = torch.nn.BCEWithLogitsLoss()\n",
    "    print(f\"inside validation data for epoch {e}\")\n",
    "    #y_hat_val_list = []\n",
    "    #y_val_list = []\n",
    "    for ind,(input_id,attention_mask) in enumerate(val_loader):\n",
    "        #print(f\"id_list{id_list}\")\n",
    "        ids = input_id.to(device=device, non_blocking=True)\n",
    "        att_mask = attention_mask.to(device=device, non_blocking=True)\n",
    "        labels = ids\n",
    "        #predictions\n",
    "        #print(f\"input_ids device = {input_ids.device}\")\n",
    "        with autocast(dtype = torch.bfloat16):\n",
    "            model_output = model(input_ids = ids ,attention_mask = att_mask, labels = labels)\n",
    "            act_loss = model_output.loss\n",
    "        \n",
    "        val_loss_accum+=act_loss.detach().item()\n",
    "        del ids,att_mask,labels,act_loss\n",
    "    \n",
    "    if val_loss_accum < global_val_loss:\n",
    "        print(f\"Val loss has decreased -->reducing the global validation loss from {global_val_loss:.2f} to {val_loss_accum:.2f}\")\n",
    "        s1 = f\"Val loss has decreased -->reducing the global validation loss from {global_val_loss:.2f} to {val_loss_accum:.2f}\"\n",
    "        global_val_loss = val_loss_accum\n",
    "        print(f\" validation loss for epoch = {e} is {val_loss_accum:.4f}\")\n",
    "        s2 = f\" validation loss for epoch = {e} is {val_loss_accum:.4f}\"\n",
    "        #print metrics and save the model\n",
    "        #y_hat_val = torch.cat(y_hat_val_list)\n",
    "        #y_val = torch.cat(y_val_list)\n",
    "        #acc_val = accuracy_score(y_val.cpu().numpy(), y_hat_val.cpu().numpy())\n",
    "        #f1_val = f1_score(y_val.cpu().numpy(), y_hat_val.cpu().numpy(), average='micro')\n",
    "        print(f\" epoch= {e} :  val loss is {val_loss_accum:.4f} \")\n",
    "        s3 = f\" epoch= {e} :  val loss is {val_loss_accum:.4f} \"\n",
    "        #save the model\n",
    "        \n",
    "        # Get the current date and time\n",
    "        current_datetime = datetime.datetime.now()\n",
    "        # Extract date and time components\n",
    "        current_date = str(current_datetime.date())\n",
    "        current_time = str(current_datetime.time()).split('.')[0]\n",
    "        file_name = 'model'+ current_date+current_time+'.pth'\n",
    "        path = os.path.join(\"model\",file_name)\n",
    "        print(f\"saving the model {file_name}\")\n",
    "        s4 = f\"saving the model {file_name}\"\n",
    "        #torch.save(model.state_dict(), path)\n",
    "        model.save_pretrained(path)\n",
    "        tokenizer.save_pretrained(path)\n",
    "        log_message = s1+s2+s3+s4\n",
    "        write_file(log_message)\n",
    "        \n",
    "        #plot_confusion_matrix(y_val.cpu().numpy(), y_hat_val.cpu().numpy(), labels)\n",
    "    else:\n",
    "        print(f\"No improvement in validation loss-->epoch= {e} and global val loss is {global_val_loss:.2f}\")\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "414dc13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_loader,val_loader,model,num_epoch = 100,device = device,tokenizer = tokenizer):\n",
    "    global global_tr_loss\n",
    "    model.train()\n",
    "    device = device\n",
    "    lr_custom = 6e-4\n",
    "    print(f\"inside train model. Device = {device}\")\n",
    "    #adding betas params per the paper\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.AdamW(params =  model.parameters(), lr= lr_custom,betas = (.9,.95),fused = True ,weight_decay = .1)\n",
    "    total_batch_size = 2**19\n",
    "    grad_accum_step = total_batch_size//(B*T)\n",
    "    \n",
    "    extra_train = .1*num_epoch\n",
    "    #m = nn.Softmax()\n",
    "    max_train_steps = int(num_epoch +extra_train )\n",
    "    import time\n",
    "    from transformers import get_linear_schedule_with_warmup\n",
    "    scheduler_cos = transformers.get_cosine_schedule_with_warmup( optimizer= optimizer, num_warmup_steps =num_epoch*.1 ,num_training_steps= num_epoch-1 ,last_epoch = -1 )\n",
    "    \n",
    "        \n",
    "    for i in range (max_train_steps):\n",
    "        epoch_start_time = time.time()\n",
    "        epoch_train_loss = 0.0\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        batch_loss = 0.0\n",
    "        for ind,(input_id,attention_mask) in enumerate(train_loader):\n",
    "            if ind == int(len(train_loader)/2):\n",
    "                batch_time = time.time()\n",
    "                duration = batch_time - epoch_start_time\n",
    "                print(f\"executing epoch:{i+1}, it took {duration/60} mins from beginning of epoch till batch#{ind}\")\n",
    "            \n",
    "            ids = input_id.to(device=device, non_blocking=True)\n",
    "            att_mask = attention_mask.to(device=device, non_blocking=True)\n",
    "            labels = ids\n",
    "            \n",
    "            with autocast(dtype = torch.bfloat16):\n",
    "                model_output = model(input_ids = ids ,attention_mask = att_mask, labels = labels)\n",
    "                act_loss = model_output.loss\n",
    "            #print(f\"inside batch_processing. ind = {ind}|loss for ind = {act_loss} |batch_loss ={batch_loss} | epoch_train_loss = {epoch_train_loss}\")\n",
    "            batch_loss +=act_loss.detach().item()\n",
    "            #print(f\"incrementing batch_loss. ind = {ind}|loss for ind = {act_loss} |batch_loss ={batch_loss} | epoch_train_loss = {epoch_train_loss}\")\n",
    "            act_loss = act_loss/grad_accum_step\n",
    "            act_loss.backward()\n",
    "            epoch_train_loss += act_loss.detach().item()\n",
    "            \n",
    "            if (ind + 1)% grad_accum_step == 0:\n",
    "                #print(f\"Gradient accum step = {ind}|batch_loss before averaging ={batch_loss} | epoch_train_loss = {epoch_train_loss}\")\n",
    "                norm = torch.nn.utils.clip_grad_norm(model.parameters() , 1.0)\n",
    "                if i <= num_epoch:\n",
    "                    optimizer.step()\n",
    "                    optimizer.zero_grad(set_to_none=True)\n",
    "                else:\n",
    "                    optimizer_reduced_lr.step()\n",
    "                    optimizer_reduced_lr.zero_grad(set_to_none=True)\n",
    "                #print(f\"Gradient accum step = {ind}|batch_loss after averaging ={batch_loss} | epoch_train_loss = {epoch_train_loss}\")\n",
    "                #print(f\"Gradient accum step = {ind}|batch_loss after averaging ={batch_loss} | epoch_train_loss = {epoch_train_loss}\")\n",
    "                                \n",
    "            del ids,att_mask,labels\n",
    "            \n",
    "        \n",
    "        \n",
    "        #batch processing complete \n",
    "        if i <= num_epoch:\n",
    "            current_lr = scheduler_cos.get_last_lr()[0]\n",
    "            scheduler_cos.step()\n",
    "        if i >= num_epoch:\n",
    "            optimizer_reduced_lr = torch.optim.AdamW(params =  model.parameters(), lr= current_lr ,betas = (.9,.95),fused = True , weight_decay=.1)\n",
    "            optimizer_reduced_lr.zero_grad(set_to_none=True)\n",
    "            scheduler_constant = transformers.get_constant_schedule_with_warmup( optimizer = optimizer_reduced_lr ,num_warmup_steps = 0, last_epoch = -1 )\n",
    "            scheduler_constant.step()\n",
    "                      \n",
    "        #mean_loss = torch.mean(torch.tensor(epoch_train_loss))\n",
    "        epoch_end_time = time.time()\n",
    "        epoch_durn = (epoch_end_time - epoch_start_time)\n",
    "        num_token = B*T*len(train_loader)\n",
    "        if (epoch_train_loss < global_tr_loss) :\n",
    "            print(f\"training loss has decreased---> reducing the global loss from {global_tr_loss:.2f} to {epoch_train_loss:.2f} | throughput = {int(num_token/epoch_durn)} tokens/second | norm = {norm:.4f} | learning rate = {current_lr:.5e}\")\n",
    "            global_tr_loss = epoch_train_loss\n",
    "            print(f\" epoch= {i+1} and  train loss is {epoch_train_loss:.2f}\")\n",
    "            #printing training metrices\n",
    "            #y_hat = torch.cat(y_hat_list)\n",
    "            #y = torch.cat(label_list)\n",
    "            #acc = accuracy_score(y.cpu().numpy(), y_hat.cpu().numpy())\n",
    "            #f1 = f1_score(y.cpu().numpy(), y_hat.cpu().numpy(), average='micro')\n",
    "            #checking validation metrices\n",
    "            if (i%4 == 0):\n",
    "                eval_model(val_loader, model, epoch = i , device = device,tokenizer = tokenizer)\n",
    "            \n",
    "        else:\n",
    "            print(f\"No improvement in training loss..the global training loss is -->{global_tr_loss:.2f} \")\n",
    "            print(f\" epoch= {i+1} and mean train loss is {epoch_train_loss:.2f}\")\n",
    "        \n",
    "        \n",
    "    \n",
    "    return model\n",
    "        \n",
    "            \n",
    "            \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c784d5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inside train model. Device = cuda:0\n",
      "executing epoch:1, it took 1.2687084952990213 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the global loss from inf to 360.04 | throughput = 136440 tokens/second | norm = 13.7512 | learning rate = 0.00000e+00\n",
      " epoch= 1 and  train loss is 360.04\n",
      "inside validation data for epoch 1\n",
      "Val loss has decreased -->reducing the global validation loss from inf to 2917.99\n",
      " validation loss for epoch = 1 is 2917.9914\n",
      " epoch= 1 :  val loss is 2917.9914 \n",
      "saving the model model2024-06-1921:04:41.pth\n",
      "[2024-06-19 21:04:41,375] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "df: /root/.triton/autotune: No such file or directory\n",
      "df: /root/.triton/autotune: No such file or directory\n",
      "/opt/conda/compiler_compat/ld: cannot find -laio\n",
      "collect2: error: ld returned 1 exit status\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93m [WARNING] \u001b[0m async_io requires the dev libaio .so object and headers but these were not found.\n",
      "\u001b[93m [WARNING] \u001b[0m async_io: please install the libaio-dev package with apt\n",
      "\u001b[93m [WARNING] \u001b[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\n",
      "\u001b[93m [WARNING] \u001b[0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH\n",
      "\u001b[93m [WARNING] \u001b[0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3\n",
      "\u001b[93m [WARNING] \u001b[0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible\n",
      "*****LOGGING INFO IN 2024-06-19.log*********\n",
      "executing epoch:2, it took 1.2211641550064087 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the global loss from 360.04 to 251.60 | throughput = 144261 tokens/second | norm = 1.6333 | learning rate = 6.00000e-05\n",
      " epoch= 2 and  train loss is 251.60\n",
      "executing epoch:3, it took 0.768336280186971 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the global loss from 251.60 to 183.37 | throughput = 186402 tokens/second | norm = 1.3231 | learning rate = 1.20000e-04\n",
      " epoch= 3 and  train loss is 183.37\n",
      "executing epoch:4, it took 0.7718747019767761 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the global loss from 183.37 to 157.10 | throughput = 185649 tokens/second | norm = 1.3199 | learning rate = 1.80000e-04\n",
      " epoch= 4 and  train loss is 157.10\n",
      "executing epoch:5, it took 0.7738141457239787 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the global loss from 157.10 to 144.23 | throughput = 185384 tokens/second | norm = 0.5897 | learning rate = 2.40000e-04\n",
      " epoch= 5 and  train loss is 144.23\n",
      "inside validation data for epoch 5\n",
      "Val loss has decreased -->reducing the global validation loss from 2917.99 to 1191.19\n",
      " validation loss for epoch = 5 is 1191.1918\n",
      " epoch= 5 :  val loss is 1191.1918 \n",
      "saving the model model2024-06-1921:11:57.pth\n",
      "*****LOGGING INFO IN 2024-06-19.log*********\n",
      "executing epoch:6, it took 0.7735873619715373 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the global loss from 144.23 to 136.54 | throughput = 185441 tokens/second | norm = 0.6578 | learning rate = 3.00000e-04\n",
      " epoch= 6 and  train loss is 136.54\n",
      "executing epoch:7, it took 0.7741116722424825 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the global loss from 136.54 to 131.57 | throughput = 185293 tokens/second | norm = 0.7058 | learning rate = 3.60000e-04\n",
      " epoch= 7 and  train loss is 131.57\n",
      "executing epoch:8, it took 0.7742275794347128 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the global loss from 131.57 to 128.20 | throughput = 185312 tokens/second | norm = 0.4553 | learning rate = 4.20000e-04\n",
      " epoch= 8 and  train loss is 128.20\n",
      "executing epoch:9, it took 0.7748098572095236 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the global loss from 128.20 to 125.47 | throughput = 185197 tokens/second | norm = 0.7330 | learning rate = 4.80000e-04\n",
      " epoch= 9 and  train loss is 125.47\n",
      "inside validation data for epoch 9\n",
      "Val loss has decreased -->reducing the global validation loss from 1191.19 to 1097.93\n",
      " validation loss for epoch = 9 is 1097.9313\n",
      " epoch= 9 :  val loss is 1097.9313 \n",
      "saving the model model2024-06-1921:18:40.pth\n",
      "*****LOGGING INFO IN 2024-06-19.log*********\n",
      "executing epoch:10, it took 0.7754518946011861 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the global loss from 125.47 to 123.59 | throughput = 185165 tokens/second | norm = 0.4768 | learning rate = 5.40000e-04\n",
      " epoch= 10 and  train loss is 123.59\n",
      "executing epoch:11, it took 0.7744049072265625 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the global loss from 123.59 to 120.58 | throughput = 185178 tokens/second | norm = 0.6189 | learning rate = 6.00000e-04\n",
      " epoch= 11 and  train loss is 120.58\n",
      "executing epoch:12, it took 0.7749155521392822 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the global loss from 120.58 to 118.38 | throughput = 184989 tokens/second | norm = 0.4838 | learning rate = 5.99813e-04\n",
      " epoch= 12 and  train loss is 118.38\n",
      "executing epoch:13, it took 0.7750106970469157 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the global loss from 118.38 to 115.84 | throughput = 185120 tokens/second | norm = 0.4670 | learning rate = 5.99253e-04\n",
      " epoch= 13 and  train loss is 115.84\n",
      "inside validation data for epoch 13\n",
      "Val loss has decreased -->reducing the global validation loss from 1097.93 to 1047.19\n",
      " validation loss for epoch = 13 is 1047.1911\n",
      " epoch= 13 :  val loss is 1047.1911 \n",
      "saving the model model2024-06-1921:25:25.pth\n",
      "*****LOGGING INFO IN 2024-06-19.log*********\n",
      "executing epoch:14, it took 0.7736477216084798 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the global loss from 115.84 to 113.03 | throughput = 185290 tokens/second | norm = 0.4550 | learning rate = 5.98319e-04\n",
      " epoch= 14 and  train loss is 113.03\n",
      "executing epoch:15, it took 0.7745419979095459 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the global loss from 113.03 to 110.03 | throughput = 185102 tokens/second | norm = 0.5359 | learning rate = 5.97015e-04\n",
      " epoch= 15 and  train loss is 110.03\n",
      "executing epoch:16, it took 0.7742270270983378 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the global loss from 110.03 to 107.41 | throughput = 185187 tokens/second | norm = 0.4490 | learning rate = 5.95340e-04\n",
      " epoch= 16 and  train loss is 107.41\n",
      "executing epoch:17, it took 0.7747551004091898 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the global loss from 107.41 to 104.52 | throughput = 185114 tokens/second | norm = 0.5200 | learning rate = 5.93297e-04\n",
      " epoch= 17 and  train loss is 104.52\n",
      "inside validation data for epoch 17\n",
      "Val loss has decreased -->reducing the global validation loss from 1047.19 to 991.21\n",
      " validation loss for epoch = 17 is 991.2140\n",
      " epoch= 17 :  val loss is 991.2140 \n",
      "saving the model model2024-06-1921:32:09.pth\n",
      "*****LOGGING INFO IN 2024-06-19.log*********\n",
      "executing epoch:18, it took 0.7755895058314005 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the global loss from 104.52 to 101.78 | throughput = 185033 tokens/second | norm = 0.4332 | learning rate = 5.90888e-04\n",
      " epoch= 18 and  train loss is 101.78\n",
      "executing epoch:19, it took 0.7748296578725179 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the global loss from 101.78 to 99.01 | throughput = 185106 tokens/second | norm = 0.4378 | learning rate = 5.88118e-04\n",
      " epoch= 19 and  train loss is 99.01\n",
      "executing epoch:20, it took 0.7751107215881348 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the global loss from 99.01 to 96.59 | throughput = 184944 tokens/second | norm = 0.4891 | learning rate = 5.84988e-04\n",
      " epoch= 20 and  train loss is 96.59\n",
      "executing epoch:21, it took 0.7746238470077514 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the global loss from 96.59 to 94.16 | throughput = 185129 tokens/second | norm = 0.5926 | learning rate = 5.81503e-04\n",
      " epoch= 21 and  train loss is 94.16\n",
      "inside validation data for epoch 21\n",
      "Val loss has decreased -->reducing the global validation loss from 991.21 to 961.67\n",
      " validation loss for epoch = 21 is 961.6711\n",
      " epoch= 21 :  val loss is 961.6711 \n",
      "saving the model model2024-06-1921:38:53.pth\n",
      "*****LOGGING INFO IN 2024-06-19.log*********\n",
      "executing epoch:22, it took 0.7751245896021525 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the global loss from 94.16 to 91.71 | throughput = 185033 tokens/second | norm = 0.4052 | learning rate = 5.77668e-04\n",
      " epoch= 22 and  train loss is 91.71\n",
      "executing epoch:23, it took 0.7756651441256205 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the global loss from 91.71 to 89.76 | throughput = 184878 tokens/second | norm = 0.4538 | learning rate = 5.73486e-04\n",
      " epoch= 23 and  train loss is 89.76\n",
      "executing epoch:24, it took 0.7752796570460002 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the global loss from 89.76 to 87.24 | throughput = 184909 tokens/second | norm = 0.5148 | learning rate = 5.68964e-04\n",
      " epoch= 24 and  train loss is 87.24\n",
      "executing epoch:25, it took 0.7742231011390686 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the global loss from 87.24 to 84.65 | throughput = 185174 tokens/second | norm = 0.3932 | learning rate = 5.64107e-04\n",
      " epoch= 25 and  train loss is 84.65\n",
      "inside validation data for epoch 25\n",
      "No improvement in validation loss-->epoch= 25 and global val loss is 961.67\n",
      "executing epoch:26, it took 0.7758965571721395 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the global loss from 84.65 to 82.43 | throughput = 184926 tokens/second | norm = 0.4931 | learning rate = 5.58921e-04\n",
      " epoch= 26 and  train loss is 82.43\n",
      "executing epoch:27, it took 0.775204328695933 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the global loss from 82.43 to 80.05 | throughput = 184991 tokens/second | norm = 0.4678 | learning rate = 5.53412e-04\n",
      " epoch= 27 and  train loss is 80.05\n",
      "executing epoch:28, it took 0.7756253163019816 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the global loss from 80.05 to 77.67 | throughput = 184819 tokens/second | norm = 0.4904 | learning rate = 5.47587e-04\n",
      " epoch= 28 and  train loss is 77.67\n",
      "executing epoch:29, it took 0.7758031686147054 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the global loss from 77.67 to 75.12 | throughput = 184920 tokens/second | norm = 0.5525 | learning rate = 5.41454e-04\n",
      " epoch= 29 and  train loss is 75.12\n",
      "inside validation data for epoch 29\n",
      "No improvement in validation loss-->epoch= 29 and global val loss is 961.67\n",
      "executing epoch:30, it took 0.7756277958552042 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the global loss from 75.12 to 72.41 | throughput = 184962 tokens/second | norm = 0.5786 | learning rate = 5.35020e-04\n",
      " epoch= 30 and  train loss is 72.41\n",
      "executing epoch:31, it took 0.7760206778844198 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the global loss from 72.41 to 69.67 | throughput = 184801 tokens/second | norm = 0.7581 | learning rate = 5.28294e-04\n",
      " epoch= 31 and  train loss is 69.67\n",
      "executing epoch:32, it took 0.7756248752276103 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the global loss from 69.67 to 67.10 | throughput = 184967 tokens/second | norm = 0.6520 | learning rate = 5.21283e-04\n",
      " epoch= 32 and  train loss is 67.10\n",
      "executing epoch:33, it took 0.7758788347244263 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the global loss from 67.10 to 64.49 | throughput = 184894 tokens/second | norm = 0.7903 | learning rate = 5.13996e-04\n",
      " epoch= 33 and  train loss is 64.49\n",
      "inside validation data for epoch 33\n",
      "No improvement in validation loss-->epoch= 33 and global val loss is 961.67\n",
      "executing epoch:34, it took 0.7761201540629069 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the global loss from 64.49 to 61.84 | throughput = 184777 tokens/second | norm = 0.8752 | learning rate = 5.06442e-04\n",
      " epoch= 34 and  train loss is 61.84\n",
      "executing epoch:35, it took 0.7755060633023579 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the global loss from 61.84 to 59.12 | throughput = 185036 tokens/second | norm = 0.9299 | learning rate = 4.98632e-04\n",
      " epoch= 35 and  train loss is 59.12\n",
      "executing epoch:36, it took 0.7753983696301778 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the global loss from 59.12 to 56.62 | throughput = 184895 tokens/second | norm = 0.8978 | learning rate = 4.90574e-04\n",
      " epoch= 36 and  train loss is 56.62\n",
      "executing epoch:37, it took 0.7761299212773641 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the global loss from 56.62 to 53.99 | throughput = 184836 tokens/second | norm = 0.9245 | learning rate = 4.82278e-04\n",
      " epoch= 37 and  train loss is 53.99\n",
      "inside validation data for epoch 37\n",
      "No improvement in validation loss-->epoch= 37 and global val loss is 961.67\n",
      "executing epoch:38, it took 0.7759356578191121 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the global loss from 53.99 to 51.23 | throughput = 184849 tokens/second | norm = 0.8708 | learning rate = 4.73756e-04\n",
      " epoch= 38 and  train loss is 51.23\n",
      "executing epoch:39, it took 0.7753793160120647 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the global loss from 51.23 to 48.64 | throughput = 184825 tokens/second | norm = 0.8957 | learning rate = 4.65017e-04\n",
      " epoch= 39 and  train loss is 48.64\n",
      "executing epoch:40, it took 0.7763649066289265 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the global loss from 48.64 to 46.52 | throughput = 184788 tokens/second | norm = 1.0845 | learning rate = 4.56072e-04\n",
      " epoch= 40 and  train loss is 46.52\n",
      "executing epoch:41, it took 0.7760235468546549 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the global loss from 46.52 to 43.88 | throughput = 184813 tokens/second | norm = 1.1657 | learning rate = 4.46933e-04\n",
      " epoch= 41 and  train loss is 43.88\n",
      "inside validation data for epoch 41\n",
      "No improvement in validation loss-->epoch= 41 and global val loss is 961.67\n",
      "executing epoch:42, it took 0.776468280951182 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the global loss from 43.88 to 41.27 | throughput = 184693 tokens/second | norm = 0.9897 | learning rate = 4.37611e-04\n",
      " epoch= 42 and  train loss is 41.27\n",
      "executing epoch:43, it took 0.7757246057192485 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the global loss from 41.27 to 39.05 | throughput = 184934 tokens/second | norm = 1.3812 | learning rate = 4.28117e-04\n",
      " epoch= 43 and  train loss is 39.05\n",
      "executing epoch:44, it took 0.7765219529469808 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the global loss from 39.05 to 36.99 | throughput = 184694 tokens/second | norm = 1.2065 | learning rate = 4.18464e-04\n",
      " epoch= 44 and  train loss is 36.99\n",
      "executing epoch:45, it took 0.7768061717351278 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the global loss from 36.99 to 34.87 | throughput = 184681 tokens/second | norm = 1.1407 | learning rate = 4.08663e-04\n",
      " epoch= 45 and  train loss is 34.87\n",
      "inside validation data for epoch 45\n",
      "No improvement in validation loss-->epoch= 45 and global val loss is 961.67\n",
      "executing epoch:46, it took 0.7755818088849386 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the global loss from 34.87 to 32.71 | throughput = 184925 tokens/second | norm = 1.2394 | learning rate = 3.98727e-04\n",
      " epoch= 46 and  train loss is 32.71\n",
      "executing epoch:47, it took 0.7756116469701131 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the global loss from 32.71 to 30.61 | throughput = 184906 tokens/second | norm = 1.2621 | learning rate = 3.88667e-04\n",
      " epoch= 47 and  train loss is 30.61\n",
      "executing epoch:48, it took 0.7760485649108887 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the global loss from 30.61 to 28.85 | throughput = 184865 tokens/second | norm = 1.3592 | learning rate = 3.78498e-04\n",
      " epoch= 48 and  train loss is 28.85\n",
      "executing epoch:49, it took 0.775436274210612 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the global loss from 28.85 to 27.16 | throughput = 184979 tokens/second | norm = 1.2800 | learning rate = 3.68230e-04\n",
      " epoch= 49 and  train loss is 27.16\n",
      "inside validation data for epoch 49\n",
      "No improvement in validation loss-->epoch= 49 and global val loss is 961.67\n",
      "executing epoch:50, it took 0.7764867146809896 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the global loss from 27.16 to 25.36 | throughput = 184748 tokens/second | norm = 1.3557 | learning rate = 3.57878e-04\n",
      " epoch= 50 and  train loss is 25.36\n",
      "executing epoch:51, it took 0.7770442008972168 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the global loss from 25.36 to 23.86 | throughput = 184667 tokens/second | norm = 1.4396 | learning rate = 3.47453e-04\n",
      " epoch= 51 and  train loss is 23.86\n",
      "executing epoch:52, it took 0.7763572374979655 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the global loss from 23.86 to 22.61 | throughput = 184724 tokens/second | norm = 1.3884 | learning rate = 3.36970e-04\n",
      " epoch= 52 and  train loss is 22.61\n",
      "executing epoch:53, it took 0.777301820119222 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the global loss from 22.61 to 20.91 | throughput = 184572 tokens/second | norm = 1.4995 | learning rate = 3.26440e-04\n",
      " epoch= 53 and  train loss is 20.91\n",
      "inside validation data for epoch 53\n",
      "No improvement in validation loss-->epoch= 53 and global val loss is 961.67\n",
      "executing epoch:54, it took 0.7763392051060994 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the global loss from 20.91 to 19.81 | throughput = 184696 tokens/second | norm = 1.1307 | learning rate = 3.15877e-04\n",
      " epoch= 54 and  train loss is 19.81\n",
      "executing epoch:55, it took 0.7766201972961426 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the global loss from 19.81 to 18.07 | throughput = 184672 tokens/second | norm = 1.3281 | learning rate = 3.05295e-04\n",
      " epoch= 55 and  train loss is 18.07\n",
      "executing epoch:56, it took 0.777403179804484 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the global loss from 18.07 to 16.84 | throughput = 184598 tokens/second | norm = 1.1754 | learning rate = 2.94705e-04\n",
      " epoch= 56 and  train loss is 16.84\n",
      "executing epoch:57, it took 0.7766314903895061 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the global loss from 16.84 to 15.65 | throughput = 184687 tokens/second | norm = 1.4847 | learning rate = 2.84123e-04\n",
      " epoch= 57 and  train loss is 15.65\n",
      "inside validation data for epoch 57\n",
      "No improvement in validation loss-->epoch= 57 and global val loss is 961.67\n",
      "executing epoch:58, it took 0.7770425756772359 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the global loss from 15.65 to 14.54 | throughput = 184652 tokens/second | norm = 1.3399 | learning rate = 2.73560e-04\n",
      " epoch= 58 and  train loss is 14.54\n",
      "executing epoch:59, it took 0.7771250446637471 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the global loss from 14.54 to 13.41 | throughput = 184594 tokens/second | norm = 1.4226 | learning rate = 2.63030e-04\n",
      " epoch= 59 and  train loss is 13.41\n",
      "executing epoch:60, it took 0.7759787718454997 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the global loss from 13.41 to 12.30 | throughput = 184827 tokens/second | norm = 1.4869 | learning rate = 2.52547e-04\n",
      " epoch= 60 and  train loss is 12.30\n",
      "executing epoch:61, it took 0.7753057797749837 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the global loss from 12.30 to 11.27 | throughput = 184956 tokens/second | norm = 1.2269 | learning rate = 2.42122e-04\n",
      " epoch= 61 and  train loss is 11.27\n",
      "inside validation data for epoch 61\n",
      "No improvement in validation loss-->epoch= 61 and global val loss is 961.67\n",
      "executing epoch:62, it took 0.7754493474960327 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the global loss from 11.27 to 10.44 | throughput = 185122 tokens/second | norm = 1.2087 | learning rate = 2.31770e-04\n",
      " epoch= 62 and  train loss is 10.44\n",
      "executing epoch:63, it took 0.7751954197883606 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the global loss from 10.44 to 9.55 | throughput = 185010 tokens/second | norm = 1.2242 | learning rate = 2.21502e-04\n",
      " epoch= 63 and  train loss is 9.55\n",
      "executing epoch:64, it took 0.7759491006533304 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the global loss from 9.55 to 8.62 | throughput = 184710 tokens/second | norm = 1.2645 | learning rate = 2.11333e-04\n",
      " epoch= 64 and  train loss is 8.62\n",
      "executing epoch:65, it took 0.7765765150388082 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the global loss from 8.62 to 8.00 | throughput = 184722 tokens/second | norm = 0.8966 | learning rate = 2.01273e-04\n",
      " epoch= 65 and  train loss is 8.00\n",
      "inside validation data for epoch 65\n",
      "No improvement in validation loss-->epoch= 65 and global val loss is 961.67\n",
      "executing epoch:66, it took 0.7764193097750346 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the global loss from 8.00 to 7.19 | throughput = 184713 tokens/second | norm = 0.8887 | learning rate = 1.91337e-04\n",
      " epoch= 66 and  train loss is 7.19\n",
      "executing epoch:67, it took 0.7768589774767558 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the global loss from 7.19 to 6.59 | throughput = 184618 tokens/second | norm = 1.0279 | learning rate = 1.81536e-04\n",
      " epoch= 67 and  train loss is 6.59\n",
      "executing epoch:68, it took 0.7773122787475586 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the global loss from 6.59 to 6.07 | throughput = 184537 tokens/second | norm = 0.8879 | learning rate = 1.71883e-04\n",
      " epoch= 68 and  train loss is 6.07\n",
      "executing epoch:69, it took 0.9335261821746826 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the global loss from 6.07 to 5.46 | throughput = 144978 tokens/second | norm = 0.8359 | learning rate = 1.62389e-04\n",
      " epoch= 69 and  train loss is 5.46\n",
      "inside validation data for epoch 69\n",
      "No improvement in validation loss-->epoch= 69 and global val loss is 961.67\n",
      "executing epoch:70, it took 1.0211469014485677 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the global loss from 5.46 to 5.01 | throughput = 139503 tokens/second | norm = 0.7227 | learning rate = 1.53067e-04\n",
      " epoch= 70 and  train loss is 5.01\n",
      "executing epoch:71, it took 1.0193563938140868 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the global loss from 5.01 to 4.56 | throughput = 140434 tokens/second | norm = 0.5962 | learning rate = 1.43928e-04\n",
      " epoch= 71 and  train loss is 4.56\n",
      "executing epoch:72, it took 1.0295479456583658 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the global loss from 4.56 to 4.16 | throughput = 138555 tokens/second | norm = 0.5523 | learning rate = 1.34983e-04\n",
      " epoch= 72 and  train loss is 4.16\n",
      "executing epoch:73, it took 1.0344288190205893 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the global loss from 4.16 to 3.82 | throughput = 139544 tokens/second | norm = 0.6038 | learning rate = 1.26244e-04\n",
      " epoch= 73 and  train loss is 3.82\n",
      "inside validation data for epoch 73\n",
      "No improvement in validation loss-->epoch= 73 and global val loss is 961.67\n",
      "executing epoch:74, it took 1.0169421831766765 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the global loss from 3.82 to 3.56 | throughput = 140526 tokens/second | norm = 0.4885 | learning rate = 1.17722e-04\n",
      " epoch= 74 and  train loss is 3.56\n",
      "executing epoch:75, it took 1.0300288836161295 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the global loss from 3.56 to 3.23 | throughput = 138797 tokens/second | norm = 0.4328 | learning rate = 1.09426e-04\n",
      " epoch= 75 and  train loss is 3.23\n",
      "executing epoch:76, it took 1.0273892800013225 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the global loss from 3.23 to 2.95 | throughput = 140054 tokens/second | norm = 0.4107 | learning rate = 1.01368e-04\n",
      " epoch= 76 and  train loss is 2.95\n",
      "executing epoch:77, it took 1.0335999727249146 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the global loss from 2.95 to 2.76 | throughput = 139248 tokens/second | norm = 0.3361 | learning rate = 9.35576e-05\n",
      " epoch= 77 and  train loss is 2.76\n",
      "inside validation data for epoch 77\n",
      "No improvement in validation loss-->epoch= 77 and global val loss is 961.67\n",
      "executing epoch:78, it took 1.0392462054888407 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the global loss from 2.76 to 2.51 | throughput = 141394 tokens/second | norm = 0.3343 | learning rate = 8.60042e-05\n",
      " epoch= 78 and  train loss is 2.51\n",
      "executing epoch:79, it took 0.7744933327039083 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the global loss from 2.51 to 2.35 | throughput = 185165 tokens/second | norm = 0.3345 | learning rate = 7.87175e-05\n",
      " epoch= 79 and  train loss is 2.35\n",
      "executing epoch:80, it took 0.7753763834635417 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the global loss from 2.35 to 2.17 | throughput = 185042 tokens/second | norm = 0.2814 | learning rate = 7.17064e-05\n",
      " epoch= 80 and  train loss is 2.17\n",
      "executing epoch:81, it took 0.7758525411287943 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the global loss from 2.17 to 2.02 | throughput = 184984 tokens/second | norm = 0.2496 | learning rate = 6.49797e-05\n",
      " epoch= 81 and  train loss is 2.02\n",
      "inside validation data for epoch 81\n",
      "No improvement in validation loss-->epoch= 81 and global val loss is 961.67\n",
      "executing epoch:82, it took 0.7767353256543478 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the global loss from 2.02 to 1.91 | throughput = 184672 tokens/second | norm = 0.2128 | learning rate = 5.85458e-05\n",
      " epoch= 82 and  train loss is 1.91\n",
      "executing epoch:83, it took 0.7762062350908915 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the global loss from 1.91 to 1.78 | throughput = 184680 tokens/second | norm = 0.1996 | learning rate = 5.24128e-05\n",
      " epoch= 83 and  train loss is 1.78\n",
      "executing epoch:84, it took 0.777440615495046 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the global loss from 1.78 to 1.65 | throughput = 184490 tokens/second | norm = 0.1845 | learning rate = 4.65882e-05\n",
      " epoch= 84 and  train loss is 1.65\n",
      "executing epoch:85, it took 0.776552700996399 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the global loss from 1.65 to 1.56 | throughput = 184670 tokens/second | norm = 0.1834 | learning rate = 4.10793e-05\n",
      " epoch= 85 and  train loss is 1.56\n",
      "inside validation data for epoch 85\n",
      "No improvement in validation loss-->epoch= 85 and global val loss is 961.67\n",
      "executing epoch:86, it took 0.7770813822746276 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the global loss from 1.56 to 1.48 | throughput = 184478 tokens/second | norm = 0.1444 | learning rate = 3.58931e-05\n",
      " epoch= 86 and  train loss is 1.48\n",
      "executing epoch:87, it took 0.7770869453748067 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the global loss from 1.48 to 1.40 | throughput = 184551 tokens/second | norm = 0.1380 | learning rate = 3.10358e-05\n",
      " epoch= 87 and  train loss is 1.40\n",
      "executing epoch:88, it took 0.7775991757710775 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the global loss from 1.40 to 1.33 | throughput = 184534 tokens/second | norm = 0.1273 | learning rate = 2.65137e-05\n",
      " epoch= 88 and  train loss is 1.33\n",
      "executing epoch:89, it took 0.7776222785313924 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the global loss from 1.33 to 1.27 | throughput = 184411 tokens/second | norm = 0.1086 | learning rate = 2.23323e-05\n",
      " epoch= 89 and  train loss is 1.27\n",
      "inside validation data for epoch 89\n"
     ]
    }
   ],
   "source": [
    "tr_model = train_model(train_loader, val_loader, model =  model,tokenizer = tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6370a15a",
   "metadata": {},
   "source": [
    "## Use this section if you want a model with Random weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6682e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig\n",
    "# Define the model name\n",
    "model_name = \"distilgpt2\"\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# Load the model configuration\n",
    "config = AutoConfig.from_pretrained(model_name)\n",
    "\n",
    "# Initialize the model with random weights\n",
    "model = AutoModelForCausalLM.from_config(config)\n",
    "\n",
    "# Check the model\n",
    "#print(model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c249afce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca53e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(df_train.iloc[178]['input_ids'], dtype = torch.long)\n",
    "att = torch.tensor(df_train.iloc[178]['attention_mask'], dtype = torch.long)\n",
    "random_out = model(input_ids = x.view(B,T) , attention_mask = att.view(B,T), labels =   x.view(B,T) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0863aeae",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e0a538",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6519f7a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5dfe6c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a1c99d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62e8f13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99adde81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ed023e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9927ebd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af58a8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0648b2a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbbb019c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
