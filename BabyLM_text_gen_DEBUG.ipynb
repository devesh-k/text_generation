{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54baafdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## reference https://huggingface.co/learn/nlp-course/en/chapter7/6?fw=pt#training-a-causal-language-model-from-scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc5c326",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "facc263e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: datasets in /opt/conda/lib/python3.8/site-packages (2.20.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.8/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.2 in /opt/conda/lib/python3.8/site-packages (from datasets) (0.23.0)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.8/site-packages (from datasets) (3.9.5)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.8/site-packages (from datasets) (6.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.8/site-packages (from datasets) (1.21.4)\n",
      "Requirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.8/site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.8/site-packages (from datasets) (1.3.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.8/site-packages (from datasets) (16.1.0)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.8/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: requests>=2.32.2 in /opt/conda/lib/python3.8/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.8/site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /opt/conda/lib/python3.8/site-packages (from datasets) (4.66.4)\n",
      "Requirement already satisfied: fsspec[http]<=2024.5.0,>=2023.1.0 in /opt/conda/lib/python3.8/site-packages (from datasets) (2024.5.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.8/site-packages (from datasets) (3.4.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.8/site-packages (from datasets) (21.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets) (21.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.8/site-packages (from huggingface-hub>=0.21.2->datasets) (4.11.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging->datasets) (3.0.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests>=2.32.2->datasets) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests>=2.32.2->datasets) (1.26.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.8/site-packages (from requests>=2.32.2->datasets) (2.0.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests>=2.32.2->datasets) (3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.8/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.8/site-packages (from pandas->datasets) (2021.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: jupyter in /opt/conda/lib/python3.8/site-packages (1.0.0)\n",
      "Requirement already satisfied: ipywidgets in /opt/conda/lib/python3.8/site-packages (8.1.3)\n",
      "Requirement already satisfied: jupyter-console in /opt/conda/lib/python3.8/site-packages (from jupyter) (6.6.3)\n",
      "Requirement already satisfied: qtconsole in /opt/conda/lib/python3.8/site-packages (from jupyter) (5.5.2)\n",
      "Requirement already satisfied: nbconvert in /opt/conda/lib/python3.8/site-packages (from jupyter) (6.3.0)\n",
      "Requirement already satisfied: notebook in /opt/conda/lib/python3.8/site-packages (from jupyter) (6.4.1)\n",
      "Requirement already satisfied: ipykernel in /opt/conda/lib/python3.8/site-packages (from jupyter) (6.29.4)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.11 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (3.0.11)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.11 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (4.0.11)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (5.14.3)\n",
      "Requirement already satisfied: comm>=0.1.3 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (7.30.0)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.47)\n",
      "Requirement already satisfied: decorator in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (5.1.0)\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (4.8.0)\n",
      "Requirement already satisfied: backcall in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: jedi>=0.16 in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.18.1)\n",
      "Requirement already satisfied: matplotlib-inline in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.3)\n",
      "Requirement already satisfied: setuptools>=18.5 in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (59.4.0)\n",
      "Requirement already satisfied: pygments in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (2.10.0)\n",
      "Requirement already satisfied: pickleshare in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.7.5)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /opt/conda/lib/python3.8/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.8/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.8/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=6.1.0->ipywidgets) (0.2.5)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in /opt/conda/lib/python3.8/site-packages (from ipykernel->jupyter) (7.1.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.8/site-packages (from ipykernel->jupyter) (21.3)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in /opt/conda/lib/python3.8/site-packages (from ipykernel->jupyter) (1.8.1)\n",
      "Requirement already satisfied: pyzmq>=24 in /opt/conda/lib/python3.8/site-packages (from ipykernel->jupyter) (26.0.3)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /opt/conda/lib/python3.8/site-packages (from ipykernel->jupyter) (5.7.2)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.8/site-packages (from ipykernel->jupyter) (5.8.0)\n",
      "Requirement already satisfied: nest-asyncio in /opt/conda/lib/python3.8/site-packages (from ipykernel->jupyter) (1.5.4)\n",
      "Requirement already satisfied: tornado>=6.1 in /opt/conda/lib/python3.8/site-packages (from ipykernel->jupyter) (6.1)\n",
      "Requirement already satisfied: entrypoints in /opt/conda/lib/python3.8/site-packages (from jupyter-client>=6.1.12->ipykernel->jupyter) (0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /opt/conda/lib/python3.8/site-packages (from jupyter-client>=6.1.12->ipykernel->jupyter) (2.8.2)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /opt/conda/lib/python3.8/site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel->jupyter) (4.2.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.8/site-packages (from python-dateutil>=2.1->jupyter-client>=6.1.12->ipykernel->jupyter) (1.16.0)\n",
      "Requirement already satisfied: bleach in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (4.1.0)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (1.5.0)\n",
      "Requirement already satisfied: nbformat>=4.4 in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (5.1.3)\n",
      "Requirement already satisfied: defusedxml in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (0.7.1)\n",
      "Requirement already satisfied: nbclient<0.6.0,>=0.5.0 in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (0.5.9)\n",
      "Requirement already satisfied: testpath in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (0.5.0)\n",
      "Requirement already satisfied: mistune<2,>=0.8.1 in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (0.8.4)\n",
      "Requirement already satisfied: jupyterlab-pygments in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (0.1.2)\n",
      "Requirement already satisfied: jinja2>=2.4 in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (3.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.8/site-packages (from jinja2>=2.4->nbconvert->jupyter) (2.0.1)\n",
      "Requirement already satisfied: ipython-genutils in /opt/conda/lib/python3.8/site-packages (from nbformat>=4.4->nbconvert->jupyter) (0.2.0)\n",
      "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /opt/conda/lib/python3.8/site-packages (from nbformat>=4.4->nbconvert->jupyter) (4.2.1)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /opt/conda/lib/python3.8/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.4->nbconvert->jupyter) (0.18.0)\n",
      "Requirement already satisfied: importlib-resources>=1.4.0 in /opt/conda/lib/python3.8/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.4->nbconvert->jupyter) (5.4.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /opt/conda/lib/python3.8/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.4->nbconvert->jupyter) (21.2.0)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /opt/conda/lib/python3.8/site-packages (from importlib-resources>=1.4.0->jsonschema!=2.5.0,>=2.4->nbformat>=4.4->nbconvert->jupyter) (3.6.0)\n",
      "Requirement already satisfied: webencodings in /opt/conda/lib/python3.8/site-packages (from bleach->nbconvert->jupyter) (0.5.1)\n",
      "Requirement already satisfied: terminado>=0.8.3 in /opt/conda/lib/python3.8/site-packages (from notebook->jupyter) (0.12.1)\n",
      "Requirement already satisfied: Send2Trash>=1.5.0 in /opt/conda/lib/python3.8/site-packages (from notebook->jupyter) (1.8.0)\n",
      "Requirement already satisfied: argon2-cffi in /opt/conda/lib/python3.8/site-packages (from notebook->jupyter) (21.1.0)\n",
      "Requirement already satisfied: prometheus-client in /opt/conda/lib/python3.8/site-packages (from notebook->jupyter) (0.12.0)\n",
      "Requirement already satisfied: cffi>=1.0.0 in /opt/conda/lib/python3.8/site-packages (from argon2-cffi->notebook->jupyter) (1.15.0)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.8/site-packages (from cffi>=1.0.0->argon2-cffi->notebook->jupyter) (2.21)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging->ipykernel->jupyter) (3.0.6)\n",
      "Requirement already satisfied: qtpy>=2.4.0 in /opt/conda/lib/python3.8/site-packages (from qtconsole->jupyter) (2.4.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#installing some libraries\n",
    "!pip install datasets\n",
    "!pip install --upgrade jupyter ipywidgets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "098d922d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch, transformers\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import PreTrainedModel, PretrainedConfig\n",
    "from transformers import AutoModel, AutoConfig,AutoModelForCausalLM, AutoConfig\n",
    "\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "#from datasets import load_dataset\n",
    "import pandas as pd, numpy as np\n",
    "from torch import cuda\n",
    "import datetime\n",
    "import warnings,itertools\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "# Ignore all warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "#pip install transformers bitsandbytes>=0.39.0 -q\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c18c493",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "model\n"
     ]
    }
   ],
   "source": [
    "#global params for training\n",
    "\n",
    "B,T = 48,1024\n",
    "epoch = 100\n",
    "random_init_wts = False\n",
    "min_text_len = 0\n",
    "# train_loss_list = []\n",
    "# val_loss_list =[]\n",
    "if cuda.is_available():\n",
    "    device = torch.device('cuda:0')\n",
    "    print(device)\n",
    "else:\n",
    "    device = 'cpu'\n",
    "#print(device)\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "#os.environ[\"MKL_DEBUG_CPU_TYPE\"] = \"5\"\n",
    "context_length = None\n",
    "global_tr_loss = torch.inf\n",
    "global_val_loss = torch.inf\n",
    "#print(global_tr_loss)\n",
    "model_path = os.path.join(\"model\")\n",
    "print(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9aa8493",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./data/unzip_text_10M'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "directory = os.path.join('.','data','unzip_text_10M')  # Replace with your directory path\n",
    "directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74210730",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_text(directory):\n",
    "    directory = os.path.join('.','data','unzip_text_10M',str(directory))  # Replace with your directory path\n",
    "    print(f\"directory :{directory}\")\n",
    "    # List all files in the directory\n",
    "    files = [f for f in os.listdir(directory) if os.path.isfile(os.path.join(directory, f))]\n",
    "    print(f\"files:{files}\")\n",
    "    text_content = []\n",
    "    # Read each file\n",
    "    total_lines = 0\n",
    "    for filenum,filename in enumerate(files):\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            #first_line = file.read()\n",
    "            #print(f\"filename :{filename}->first few lines {first_line}\")\n",
    "            #continue\n",
    "            #lines_list = [line.strip() for line in open(file_path, 'r')]\n",
    "            text = file.read()\n",
    "            text_content.append(text)\n",
    "            print(f\"the file:{filename} has been appeneded to the uber list and its length is {len(text_content)} \")\n",
    "            #total_lines+=len(lines_list)\n",
    "            #text_content.append(lines_list)\n",
    "    \n",
    "    flattened_list = ''.join(text_content)\n",
    "    assert (len(flattened_list) == total_lines , f\"Expected {len(flattened_list)} to be equal to {total_lines}\" )\n",
    "    \n",
    "    return flattened_list\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4209840d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "directory :./data/unzip_text_10M/train_10M\n",
      "files:['switchboard.train', 'simple_wiki.train', 'open_subtitles.train', 'gutenberg.train', 'childes.train', 'bnc_spoken.train']\n",
      "the file:switchboard.train has been appeneded to the uber list and its length is 1 \n",
      "the file:simple_wiki.train has been appeneded to the uber list and its length is 2 \n",
      "the file:open_subtitles.train has been appeneded to the uber list and its length is 3 \n",
      "the file:gutenberg.train has been appeneded to the uber list and its length is 4 \n",
      "the file:childes.train has been appeneded to the uber list and its length is 5 \n",
      "the file:bnc_spoken.train has been appeneded to the uber list and its length is 6 \n"
     ]
    }
   ],
   "source": [
    "train_list = read_text(\"train_10M\")\n",
    "#print(train_dict)\n",
    "#val_list = read_text(\"dev\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d13c691",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54215049"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ea4c822",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1103\n"
     ]
    }
   ],
   "source": [
    "chunks = len(train_list)//(B*T)\n",
    "print(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "113ba9d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17191971 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilgpt2\",return_tensors = \"pt\" , truncate = True, max_length  = context_length ,return_overflowing_tokens=True , padding = False,)\n",
    "enc_train = tokenizer(train_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "281656f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.1535097982657136"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comp_ratio = len(train_list)/len(enc_train['input_ids'])\n",
    "comp_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "655b5963",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "directory :./data/unzip_text_10M/dev\n",
      "files:['switchboard.dev', 'simple_wiki.dev', 'open_subtitles.dev', 'gutenberg.dev', 'childes.dev', 'bnc_spoken.dev']\n",
      "the file:switchboard.dev has been appeneded to the uber list and its length is 1 \n",
      "the file:simple_wiki.dev has been appeneded to the uber list and its length is 2 \n",
      "the file:open_subtitles.dev has been appeneded to the uber list and its length is 3 \n",
      "the file:gutenberg.dev has been appeneded to the uber list and its length is 4 \n",
      "the file:childes.dev has been appeneded to the uber list and its length is 5 \n",
      "the file:bnc_spoken.dev has been appeneded to the uber list and its length is 6 \n"
     ]
    }
   ],
   "source": [
    "val_list = read_text(\"dev\")\n",
    "enc_val = tokenizer(val_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc0caeb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "352d9213",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df_from_list(enc , B= B, T = T):\n",
    "    chunk_size = B*T\n",
    "    long_list_inp = enc['input_ids']\n",
    "    long_list_attention = enc['attention_mask']\n",
    "\n",
    "    # Step 3: Split the list into chunks and pad the last chunk if necessary\n",
    "    chunks_inp = [long_list_inp[i:i + chunk_size] for i in range(0, len(long_list_inp), chunk_size)]\n",
    "    chunks_att = [long_list_attention[i:i + chunk_size] for i in range(0, len(long_list_attention), chunk_size)]\n",
    "    df = pd.DataFrame({'input_ids': chunks_inp,'attention_mask':chunks_att})\n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e24c5611",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the dataframe is = 350\n",
      "Length of the VALDATION dataframe is = 355\n"
     ]
    }
   ],
   "source": [
    "df_train_temp = get_df_from_list(enc_train)\n",
    "# Display the DataFrame\n",
    "df_train_temp.head()\n",
    "print(f\"Length of the dataframe is = {len(df_train_temp)}\")\n",
    "\n",
    "df_val_temp = get_df_from_list(enc_val)\n",
    "# Display the DataFrame\n",
    "df_val_temp.head()\n",
    "print(f\"Length of the VALDATION dataframe is = {len(df_val_temp)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "32d2ffd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_df(df,eos_char = tokenizer.eos_token_id,B = B, T = T):\n",
    "    for ind,row in df.iterrows():\n",
    "        if len(row['input_ids']) != B*T :\n",
    "            print(f\"row = {ind} and input_id length = {len(row['input_ids'])}\")\n",
    "            print(f\"row = {ind} and attention length = {len(row['attention_mask'])}\")\n",
    "            pad_len = B*T - len(row['input_ids'])\n",
    "            print(f\"padding the row index {ind} with {pad_len} character\")\n",
    "            row['input_ids'] = row['input_ids']+ [eos_char] * pad_len\n",
    "            #attention mask should be padded to 0\n",
    "            row['attention_mask'] = row['attention_mask']+ [0] * pad_len\n",
    "            print(\"#### POST CONCAT####\")\n",
    "            print(f\"row = {ind} and input_id length = {len(row['input_ids'])}\")\n",
    "            print(f\"row = {ind} and attention length ={len(row['attention_mask'])}\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def verify_len(df):\n",
    "    row_ind = []\n",
    "    for ind,row in df.iterrows():\n",
    "        if len(row['input_ids']) != B*T :\n",
    "            row_ind.append(ind)\n",
    "        else:\n",
    "            continue\n",
    "    if len(row_ind) !=0:\n",
    "        print(\"CONCATENATION Did not work\")\n",
    "    else:\n",
    "        print(\"CONCATENATION worked\")\n",
    "        \n",
    "            \n",
    "        \n",
    "    \n",
    "        \n",
    "        \n",
    "   \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "45c53e9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row = 349 and input_id length = 37923\n",
      "row = 349 and attention length = 37923\n",
      "padding the row index 349 with 11229 character\n",
      "#### POST CONCAT####\n",
      "row = 349 and input_id length = 49152\n",
      "row = 349 and attention length =49152\n",
      "CONCATENATION worked\n",
      "row = 354 and input_id length = 13588\n",
      "row = 354 and attention length = 13588\n",
      "padding the row index 354 with 35564 character\n",
      "#### POST CONCAT####\n",
      "row = 354 and input_id length = 49152\n",
      "row = 354 and attention length =49152\n",
      "CONCATENATION worked\n"
     ]
    }
   ],
   "source": [
    "df_train  = pad_df(df_train_temp)\n",
    "verify_len(df_train)\n",
    "\n",
    "df_val  = pad_df(df_val_temp)\n",
    "verify_len(df_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "115b4605",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_ids</th>\n",
       "      <th>attention_mask</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[32, 25, 197, 40, 1101, 1654, 484, 389, 13, 19...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[314, 892, 340, 6774, 503, 262, 5290, 287, 262...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[318, 407, 922, 329, 262, 1200, 2035, 13, 198,...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[32, 25, 197, 40, 1612, 11, 345, 760, 11, 314,...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[25, 197, 2396, 314, 466, 21099, 326, 11, 475,...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           input_ids  \\\n",
       "0  [32, 25, 197, 40, 1101, 1654, 484, 389, 13, 19...   \n",
       "1  [314, 892, 340, 6774, 503, 262, 5290, 287, 262...   \n",
       "2  [318, 407, 922, 329, 262, 1200, 2035, 13, 198,...   \n",
       "3  [32, 25, 197, 40, 1612, 11, 345, 760, 11, 314,...   \n",
       "4  [25, 197, 2396, 314, 466, 21099, 326, 11, 475,...   \n",
       "\n",
       "                                      attention_mask  \n",
       "0  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "1  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "2  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "3  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "4  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed231c11",
   "metadata": {},
   "source": [
    "## Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7bbf6b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the tokenizer:\n",
    "model_name = \"distilgpt2\"\n",
    "if random_init_wts:\n",
    "    config = AutoConfig.from_pretrained(model_name, vocab_size = 50304)\n",
    "    # Initialize the model with random weights\n",
    "    model = AutoModelForCausalLM.from_config(config)\n",
    "else:\n",
    "    #model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\"distilgpt2\")\n",
    "\n",
    "model = torch.compile(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf8b2dc",
   "metadata": {},
   "source": [
    "### Data loaders and Dataset for batched training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "855e381a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataset_pyt(Dataset):\n",
    "    def __init__(self, df, B = B, T = T ):\n",
    "        self.df = df\n",
    "                                        \n",
    "    def __getitem__(self, idx):\n",
    "        #print(f\"inside loader...idx ->{idx}\")\n",
    "        input_id_temp = torch.tensor(self.df.iloc[idx]['input_ids'],dtype = torch.long)\n",
    "        #print(f\"length of text ->{len(text)}\")\n",
    "        #print(f\"text ->{text}\")\n",
    "        #encodings = tokenizer(text, truncation=True, max_length= self.max_length, return_overflowing_tokens=True, padding = 'max_length',return_tensors='pt')\n",
    "        att_mask = torch.tensor(self.df.iloc[idx]['attention_mask'],dtype = torch.long)\n",
    "        # check the length of the encoded list\n",
    "        \n",
    "        #x_dict['input_id'] = input_ids_list\n",
    "        #x_dict['attention_mask'] = input_ids_list\n",
    "                \n",
    "        #print(f\"x_dict = {x_dict}\")             \n",
    "#       print(f\"inside the loader and input_id = {input_ids} and its shape is {input_ids.shape}\")\n",
    "        #labels = input_id_list\n",
    "        #input_ids = torch.tensor(input_id_list)\n",
    "        #attention_mask = torch.tensor(attention_mask_list)\n",
    "        #labels = torch.tensor(labels)\n",
    "        #print(f\"inside the loader and input_id shape= {input_ids.shape} attention_mask_shape is {attention_mask.shape} and label shape is {labels.shape}\")\n",
    "        #print(f\"encoding = {encodings}\")\n",
    "        input_id =   input_id_temp.view(B,T)    \n",
    "        attention_mask = att_mask.view(B,T)   \n",
    "        \n",
    "        return input_id, attention_mask\n",
    "        \n",
    "    def __len__(self):\n",
    "        #return the length of the dataframe\n",
    "        return len(self.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a5272e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_dataset = dataset_pyt(filtered_df,tokenizer = tokenizer)\n",
    "train_dataset = dataset_pyt(df_train)\n",
    "val_dataset = dataset_pyt(df_val)\n",
    "#test_dataset = dataset_pyt(df_test,tokenizer = tokenizer)\n",
    "\n",
    "train_loader = DataLoader(train_dataset,batch_size = 1, shuffle = True , num_workers = 4, pin_memory = True)\n",
    "val_loader = DataLoader(val_dataset,batch_size = 1, shuffle = True , num_workers = 4, pin_memory = True)\n",
    "#test_loader = DataLoader(test_dataset,batch_size = batch_size, shuffle = False, collate_fn = custom_collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "140bc131",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the train loader is 350\n",
      "num_tokens= 17203200\n"
     ]
    }
   ],
   "source": [
    "print(f\"Length of the train loader is {len(train_loader)}\")\n",
    "print(f\"num_tokens= {B*T*len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8ab59d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_output = model(input_ids = inp ,attention_mask = att, labels = inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "14a820c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad\n",
    "def eval_model(val_loader, model, epoch , device = device,tokenizer = tokenizer):\n",
    "    global global_val_loss\n",
    "    #m = nn.Softmax()\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    e = epoch+1\n",
    "    val_loss_list = []\n",
    "    #criterion = torch.nn.BCEWithLogitsLoss()\n",
    "    print(f\"inside validation data for epoch {e}\")\n",
    "    #y_hat_val_list = []\n",
    "    #y_val_list = []\n",
    "    for ind,(input_id,attention_mask) in enumerate(val_loader):\n",
    "        #print(f\"id_list{id_list}\")\n",
    "        ids = input_id.to(device=device, non_blocking=True)\n",
    "        att_mask = attention_mask.to(device=device, non_blocking=True)\n",
    "        labels = ids\n",
    "        #predictions\n",
    "        #print(f\"input_ids device = {input_ids.device}\")\n",
    "        with autocast(dtype = torch.bfloat16):\n",
    "            model_output = model(input_ids = ids ,attention_mask = att_mask, labels = labels)\n",
    "            act_loss = model_output.loss\n",
    "        \n",
    "        val_loss_list.append(act_loss)\n",
    "        del ids,att_mask,labels\n",
    "                    \n",
    "    mean_val_loss = torch.mean(torch.tensor(val_loss_list))\n",
    "    if mean_val_loss < global_val_loss:\n",
    "        print(f\"Val loss has decreased -->reducing the global validation loss from {global_val_loss:.2f} to {mean_val_loss:.2f}\")\n",
    "        global_val_loss = mean_val_loss\n",
    "        print(f\" validation loss for epoch = {e} is {torch.mean(torch.tensor(val_loss_list)):.4f}\")\n",
    "        #print metrics and save the model\n",
    "        #y_hat_val = torch.cat(y_hat_val_list)\n",
    "        #y_val = torch.cat(y_val_list)\n",
    "        #acc_val = accuracy_score(y_val.cpu().numpy(), y_hat_val.cpu().numpy())\n",
    "        #f1_val = f1_score(y_val.cpu().numpy(), y_hat_val.cpu().numpy(), average='micro')\n",
    "        print(f\" epoch= {e} : mean val loss is {torch.mean(torch.tensor(mean_val_loss)):.4f} \")\n",
    "        #save the model\n",
    "        \n",
    "        # Get the current date and time\n",
    "        current_datetime = datetime.datetime.now()\n",
    "        # Extract date and time components\n",
    "        current_date = str(current_datetime.date())\n",
    "        current_time = str(current_datetime.time()).split('.')[0]\n",
    "        file_name = 'model'+ current_date+current_time+'.pth'\n",
    "        path = os.path.join(\"model\",file_name)\n",
    "        print(f\"saving the model {file_name}\")\n",
    "        #torch.save(model.state_dict(), path)\n",
    "        model.save_pretrained(path)\n",
    "        tokenizer.save_pretrained(path)\n",
    "        \n",
    "        #plot_confusion_matrix(y_val.cpu().numpy(), y_hat_val.cpu().numpy(), labels)\n",
    "    else:\n",
    "        print(f\"No improvement in validation loss-->epoch= {e} and global val loss is {global_val_loss:.2f}\")\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a3f36dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_loader,val_loader,model,num_epoch = 30,device = device,tokenizer = tokenizer):\n",
    "    global global_tr_loss\n",
    "    model.train()\n",
    "    device = device\n",
    "    lr_custom = 6e-4\n",
    "    print(f\"inside train model. Device = {device}\")\n",
    "    #adding betas params per the paper\n",
    "    optimizer = torch.optim.AdamW(params =  model.parameters(), lr= lr_custom,betas = (.9,.95),fused = True ,weight_decay = .1)\n",
    "    optimizer_reduced_lr = torch.optim.AdamW(params =  model.parameters(), lr= .1*lr_custom ,betas = (.9,.95),fused = True , weight_decay=.1)\n",
    "    total_batch_size = 2**19\n",
    "    grad_accum_step = total_batch_size//(B*T)\n",
    "    model.to(device)\n",
    "    extra_train = .1*num_epoch\n",
    "    #m = nn.Softmax()\n",
    "    max_train_steps = num_epoch +extra_train \n",
    "    import time\n",
    "    from transformers import get_linear_schedule_with_warmup\n",
    "    scheduler_cos = transformers.get_cosine_schedule_with_warmup( optimizer= optimizer, num_warmup_steps =num_epoch*.1 ,num_training_steps= num_epoch-1 ,last_epoch = -1 )\n",
    "    scheduler_constant = transformers.get_constant_schedule_with_warmup( optimizer = optimizer_reduced_lr ,num_warmup_steps = 0, last_epoch = -1 )\n",
    "    \n",
    "    for i in range (max_train_steps):\n",
    "        epoch_start_time = time.time()\n",
    "        #y_hat_list =[]\n",
    "        #label_list = []\n",
    "        epoch_train_loss = []\n",
    "        for ind,(input_id,attention_mask) in enumerate(train_loader):\n",
    "            #print(f\"x_dict = {x_dict}\")\n",
    "            \n",
    "            #print(f\"id_list{id_list}\")\n",
    "            if ind == int(len(train_loader)/2):\n",
    "                batch_time = time.time()\n",
    "                duration = batch_time - epoch_start_time\n",
    "                print(f\"executing epoch:{i+1}, it took {duration/60} mins from beginning of epoch till batch#{ind}\")\n",
    "            \n",
    "            \n",
    "            ids = input_id.to(device=device, non_blocking=True)\n",
    "            att_mask = attention_mask.to(device=device, non_blocking=True)\n",
    "            labels = ids\n",
    "            #predictions\n",
    "            #print(f\"shape of ids = {ids.shape} and shape of att_mask = {att_mask.shape}\")\n",
    "            for micro_step in range(grad_accum_step):\n",
    "                with autocast(dtype = torch.bfloat16):\n",
    "                    model_output = model(input_ids = ids ,attention_mask = att_mask, labels = labels)\n",
    "                    act_loss = model_output.loss\n",
    "            \n",
    "                act_loss.backward()\n",
    "                act_loss = act_loss/grad_accum_step\n",
    "                \n",
    "            #print(f\"model_output->{model_output}\")\n",
    "            #probs = m(logits)\n",
    "            #y_hat_list.append(torch.argmax(probs , dim = 1))\n",
    "            #label_list.append(torch.argmax(lab, dim = 1))\n",
    "            \n",
    "            #loss calculation                   \n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            optimizer_reduced_lr.zero_grad(set_to_none=True)\n",
    "            \n",
    "            norm = torch.nn.utils.clip_grad_norm(model.parameters() , 1.0)\n",
    "            if i <= max_train_steps:\n",
    "                current_lr = scheduler_cos.get_last_lr()[0]\n",
    "                optimizer.step()\n",
    "                scheduler_cos.step()\n",
    "                \n",
    "            else:\n",
    "                current_lr = scheduler_constant.get_last_lr()[0]\n",
    "                optimizer_reduced_lr.step()\n",
    "                scheduler_constant.step()\n",
    "                \n",
    "                \n",
    "            epoch_train_loss.append(act_loss)\n",
    "            #print(f\"current LR->{scheduler.get_last_lr()}\")\n",
    "            \n",
    "            del ids,att_mask,labels\n",
    "            \n",
    "        #batch processing complete    \n",
    "        mean_loss = torch.mean(torch.tensor(epoch_train_loss))\n",
    "        epoch_end_time = time.time()\n",
    "        epoch_durn = (epoch_end_time - epoch_start_time)\n",
    "        num_token = B*T*len(train_loader)\n",
    "        if mean_loss < global_tr_loss:\n",
    "            print(f\"training loss has decreased---> reducing the global loss from {global_tr_loss:.2f} to {mean_loss:.2f} | throughput = {int(num_token/epoch_durn)} tokens/second | norm = {norm:.4f} | learning rate = {current_lr}\")\n",
    "            global_tr_loss = mean_loss\n",
    "            print(f\" epoch= {i+1} and mean train loss is {torch.mean(torch.tensor(epoch_train_loss)):.2f}\")\n",
    "            #printing training metrices\n",
    "            #y_hat = torch.cat(y_hat_list)\n",
    "            #y = torch.cat(label_list)\n",
    "            #acc = accuracy_score(y.cpu().numpy(), y_hat.cpu().numpy())\n",
    "            #f1 = f1_score(y.cpu().numpy(), y_hat.cpu().numpy(), average='micro')\n",
    "            print(f\" epoch= {i+1} : mean train loss is {torch.mean(torch.tensor(epoch_train_loss)):.4f} \")\n",
    "            #checking validation metrices\n",
    "            eval_model(val_loader, model, epoch = i , device = device,tokenizer = tokenizer)\n",
    "            \n",
    "        else:\n",
    "            print(f\"No improvement in training loss..the global training loss is -->{global_tr_loss:.2f} \")\n",
    "            print(f\" epoch= {i+1} and mean train loss is {torch.mean(torch.tensor(epoch_train_loss)):.2f}\")\n",
    "        \n",
    "        \n",
    "    \n",
    "    return model\n",
    "        \n",
    "            \n",
    "            \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6192d48e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inside train model. Device = cuda:0\n",
      "executing epoch:1, it took 1.606126594543457 mins from beginning of epoch till batch#175\n",
      "training loss has decreased---> reducing the global loss from inf to 3.11 and throughput = 103719 tokens/second | norm = 9.4579\n",
      " epoch= 1 and mean train loss is 3.11\n",
      " epoch= 1 : mean train loss is 3.1145 \n",
      "inside validation data for epoch 1\n",
      "Val loss has decreased -->reducing the global validation loss from inf to 2.92\n",
      " validation loss for epoch = 1 is 2.9163\n",
      " epoch= 1 : mean val loss is 2.9163 \n",
      "saving the model model2024-06-1818:31:33.pth\n",
      "executing epoch:2, it took 1.2919510841369628 mins from beginning of epoch till batch#175\n",
      "training loss has decreased---> reducing the global loss from 3.11 to 2.71 and throughput = 120130 tokens/second | norm = 2.7163\n",
      " epoch= 2 and mean train loss is 2.71\n",
      " epoch= 2 : mean train loss is 2.7144 \n",
      "inside validation data for epoch 2\n",
      "Val loss has decreased -->reducing the global validation loss from 2.92 to 2.79\n",
      " validation loss for epoch = 2 is 2.7924\n",
      " epoch= 2 : mean val loss is 2.7924 \n",
      "saving the model model2024-06-1818:34:43.pth\n",
      "executing epoch:3, it took 1.0795535922050477 mins from beginning of epoch till batch#175\n",
      "training loss has decreased---> reducing the global loss from 2.71 to 2.64 and throughput = 131585 tokens/second | norm = 0.7286\n",
      " epoch= 3 and mean train loss is 2.64\n",
      " epoch= 3 : mean train loss is 2.6423 \n",
      "inside validation data for epoch 3\n",
      "Val loss has decreased -->reducing the global validation loss from 2.79 to 2.77\n",
      " validation loss for epoch = 3 is 2.7660\n",
      " epoch= 3 : mean val loss is 2.7660 \n",
      "saving the model model2024-06-1818:37:40.pth\n",
      "executing epoch:4, it took 1.079576353232066 mins from beginning of epoch till batch#175\n",
      "training loss has decreased---> reducing the global loss from 2.64 to 2.60 and throughput = 131627 tokens/second | norm = 1.3452\n",
      " epoch= 4 and mean train loss is 2.60\n",
      " epoch= 4 : mean train loss is 2.5980 \n",
      "inside validation data for epoch 4\n",
      "Val loss has decreased -->reducing the global validation loss from 2.77 to 2.74\n",
      " validation loss for epoch = 4 is 2.7416\n",
      " epoch= 4 : mean val loss is 2.7416 \n",
      "saving the model model2024-06-1818:40:37.pth\n",
      "executing epoch:5, it took 1.0779240528742473 mins from beginning of epoch till batch#175\n",
      "training loss has decreased---> reducing the global loss from 2.60 to 2.56 and throughput = 131738 tokens/second | norm = 1.6203\n",
      " epoch= 5 and mean train loss is 2.56\n",
      " epoch= 5 : mean train loss is 2.5605 \n",
      "inside validation data for epoch 5\n",
      "Val loss has decreased -->reducing the global validation loss from 2.74 to 2.73\n",
      " validation loss for epoch = 5 is 2.7256\n",
      " epoch= 5 : mean val loss is 2.7256 \n",
      "saving the model model2024-06-1818:43:35.pth\n",
      "executing epoch:6, it took 1.079982089996338 mins from beginning of epoch till batch#175\n",
      "training loss has decreased---> reducing the global loss from 2.56 to 2.53 and throughput = 131574 tokens/second | norm = 1.9975\n",
      " epoch= 6 and mean train loss is 2.53\n",
      " epoch= 6 : mean train loss is 2.5305 \n",
      "inside validation data for epoch 6\n",
      "Val loss has decreased -->reducing the global validation loss from 2.73 to 2.72\n",
      " validation loss for epoch = 6 is 2.7155\n",
      " epoch= 6 : mean val loss is 2.7155 \n",
      "saving the model model2024-06-1818:46:32.pth\n",
      "executing epoch:7, it took 1.0793748418490092 mins from beginning of epoch till batch#175\n",
      "training loss has decreased---> reducing the global loss from 2.53 to 2.50 and throughput = 131755 tokens/second | norm = 1.8301\n",
      " epoch= 7 and mean train loss is 2.50\n",
      " epoch= 7 : mean train loss is 2.5045 \n",
      "inside validation data for epoch 7\n",
      "Val loss has decreased -->reducing the global validation loss from 2.72 to 2.71\n",
      " validation loss for epoch = 7 is 2.7124\n",
      " epoch= 7 : mean val loss is 2.7124 \n",
      "saving the model model2024-06-1818:49:29.pth\n",
      "executing epoch:8, it took 1.0789538939793906 mins from beginning of epoch till batch#175\n",
      "training loss has decreased---> reducing the global loss from 2.50 to 2.48 and throughput = 131654 tokens/second | norm = 2.8079\n",
      " epoch= 8 and mean train loss is 2.48\n",
      " epoch= 8 : mean train loss is 2.4830 \n",
      "inside validation data for epoch 8\n",
      "Val loss has decreased -->reducing the global validation loss from 2.71 to 2.70\n",
      " validation loss for epoch = 8 is 2.6984\n",
      " epoch= 8 : mean val loss is 2.6984 \n",
      "saving the model model2024-06-1818:52:27.pth\n",
      "executing epoch:9, it took 1.0811640898386636 mins from beginning of epoch till batch#175\n",
      "training loss has decreased---> reducing the global loss from 2.48 to 2.46 and throughput = 131525 tokens/second | norm = 2.2150\n",
      " epoch= 9 and mean train loss is 2.46\n",
      " epoch= 9 : mean train loss is 2.4629 \n",
      "inside validation data for epoch 9\n",
      "Val loss has decreased -->reducing the global validation loss from 2.70 to 2.69\n",
      " validation loss for epoch = 9 is 2.6898\n",
      " epoch= 9 : mean val loss is 2.6898 \n",
      "saving the model model2024-06-1818:55:25.pth\n",
      "executing epoch:10, it took 1.0799275318781534 mins from beginning of epoch till batch#175\n",
      "training loss has decreased---> reducing the global loss from 2.46 to 2.44 and throughput = 131597 tokens/second | norm = 1.8929\n",
      " epoch= 10 and mean train loss is 2.44\n",
      " epoch= 10 : mean train loss is 2.4421 \n",
      "inside validation data for epoch 10\n",
      "Val loss has decreased -->reducing the global validation loss from 2.69 to 2.69\n",
      " validation loss for epoch = 10 is 2.6886\n",
      " epoch= 10 : mean val loss is 2.6886 \n",
      "saving the model model2024-06-1818:58:22.pth\n",
      "executing epoch:11, it took 1.0791091561317443 mins from beginning of epoch till batch#175\n",
      "training loss has decreased---> reducing the global loss from 2.44 to 2.42 and throughput = 131752 tokens/second | norm = 3.1193\n",
      " epoch= 11 and mean train loss is 2.42\n",
      " epoch= 11 : mean train loss is 2.4244 \n",
      "inside validation data for epoch 11\n",
      "Val loss has decreased -->reducing the global validation loss from 2.69 to 2.69\n",
      " validation loss for epoch = 11 is 2.6862\n",
      " epoch= 11 : mean val loss is 2.6862 \n",
      "saving the model model2024-06-1819:01:19.pth\n",
      "executing epoch:12, it took 1.0798524022102356 mins from beginning of epoch till batch#175\n",
      "training loss has decreased---> reducing the global loss from 2.42 to 2.41 and throughput = 131692 tokens/second | norm = 1.5282\n",
      " epoch= 12 and mean train loss is 2.41\n",
      " epoch= 12 : mean train loss is 2.4088 \n",
      "inside validation data for epoch 12\n",
      "Val loss has decreased -->reducing the global validation loss from 2.69 to 2.68\n",
      " validation loss for epoch = 12 is 2.6796\n",
      " epoch= 12 : mean val loss is 2.6796 \n",
      "saving the model model2024-06-1819:04:16.pth\n",
      "executing epoch:13, it took 1.0783718029658 mins from beginning of epoch till batch#175\n",
      "training loss has decreased---> reducing the global loss from 2.41 to 2.39 and throughput = 131804 tokens/second | norm = 2.1521\n",
      " epoch= 13 and mean train loss is 2.39\n",
      " epoch= 13 : mean train loss is 2.3939 \n",
      "inside validation data for epoch 13\n",
      "No improvement in validation loss-->epoch= 13 and global val loss is 2.68\n",
      "executing epoch:14, it took 1.0769390384356181 mins from beginning of epoch till batch#175\n",
      "training loss has decreased---> reducing the global loss from 2.39 to 2.38 and throughput = 131984 tokens/second | norm = 1.6284\n",
      " epoch= 14 and mean train loss is 2.38\n",
      " epoch= 14 : mean train loss is 2.3807 \n",
      "inside validation data for epoch 14\n",
      "No improvement in validation loss-->epoch= 14 and global val loss is 2.68\n",
      "executing epoch:15, it took 1.0772146224975585 mins from beginning of epoch till batch#175\n",
      "training loss has decreased---> reducing the global loss from 2.38 to 2.37 and throughput = 131984 tokens/second | norm = 1.6294\n",
      " epoch= 15 and mean train loss is 2.37\n",
      " epoch= 15 : mean train loss is 2.3682 \n",
      "inside validation data for epoch 15\n",
      "No improvement in validation loss-->epoch= 15 and global val loss is 2.68\n",
      "executing epoch:16, it took 1.077094316482544 mins from beginning of epoch till batch#175\n",
      "training loss has decreased---> reducing the global loss from 2.37 to 2.36 and throughput = 131991 tokens/second | norm = 1.7991\n",
      " epoch= 16 and mean train loss is 2.36\n",
      " epoch= 16 : mean train loss is 2.3562 \n",
      "inside validation data for epoch 16\n",
      "No improvement in validation loss-->epoch= 16 and global val loss is 2.68\n",
      "executing epoch:17, it took 1.0767915884653727 mins from beginning of epoch till batch#175\n",
      "training loss has decreased---> reducing the global loss from 2.36 to 2.35 and throughput = 132015 tokens/second | norm = 1.9376\n",
      " epoch= 17 and mean train loss is 2.35\n",
      " epoch= 17 : mean train loss is 2.3465 \n",
      "inside validation data for epoch 17\n",
      "No improvement in validation loss-->epoch= 17 and global val loss is 2.68\n",
      "executing epoch:18, it took 1.0767406185468038 mins from beginning of epoch till batch#175\n",
      "training loss has decreased---> reducing the global loss from 2.35 to 2.34 and throughput = 132019 tokens/second | norm = 1.6975\n",
      " epoch= 18 and mean train loss is 2.34\n",
      " epoch= 18 : mean train loss is 2.3369 \n",
      "inside validation data for epoch 18\n",
      "No improvement in validation loss-->epoch= 18 and global val loss is 2.68\n",
      "executing epoch:19, it took 1.076697834332784 mins from beginning of epoch till batch#175\n",
      "training loss has decreased---> reducing the global loss from 2.34 to 2.33 and throughput = 132018 tokens/second | norm = 1.3990\n",
      " epoch= 19 and mean train loss is 2.33\n",
      " epoch= 19 : mean train loss is 2.3286 \n",
      "inside validation data for epoch 19\n",
      "No improvement in validation loss-->epoch= 19 and global val loss is 2.68\n",
      "executing epoch:20, it took 1.0770739634831747 mins from beginning of epoch till batch#175\n",
      "training loss has decreased---> reducing the global loss from 2.33 to 2.32 and throughput = 131985 tokens/second | norm = 1.6379\n",
      " epoch= 20 and mean train loss is 2.32\n",
      " epoch= 20 : mean train loss is 2.3214 \n",
      "inside validation data for epoch 20\n",
      "No improvement in validation loss-->epoch= 20 and global val loss is 2.68\n",
      "executing epoch:21, it took 1.0774377822875976 mins from beginning of epoch till batch#175\n",
      "training loss has decreased---> reducing the global loss from 2.32 to 2.31 and throughput = 131967 tokens/second | norm = 1.6097\n",
      " epoch= 21 and mean train loss is 2.31\n",
      " epoch= 21 : mean train loss is 2.3150 \n",
      "inside validation data for epoch 21\n",
      "No improvement in validation loss-->epoch= 21 and global val loss is 2.68\n",
      "executing epoch:22, it took 1.0767708818117778 mins from beginning of epoch till batch#175\n",
      "training loss has decreased---> reducing the global loss from 2.31 to 2.31 and throughput = 132005 tokens/second | norm = 1.2580\n",
      " epoch= 22 and mean train loss is 2.31\n",
      " epoch= 22 : mean train loss is 2.3095 \n",
      "inside validation data for epoch 22\n",
      "No improvement in validation loss-->epoch= 22 and global val loss is 2.68\n",
      "executing epoch:23, it took 1.0773974776268005 mins from beginning of epoch till batch#175\n",
      "training loss has decreased---> reducing the global loss from 2.31 to 2.31 and throughput = 131961 tokens/second | norm = 1.4386\n",
      " epoch= 23 and mean train loss is 2.31\n",
      " epoch= 23 : mean train loss is 2.3050 \n",
      "inside validation data for epoch 23\n",
      "No improvement in validation loss-->epoch= 23 and global val loss is 2.68\n",
      "executing epoch:24, it took 1.0771894534428914 mins from beginning of epoch till batch#175\n",
      "training loss has decreased---> reducing the global loss from 2.31 to 2.30 and throughput = 131972 tokens/second | norm = 1.2988\n",
      " epoch= 24 and mean train loss is 2.30\n",
      " epoch= 24 : mean train loss is 2.3012 \n",
      "inside validation data for epoch 24\n",
      "No improvement in validation loss-->epoch= 24 and global val loss is 2.68\n",
      "executing epoch:25, it took 1.0772413849830627 mins from beginning of epoch till batch#175\n",
      "training loss has decreased---> reducing the global loss from 2.30 to 2.30 and throughput = 131976 tokens/second | norm = 0.6021\n",
      " epoch= 25 and mean train loss is 2.30\n",
      " epoch= 25 : mean train loss is 2.2981 \n",
      "inside validation data for epoch 25\n",
      "No improvement in validation loss-->epoch= 25 and global val loss is 2.68\n",
      "executing epoch:26, it took 1.0773653944333395 mins from beginning of epoch till batch#175\n",
      "training loss has decreased---> reducing the global loss from 2.30 to 2.30 and throughput = 131978 tokens/second | norm = 2.1371\n",
      " epoch= 26 and mean train loss is 2.30\n",
      " epoch= 26 : mean train loss is 2.2958 \n",
      "inside validation data for epoch 26\n",
      "No improvement in validation loss-->epoch= 26 and global val loss is 2.68\n",
      "executing epoch:27, it took 1.0773533582687378 mins from beginning of epoch till batch#175\n",
      "training loss has decreased---> reducing the global loss from 2.30 to 2.29 and throughput = 131950 tokens/second | norm = 1.1494\n",
      " epoch= 27 and mean train loss is 2.29\n",
      " epoch= 27 : mean train loss is 2.2942 \n",
      "inside validation data for epoch 27\n",
      "No improvement in validation loss-->epoch= 27 and global val loss is 2.68\n",
      "executing epoch:28, it took 1.0773141344388326 mins from beginning of epoch till batch#175\n",
      "training loss has decreased---> reducing the global loss from 2.29 to 2.29 and throughput = 131937 tokens/second | norm = 1.3886\n",
      " epoch= 28 and mean train loss is 2.29\n",
      " epoch= 28 : mean train loss is 2.2929 \n",
      "inside validation data for epoch 28\n",
      "No improvement in validation loss-->epoch= 28 and global val loss is 2.68\n",
      "executing epoch:29, it took 1.0761814355850219 mins from beginning of epoch till batch#175\n",
      "training loss has decreased---> reducing the global loss from 2.29 to 2.29 and throughput = 132160 tokens/second | norm = 0.5189\n",
      " epoch= 29 and mean train loss is 2.29\n",
      " epoch= 29 : mean train loss is 2.2923 \n",
      "inside validation data for epoch 29\n",
      "No improvement in validation loss-->epoch= 29 and global val loss is 2.68\n",
      "executing epoch:30, it took 1.0794860323270161 mins from beginning of epoch till batch#175\n",
      "training loss has decreased---> reducing the global loss from 2.29 to 2.29 and throughput = 131636 tokens/second | norm = 2.2165\n",
      " epoch= 30 and mean train loss is 2.29\n",
      " epoch= 30 : mean train loss is 2.2918 \n",
      "inside validation data for epoch 30\n",
      "No improvement in validation loss-->epoch= 30 and global val loss is 2.68\n"
     ]
    }
   ],
   "source": [
    "tr_model = train_model(train_loader, val_loader, model =  model,tokenizer = tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40738962",
   "metadata": {},
   "source": [
    "## Use this section if you want a model with Random weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53c4691",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig\n",
    "# Define the model name\n",
    "model_name = \"distilgpt2\"\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# Load the model configuration\n",
    "config = AutoConfig.from_pretrained(model_name)\n",
    "\n",
    "# Initialize the model with random weights\n",
    "model = AutoModelForCausalLM.from_config(config)\n",
    "\n",
    "# Check the model\n",
    "#print(model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ffb562",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89eb81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(df_train.iloc[178]['input_ids'], dtype = torch.long)\n",
    "att = torch.tensor(df_train.iloc[178]['attention_mask'], dtype = torch.long)\n",
    "random_out = model(input_ids = x.view(B,T) , attention_mask = att.view(B,T), labels =   x.view(B,T) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9ea992",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a482c823",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4f41b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196306af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5becdc54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2235afd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300b5712",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa992d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6121cd8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0526f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f923100",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bff47db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
