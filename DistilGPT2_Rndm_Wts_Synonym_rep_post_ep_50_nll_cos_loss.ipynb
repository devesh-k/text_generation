{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bff88b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "## reference https://huggingface.co/learn/nlp-course/en/chapter7/6?fw=pt#training-a-causal-language-model-from-scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d77fef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e79e0430",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: jupyter in /opt/conda/lib/python3.8/site-packages (1.0.0)\n",
      "Requirement already satisfied: ipywidgets in /opt/conda/lib/python3.8/site-packages (8.1.3)\n",
      "Requirement already satisfied: jupyter-console in /opt/conda/lib/python3.8/site-packages (from jupyter) (6.6.3)\n",
      "Requirement already satisfied: qtconsole in /opt/conda/lib/python3.8/site-packages (from jupyter) (5.5.2)\n",
      "Requirement already satisfied: ipykernel in /opt/conda/lib/python3.8/site-packages (from jupyter) (6.29.5)\n",
      "Requirement already satisfied: nbconvert in /opt/conda/lib/python3.8/site-packages (from jupyter) (6.3.0)\n",
      "Requirement already satisfied: notebook in /opt/conda/lib/python3.8/site-packages (from jupyter) (6.4.1)\n",
      "Requirement already satisfied: comm>=0.1.3 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (5.14.3)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (7.30.0)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.11 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (3.0.11)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.11 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (4.0.11)\n",
      "Requirement already satisfied: decorator in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (5.1.0)\n",
      "Requirement already satisfied: pickleshare in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.7.5)\n",
      "Requirement already satisfied: setuptools>=18.5 in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (59.4.0)\n",
      "Requirement already satisfied: jedi>=0.16 in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.18.1)\n",
      "Requirement already satisfied: backcall in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: matplotlib-inline in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.3)\n",
      "Requirement already satisfied: pygments in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (2.10.0)\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (4.8.0)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.47)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /opt/conda/lib/python3.8/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.8/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.8/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=6.1.0->ipywidgets) (0.2.5)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.8/site-packages (from ipykernel->jupyter) (5.8.0)\n",
      "Requirement already satisfied: pyzmq>=24 in /opt/conda/lib/python3.8/site-packages (from ipykernel->jupyter) (26.0.3)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.8/site-packages (from ipykernel->jupyter) (21.3)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /opt/conda/lib/python3.8/site-packages (from ipykernel->jupyter) (5.7.2)\n",
      "Requirement already satisfied: nest-asyncio in /opt/conda/lib/python3.8/site-packages (from ipykernel->jupyter) (1.5.4)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in /opt/conda/lib/python3.8/site-packages (from ipykernel->jupyter) (1.8.2)\n",
      "Requirement already satisfied: tornado>=6.1 in /opt/conda/lib/python3.8/site-packages (from ipykernel->jupyter) (6.1)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in /opt/conda/lib/python3.8/site-packages (from ipykernel->jupyter) (7.1.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /opt/conda/lib/python3.8/site-packages (from jupyter-client>=6.1.12->ipykernel->jupyter) (2.8.2)\n",
      "Requirement already satisfied: entrypoints in /opt/conda/lib/python3.8/site-packages (from jupyter-client>=6.1.12->ipykernel->jupyter) (0.3)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /opt/conda/lib/python3.8/site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel->jupyter) (4.2.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.8/site-packages (from python-dateutil>=2.1->jupyter-client>=6.1.12->ipykernel->jupyter) (1.16.0)\n",
      "Requirement already satisfied: testpath in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (0.5.0)\n",
      "Requirement already satisfied: jupyterlab-pygments in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (0.1.2)\n",
      "Requirement already satisfied: nbclient<0.6.0,>=0.5.0 in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (0.5.9)\n",
      "Requirement already satisfied: jinja2>=2.4 in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (3.0.3)\n",
      "Requirement already satisfied: bleach in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (4.1.0)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (1.5.0)\n",
      "Requirement already satisfied: nbformat>=4.4 in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (5.1.3)\n",
      "Requirement already satisfied: defusedxml in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (0.7.1)\n",
      "Requirement already satisfied: mistune<2,>=0.8.1 in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (0.8.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.8/site-packages (from jinja2>=2.4->nbconvert->jupyter) (2.0.1)\n",
      "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /opt/conda/lib/python3.8/site-packages (from nbformat>=4.4->nbconvert->jupyter) (4.2.1)\n",
      "Requirement already satisfied: ipython-genutils in /opt/conda/lib/python3.8/site-packages (from nbformat>=4.4->nbconvert->jupyter) (0.2.0)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /opt/conda/lib/python3.8/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.4->nbconvert->jupyter) (0.18.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /opt/conda/lib/python3.8/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.4->nbconvert->jupyter) (21.2.0)\n",
      "Requirement already satisfied: importlib-resources>=1.4.0 in /opt/conda/lib/python3.8/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.4->nbconvert->jupyter) (5.4.0)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /opt/conda/lib/python3.8/site-packages (from importlib-resources>=1.4.0->jsonschema!=2.5.0,>=2.4->nbformat>=4.4->nbconvert->jupyter) (3.6.0)\n",
      "Requirement already satisfied: webencodings in /opt/conda/lib/python3.8/site-packages (from bleach->nbconvert->jupyter) (0.5.1)\n",
      "Requirement already satisfied: argon2-cffi in /opt/conda/lib/python3.8/site-packages (from notebook->jupyter) (21.1.0)\n",
      "Requirement already satisfied: Send2Trash>=1.5.0 in /opt/conda/lib/python3.8/site-packages (from notebook->jupyter) (1.8.0)\n",
      "Requirement already satisfied: terminado>=0.8.3 in /opt/conda/lib/python3.8/site-packages (from notebook->jupyter) (0.12.1)\n",
      "Requirement already satisfied: prometheus-client in /opt/conda/lib/python3.8/site-packages (from notebook->jupyter) (0.12.0)\n",
      "Requirement already satisfied: cffi>=1.0.0 in /opt/conda/lib/python3.8/site-packages (from argon2-cffi->notebook->jupyter) (1.15.0)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.8/site-packages (from cffi>=1.0.0->argon2-cffi->notebook->jupyter) (2.21)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging->ipykernel->jupyter) (3.0.6)\n",
      "Requirement already satisfied: qtpy>=2.4.0 in /opt/conda/lib/python3.8/site-packages (from qtconsole->jupyter) (2.4.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#installing some libraries\n",
    "#!pip install datasets\n",
    "!pip install --upgrade jupyter ipywidgets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5fe9c7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch, transformers\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import PreTrainedModel, PretrainedConfig\n",
    "from transformers import AutoModel, AutoConfig,AutoModelForCausalLM, AutoConfig,GPT2Config,GPT2Tokenizer\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "#from datasets import load_dataset\n",
    "import pandas as pd, numpy as np\n",
    "from torch.cuda.amp import autocast\n",
    "from torch import cuda\n",
    "import datetime\n",
    "import warnings,itertools\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "import json\n",
    "# Ignore all warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "#pip install transformers bitsandbytes>=0.39.0 -q\n",
    "import zipfile,logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de6bcd5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "import random\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc059685",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb1bde34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "model\n"
     ]
    }
   ],
   "source": [
    "#global params for training\n",
    "\n",
    "B,T = 32,1024\n",
    "epoch = 50\n",
    "nll_train = epoch/2\n",
    "random_init_wts = True\n",
    "min_text_len = 0\n",
    "# hard coded com\n",
    "comp_ratio = 3\n",
    "# train_loss_list = []\n",
    "# val_loss_list =[]\n",
    "if cuda.is_available():\n",
    "    device = torch.device('cuda:0')\n",
    "    print(device)\n",
    "else:\n",
    "    device = 'cpu'\n",
    "#print(device)\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "#os.environ[\"MKL_DEBUG_CPU_TYPE\"] = \"5\"\n",
    "\n",
    "#print(global_tr_loss)\n",
    "model_path = os.path.join(\"model\")\n",
    "print(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73bc71c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./data/unzip_text_10M'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "directory = os.path.join('.','data','unzip_text_10M')  # Replace with your directory path\n",
    "directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c0cb4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_text(directory):\n",
    "    directory = os.path.join('.','data','unzip_text_10M',str(directory))  # Replace with your directory path\n",
    "    print(f\"directory :{directory}\")\n",
    "    # List all files in the directory\n",
    "    files = [f for f in os.listdir(directory) if os.path.isfile(os.path.join(directory, f))]\n",
    "    print(f\"files:{files}\")\n",
    "    text_content = []\n",
    "    # Read each file\n",
    "    total_lines = 0\n",
    "    for filenum,filename in enumerate(files):\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            #first_line = file.read()\n",
    "            #print(f\"filename :{filename}->first few lines {first_line}\")\n",
    "            #continue\n",
    "            #lines_list = [line.strip() for line in open(file_path, 'r')]\n",
    "            text = file.read()\n",
    "            text_content.append(text)\n",
    "            print(f\"the file:{filename} has been appeneded to the uber list and its length is {len(text_content)} \")\n",
    "            #total_lines+=len(lines_list)\n",
    "            #text_content.append(lines_list)\n",
    "    \n",
    "    flattened_list = ''.join(text_content)\n",
    "    assert (len(flattened_list) == total_lines , f\"Expected {len(flattened_list)} to be equal to {total_lines}\" )\n",
    "    \n",
    "    return flattened_list\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "69b70093",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "directory :./data/unzip_text_10M/train_10M\n",
      "files:['switchboard.train', 'simple_wiki.train', 'open_subtitles.train', 'gutenberg.train', 'childes.train', 'bnc_spoken.train']\n",
      "the file:switchboard.train has been appeneded to the uber list and its length is 1 \n",
      "the file:simple_wiki.train has been appeneded to the uber list and its length is 2 \n",
      "the file:open_subtitles.train has been appeneded to the uber list and its length is 3 \n",
      "the file:gutenberg.train has been appeneded to the uber list and its length is 4 \n",
      "the file:childes.train has been appeneded to the uber list and its length is 5 \n",
      "the file:bnc_spoken.train has been appeneded to the uber list and its length is 6 \n"
     ]
    }
   ],
   "source": [
    "train_list = read_text(\"train_10M\")\n",
    "#print(train_dict)\n",
    "#val_list = read_text(\"dev\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f398b995",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54215049"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4dba9da4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1654\n"
     ]
    }
   ],
   "source": [
    "chunks = len(train_list)//(B*T)\n",
    "print(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "21290731",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(\"distilgpt2\")\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "49842644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50257\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.pad_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "327b43c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "directory :./data/unzip_text_10M/dev\n",
      "files:['switchboard.dev', 'simple_wiki.dev', 'open_subtitles.dev', 'gutenberg.dev', 'childes.dev', 'bnc_spoken.dev']\n",
      "the file:switchboard.dev has been appeneded to the uber list and its length is 1 \n",
      "the file:simple_wiki.dev has been appeneded to the uber list and its length is 2 \n",
      "the file:open_subtitles.dev has been appeneded to the uber list and its length is 3 \n",
      "the file:gutenberg.dev has been appeneded to the uber list and its length is 4 \n",
      "the file:childes.dev has been appeneded to the uber list and its length is 5 \n",
      "the file:bnc_spoken.dev has been appeneded to the uber list and its length is 6 \n"
     ]
    }
   ],
   "source": [
    "val_list = read_text(\"dev\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bc763319",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50256\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d539aead",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.658591848769283"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.random()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d0a77b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b8fa99de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ranked_synonyms(word, pos):\n",
    "    synonyms = []\n",
    "    for syn in wordnet.synsets(word, pos=pos):\n",
    "        for lemma in syn.lemmas():\n",
    "            if lemma.name() != word:\n",
    "                synonyms.append((lemma.name(), syn.wup_similarity(syn)))\n",
    "    \n",
    "    # Sort synonyms by similarity score in descending order\n",
    "    ranked_synonyms = sorted(set(synonyms), key=lambda x: x[1] if x[1] is not None else 0, reverse=True)\n",
    "    return [syn for syn, _ in ranked_synonyms]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3f6d6d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def replace_verbs_with_synonyms(text):\n",
    "    words = word_tokenize(text)\n",
    "    tagged = pos_tag(words)\n",
    "    result = []\n",
    "    for word, pos in tagged:\n",
    "        if pos =='VB':\n",
    "            wordnet_pos = get_wordnet_pos(pos)\n",
    "            lemma = lemmatizer.lemmatize(word, wordnet_pos)\n",
    "            synonyms = get_ranked_synonyms(lemma, wordnet_pos)\n",
    "            if synonyms:\n",
    "                replacement = random.choice(synonyms)  # Choose from top 3 synonyms\n",
    "                #print(f\"word = {word}|replacement = {replacement}\")\n",
    "                result.append(replacement)\n",
    "            else:\n",
    "                result.append(word)\n",
    "        else:\n",
    "            result.append(word)\n",
    "    return \" \".join(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cdfe26eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = train_list[584:1000]\n",
    "repl = replace_verbs_with_synonyms(sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "734c2e0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "like Thumper that , boy , I could just sound out and stupefy all kinds of game B : Did it process ? A : Yeah , A : except we live in Plano , Texas now B : No , B : right . A : so B : I , um , I had a , for many years I had a dog that was part Springer Spaniel . B : I just sleep_with them . B : Her name was Molly , B : but she is n't alive any more B : We had her for , um , fifteen years , I think , my family did , and just loved her . B : She was the greatest ,\n"
     ]
    }
   ],
   "source": [
    "print(repl)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7caed38c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "like Thumper that, boy, I could just go out and get all kinds of game\n",
      "B:\tDid it work?\n",
      "A:\tYeah,\n",
      "A:\texcept we live in Plano, Texas now\n",
      "B:\tNo,\n",
      "B:\tright.\n",
      "A:\tso\n",
      "B:\tI, um, I had a, for many years I had a dog that was part Springer Spaniel.\n",
      "B:\tI just love them.\n",
      "B:\tHer name was Molly,\n",
      "B:\tbut she isn't alive any more\n",
      "B:\tWe had her for, um, fifteen years, I think, my family did, and just loved her.\n",
      "B:\tShe was the greatest,\n"
     ]
    }
   ],
   "source": [
    "print(sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213ef5b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b702e3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7f20eb24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequences(token_ids_list, max_length = B*T, tokenizer = tokenizer):\n",
    "    padded_sequences = tokenizer.pad(\n",
    "        {\"input_ids\": token_ids_list},\n",
    "        padding='max_length',\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    return padded_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f91d50f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text (text,tokenizer = tokenizer,max_length = B*T):\n",
    "    #print(f\"inside tokenize_text\")\n",
    "    enc = tokenizer(text,padding='max_length',truncation=True,max_length=max_length,return_tensors=\"pt\",return_attention_mask=True)\n",
    "    input_id = enc['input_ids']\n",
    "    att_mask = enc['attention_mask']\n",
    "    \n",
    "    # now concatenate these lists to B*T\n",
    "    input_id = torch.squeeze(input_id, dim = 0).to(dtype = torch.long)\n",
    "    att_mask = torch.squeeze(att_mask, dim = 0).to(dtype = torch.bool)\n",
    "    return input_id,att_mask\n",
    "    \n",
    "    \n",
    "    \n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5301099a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test the tokenizer:\n",
    "model_name = 'distilgpt2'\n",
    "if random_init_wts:\n",
    "    config = AutoConfig.from_pretrained(model_name, vocab_size = 50304)\n",
    "    # Initialize the model with random weights\n",
    "    model = AutoModelForCausalLM.from_config(config)\n",
    "else:\n",
    "    #model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "# model.resize_token_embeddings(len(tokenizer))\n",
    "# model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "    \n",
    "model = torch.compile(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d872ca",
   "metadata": {},
   "source": [
    "### Data loaders and Dataset for batched training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "99fc280b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataset_pyt_tr_syn(Dataset):\n",
    "    def __init__(self, text_list, B = B, T = T, tokenizer = tokenizer, prob = .5,synonym_flag = False):\n",
    "        self.text_list = text_list\n",
    "        #print(f\"Value of B {B}\")\n",
    "        self.prob = prob\n",
    "        \n",
    "        \n",
    "                                        \n",
    "    def __getitem__(self, idx):\n",
    "        #words_list[start_index:start_index + n]\n",
    "        chunk = self.text_list[idx:idx + B*T*comp_ratio]\n",
    "        #print(f\"length of chunk = {len(chunk)}\")\n",
    "        random_num = random.random()\n",
    "        #print(f\"The random number generated = {random_num}\")\n",
    "        if random_num > self.prob:\n",
    "            chunk = replace_verbs_with_synonyms(chunk)\n",
    "            \n",
    "        inp,att = tokenize_text(chunk, tokenizer)\n",
    "        input_id = inp.view(B,T)\n",
    "        attention_mask = att.view(B,T)\n",
    "        #print(f\"shape of input_id = {input_id.shape}| shape of attention = {attention_mask.shape}\")\n",
    "        \n",
    "        return input_id,attention_mask\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        #return the length of the dataframe\n",
    "        num_chunks = comp_ratio*B*T\n",
    "        return len(self.text_list)//num_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b17e813c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataset_pyt_tr_no_syn(Dataset):\n",
    "    def __init__(self, text_list, B = B, T = T, tokenizer = tokenizer, comp_ratio = comp_ratio):\n",
    "        self.text_list = text_list\n",
    "        #print(f\"Value of B {B}\")\n",
    "                                                \n",
    "    def __getitem__(self, idx):\n",
    "        #words_list[start_index:start_index + n]\n",
    "        chunk = self.text_list[idx:idx + B*T*comp_ratio]\n",
    "        #print(f\"length of chunk = {len()}\")\n",
    "        inp,att = tokenize_text(chunk, tokenizer)\n",
    "        input_id = inp.view(B,T)\n",
    "        attention_mask = att.view(B,T)\n",
    "        #print(f\"shape of input_id = {input_id.shape}| shape of attention = {attention_mask.shape}\")\n",
    "        \n",
    "        return input_id,attention_mask\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        #return the length of the dataframe\n",
    "        num_chunks = comp_ratio*B*T\n",
    "        return len(self.text_list)//num_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "312ec88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataset_pyt_val(Dataset):\n",
    "    def __init__(self, text_list, B = B, T = T, tokenizer = tokenizer, comp_ratio = comp_ratio):\n",
    "        self.text_list = text_list\n",
    "        #print(f\"Value of B {B}\")\n",
    "                                                \n",
    "    def __getitem__(self, idx):\n",
    "        #words_list[start_index:start_index + n]\n",
    "        chunk = self.text_list[idx:idx + B*T*comp_ratio]\n",
    "        #print(f\"length of chunk = {len()}\")\n",
    "        inp,att = tokenize_text(chunk, tokenizer)\n",
    "        input_id = inp.view(B,T)\n",
    "        attention_mask = att.view(B,T)\n",
    "        #print(f\"shape of input_id = {input_id.shape}| shape of attention = {attention_mask.shape}\")\n",
    "        \n",
    "        return input_id,attention_mask\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        #return the length of the dataframe\n",
    "        num_chunks = comp_ratio*B*T\n",
    "        return len(self.text_list)//num_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ded48ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_dataset = dataset_pyt(filtered_df,tokenizer = tokenizer)\n",
    "train_dataset_syn = dataset_pyt_tr_syn(train_list)\n",
    "train_dataset_no_syn = dataset_pyt_tr_no_syn(train_list)\n",
    "\n",
    "val_dataset = dataset_pyt_val(val_list)\n",
    "\n",
    "train_loader_syn = DataLoader(train_dataset_syn,batch_size = 1, shuffle = True , num_workers = 4, pin_memory = True)\n",
    "train_loader_no_syn = DataLoader(train_dataset_no_syn,batch_size = 1, shuffle = True , num_workers = 4, pin_memory = True)\n",
    "\n",
    "\n",
    "#train_loader = DataLoader(train_dataset,batch_size = 1, shuffle = True , num_workers = 4, pin_memory = True)\n",
    "val_loader = DataLoader(val_dataset,batch_size = 1, shuffle = True , num_workers = 4, pin_memory = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1ce08f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x,y,z = train_dataset[0]\n",
    "# print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ad5816a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer vocabulary size: 50258\n",
      "EOS token ID: 50256\n",
      "PAD token ID: 50257\n"
     ]
    }
   ],
   "source": [
    "print(f\"Tokenizer vocabulary size: {len(tokenizer)}\")\n",
    "print(f\"EOS token ID: {tokenizer.eos_token_id}\")\n",
    "print(f\"PAD token ID: {tokenizer.pad_token_id}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f07ec12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_file(log_message, model_name = \"GPT2\" ,random_init_wts = random_init_wts ):\n",
    "    current_datetime = datetime.datetime.now()\n",
    "    # Extract date and time components\n",
    "    current_date = str(current_datetime.date())\n",
    "    log_file = model_name +'_COS_SIM_'+'random_init_wts'+ '_'+str(random_init_wts)+'_' +current_date+'.log'\n",
    "    print(f\"*****LOGGING INFO IN {log_file}*********\")\n",
    "    filepath = os.path.join(\"model\",log_file)\n",
    "    logging.basicConfig(filename=filepath, \n",
    "                    filemode='a',  # Overwrite the log file each time\n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s', \n",
    "                    level=logging.DEBUG)\n",
    "    logger = logging.getLogger()\n",
    "    logger.info(log_message)\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "513ecd29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the train loader is 551\n",
      "Length of the val loader is 575\n",
      "num_tokens= 18055168\n"
     ]
    }
   ],
   "source": [
    "print(f\"Length of the train loader is {len(train_loader_syn)}\")\n",
    "print(f\"Length of the val loader is {len(val_loader)}\")\n",
    "print(f\"num_tokens= {B*T*len(train_loader_syn)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6beeceaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#emb,att,inp = train_dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "783314b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class check_train_metrics:\n",
    "    def __init__(self, patience=25, min_delta=0 , B = T, T = T,best_loss = torch.inf,early_stop = False):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = best_loss\n",
    "        self.early_stop = early_stop\n",
    "        self.B = B\n",
    "        self.T = T\n",
    "        self.improvement = None\n",
    "\n",
    "    def __call__(self, loss, epoch , epoch_durn, norm , current_lr, num_token):\n",
    "        if self.best_loss - loss > self.min_delta:\n",
    "            \n",
    "            print(f\"training loss has decreased---> reducing the best loss from {self.best_loss:.2f} to {loss:.2f} | throughput = {int(num_token/epoch_durn)} tokens/second | norm = {norm:.4f} | learning rate = {current_lr:.5e}\")\n",
    "            self.best_loss = loss\n",
    "            self.counter = 0\n",
    "            self.improvement = True\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            self.improvement = False\n",
    "            print(f\"No improvement in training  loss-->epoch= {epoch} and best loss is {self.best_loss:.2f}|current_loss = {loss}|counter = {self.counter}\")\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9c94cd22",
   "metadata": {},
   "outputs": [],
   "source": [
    "class check_val_metrics:\n",
    "    def __init__(self, patience=25, min_delta=0, best_loss = torch.inf,early_stop = False):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = best_loss\n",
    "        self.early_stop = early_stop\n",
    "        \n",
    "\n",
    "    def __call__(self, loss, epoch , model, tokenizer):\n",
    "        if self.best_loss - loss > self.min_delta:\n",
    "            print(f\"Val loss has decreased -->reducing the global validation loss from {self.best_loss:.2f} to {loss:.2f}\")\n",
    "            s1 = (f\"Val loss has decreased -->reducing the global validation loss from {self.best_loss:.2f} to {loss:.2f}\")\n",
    "            print(f\" validation loss for epoch = {epoch} is {loss:.4f}\")\n",
    "            self.best_loss = loss\n",
    "            s2 = f\" validation loss for epoch = {epoch} is {loss:.4f}\"\n",
    "            print(f\" epoch= {epoch} :  val loss is {loss:.4f} \")\n",
    "            s3 = f\" epoch= {epoch} :  val loss is {loss:.4f} \"\n",
    "            #save the model\n",
    "            # Get the current date and time\n",
    "            current_datetime = datetime.datetime.now()\n",
    "            # Extract date and time components\n",
    "            current_date = str(current_datetime.date())\n",
    "            current_time = str(current_datetime.time()).split('.')[0]\n",
    "            file_name = 'model'+ current_date+current_time+'.pth'\n",
    "            path = os.path.join(\"model\",file_name)\n",
    "            print(f\"saving the model {file_name}\")\n",
    "            s4 = f\"saving the model {file_name}\"\n",
    "            #torch.save(model.state_dict(), path)\n",
    "            model.save_pretrained(path)\n",
    "            tokenizer.save_pretrained(path)\n",
    "            log_message = s1+s2+s3+s4\n",
    "            write_file(log_message)\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            print(f\"No improvement in validation loss-->epoch= {epoch} and best val loss is {self.best_loss:.2f}|current_Val loss = {loss}|counter = {self.counter}\")\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f6325504",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_output = model(input_ids = inp ,attention_mask = att, labels = inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e5e4f700",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad\n",
    "\n",
    "def eval_model(val_loader, model, epoch , device = device,tokenizer = tokenizer):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    e = epoch+1\n",
    "    val_loss_accum = 0.0\n",
    "    embedding_layer = model.transformer.wte\n",
    "    print(f\"inside validation data for epoch {e}\")\n",
    "    for ind,(input_id,attention_mask) in enumerate(val_loader):\n",
    "        ids = input_id.to(device=device, non_blocking=True)\n",
    "        ids = torch.squeeze(ids, dim = 0)\n",
    "        att_mask = attention_mask.to(device=device, non_blocking=True)\n",
    "        att_mask =  torch.squeeze(att_mask, dim = 0)\n",
    "        labels = ids.clone().to(device)\n",
    "        with autocast(dtype = torch.bfloat16):\n",
    "            model_output = model(input_ids = ids ,attention_mask = att_mask, labels = labels)\n",
    "            total_loss = model_output.loss\n",
    "            \n",
    "    \n",
    "    \n",
    "        val_loss_accum+= total_loss.detach().item()\n",
    "        del att_mask,labels,model_output,total_loss,ids\n",
    "    return val_loss_accum        \n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0769d4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss(input_id,attention_mask,model,cos_loss = False,device = device):\n",
    "    model.to(device)\n",
    "    embedding_layer = model.transformer.wte\n",
    "    ids = input_id.to(device=device, non_blocking=True)\n",
    "    ids = torch.squeeze(ids, dim = 0)\n",
    "    att_mask = attention_mask.to(device=device, non_blocking=True)\n",
    "    att_mask =  torch.squeeze(att_mask, dim = 0)\n",
    "    labels = ids.clone().to(device)\n",
    "    if cos_loss == False:\n",
    "        with autocast(dtype = torch.bfloat16):\n",
    "            model_output = model(input_ids = ids ,attention_mask = att_mask, labels = labels)\n",
    "            total_loss = model_output.loss\n",
    "        return total_loss\n",
    "    else:\n",
    "        with autocast(dtype = torch.bfloat16):\n",
    "            input_emb = embedding_layer(ids)\n",
    "            model_output = model(input_ids = ids ,attention_mask = att_mask, labels = labels)\n",
    "            logits = model_output.logits\n",
    "            predictions = torch.argmax(logits, dim=-1)\n",
    "            #print(f\"predictions = {predictions}\")\n",
    "            prediction_embeddings = embedding_layer(predictions)\n",
    "            cos_sim = F.cosine_similarity(torch.squeeze(prediction_embeddings,dim = 0), torch.squeeze(input_emb,dim = 0), dim=1)\n",
    "            cos_loss = 1- cos_sim.mean()\n",
    "            total_loss = cos_loss\n",
    "        return total_loss\n",
    "    \n",
    "        \n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "72ae1d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_model(train_loader_no_syn,train_loader_syn,val_loader,model,num_epoch = epoch,device = device,tokenizer = tokenizer):\n",
    "    #model.train()\n",
    "    device = device\n",
    "    lr_custom = 1e-5\n",
    "    print(f\"inside train model. Device = {device}\")\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.AdamW(params =  model.parameters(), lr= lr_custom,fused = True ,weight_decay = .1)\n",
    "      \n",
    "    extra_train = .1*num_epoch\n",
    "    max_train_steps = int(num_epoch +extra_train )\n",
    "    import time\n",
    "    from transformers import get_linear_schedule_with_warmup\n",
    "    total_steps = len(train_loader_no_syn) * num_epoch\n",
    "    scheduler_cos = transformers.get_cosine_schedule_with_warmup( optimizer= optimizer, num_warmup_steps =int(total_steps * 0.1) ,num_training_steps= total_steps )\n",
    "        \n",
    "    epoch_train_log = []\n",
    "    epoch_val_log = []\n",
    "    validate_val_metric = check_val_metrics()\n",
    "    validate_train_metric = check_train_metrics()\n",
    "    embedding_layer = model.transformer.wte\n",
    "    for i in range (max_train_steps):\n",
    "        \n",
    "        epoch_start_time = time.time()\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        # we use 2 schedulers - the first LR scheduler uses a cosine decay for 100 epochs the second scheduler takes the last LR from cosine scheduler and then maintains that LR for the next 10 epochs\n",
    "        if i >= num_epoch:\n",
    "            optimizer_reduced_lr = torch.optim.AdamW(params =  model.parameters(), lr= current_lr ,fused = True , weight_decay=.1)\n",
    "            scheduler_constant = transformers.get_constant_schedule_with_warmup( optimizer = optimizer_reduced_lr ,num_warmup_steps = 0, last_epoch = -1 )\n",
    "        \n",
    "        epoch_train_loss = 0       \n",
    "        if i < nll_train:# for first 50% of the epochs do not replace synonym and calculate cos similarity\n",
    "            for ind,(input_id,attention_mask) in enumerate(train_loader_no_syn):\n",
    "                if ind == int(len(train_loader_no_syn)/2):\n",
    "                    batch_time = time.time()\n",
    "                    duration = batch_time - epoch_start_time\n",
    "                    print(f\"executing epoch:{i+1}, it took {duration/60} mins from beginning of epoch till batch#{ind}\")\n",
    "                total_loss = calc_loss(input_id,attention_mask,model,cos_loss = False)\n",
    "                total_loss.backward()\n",
    "                epoch_train_loss += total_loss.detach().item()\n",
    "                norm = torch.nn.utils.clip_grad_norm(model.parameters() , 1.0)\n",
    "                optimizer.step()\n",
    "                scheduler_cos.step()\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "            \n",
    "            \n",
    "        else:#for next 50% of the epochs replace synonym and calculate cos similarity\n",
    "            for ind,(input_id,attention_mask) in enumerate(train_loader_syn):\n",
    "                if ind == int(len(train_loader_syn)/2):\n",
    "                    batch_time = time.time()\n",
    "                    duration = batch_time - epoch_start_time\n",
    "                    print(f\"executing epoch:{i+1}, it took {duration/60} mins from beginning of epoch till batch#{ind}\")\n",
    "                total_loss = calc_loss(input_id,attention_mask,model,cos_loss = True)\n",
    "                total_loss.backward()\n",
    "                epoch_train_loss += total_loss.detach().item()\n",
    "                norm = torch.nn.utils.clip_grad_norm(model.parameters() , 1.0)\n",
    "                         \n",
    "            if i <= num_epoch:\n",
    "                optimizer.step()\n",
    "                scheduler_cos.step()\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "            else:\n",
    "                optimizer_reduced_lr.step()\n",
    "                optimizer_reduced_lr.zero_grad(set_to_none=True)\n",
    "                scheduler_constant.step()\n",
    "                \n",
    "                         \n",
    "            #del att_mask,labels,model_output,ids\n",
    "            \n",
    "        #batch processing complete \n",
    "        #print(f\"batch processing complete , lambda = {lambda_val} |total_loss for batch= {total_loss}\")\n",
    "        \n",
    "        if i <= num_epoch:\n",
    "            current_lr = scheduler_cos.get_last_lr()[0]\n",
    "        epoch_end_time = time.time()\n",
    "        epoch_durn = (epoch_end_time - epoch_start_time)\n",
    "        num_token = B*T*len(train_loader_no_syn)\n",
    "        epoch_train_log.append(epoch_train_loss)\n",
    "        validate_train_metric(epoch_train_loss, i , epoch_durn, norm , current_lr, num_token)\n",
    "        \n",
    "        if validate_train_metric.improvement:\n",
    "            val_loss= eval_model(val_loader, model, epoch = i, device = device,tokenizer = tokenizer)\n",
    "            epoch_val_log.append(val_loss)\n",
    "            validate_val_metric(val_loss, i , model, tokenizer)\n",
    "            if validate_train_metric.early_stop or validate_val_metric.early_stop :\n",
    "                print(f\"early stopping trigerred either from training data or val data | train_counter = {validate_train_metric.counter}|val_counter = {validate_val_metric.counter}\")\n",
    "                break\n",
    "        else:\n",
    "            if validate_val_metric.early_stop:\n",
    "                print(f\"early stopping trigerred from validation data\")\n",
    "                break\n",
    "              \n",
    "    \n",
    "    return model,epoch_train_log,epoch_val_log\n",
    "        \n",
    "            \n",
    "            \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ab02818b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inside train model. Device = cuda:0\n",
      "executing epoch:1, it took 1.218025263150533 mins from beginning of epoch till batch#275\n",
      "training loss has decreased---> reducing the best loss from inf to 4164.22 | throughput = 144844 tokens/second | norm = 1.7305 | learning rate = 2.00000e-06\n",
      "inside validation data for epoch 1\n",
      "Val loss has decreased -->reducing the global validation loss from inf to 3069.97\n",
      " validation loss for epoch = 0 is 3069.9703\n",
      " epoch= 0 :  val loss is 3069.9703 \n",
      "saving the model model2024-07-2917:04:23.pth\n",
      "*****LOGGING INFO IN GPT2_COS_SIM_random_init_wts_True_2024-07-29.log*********\n",
      "executing epoch:2, it took 0.9777637799580892 mins from beginning of epoch till batch#275\n",
      "training loss has decreased---> reducing the best loss from 4164.22 to 2446.68 | throughput = 169548 tokens/second | norm = 3.6379 | learning rate = 4.00000e-06\n",
      "inside validation data for epoch 2\n",
      "Val loss has decreased -->reducing the global validation loss from 3069.97 to 2528.96\n",
      " validation loss for epoch = 1 is 2528.9634\n",
      " epoch= 1 :  val loss is 2528.9634 \n",
      "saving the model model2024-07-2917:06:44.pth\n",
      "*****LOGGING INFO IN GPT2_COS_SIM_random_init_wts_True_2024-07-29.log*********\n",
      "executing epoch:3, it took 0.7992901841799418 mins from beginning of epoch till batch#275\n",
      "training loss has decreased---> reducing the best loss from 2446.68 to 1899.55 | throughput = 188300 tokens/second | norm = 7.6356 | learning rate = 6.00000e-06\n",
      "inside validation data for epoch 3\n",
      "Val loss has decreased -->reducing the global validation loss from 2528.96 to 2366.57\n",
      " validation loss for epoch = 2 is 2366.5674\n",
      " epoch= 2 :  val loss is 2366.5674 \n",
      "saving the model model2024-07-2917:08:54.pth\n",
      "*****LOGGING INFO IN GPT2_COS_SIM_random_init_wts_True_2024-07-29.log*********\n",
      "executing epoch:4, it took 0.8002602299054463 mins from beginning of epoch till batch#275\n",
      "training loss has decreased---> reducing the best loss from 1899.55 to 1543.25 | throughput = 188141 tokens/second | norm = 14.5250 | learning rate = 8.00000e-06\n",
      "inside validation data for epoch 4\n",
      "Val loss has decreased -->reducing the global validation loss from 2366.57 to 2341.52\n",
      " validation loss for epoch = 3 is 2341.5245\n",
      " epoch= 3 :  val loss is 2341.5245 \n",
      "saving the model model2024-07-2917:11:04.pth\n",
      "*****LOGGING INFO IN GPT2_COS_SIM_random_init_wts_True_2024-07-29.log*********\n",
      "executing epoch:5, it took 0.8008840282758077 mins from beginning of epoch till batch#275\n",
      "training loss has decreased---> reducing the best loss from 1543.25 to 1219.80 | throughput = 187728 tokens/second | norm = 13.9302 | learning rate = 1.00000e-05\n",
      "inside validation data for epoch 5\n",
      "No improvement in validation loss-->epoch= 4 and best val loss is 2341.52|current_Val loss = 2422.6836438179016|counter = 1\n",
      "executing epoch:6, it took 0.8028137723604838 mins from beginning of epoch till batch#275\n",
      "training loss has decreased---> reducing the best loss from 1219.80 to 918.61 | throughput = 187726 tokens/second | norm = 17.1832 | learning rate = 9.98782e-06\n",
      "inside validation data for epoch 6\n",
      "No improvement in validation loss-->epoch= 5 and best val loss is 2341.52|current_Val loss = 2529.3963022232056|counter = 2\n",
      "executing epoch:7, it took 0.8012150327364603 mins from beginning of epoch till batch#275\n",
      "training loss has decreased---> reducing the best loss from 918.61 to 682.01 | throughput = 187704 tokens/second | norm = 17.5369 | learning rate = 9.95134e-06\n",
      "inside validation data for epoch 7\n",
      "No improvement in validation loss-->epoch= 6 and best val loss is 2341.52|current_Val loss = 2660.2573494911194|counter = 3\n",
      "executing epoch:8, it took 0.8031388401985169 mins from beginning of epoch till batch#275\n",
      "training loss has decreased---> reducing the best loss from 682.01 to 492.30 | throughput = 187716 tokens/second | norm = 14.9529 | learning rate = 9.89074e-06\n",
      "inside validation data for epoch 8\n",
      "No improvement in validation loss-->epoch= 7 and best val loss is 2341.52|current_Val loss = 2791.3667426109314|counter = 4\n",
      "executing epoch:9, it took 0.8014885107676188 mins from beginning of epoch till batch#275\n",
      "training loss has decreased---> reducing the best loss from 492.30 to 327.56 | throughput = 187697 tokens/second | norm = 15.1978 | learning rate = 9.80631e-06\n",
      "inside validation data for epoch 9\n",
      "No improvement in validation loss-->epoch= 8 and best val loss is 2341.52|current_Val loss = 2943.458873271942|counter = 5\n",
      "executing epoch:10, it took 0.8040260513623555 mins from beginning of epoch till batch#275\n",
      "training loss has decreased---> reducing the best loss from 327.56 to 194.74 | throughput = 187371 tokens/second | norm = 13.2844 | learning rate = 9.69846e-06\n",
      "inside validation data for epoch 10\n",
      "No improvement in validation loss-->epoch= 9 and best val loss is 2341.52|current_Val loss = 3051.364125728607|counter = 6\n",
      "executing epoch:11, it took 0.8017086744308471 mins from beginning of epoch till batch#275\n",
      "training loss has decreased---> reducing the best loss from 194.74 to 103.97 | throughput = 187890 tokens/second | norm = 6.6198 | learning rate = 9.56773e-06\n",
      "inside validation data for epoch 11\n",
      "No improvement in validation loss-->epoch= 10 and best val loss is 2341.52|current_Val loss = 3126.061797142029|counter = 7\n",
      "executing epoch:12, it took 0.8021661202112834 mins from beginning of epoch till batch#275\n",
      "training loss has decreased---> reducing the best loss from 103.97 to 54.42 | throughput = 187719 tokens/second | norm = 5.3473 | learning rate = 9.41474e-06\n",
      "inside validation data for epoch 12\n",
      "No improvement in validation loss-->epoch= 11 and best val loss is 2341.52|current_Val loss = 3248.429584980011|counter = 8\n",
      "executing epoch:13, it took 0.8017637610435486 mins from beginning of epoch till batch#275\n",
      "training loss has decreased---> reducing the best loss from 54.42 to 26.03 | throughput = 187841 tokens/second | norm = 6.2339 | learning rate = 9.24024e-06\n",
      "inside validation data for epoch 13\n",
      "No improvement in validation loss-->epoch= 12 and best val loss is 2341.52|current_Val loss = 3398.951030731201|counter = 9\n",
      "executing epoch:14, it took 0.8019396503766377 mins from beginning of epoch till batch#275\n",
      "training loss has decreased---> reducing the best loss from 26.03 to 15.03 | throughput = 187778 tokens/second | norm = 1.5946 | learning rate = 9.04508e-06\n",
      "inside validation data for epoch 14\n",
      "No improvement in validation loss-->epoch= 13 and best val loss is 2341.52|current_Val loss = 3419.06934261322|counter = 10\n",
      "executing epoch:15, it took 0.8016936222712199 mins from beginning of epoch till batch#275\n",
      "training loss has decreased---> reducing the best loss from 15.03 to 10.21 | throughput = 187810 tokens/second | norm = 1.9788 | learning rate = 8.83022e-06\n",
      "inside validation data for epoch 15\n",
      "No improvement in validation loss-->epoch= 14 and best val loss is 2341.52|current_Val loss = 3526.202046394348|counter = 11\n",
      "executing epoch:16, it took 0.8015128970146179 mins from beginning of epoch till batch#275\n",
      "training loss has decreased---> reducing the best loss from 10.21 to 7.67 | throughput = 187834 tokens/second | norm = 2.3649 | learning rate = 8.59670e-06\n",
      "inside validation data for epoch 16\n",
      "No improvement in validation loss-->epoch= 15 and best val loss is 2341.52|current_Val loss = 3545.405498981476|counter = 12\n",
      "executing epoch:17, it took 0.8040762782096863 mins from beginning of epoch till batch#275\n",
      "training loss has decreased---> reducing the best loss from 7.67 to 6.27 | throughput = 187413 tokens/second | norm = 1.3196 | learning rate = 8.34565e-06\n",
      "inside validation data for epoch 17\n",
      "No improvement in validation loss-->epoch= 16 and best val loss is 2341.52|current_Val loss = 3585.2045092582703|counter = 13\n",
      "executing epoch:18, it took 0.8006890654563904 mins from beginning of epoch till batch#275\n",
      "training loss has decreased---> reducing the best loss from 6.27 to 5.29 | throughput = 187936 tokens/second | norm = 0.6941 | learning rate = 8.07831e-06\n",
      "inside validation data for epoch 18\n",
      "No improvement in validation loss-->epoch= 17 and best val loss is 2341.52|current_Val loss = 3580.9300861358643|counter = 14\n",
      "executing epoch:19, it took 0.8038269837697347 mins from beginning of epoch till batch#275\n",
      "training loss has decreased---> reducing the best loss from 5.29 to 4.68 | throughput = 187488 tokens/second | norm = 0.7343 | learning rate = 7.79596e-06\n",
      "inside validation data for epoch 19\n",
      "No improvement in validation loss-->epoch= 18 and best val loss is 2341.52|current_Val loss = 3572.5783100128174|counter = 15\n",
      "executing epoch:20, it took 0.8011672218640645 mins from beginning of epoch till batch#275\n",
      "training loss has decreased---> reducing the best loss from 4.68 to 4.13 | throughput = 187957 tokens/second | norm = 1.0625 | learning rate = 7.50000e-06\n",
      "inside validation data for epoch 20\n",
      "No improvement in validation loss-->epoch= 19 and best val loss is 2341.52|current_Val loss = 3520.558793067932|counter = 16\n",
      "executing epoch:21, it took 0.8038326938947041 mins from beginning of epoch till batch#275\n",
      "training loss has decreased---> reducing the best loss from 4.13 to 3.74 | throughput = 187362 tokens/second | norm = 0.4275 | learning rate = 7.19186e-06\n",
      "inside validation data for epoch 21\n",
      "No improvement in validation loss-->epoch= 20 and best val loss is 2341.52|current_Val loss = 3604.653905391693|counter = 17\n",
      "executing epoch:22, it took 0.800839610894521 mins from beginning of epoch till batch#275\n",
      "No improvement in training  loss-->epoch= 21 and best loss is 3.74|current_loss = 3.775063486304134|counter = 1\n",
      "executing epoch:23, it took 0.8013913790384929 mins from beginning of epoch till batch#275\n",
      "training loss has decreased---> reducing the best loss from 3.74 to 3.25 | throughput = 187618 tokens/second | norm = 0.4430 | learning rate = 6.54508e-06\n",
      "inside validation data for epoch 23\n",
      "No improvement in validation loss-->epoch= 22 and best val loss is 2341.52|current_Val loss = 3641.1873993873596|counter = 18\n",
      "executing epoch:24, it took 0.803203547000885 mins from beginning of epoch till batch#275\n",
      "No improvement in training  loss-->epoch= 23 and best loss is 3.25|current_loss = 3.434132097288966|counter = 1\n",
      "executing epoch:25, it took 0.8015684088071188 mins from beginning of epoch till batch#275\n",
      "training loss has decreased---> reducing the best loss from 3.25 to 3.03 | throughput = 187869 tokens/second | norm = 0.5871 | learning rate = 5.86824e-06\n",
      "inside validation data for epoch 25\n",
      "No improvement in validation loss-->epoch= 24 and best val loss is 2341.52|current_Val loss = 3635.616772174835|counter = 19\n",
      "executing epoch:26, it took 1.0906076709429422 mins from beginning of epoch till batch#275\n",
      "No improvement in training  loss-->epoch= 25 and best loss is 3.03|current_loss = 524.4395244717598|counter = 1\n",
      "executing epoch:27, it took 1.1303890188535055 mins from beginning of epoch till batch#275\n",
      "No improvement in training  loss-->epoch= 26 and best loss is 3.03|current_loss = 524.4288319945335|counter = 2\n",
      "executing epoch:28, it took 1.0653964201609294 mins from beginning of epoch till batch#275\n",
      "No improvement in training  loss-->epoch= 27 and best loss is 3.03|current_loss = 523.9547559022903|counter = 3\n",
      "executing epoch:29, it took 1.0818988680839539 mins from beginning of epoch till batch#275\n",
      "No improvement in training  loss-->epoch= 28 and best loss is 3.03|current_loss = 524.1681588292122|counter = 4\n",
      "executing epoch:30, it took 1.0705085436503092 mins from beginning of epoch till batch#275\n",
      "No improvement in training  loss-->epoch= 29 and best loss is 3.03|current_loss = 523.9072570204735|counter = 5\n",
      "executing epoch:31, it took 1.1170915881792705 mins from beginning of epoch till batch#275\n",
      "No improvement in training  loss-->epoch= 30 and best loss is 3.03|current_loss = 523.5231085419655|counter = 6\n",
      "executing epoch:32, it took 1.100933841864268 mins from beginning of epoch till batch#275\n",
      "No improvement in training  loss-->epoch= 31 and best loss is 3.03|current_loss = 522.7557541131973|counter = 7\n",
      "executing epoch:33, it took 1.115037985642751 mins from beginning of epoch till batch#275\n",
      "No improvement in training  loss-->epoch= 32 and best loss is 3.03|current_loss = 522.3619467616081|counter = 8\n",
      "executing epoch:34, it took 1.0556981245676675 mins from beginning of epoch till batch#275\n",
      "No improvement in training  loss-->epoch= 33 and best loss is 3.03|current_loss = 521.8228106498718|counter = 9\n",
      "executing epoch:35, it took 1.1551751732826232 mins from beginning of epoch till batch#275\n",
      "No improvement in training  loss-->epoch= 34 and best loss is 3.03|current_loss = 522.0621215105057|counter = 10\n",
      "executing epoch:36, it took 1.164169998963674 mins from beginning of epoch till batch#275\n",
      "No improvement in training  loss-->epoch= 35 and best loss is 3.03|current_loss = 521.4140948057175|counter = 11\n",
      "executing epoch:37, it took 1.0214580734570822 mins from beginning of epoch till batch#275\n",
      "No improvement in training  loss-->epoch= 36 and best loss is 3.03|current_loss = 520.4938191771507|counter = 12\n",
      "executing epoch:38, it took 1.0696845014890035 mins from beginning of epoch till batch#275\n",
      "No improvement in training  loss-->epoch= 37 and best loss is 3.03|current_loss = 520.0646504163742|counter = 13\n",
      "executing epoch:39, it took 1.1114928126335144 mins from beginning of epoch till batch#275\n",
      "No improvement in training  loss-->epoch= 38 and best loss is 3.03|current_loss = 520.092330634594|counter = 14\n",
      "executing epoch:40, it took 1.170074498653412 mins from beginning of epoch till batch#275\n",
      "No improvement in training  loss-->epoch= 39 and best loss is 3.03|current_loss = 519.359686434269|counter = 15\n",
      "executing epoch:41, it took 1.1356784462928773 mins from beginning of epoch till batch#275\n",
      "No improvement in training  loss-->epoch= 40 and best loss is 3.03|current_loss = 519.0869126319885|counter = 16\n",
      "executing epoch:42, it took 1.1515644192695618 mins from beginning of epoch till batch#275\n",
      "No improvement in training  loss-->epoch= 41 and best loss is 3.03|current_loss = 518.6740053296089|counter = 17\n",
      "executing epoch:43, it took 1.117567201455434 mins from beginning of epoch till batch#275\n",
      "No improvement in training  loss-->epoch= 42 and best loss is 3.03|current_loss = 518.0400879383087|counter = 18\n",
      "executing epoch:44, it took 1.1105666836102803 mins from beginning of epoch till batch#275\n",
      "No improvement in training  loss-->epoch= 43 and best loss is 3.03|current_loss = 517.5400150418282|counter = 19\n",
      "executing epoch:45, it took 1.1037948489189149 mins from beginning of epoch till batch#275\n",
      "No improvement in training  loss-->epoch= 44 and best loss is 3.03|current_loss = 517.1272051334381|counter = 20\n",
      "executing epoch:46, it took 1.097184149424235 mins from beginning of epoch till batch#275\n",
      "No improvement in training  loss-->epoch= 45 and best loss is 3.03|current_loss = 517.0782651305199|counter = 21\n",
      "executing epoch:47, it took 1.1184155146280925 mins from beginning of epoch till batch#275\n",
      "No improvement in training  loss-->epoch= 46 and best loss is 3.03|current_loss = 516.4167312383652|counter = 22\n",
      "executing epoch:48, it took 1.1333905816078187 mins from beginning of epoch till batch#275\n",
      "No improvement in training  loss-->epoch= 47 and best loss is 3.03|current_loss = 516.0627773404121|counter = 23\n",
      "executing epoch:49, it took 1.0954061587651571 mins from beginning of epoch till batch#275\n",
      "No improvement in training  loss-->epoch= 48 and best loss is 3.03|current_loss = 514.9559451341629|counter = 24\n",
      "executing epoch:50, it took 1.1188763737678529 mins from beginning of epoch till batch#275\n",
      "No improvement in training  loss-->epoch= 49 and best loss is 3.03|current_loss = 515.3600623607635|counter = 25\n",
      "executing epoch:51, it took 1.1356265823046365 mins from beginning of epoch till batch#275\n",
      "No improvement in training  loss-->epoch= 50 and best loss is 3.03|current_loss = 514.0999896526337|counter = 26\n",
      "executing epoch:52, it took 1.170199743906657 mins from beginning of epoch till batch#275\n",
      "No improvement in training  loss-->epoch= 51 and best loss is 3.03|current_loss = 513.8906639814377|counter = 27\n",
      "executing epoch:53, it took 1.0274434407552084 mins from beginning of epoch till batch#275\n",
      "No improvement in training  loss-->epoch= 52 and best loss is 3.03|current_loss = 514.1286872029305|counter = 28\n",
      "executing epoch:54, it took 1.0615364114443462 mins from beginning of epoch till batch#275\n",
      "No improvement in training  loss-->epoch= 53 and best loss is 3.03|current_loss = 513.2887754440308|counter = 29\n",
      "executing epoch:55, it took 1.063291072845459 mins from beginning of epoch till batch#275\n",
      "No improvement in training  loss-->epoch= 54 and best loss is 3.03|current_loss = 513.6315989494324|counter = 30\n"
     ]
    }
   ],
   "source": [
    "tr_model,epoch_train_log,epoch_val_log = train_model(train_loader_no_syn,train_loader_syn, val_loader,model=model,tokenizer = tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c4f6098f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4164.220906257629, 2446.677306175232, 1899.5483124256134, 1543.2535059452057, 1219.8036308288574, 918.6114240884781, 682.0139791965485, 492.297170817852, 327.5625470876694, 194.7352360635996, 103.96937954425812, 54.4150490835309, 26.0308432020247, 15.029132531955838, 10.210197267122567, 7.6729622120037675, 6.270899328403175, 5.290666606277227, 4.680097742006183, 4.128032915759832, 3.743214990478009, 3.775063486304134, 3.2457104476634413, 3.434132097288966, 3.0346558925230056, 524.4395244717598, 524.4288319945335, 523.9547559022903, 524.1681588292122, 523.9072570204735, 523.5231085419655, 522.7557541131973, 522.3619467616081, 521.8228106498718, 522.0621215105057, 521.4140948057175, 520.4938191771507, 520.0646504163742, 520.092330634594, 519.359686434269, 519.0869126319885, 518.6740053296089, 518.0400879383087, 517.5400150418282, 517.1272051334381, 517.0782651305199, 516.4167312383652, 516.0627773404121, 514.9559451341629, 515.3600623607635, 514.0999896526337, 513.8906639814377, 514.1286872029305, 513.2887754440308, 513.6315989494324]\n"
     ]
    }
   ],
   "source": [
    "print(epoch_train_log)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c8e47b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json , os\n",
    "# path_var_train_log = os.path.join(\".\",\"Tokenizers_and_loss_vals\",\"epoch_train_plain_loss.json\")\n",
    "# path_var_val_log = os.path.join(\".\",\"Tokenizers_and_loss_vals\",\"epoch_val_val_loss_.json\")\n",
    "\n",
    "# #print(path_var)\n",
    "# #Write the list to a JSON file\n",
    "# with open(path_var_train_log, \"w\") as file:\n",
    "#     json.dump(epoch_train_log, file)\n",
    "\n",
    "# with open(path_var_val_log, \"w\") as file:\n",
    "#     json.dump(epoch_val_log, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "deb9877c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(path_var_train_log, \"r\") as file:\n",
    "#     train_loss = json.load(file)\n",
    "# with open(path_var_val_log, \"r\") as file:\n",
    "#     val_loss = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e8179516",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f4e60f4cf70>]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_values = range(len(epoch_train_log))\n",
    "plt.plot(x_values, epoch_train_log, label='Train_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "264c4de4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f4e60c9cb20>]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD4CAYAAAAAczaOAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8/fFQqAAAACXBIWXMAAAsTAAALEwEAmpwYAAAqWklEQVR4nO3deXxU5dn/8c+VlayErEIIhCVhEwkQFhesK+LSUlvrWkRFsda2au3TWve6PVX709ZqfR5aULAo4o4rRcWqD8q+hi2RJSEsSchCFrJfvz/mYCMQksAkJ5m53q9XXpm5z5mZ64zDN8f7vuc+oqoYY4zxDwFuF2CMMabjWOgbY4wfsdA3xhg/YqFvjDF+xELfGGP8SJDbBRxLfHy8pqamul2GMcZ0KStXrixS1YSjbevUoZ+amsqKFSvcLsMYY7oUEdnZ3Dbr3jHGGD9ioW+MMX7EQt8YY/yIhb4xxvgRC31jjPEjFvrGGONHLPSNMcaPdOp5+sYY01U1NCqfby1kQ34Zkd2CiO4WTFS3IKLDgr9zOyo0iIAA6bC6LPSNMcaL9h2oZv7yPOYtzyO/9GCrHhMV6vwBcP44RIcFMeikKP7rgsFer89C3xhjTlBjo/J5diEvL83lk80FNDQqpw+M4+6LhnDO4ESq6xo4UF3HgYP1lFfXfXv7QHUdB6rrOXCwjvJq5/7BOnaXVhMaHNgutVroG2PMcSoor+a1Fbt4ZVkuu0oOEhcRwo0T+nHVmD6kxkd8u19YSCA9IkJcrPQ/Wgx9EekGfA6EOvu/rqoPiIgAjwA/ARqA51X1Gaf9L8BFQBVwnaqucp5rKnCv89SPqOpsbx+QMca0p8ZG5cucIl5emsvHm/ZR36ic2j+O300azMRhSYQGtc8Zure05ky/BjhHVStEJBj4UkQ+BIYAKcBgVW0UkURn/wuBNOdnHPA8ME5EYoEHgExAgZUiskBVS7x7SMYY432F5TW8tjKPecvyyC2uokd4MDec0Y8rx6TQPyHS7fJarcXQV8+V0yucu8HOjwK3AFeraqOzX4Gzz2RgjvO4r0UkRkR6AmcBi1S1GEBEFgGTgFe8dzjGGON9L/7fdh79YBN1Dcq4frHcOTGdSSef1OnP6o+mVX36IhIIrAQGAs+p6lIRGQBcISKXAoXAr1Q1G0gG8po8fJfT1ly7McZ0SqrK0x9n88wn2Zw3JJG7LhzCwMSuc1Z/NK0KfVVtADJEJAZ4S0ROxtPHX62qmSLyI2AWMOFECxKR6cB0gD59+pzo0xljzHFpbFQefDeLOV/t5PLM3jx26XCCArv+91nbNHtHVUtFZDGebpldwJvOpreAF5zb+Xj6+g/p7bTl4+niadr+2VFeYwYwAyAzM1PbUp8xpuMdrG3grdX5vLhkOzuKqggPDSQiJIjwkEDCQ4OICAkkPCSIiFDnd9P20CAiQwOJCQshISqU+MhQ4iJDCHY5XGvrG/nNa2tZsHY308/sz+8vHIxnjkrX15rZOwlAnRP4YcD5wOPA28DZwHbge8BW5yELgF+IyDw8A7llqrpHRBYCj4lID2e/icDvvXkwxpiOs7esmjlf7eDlZbmUVtUxrFc015+eysG6BiprGqiqraeytoGqmnpKqg567jvtVbUNx3zuHuHB3/4RaPo7ITKUeOd3YrSn3dsO1jZwy9yVfLalkN9NGswtZw3w+mu4qTVn+j2B2U6/fgAwX1XfE5Evgbkicgeegd4bnf0/wDNdMwfPlM3rAVS1WEQeBpY7+z10aFDXGNN1rMkrZdaX2/lg/R4aVJk4NIkbTu/H2H6xrT4bbmxUzx8H5w9BSVUtheU1FFXUHPa7ltW5pRSW13Cw7sg/FBPS4rn34qEMOinKK8dWVlXHDbOXszq3hP/+0XCuGut7XczimWTTOWVmZqpdI9cY99U3NPJR1l5mfbmdVbmlRIYGccWYFKaemkqfuPAOqaGypv47fxSy91Xw9y+2UVFTzzXj+nLH+enEnsAXoAoOVHPtrGVsK6zkz1dmcNHwnl6svmOJyEpVzTzqNgt9Y0xzyqrqeGV5LnOW7GB3WTV948K57rRULhvdm6huwW6XR0llLU9/vJW5S3OJCAnktvPSmTK+LyFBbRsTyN1fxU9nLqWoooYZUzI5Iy2+nSruGBb6xpg2ySmo4MUl23ljZT4H6xo4tX8cN5zRj3MGJxLYgStCttbWfeU8/N5Gvsguon98BPdeMoSzByW2qrtp894DTJm5jLqGRl68fiwZKTHtX3A7s9A3xjSrvqGR7IIK1uaVsnZXKWvyyti05wAhgQFMzujF9af3Y2ivaLfLbJGqsnhLAY+8t4ltRZVMSIvnvkuGkp7UfH//yp3FXP/CcsJDgnhp2ljSjrFvV2Khb4wBPMGYX3qQtXllrMkrYW1eGevzy74dJI3uFsSIlBjG94/j8swUEqK8PzumvdXWN/LS1zv5y8dbqaxt4JpxfbjjvPQjFjz7bEsBP/vnSnp2D+OlaWPp3aNjxiY6goW+MX6qtKqWtbvKPGfxzpl8UUUtACFBAQzrFc2I3jFkpMQwIiWG1Lhwn5mPXlxZy5+b9Pfffl46U07tS3BgAAvW7ubXr65h0ElRzL5hbLtM/XSThb4xfmj2kh08+G4WqiACAxIivw33jN4xDDopqs0Dnl3Rd/r7EyI4f0gSM77YxpjUWP4xNZPoTjAg7W0W+sb4mY837uOml1bwvfQEpk/oz8m9u/tkuLWWqvLp5gIefd/T33/ekCSevXok3drpQiVuO1bo20VUjPExWbvL+NW81QzrFc3frhlFeIj9MxcRzh2SxIS0BJbvKGZsv1jXl3pwi30ajPEhBQequXH2CqK7BTNz6hgL/MOEBAVw+sCuPQf/RNknwhgfUVVbz7TZKyg7WMdrPzuVpOhubpdkOiELfWN8QGOj8utX17Jhdxl/n5LJsF7d3S7JdFL+2alljI95YuEWPsrayz0XDeG8oUlul2M6MQt9Y7q4V5fn8j///oarx/Vh2hn93C7HdHIW+sZ0YUu+KeKetzYwIS2eP/xgmM98scq0Hwt9Y7qobworuOWfq0iNj+DZq0f57RRE0zb2KTGmCyqprOWGF5cTFCC8cN0Yuof57xevTNvY7B1jupia+gZu/udK9pRV88pN40iJ9Z2Fwkz7szN9Y7oQVeXuNzewbHsxT152CqP7xrpdkuliLPSN6UL+9tk3vLFqF7efl8bkjGS3yzFdkIW+MV3E++v28OTCLUzO6MVt56a5XY7poiz0jekCVueW8Ov5axjdtweP//gUm5ppjpsN5BrTSe2vqGF1bimr80p4dXkeidGhzJgy2meXAzYdo8XQF5FuwOdAqLP/66r6QJPtzwA3qGqkcz8UmAOMBvYDV6jqDmfb74FpQAPwK1Vd6NWjMaaLqm9oZPPeclbnlrAqt5TVuSXs2F8FQFCAMLx3d5687BTifOwKT6bjteZMvwY4R1UrRCQY+FJEPlTVr0UkE+hx2P7TgBJVHSgiVwKPA1eIyFDgSmAY0Av4WETSVbXBe4djTNdQVFHDqp0lrM4rZdXOEtbt+s91auMjQxnVJ4Yrx/ZhVJ8eDE/uTliInd0b72gx9NVzaa0K526w86MiEgg8CVwNXNrkIZOBB53brwPPiqcDcjIwT1VrgO0ikgOMBb7ywnEY0+nVNTTyyHsbWbylkNzi/5zFD+sVzRVjUhjZJ4ZRfXrQu0eY9dmbdtOqPn0n4FcCA4HnVHWpiNwGLFDVPYd9QJOBPABVrReRMiDOaf+6yX67nLbDX2s6MB2gT58+bT4gYzqr2Ut2MPurnZw3JJGfjvecxZ+c3N366E2HalXoO10wGSISA7wlImcCPwHO8nZBqjoDmAGea+R6+/mNccO+A9U8vWgrZw9K4O/XZtqZvHFNm6ZsqmopsBg4G89Zf46I7ADCne4agHwgBUBEgoDueAZ0v2139HbajPF5j7y/ibpG5UFbCdO4rMXQF5EE5wwfEQkDzgdWqupJqpqqqqlAlaoOdB6yAJjq3L4M+NQZF1gAXCkioSLSD0gDlnn1aIzphP4vp4h31+7m52cNoG9chNvlGD/Xmu6dnsBsp18/AJivqu8dY/+ZwEvOmX8xnhk7qGqWiMwHNgL1wK02c8f4utr6Ru5/ZwN9YsP52fcGuF2OMa2avbMOGNnCPpFNblfj6e8/2n6PAo+2sUZjuqyZX27nm8JKXrhujA3Ymk7BlmEwpp3klx7kmU+ymTg0ibMHJ7pdjjGAhb4x7ebhdzeiKPd/f6jbpRjzLQt9Y9rBZ1sK+ChrL788J43ePewiJ6bzsNA3xsuq6xp4cEEW/eMjuHFCP7fLMeY7bJVNY7xsxufb2LG/ipemjSU0yAZvTediZ/rGeFFecRXPLc7h4uE9mZCW4HY5xhzBQt8YL/rDu1kEBgj3XjLE7VKMOSoLfWO85OON+/h4UwG3n5dGz+5hbpdjzFFZ6BvjBQdrG3jw3SzSEiO5/nQbvDWdlw3kGuMFz3+Ww66Sg7xy03iCA+1cynRe9uk05gRtL6rkf/69jR9m9OLUAXFul2PMMVnoG3MCVJUHFmQRGhTA3RfZ4K3p/Cz0jTkBC7P28vnWQu44P53E6G5ul2NMiyz0jTlOVbX1PPTuRob0jObaU/u6XY4xrWKhb8xxeuaTHHaXVfPw5GEE2eCt6SLsk2rMccgpKOcfX2zjstG9yUyNdbscY1rNQt+YNmpsVO59ewPhIYHcdeFgt8sxpk0s9I1po+f//Q1fbyvmnouHEB8Z6nY5xrSJhb4xbbB8RzFPLdrK90f04vLMFLfLMabNLPSNaaWSylp+9cpqevcI47FLT0ZE3C7JmDazZRiMaQVV5c7X1rK/opY3f34aUd2C3S7JmOPS4pm+iHQTkWUislZEskTkD077XBHZIiIbRGSWiAQ77SIiz4hIjoisE5FRTZ5rqohkOz9T2++wjPGumV9u59PNBdxz8RBOTu7udjnGHLfWdO/UAOeo6gggA5gkIuOBucBgYDgQBtzo7H8hkOb8TAeeBxCRWOABYBwwFnhARHp47UiMaSerc0v444ebuWBYkn0Jy3R5LYa+elQ4d4OdH1XVD5xtCiwDejv7TAbmOJu+BmJEpCdwAbBIVYtVtQRYBEzy9gEZ401lVXX84uXVnNS9G0/8eIT145sur1UDuSISKCJrgAI8wb20ybZgYArwkdOUDOQ1efgup6259sNfa7qIrBCRFYWFhW04FGO8S1X53Rvr2Hegmr9eNZLu4daPb7q+VoW+qjaoagaes/mxInJyk81/Az5X1S+8UZCqzlDVTFXNTEiwa4wa97z09U4+ytrL7yYNZmQf64k0vqFNUzZVtRRYjNMtIyIPAAnAr5vslg80ncDc22lrrt2YTmdDfhmPvLeJcwYnMu0MuxKW8R2tmb2TICIxzu0w4Hxgs4jciKef/ipVbWzykAXAtc4snvFAmaruARYCE0WkhzOAO9FpM6ZTKa+u4xcvryI2IoQ//WQEAQHWj298R2vm6fcEZotIIJ4/EvNV9T0RqQd2Al85g1tvqupDwAfARUAOUAVcD6CqxSLyMLDced6HVLXYq0djzAlSVe5+awN5JQeZN308sREhbpdkjFe1GPqqug4YeZT2oz7Wmc1zazPbZgGz2lijMR1m3vI83l27m/+6YBBjbPVM44NsGQZjHJv3HuDBBVlMSIvnlu8NcLscY9qFhb4xeK6CdevcVUSHBfPU5RnWj298lq29Ywxw39tZbCuqZO60cSRE2XLJxnfZmb7xe6+v3MUbq3bxy3PSOG1gvNvlGNOuLPSNX8spqOC+tzcwrl8st52b5nY5xrQ7C33jtzbuPsDUWcsICwnkmatGEmj9+MYPWOgbv/Th+j38+PklNDQqs68fS1J0N7dLMqZD2ECu8SuNjcozn2bz54+zyUiJYcaU0SRa4Bs/YqFv/EZVbT13zl/Lhxv28qNRyTx26XC6BQe6XZYxHcpC3/iFXSVV3DRnJVv2HuDei4cw7Yx+tja+8UsW+sbnLd9RzM9eWkltQyOzrhvDWYMS3S7JGNdY6BufNm9ZLve9s4HePcL5x9RMBiREul2SMa6y0Dc+qb6hkUfe38SLS3YwIS2eZ68aZVe+MgYfnbK5v6KG376+lq++2e92KcYFpVW1XPfCcl5csoMbz+jHC9eNscA3xuGTZ/rhIUG8tnIXyTHhnDogzu1yTAfK3lfOjXNWsKe0micuO4XLM1NafpAxfsQnQz8sJJCUHuFsLSh3uxTTgT7ZtI/b5q2hW3Agr0wfx+i+th6+MYfzydAHSE+KZOteC31/8fLSXO55ez3DekUzY0omvWLC3C7JmE7JJ/v0AdKSotheVEltfWPLO5su7bMtBdz79nrOSk/gtZtPs8A35hh8NvTTkyKpb1R27K90uxTTjrbsLecXL69m0EnRPHv1KMJC7Bu2xhyLz4Z+WmIUAFv3WRePryqqqGHa7OWEhQQyc2omEaE+21tpjNf4bOgPTIwkQGDrvgq3SzHtoLqugZtfWklheQ3/uNb68I1pLZ89NeoWHEif2HCy7Uzf56gqv39zPSt3lvDs1SMZkRLjdknGdBktnumLSDcRWSYia0UkS0T+4LT3E5GlIpIjIq+KSIjTHurcz3G2pzZ5rt877VtE5IJ2OypHWlKUde/4oOcW5/DW6nzuPD+dS07p5XY5xnQpreneqQHOUdURQAYwSUTGA48DT6vqQKAEmObsPw0ocdqfdvZDRIYCVwLDgEnA30SkXUfd0pMi2bG/ipr6hvZ8GdOBPli/hz/9ays/zOjFL84Z6HY5xnQ5LYa+ehzqGA92fhQ4B3jdaZ8N/NC5Pdm5j7P9XPGsYTsZmKeqNaq6HcgBxnrjIJqTnhRFQ6Oyvchm8PiCtXml/Hr+Gkb37cEff3yKLY1szHFo1UCuiASKyBqgAFgEfAOUqmq9s8suINm5nQzkATjby4C4pu1HeUzT15ouIitEZEVhYWGbD6ip/8zgscHcrm536UFunLOC+MhQ/nfKaLv4iTHHqVWhr6oNqpoB9MZzdj64vQpS1RmqmqmqmQkJCSf0XP0TIggQbDC3i6usqefG2Ss4WNvArOvGEB8Z6nZJxnRZbZqyqaqlwGLgVCBGRA7N/ukN5Du384EUAGd7d2B/0/ajPKZddAsOJDUuwgZzu7DGRuX2V9ewee8Bnr16JOlJUW6XZEyX1prZOwkiEuPcDgPOBzbhCf/LnN2mAu84txc493G2f6qq6rRf6czu6QekAcu8dBzNSkuKJNu6d7qsxxduZtHGfdx/yVC74pUxXtCaefo9gdnOTJsAYL6qviciG4F5IvIIsBqY6ew/E3hJRHKAYjwzdlDVLBGZD2wE6oFbVbXdp9WkJ0WxaOM+qusarB+4i5m/PI///fc2pozvy9TTUt0uxxif0GLoq+o6YORR2rdxlNk3qloN/KSZ53oUeLTtZR6/tKQoGhW2FVYytFd0R760OQFfb9vP3W+tZ0JaPA98f6jN1DHGS3x2GYZD0pM810TNtrX1u4wdRZX87J8rSY2P4NmrRxEU6PMfU2M6jM//a+oXH0FggNhgbhdRdrCOG2YvR4BZU8fQPcwuc2iMN/ns2juHhAYFkhoXbnP1u4jH3t9E7v4qXr5pPH3iwt0uxxif4/Nn+uAZzLW5+p3fqtwSXl2Rx7Qz+jG2n13q0Jj24Behn5YUxc7iKqrrbA2ezqqhUbn/nQ0kRYfyy3PT3C7HGJ/lF6E/KCkKVcgpsC6ezuqVZblsyD/APRcPJdIuhmJMu/GL0LcZPJ1bcWUtTy7cwqn94/j+KT3dLscYn+YXoZ8aH0FwoNhgbif15MLNVNbU84fJw2w+vjHtzC9CPzgwgH7xETaY2wmtzStl3vI8rj891dbVMaYD+EXog2cwd4uFfqfS0Kjc984GEiJDue28dLfLMcYv+E3opydGkVd8kKra+pZ3Nh3i1eV5rNtVxj0XD7HBW2M6iP+EvjOYazN4OoeSylqeWLiZcf1i+cEIu86tMR3Fb0I/LcmuotWZPPmvLZRX1/PQ5JNt8NaYDuQ3oZ8aF05IYIAN5nYC63aV8sqyXKaemsqgk2zw1piO5DehHxQYQP8Eu4qW2xoblfvfySIuIpTbz7dv3hrT0fwm9MHTxWPdO+56bWUea/JKufuiwUR3sxU0jelofhX66YmR5JcepLLGZvC4obSqlsc/2sKY1B5cOjLZ7XKM8Ut+FfqHBnOzbQaPK/70ry2UHayzwVtjXORXoX9o2qb163e8DfllzF2ay5TxfRnS0y5baYxb/Cr0+8ZFEBJkM3g6WqPzzdu4iBDuON++eWuMm/wq9AMDhAEJkTaY28FeX7WL1bml3HXhELv8oTEuazH0RSRFRBaLyEYRyRKR25z2DBH5WkTWiMgKERnrtIuIPCMiOSKyTkRGNXmuqSKS7fxMbb/Dal56UqSd6Xegsqo6Hv9wM6P79uBHNnhrjOtac6ZfD9ypqkOB8cCtIjIUeAL4g6pmAPc79wEuBNKcn+nA8wAiEgs8AIwDxgIPiEgP7x1K66QnRbG7rJry6rqOfmm/9NSiLZRU1fLQ5GEEBNjgrTFuazH0VXWPqq5ybpcDm4BkQIFDI3Ldgd3O7cnAHPX4GogRkZ7ABcAiVS1W1RJgETDJq0fTCmmJhy6oYl087S1rdxkvfb2Tn47vy7Be3d0uxxgDtGlpQxFJBUYCS4HbgYUi8ic8fzxOc3ZLBvKaPGyX09Zc++GvMR3P/yHQp0+ftpTXKofWbM/eV86oPh3+Pxp+49A3b3uEh3Dn+YPcLscY42j1QK6IRAJvALer6gHgFuAOVU0B7gBmeqMgVZ2hqpmqmpmQkOCNp/yOlNhwQoMCbDC3nc1dlsvKnSX8btJguofb4K0xnUWrQl9EgvEE/lxVfdNpngocuv0ann56gHwgpcnDezttzbV3qMAAYWBipM3Vb0fbCit47P1NTEiL5yeZvd0uxxjTRGtm7wies/hNqvpUk027ge85t88Bsp3bC4BrnVk844EyVd0DLAQmikgPZwB3otPW4dKTosi2M/12Ud/QyB3z1xISFMCTl42wb94a08m0pk//dGAKsF5E1jhtdwM3AX8RkSCgGqcfHvgAuAjIAaqA6wFUtVhEHgaWO/s9pKrF3jiItkpLiuSt1fmUHayzeeNe9rfPvmFtXinPXj2Sk7p3c7scY8xhWgx9Vf0SaO50bfRR9lfg1maeaxYwqy0Ftof0RM9gbk5BOaP7xrpcje9Ym1fKXz7JZnJGLy45xa6GZUxn5FffyD0k3a6i5XUHaxu4Y/4aEqNCeegHJ7tdjjGmGX55NerePcIICw60wVwvevyjzWwrrGTujeNsto4xnZhfnukHODN4bDDXO77ILuTFJTu4/vRUTh8Y73Y5xphj8MvQB08Xj53pn7jSqlp+89paBiZG8rtJg90uxxjTAj8O/UgKymsorap1u5Qu7b53sthfUcvTl2fQLTjQ7XKMMS3w49C3wdwT9c6afN5du5vbzk1jeG9bW8eYrsBvQz/NrqJ1QvaUHeS+tzcwsk8Mt5w1wO1yjDGt5LehnxwTRkRIoK2tfxwaG5X/em0ddQ3K05dnEBTotx8jY7ocv/3XKiIMTIqy7p3jMOerHXyZU8S9lwwhNT7C7XKMMW3gt6EPkJ4YSXaBnem3RU5BOf/94WbOHpTA1WO9v/S1MaZ9+XfoJ0VRVFFLcaXN4GmNuoZG7nh1LeEhgTx+2Sm2mJoxXZBfh74N5rbNXz/JZn1+Gf/9o+EkRtliasZ0RX4d+k2vomWObVVuCc8uzuFHo5KZdHJPt8sxxhwnvw79nt27ERUaZIO5LaiqrefXr66hZ/cwHvzBMLfLMcacAL9ccO0Qzwweu4pWSx59fxM7i6t45abxRHezxdSM6cr8+kwfPGvrZxfYmX5zFm8pYO7SXG48ox/j+8e5XY4x5gT5feinJUVSXFlLUUWN26V0OsWVtfz29XUMSorizomD3C7HGOMFfh/6/1mDx7p4mlJV7n5zPaVVtTx9hS2mZoyvsND/dgaPdfE09eaqfD7K2sudEwcxtFe02+UYY7zE70M/KTqUqG5BdqbfRF5xFQ8syGJsaiw3TejvdjnGGC/y+9AXEdKTouxM39HQqNz52loA/t/lIwgMsG/dGuNLWgx9EUkRkcUislFEskTktibbfikim532J5q0/15EckRki4hc0KR9ktOWIyJ3ef9wjk96UiRbC8pRVbdLcd3ML7exbHsxD3x/KCmx4W6XY4zxstbM068H7lTVVSISBawUkUVAEjAZGKGqNSKSCCAiQ4ErgWFAL+BjEUl3nus54HxgF7BcRBao6kbvHlLbpSVG8UpVHoUVNX69vMCmPQf408KtXDAsictG93a7HGNMO2gx9FV1D7DHuV0uIpuAZOAm4I+qWuNsK3AeMhmY57RvF5EcYKyzLUdVtwGIyDxnX9dDv+lgrr+Gfk19A3e8uobosGAeu3S4LaZmjI9qU5++iKQCI4GlQDowQUSWisi/RWSMs1sykNfkYbuctubaXZduC6/x1L+2snlvOU9cNpy4yFC3yzHGtJNWL8MgIpHAG8DtqnpARIKAWGA8MAaYLyInPNVDRKYD0wH69OmY9doTokLpHhbst2vwfL1tPzO+2MbV4/pwzuAkt8sxxrSjVp3pi0gwnsCfq6pvOs27gDfVYxnQCMQD+UBKk4f3dtqaa/8OVZ2hqpmqmpmQkNDW4zkunhk8kX652uaB6jrunL+WvrHh3HPRELfLMca0s9bM3hFgJrBJVZ9qsult4Gxnn3QgBCgCFgBXikioiPQD0oBlwHIgTUT6iUgInsHeBV48lhOSlhTF1n3+N4PnDws2sqfsIE9dkUFEqF+vv2eMX2jNv/LTgSnAehFZ47TdDcwCZonIBqAWmKqexMwSkfl4BmjrgVtVtQFARH4BLAQCgVmqmuXNgzkR6YmRHKiup6C8hqRo/xjM/XD9Ht5YtYtfnTOQUX16uF2OMaYDtGb2zpdAc1M5ftrMYx4FHj1K+wfAB20psKMcmsGzZW+5X4R+wYFq7n5rPcOTu/PLc9PcLscY00H8/hu5h6T50cJrqspv31hHVW0DT1+RQXCgfQyM8Rf2r90RHxlCbESIXyzHMHdpLp9tKeTui4YwMDHS7XKMMR3IQt8hIqQlepZj8GXbiyp59P1NTEiLZ8r4vm6XY4zpYBb6TaQnRZGzr8JnZ/DUNzRyx6trCAkK4MnLRhBgi6kZ43cs9JtIT4qkvKaePWXVbpfSLp5b/A1r8kp59NKTOam77w9WG2OOZKHfxLDk7gAs2rjP5Uq875NN+/jLJ1v5YUYvLjmll9vlGGNcYqHfxMiUGMb1i+Uvn2RzoLrO7XK8ZuPuA/zyldUM7RXNYz8a7nY5xhgXWeg3ISLcd8lQSqpqeW5xjtvleMW+A9VMm72c6G7BzJw6hvAQ+9atMf7MQv8wJyd359KRybzw5Q7yiqvcLueEVNXWM232csoO1jHzuky/+NKZMebYLPSP4r8uGERAADz+0Wa3SzluDY3KbfPWsHH3AZ69eiTDenV3uyRjTCdgoX8UPbuHMX1Cf95bt4dVuSVul3NcHv9oM4s27uO+S4bacsnGmG9Z6Dfj5u8NICEqlEfe29jl5u2/vDSXGZ9v49pT+3Ldaalul2OM6UQs9JsRERrEbyamsyq3lPfX73G7nFb7IruQ+97ZwFmDErj/kqF22UNjzHdY6B/DZaNTGHxSFI9/tJnquga3y2lR9r5yfv7PVaQlRvLXq0YSZAupGWMOY6lwDIEBwr0XDyWv+CCzl+xwu5xjKqqo4foXlxMaHMjM68YQ1S3Y7ZKMMZ2QhX4LzkiL5+xBCTy7OIfiylq3yzmq6roGbpqzgqKKGmZOzSQ5JsztkowxnZSFfivcfdEQqmob+MvHW90u5QiNjcpvXlvL6txSnr48gxEpMW6XZIzpxCz0WyEtKYqrxqbwz6W55BR0rvX2n/54K++t28NdFw7mwuE93S7HGNPJWei30u3npRMWHMgfP9zkdinfemPlLv76aQ5XZKZw85n93S7HGNMFWOi3UnxkKLeePZCPNxWwJKfI7XJYum0/d725jtMGxPHwD0+2qZnGmFax0G+D609PJTkmjEfe30RDo3tf2NpeVMnN/1xJSmw4z18zmpAg+89ojGkdS4s26BYcyG8nDWLjngO8uWqXKzVk7yvnmr9/TYAIL1w3hu7hNjXTGNN6LYa+iKSIyGIR2SgiWSJy22Hb7xQRFZF4576IyDMikiMi60RkVJN9p4pItvMz1fuH0/5+MKIXGSkxPLlwC1W19R362st3FPPj55dQ26DMuWEsfeMiOvT1jTFdX2vO9OuBO1V1KDAeuFVEhoLnDwIwEchtsv+FQJrzMx143tk3FngAGAeMBR4QkR5eOo4O41lzfwgF5TXM+Hxbh73uRxv2cM0/lhIfGcpbPz+Nk5Nt1UxjTNu1GPqqukdVVzm3y4FNQLKz+Wngt0DTDu7JwBz1+BqIEZGewAXAIlUtVtUSYBEwyXuH0nFG943l4uE9+d9/b2Pfgfa/nu6cr3Zwy9xVDOsVzeu3nEZKbHi7v6Yxxje1qU9fRFKBkcBSEZkM5Kvq2sN2Swbymtzf5bQ11374a0wXkRUisqKwsLAt5XWo300aTEOj8qeFW9rtNVSVJz7azP3vZHHu4ERevnE8sREh7fZ6xhjf1+rQF5FI4A3gdjxdPncD93u7IFWdoaqZqpqZkJDg7af3mj5x4Vx3eiqvr9pF1u4yrz9/XUMjd762lr999g1XjU3hf346mrCQQK+/jjHGv7Qq9EUkGE/gz1XVN4EBQD9grYjsAHoDq0TkJCAfSGny8N5OW3PtXdatZw8kJiyYR9/f5NU19ytq6pk2ewVvrsrn1+en89ilw23FTGOMV7Rm9o4AM4FNqvoUgKquV9VEVU1V1VQ8XTWjVHUvsAC41pnFMx4oU9U9wEJgooj0cAZwJzptXVb3sGBuOzeNJd/s59PNBV55zsLyGq6a8TX/l1PE4z8ezq/OTbMvXhljvCaoFfucDkwB1ovIGqftblX9oJn9PwAuAnKAKuB6AFUtFpGHgeXOfg+pavHxFt5ZXDO+L3O+2slD722koqae0wfGEx8ZelzPtb2okmtnLaWovJa/XzvaLnNojPE66cyXAszMzNQVK1a4XUaLlnxTxK1zV1FSVQfAycnRTEhL4My0BEb37dGqb8yuySvlhhc9fw9nXTeGDFst0xhznERkpapmHnWbhb53NDQqG/LL+CK7kM+3FrEqt4T6RiU8JJBT+8cxIS2eM9MT6BcfcUR3zaeb93Hr3NUkRIUy+4ax9Iu3L10ZY46fhb4Lyqvr+Oqb/XyRXcTn2YXs3F8FQHJMGGemJ3BmWjynDYznw/V7uOftDQztGc2s68aQEHV8XUPGGHOIhX4nsHN/JZ9nF/HF1kKWfLOfipp6AgQaFc5MT+D5a0YREdqaIRZjjDm2Y4W+pUwH6RsXwZS4CKaM70tdQyNr8kr5fGshoUEB3Py9AQTblExjTAew0HdBcGAAY1JjGZMa63Ypxhg/Y6eXxhjjRyz0jTHGj1joG2OMH7HQN8YYP2Khb4wxfsRC3xhj/IiFvjHG+BELfWOM8SOdehkGESkEdp7AU8QDRV4qx1fYe3Ike0+OZO/JkbrSe9JXVY966cFOHfonSkRWNLf+hL+y9+RI9p4cyd6TI/nKe2LdO8YY40cs9I0xxo/4eujPcLuATsjekyPZe3Ike0+O5BPviU/36RtjjPkuXz/TN8YY04SFvjHG+BGfDH0RmSQiW0QkR0TucruezkBEdojIehFZIyK+cQ3K4yAis0SkQEQ2NGmLFZFFIpLt/O7hZo0drZn35EERyXc+L2tE5CI3a+xoIpIiIotFZKOIZInIbU57l/+s+Fzoi0gg8BxwITAUuEpEhrpbVadxtqpm+MJc4xPwIjDpsLa7gE9UNQ34xLnvT17kyPcE4Gnn85Khqh90cE1uqwfuVNWhwHjgVidHuvxnxedCHxgL5KjqNlWtBeYBk12uyXQSqvo5UHxY82RgtnN7NvDDjqzJbc28J35NVfeo6irndjmwCUjGBz4rvhj6yUBek/u7nDZ/p8C/RGSliEx3u5hOJklV9zi39wJJbhbTifxCRNY53T9drhvDW0QkFRgJLMUHPiu+GPrm6M5Q1VF4ur1uFZEz3S6oM1LPHGabxwzPAwOADGAP8P9crcYlIhIJvAHcrqoHmm7rqp8VXwz9fCClyf3eTptfU9V853cB8BaebjDjsU9EegI4vwtcrsd1qrpPVRtUtRH4O374eRGRYDyBP1dV33Sau/xnxRdDfzmQJiL9RCQEuBJY4HJNrhKRCBGJOnQbmAhsOPaj/MoCYKpzeyrwjou1dAqHgs1xKX72eRERAWYCm1T1qSabuvxnxSe/ketML/szEAjMUtVH3a3IXSLSH8/ZPUAQ8LK/vici8gpwFp5lcvcBDwBvA/OBPniW8r5cVf1mYLOZ9+QsPF07CuwAbm7Sl+3zROQM4AtgPdDoNN+Np1+/S39WfDL0jTHGHJ0vdu8YY4xphoW+Mcb4EQt9Y4zxIxb6xhjjRyz0jTHGj1joG2OMH7HQN8YYP/L/AVzgoAiksKfGAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#epoch_val_log= [t.detach().cpu().numpy() for t in epoch_val_log]\n",
    "x_values_val = range(len(epoch_val_log))\n",
    "plt.plot(x_values_val, epoch_val_log, label='val_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d4a392d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f4e57c41070>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "min_length = min(len(epoch_train_log), len(epoch_val_log))\n",
    "list1 = epoch_train_log[:min_length]\n",
    "list2 = epoch_val_log[:min_length]\n",
    "\n",
    "# Create x-axis values\n",
    "x_values = range(min_length)\n",
    "\n",
    "# Plot the lists\n",
    "plt.plot(x_values, list1, label='Train_loss')\n",
    "plt.plot(x_values, list2, label='Val_loss')\n",
    "\n",
    "# Adding labels and title\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Train Loss and Val_Loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b93def",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b743f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77d3861",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a115fcc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a812a102",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8eeaec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
