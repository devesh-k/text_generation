{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8982cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## reference https://huggingface.co/learn/nlp-course/en/chapter7/6?fw=pt#training-a-causal-language-model-from-scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a164f0bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3eeb3af6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: datasets in /opt/conda/lib/python3.8/site-packages (2.20.0)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.8/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.8/site-packages (from datasets) (3.9.5)\n",
      "Requirement already satisfied: requests>=2.32.2 in /opt/conda/lib/python3.8/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.8/site-packages (from datasets) (1.3.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.8/site-packages (from datasets) (1.21.4)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.8/site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.2 in /opt/conda/lib/python3.8/site-packages (from datasets) (0.23.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.8/site-packages (from datasets) (6.0)\n",
      "Requirement already satisfied: fsspec[http]<=2024.5.0,>=2023.1.0 in /opt/conda/lib/python3.8/site-packages (from datasets) (2024.5.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.8/site-packages (from datasets) (21.3)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.8/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.8/site-packages (from datasets) (17.0.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.8/site-packages (from datasets) (3.4.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.8/site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /opt/conda/lib/python3.8/site-packages (from datasets) (4.66.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets) (21.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.8/site-packages (from huggingface-hub>=0.21.2->datasets) (4.11.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging->datasets) (3.0.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests>=2.32.2->datasets) (1.26.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.8/site-packages (from requests>=2.32.2->datasets) (2.0.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests>=2.32.2->datasets) (3.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests>=2.32.2->datasets) (2021.10.8)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.8/site-packages (from pandas->datasets) (2021.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.8/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: jupyter in /opt/conda/lib/python3.8/site-packages (1.0.0)\n",
      "Requirement already satisfied: ipywidgets in /opt/conda/lib/python3.8/site-packages (8.1.3)\n",
      "Requirement already satisfied: notebook in /opt/conda/lib/python3.8/site-packages (from jupyter) (6.4.1)\n",
      "Requirement already satisfied: ipykernel in /opt/conda/lib/python3.8/site-packages (from jupyter) (6.29.5)\n",
      "Requirement already satisfied: nbconvert in /opt/conda/lib/python3.8/site-packages (from jupyter) (6.3.0)\n",
      "Requirement already satisfied: jupyter-console in /opt/conda/lib/python3.8/site-packages (from jupyter) (6.6.3)\n",
      "Requirement already satisfied: qtconsole in /opt/conda/lib/python3.8/site-packages (from jupyter) (5.5.2)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (5.14.3)\n",
      "Requirement already satisfied: comm>=0.1.3 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.11 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (3.0.11)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.11 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (4.0.11)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (7.30.0)\n",
      "Requirement already satisfied: matplotlib-inline in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.3)\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (4.8.0)\n",
      "Requirement already satisfied: pickleshare in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.7.5)\n",
      "Requirement already satisfied: backcall in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: jedi>=0.16 in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.18.1)\n",
      "Requirement already satisfied: pygments in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (2.10.0)\n",
      "Requirement already satisfied: setuptools>=18.5 in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (59.4.0)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.47)\n",
      "Requirement already satisfied: decorator in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (5.1.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /opt/conda/lib/python3.8/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.8/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.8/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=6.1.0->ipywidgets) (0.2.5)\n",
      "Requirement already satisfied: pyzmq>=24 in /opt/conda/lib/python3.8/site-packages (from ipykernel->jupyter) (26.0.3)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /opt/conda/lib/python3.8/site-packages (from ipykernel->jupyter) (5.7.2)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in /opt/conda/lib/python3.8/site-packages (from ipykernel->jupyter) (7.1.0)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in /opt/conda/lib/python3.8/site-packages (from ipykernel->jupyter) (1.8.2)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.8/site-packages (from ipykernel->jupyter) (5.8.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.8/site-packages (from ipykernel->jupyter) (21.3)\n",
      "Requirement already satisfied: tornado>=6.1 in /opt/conda/lib/python3.8/site-packages (from ipykernel->jupyter) (6.1)\n",
      "Requirement already satisfied: nest-asyncio in /opt/conda/lib/python3.8/site-packages (from ipykernel->jupyter) (1.5.4)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /opt/conda/lib/python3.8/site-packages (from jupyter-client>=6.1.12->ipykernel->jupyter) (2.8.2)\n",
      "Requirement already satisfied: entrypoints in /opt/conda/lib/python3.8/site-packages (from jupyter-client>=6.1.12->ipykernel->jupyter) (0.3)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /opt/conda/lib/python3.8/site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel->jupyter) (4.2.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.8/site-packages (from python-dateutil>=2.1->jupyter-client>=6.1.12->ipykernel->jupyter) (1.16.0)\n",
      "Requirement already satisfied: mistune<2,>=0.8.1 in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (0.8.4)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (1.5.0)\n",
      "Requirement already satisfied: testpath in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (0.5.0)\n",
      "Requirement already satisfied: jinja2>=2.4 in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (3.0.3)\n",
      "Requirement already satisfied: nbformat>=4.4 in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (5.1.3)\n",
      "Requirement already satisfied: bleach in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (4.1.0)\n",
      "Requirement already satisfied: nbclient<0.6.0,>=0.5.0 in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (0.5.9)\n",
      "Requirement already satisfied: jupyterlab-pygments in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (0.1.2)\n",
      "Requirement already satisfied: defusedxml in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (0.7.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.8/site-packages (from jinja2>=2.4->nbconvert->jupyter) (2.0.1)\n",
      "Requirement already satisfied: ipython-genutils in /opt/conda/lib/python3.8/site-packages (from nbformat>=4.4->nbconvert->jupyter) (0.2.0)\n",
      "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /opt/conda/lib/python3.8/site-packages (from nbformat>=4.4->nbconvert->jupyter) (4.2.1)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /opt/conda/lib/python3.8/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.4->nbconvert->jupyter) (21.2.0)\n",
      "Requirement already satisfied: importlib-resources>=1.4.0 in /opt/conda/lib/python3.8/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.4->nbconvert->jupyter) (5.4.0)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /opt/conda/lib/python3.8/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.4->nbconvert->jupyter) (0.18.0)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /opt/conda/lib/python3.8/site-packages (from importlib-resources>=1.4.0->jsonschema!=2.5.0,>=2.4->nbformat>=4.4->nbconvert->jupyter) (3.6.0)\n",
      "Requirement already satisfied: webencodings in /opt/conda/lib/python3.8/site-packages (from bleach->nbconvert->jupyter) (0.5.1)\n",
      "Requirement already satisfied: terminado>=0.8.3 in /opt/conda/lib/python3.8/site-packages (from notebook->jupyter) (0.12.1)\n",
      "Requirement already satisfied: prometheus-client in /opt/conda/lib/python3.8/site-packages (from notebook->jupyter) (0.12.0)\n",
      "Requirement already satisfied: argon2-cffi in /opt/conda/lib/python3.8/site-packages (from notebook->jupyter) (21.1.0)\n",
      "Requirement already satisfied: Send2Trash>=1.5.0 in /opt/conda/lib/python3.8/site-packages (from notebook->jupyter) (1.8.0)\n",
      "Requirement already satisfied: cffi>=1.0.0 in /opt/conda/lib/python3.8/site-packages (from argon2-cffi->notebook->jupyter) (1.15.0)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.8/site-packages (from cffi>=1.0.0->argon2-cffi->notebook->jupyter) (2.21)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging->ipykernel->jupyter) (3.0.6)\n",
      "Requirement already satisfied: qtpy>=2.4.0 in /opt/conda/lib/python3.8/site-packages (from qtconsole->jupyter) (2.4.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#installing some libraries\n",
    "!pip install datasets\n",
    "!pip install --upgrade jupyter ipywidgets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f485606",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch, transformers\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import PreTrainedModel, PretrainedConfig\n",
    "from transformers import AutoModel, AutoConfig,AutoModelForCausalLM, AutoConfig,GPT2Config\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "#from datasets import load_dataset\n",
    "import pandas as pd, numpy as np\n",
    "from torch.cuda.amp import autocast\n",
    "from torch import cuda\n",
    "import datetime\n",
    "import warnings,itertools\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "import json\n",
    "# Ignore all warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "#pip install transformers bitsandbytes>=0.39.0 -q\n",
    "import zipfile,logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "392d185e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, GPT2Config\n",
    "from nltk import pos_tag, word_tokenize\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "import random\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e971867",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "model\n"
     ]
    }
   ],
   "source": [
    "#global params for training\n",
    "\n",
    "B,T = 64,1024\n",
    "epoch = 100\n",
    "random_init_wts = True\n",
    "min_text_len = 0\n",
    "# train_loss_list = []\n",
    "# val_loss_list =[]\n",
    "if cuda.is_available():\n",
    "    device = torch.device('cuda:0')\n",
    "    print(device)\n",
    "else:\n",
    "    device = 'cpu'\n",
    "#print(device)\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "#os.environ[\"MKL_DEBUG_CPU_TYPE\"] = \"5\"\n",
    "\n",
    "#print(global_tr_loss)\n",
    "model_path = os.path.join(\"model\")\n",
    "print(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43cc6df3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./data/unzip_text_10M'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "directory = os.path.join('.','data','unzip_text_10M')  # Replace with your directory path\n",
    "directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35fc23dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_text(directory):\n",
    "    directory = os.path.join('.','data','unzip_text_10M',str(directory))  # Replace with your directory path\n",
    "    print(f\"directory :{directory}\")\n",
    "    # List all files in the directory\n",
    "    files = [f for f in os.listdir(directory) if os.path.isfile(os.path.join(directory, f))]\n",
    "    print(f\"files:{files}\")\n",
    "    text_content = []\n",
    "    # Read each file\n",
    "    total_lines = 0\n",
    "    for filenum,filename in enumerate(files):\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            #first_line = file.read()\n",
    "            #print(f\"filename :{filename}->first few lines {first_line}\")\n",
    "            #continue\n",
    "            #lines_list = [line.strip() for line in open(file_path, 'r')]\n",
    "            text = file.read()\n",
    "            text_content.append(text)\n",
    "            print(f\"the file:{filename} has been appeneded to the uber list and its length is {len(text_content)} \")\n",
    "            #total_lines+=len(lines_list)\n",
    "            #text_content.append(lines_list)\n",
    "    \n",
    "    flattened_list = ''.join(text_content)\n",
    "    assert (len(flattened_list) == total_lines , f\"Expected {len(flattened_list)} to be equal to {total_lines}\" )\n",
    "    \n",
    "    return flattened_list\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a7ceba78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "directory :./data/unzip_text_10M/train_10M\n",
      "files:['switchboard.train', 'simple_wiki.train', 'open_subtitles.train', 'gutenberg.train', 'childes.train', 'bnc_spoken.train']\n",
      "the file:switchboard.train has been appeneded to the uber list and its length is 1 \n",
      "the file:simple_wiki.train has been appeneded to the uber list and its length is 2 \n",
      "the file:open_subtitles.train has been appeneded to the uber list and its length is 3 \n",
      "the file:gutenberg.train has been appeneded to the uber list and its length is 4 \n",
      "the file:childes.train has been appeneded to the uber list and its length is 5 \n",
      "the file:bnc_spoken.train has been appeneded to the uber list and its length is 6 \n"
     ]
    }
   ],
   "source": [
    "train_list = read_text(\"train_10M\")\n",
    "#print(train_dict)\n",
    "#val_list = read_text(\"dev\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f31d9aaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54215049"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3b548d0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "827\n"
     ]
    }
   ],
   "source": [
    "chunks = len(train_list)//(B*T)\n",
    "print(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6a7c79a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\",return_tensors = \"pt\" , truncate = True, return_overflowing_tokens=True , padding = False,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5eef6869",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "directory :./data/unzip_text_10M/dev\n",
      "files:['switchboard.dev', 'simple_wiki.dev', 'open_subtitles.dev', 'gutenberg.dev', 'childes.dev', 'bnc_spoken.dev']\n",
      "the file:switchboard.dev has been appeneded to the uber list and its length is 1 \n",
      "the file:simple_wiki.dev has been appeneded to the uber list and its length is 2 \n",
      "the file:open_subtitles.dev has been appeneded to the uber list and its length is 3 \n",
      "the file:gutenberg.dev has been appeneded to the uber list and its length is 4 \n",
      "the file:childes.dev has been appeneded to the uber list and its length is 5 \n",
      "the file:bnc_spoken.dev has been appeneded to the uber list and its length is 6 \n"
     ]
    }
   ],
   "source": [
    "val_list = read_text(\"dev\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1639cd88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17191971 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "enc_train = tokenizer(train_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cdc7a89e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.1535097982657136"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comp_ratio = len(train_list)/len(enc_train['input_ids'])\n",
    "comp_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d1b88c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_val = tokenizer(val_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f18e032a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df_from_list(enc , B= B, T = T, val= False):\n",
    "    chunk_size = B*T\n",
    "    if val:\n",
    "        chunk_size = 2*B*T\n",
    "        \n",
    "    long_list_inp = enc['input_ids']\n",
    "    long_list_attention = enc['attention_mask']\n",
    "\n",
    "    # Step 3: Split the list into chunks and pad the last chunk if necessary\n",
    "    chunks_inp = [long_list_inp[i:i + chunk_size] for i in range(0, len(long_list_inp), chunk_size)]\n",
    "    chunks_att = [long_list_attention[i:i + chunk_size] for i in range(0, len(long_list_attention), chunk_size)]\n",
    "    df = pd.DataFrame({'input_ids': chunks_inp,'attention_mask':chunks_att})\n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9a583ae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the dataframe is = 263\n"
     ]
    }
   ],
   "source": [
    "df_train_temp = get_df_from_list(enc_train)\n",
    "# Display the DataFrame\n",
    "df_train_temp.head()\n",
    "print(f\"Length of the dataframe is = {len(df_train_temp)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f6828afe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the VALDATION dataframe is = 266\n"
     ]
    }
   ],
   "source": [
    "df_val_temp = get_df_from_list(enc_val)\n",
    "# Display the DataFrame\n",
    "df_val_temp.head()\n",
    "print(f\"Length of the VALDATION dataframe is = {len(df_val_temp)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cf1bc946",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_df(df,eos_char = tokenizer.eos_token_id,B = B, T = T,val = False):\n",
    "    if val:\n",
    "        B = 2*B\n",
    "    for ind,row in df.iterrows():\n",
    "        if len(row['input_ids']) != B*T :\n",
    "            print(f\"row = {ind} and input_id length = {len(row['input_ids'])}\")\n",
    "            print(f\"row = {ind} and attention length = {len(row['attention_mask'])}\")\n",
    "            pad_len = B*T - len(row['input_ids'])\n",
    "            print(f\"padding the row index {ind} with {pad_len} character\")\n",
    "            row['input_ids'] = row['input_ids']+ [eos_char] * pad_len\n",
    "            #attention mask should be padded to 0\n",
    "            row['attention_mask'] = row['attention_mask']+ [0] * pad_len\n",
    "            print(\"#### POST CONCAT####\")\n",
    "            print(f\"row = {ind} and input_id length = {len(row['input_ids'])}\")\n",
    "            print(f\"row = {ind} and attention length ={len(row['attention_mask'])}\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def verify_len(df,val = False, B= B):\n",
    "    row_ind = []\n",
    "    if val:\n",
    "        B = 2*B\n",
    "    for ind,row in df.iterrows():\n",
    "        if len(row['input_ids']) != B*T :\n",
    "            row_ind.append(ind)\n",
    "        else:\n",
    "            continue\n",
    "    if len(row_ind) !=0:\n",
    "        print(\"CONCATENATION Did not work\")\n",
    "    else:\n",
    "        print(\"CONCATENATION worked\")\n",
    "        \n",
    "            \n",
    "        \n",
    "    \n",
    "        \n",
    "        \n",
    "   \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b7f61b5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row = 262 and input_id length = 21539\n",
      "row = 262 and attention length = 21539\n",
      "padding the row index 262 with 43997 character\n",
      "#### POST CONCAT####\n",
      "row = 262 and input_id length = 65536\n",
      "row = 262 and attention length =65536\n",
      "CONCATENATION worked\n"
     ]
    }
   ],
   "source": [
    "df_train  = pad_df(df_train_temp)\n",
    "verify_len(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5bc64ea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row = 265 and input_id length = 46356\n",
      "row = 265 and attention length = 46356\n",
      "padding the row index 265 with 19180 character\n",
      "#### POST CONCAT####\n",
      "row = 265 and input_id length = 65536\n",
      "row = 265 and attention length =65536\n",
      "CONCATENATION worked\n"
     ]
    }
   ],
   "source": [
    "df_val  = pad_df(df_val_temp)\n",
    "verify_len(df_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dca17809",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_ids</th>\n",
       "      <th>attention_mask</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[32, 25, 197, 40, 1101, 1654, 484, 389, 13, 19...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[314, 1612, 11, 340, 338, 1611, 286, 43244, 11...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[198, 32, 25, 197, 1870, 11, 21480, 11, 314, 4...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[25, 197, 2396, 314, 466, 21099, 326, 11, 475,...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[329, 6835, 10807, 13, 198, 32610, 1810, 13, 1...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           input_ids  \\\n",
       "0  [32, 25, 197, 40, 1101, 1654, 484, 389, 13, 19...   \n",
       "1  [314, 1612, 11, 340, 338, 1611, 286, 43244, 11...   \n",
       "2  [198, 32, 25, 197, 1870, 11, 21480, 11, 314, 4...   \n",
       "3  [25, 197, 2396, 314, 466, 21099, 326, 11, 475,...   \n",
       "4  [329, 6835, 10807, 13, 198, 32610, 1810, 13, 1...   \n",
       "\n",
       "                                      attention_mask  \n",
       "0  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "1  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "2  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "3  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "4  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "505d1edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_file(log_message, model_name = \"GPT2\" ,random_init_wts = random_init_wts ):\n",
    "    current_datetime = datetime.datetime.now()\n",
    "    # Extract date and time components\n",
    "    current_date = str(current_datetime.date())\n",
    "    log_file = model_name +'_COS_SIM_'+'random_init_wts'+ '_'+str(random_init_wts)+'_' +current_date+'.log'\n",
    "    print(f\"*****LOGGING INFO IN {log_file}*********\")\n",
    "    filepath = os.path.join(\"model\",log_file)\n",
    "    logging.basicConfig(filename=filepath, \n",
    "                    filemode='a',  # Overwrite the log file each time\n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s', \n",
    "                    level=logging.DEBUG)\n",
    "    logger = logging.getLogger()\n",
    "    logger.info(log_message)\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "31baab7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_token_pos_mapping(tokenizer):\n",
    "    token_pos_map = {}\n",
    "    for word, token_id in tokenizer.get_vocab().items():\n",
    "        if word.startswith('Ġ'):  # GPT-2 uses 'Ġ' to denote word beginnings\n",
    "            word = word[1:]\n",
    "            #print(f\"word = {word}|token_id = {token_id}\")\n",
    "        pos = pos_tag([word])[0][1]\n",
    "        #print(f\"pos = {pos}\")\n",
    "        token_pos_map[token_id] = pos\n",
    "    return token_pos_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8e15b110",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "token_pos_map = create_token_pos_mapping(tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "04a1e789",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_verb_synonyms(word):\n",
    "    synsets = wordnet.synsets(word, pos=wordnet.VERB)\n",
    "    synonyms = list(set([lemma.name() for synset in synsets for lemma in synset.lemmas() if lemma.name() != word]))\n",
    "    return synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6e845477",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CustomGPT2Model(AutoModelForCausalLM):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        \n",
    "    def forward(self, input_ids=None, past_key_values=None, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None, inputs_embeds=None, labels=None, use_cache=None, output_attentions=None, output_hidden_states=None, return_dict=None):\n",
    "        if inputs_embeds is None and input_ids is None:\n",
    "            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n",
    "        \n",
    "        return super().forward(\n",
    "            input_ids=input_ids,\n",
    "            past_key_values=past_key_values,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            labels=labels,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "config = GPT2Config.from_pretrained('distilgpt2')\n",
    "model = CustomGPT2Model.from_pretrained('distilgpt2', config=config)\n",
    "tune_model = torch.compile(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8f6b8309",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(50257, 768)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tune_model.transformer.wte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "50685343",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_input_with_synonym_embeddings(input_ids, model, tokenizer, token_pos_map, replacement_prob=0.5):\n",
    "    model.to('cuda')\n",
    "    input_ids = input_ids.to('cuda')  # Move input_ids to GPU once\n",
    "\n",
    "    with torch.no_grad():\n",
    "        original_embeddings = model.transformer.wte(input_ids).clone()\n",
    "\n",
    "    embed_list = []\n",
    "\n",
    "    for i, token_id in enumerate(input_ids):\n",
    "        token_id = token_id.item()  # Convert tensor to integer\n",
    "        if token_pos_map.get(token_id, '').startswith('VB') and random.random() < replacement_prob:\n",
    "            word = tokenizer.decode([token_id])\n",
    "            synonyms = get_verb_synonyms(word)\n",
    "\n",
    "            if synonyms:\n",
    "                synonym = random.choice(synonyms)\n",
    "                synonym_tokens = tokenizer.encode(synonym, add_special_tokens=False)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    synonym_embeddings = model.transformer.wte(torch.tensor(synonym_tokens).to('cuda'))\n",
    "\n",
    "                mean_synonym_embedding = synonym_embeddings.mean(dim=0)\n",
    "                embed_list.append(mean_synonym_embedding)\n",
    "\n",
    "            else:\n",
    "                embed_list.append(model.transformer.wte(torch.tensor([token_id]).to('cuda')).squeeze(0))\n",
    "        else:\n",
    "            embed_list.append(model.transformer.wte(torch.tensor([token_id]).to('cuda')).squeeze(0))\n",
    "\n",
    "    return torch.stack(embed_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ca76b7",
   "metadata": {},
   "source": [
    "### Data loaders and Dataset for batched training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "80e3be9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataset_pyt_train(Dataset):\n",
    "    def __init__(self, df, B = B, T = T, model = model, tokenizer = tokenizer, token_pos_map = create_token_pos_mapping(tokenizer)):\n",
    "        self.df = df\n",
    "        print(f\"Value of B {B}\")\n",
    "                                        \n",
    "    def __getitem__(self, idx):\n",
    "        #print(f\"inside loader...idx ->{idx}\")\n",
    "        input_id_temp = torch.tensor(self.df.iloc[idx]['input_ids'],dtype = torch.long).to(device)\n",
    "        att_mask = torch.tensor(self.df.iloc[idx]['attention_mask'],dtype = torch.long).to(device)\n",
    "        #if the input index is even get the replacement embedding\n",
    "        \n",
    "        if idx %2 == 0:\n",
    "            inp_emb= prepare_input_with_synonym_embeddings(input_id_temp, model = model, tokenizer = tokenizer, token_pos_map = create_token_pos_mapping(tokenizer), replacement_prob=0.1)\n",
    "        else:\n",
    "            inp_emb = model.transformer.wte(input_id_temp)\n",
    "            \n",
    "                \n",
    "        inp_emb =   inp_emb.view(-1,T,768)    \n",
    "        attention_mask = att_mask.view(B,T)   \n",
    "        labels = input_id_temp.view(B,T)\n",
    "        return inp_emb, attention_mask,labels\n",
    "        \n",
    "    def __len__(self):\n",
    "        #return the length of the dataframe\n",
    "        return len(self.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ecfbbbd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataset_pyt_val(Dataset):\n",
    "    def __init__(self, df, B = B, T = T, model = model ):\n",
    "        self.df = df\n",
    "                                        \n",
    "    def __getitem__(self, idx):\n",
    "        #print(f\"inside loader...idx ->{idx}\")\n",
    "        input_id_temp = torch.tensor(self.df.iloc[idx]['input_ids'],dtype = torch.long).to(device)\n",
    "        att_mask = torch.tensor(self.df.iloc[idx]['attention_mask'],dtype = torch.long).to(device)\n",
    "               \n",
    "        model.to(device)\n",
    "        inp_emb = model.transformer.wte(input_id_temp)\n",
    "        inp_emb =   inp_emb.view(-1,T,768)    \n",
    "        attention_mask = att_mask.view(B,T)   \n",
    "        labels = input_id_temp.view(B,T)\n",
    "        return inp_emb, attention_mask,labels\n",
    "        \n",
    "    def __len__(self):\n",
    "        #return the length of the dataframe\n",
    "        return len(self.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a11eabe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value of B 64\n"
     ]
    }
   ],
   "source": [
    "#train_dataset = dataset_pyt(filtered_df,tokenizer = tokenizer)\n",
    "train_dataset = dataset_pyt_train(df_train)\n",
    "val_dataset = dataset_pyt_val(df_val)\n",
    "#test_dataset = dataset_pyt(df_test,tokenizer = tokenizer)\n",
    "\n",
    "train_loader = DataLoader(train_dataset,batch_size = 1, shuffle = True , num_workers = 0, pin_memory = False)\n",
    "val_loader = DataLoader(val_dataset,batch_size = 1, shuffle = True , num_workers = 0, pin_memory = False)\n",
    "#test_loader = DataLoader(test_dataset,batch_size = batch_size, shuffle = False, collate_fn = custom_collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ac2272ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the train loader is 263\n",
      "Length of the val loader is 266\n",
      "num_tokens= 17235968\n"
     ]
    }
   ],
   "source": [
    "print(f\"Length of the train loader is {len(train_loader)}\")\n",
    "print(f\"Length of the val loader is {len(val_loader)}\")\n",
    "print(f\"num_tokens= {B*T*len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f614a70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#emb,att,inp = train_dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730c5724",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e566d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b0eb788b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class check_train_metrics:\n",
    "    def __init__(self, patience=10, min_delta=0 , B = T, T = T,best_loss = torch.inf,early_stop = False):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = best_loss\n",
    "        self.early_stop = early_stop\n",
    "        self.B = B\n",
    "        self.T = T\n",
    "        self.improvement = None\n",
    "\n",
    "    def __call__(self, loss, epoch , epoch_durn, norm , current_lr, num_token):\n",
    "        if self.best_loss - loss > self.min_delta:\n",
    "            \n",
    "            print(f\"training loss has decreased---> reducing the best loss from {self.best_loss:.2f} to {loss:.2f} | throughput = {int(num_token/epoch_durn)} tokens/second | norm = {norm:.4f} | learning rate = {current_lr:.5e}\")\n",
    "            self.best_loss = loss\n",
    "            self.counter = 0\n",
    "            self.improvement = True\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            self.improvement = False\n",
    "            print(f\"No improvement in training  loss-->epoch= {epoch} and best loss is {self.best_loss:.2f}|current_loss = {loss}|counter = {self.counter}\")\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9adc5782",
   "metadata": {},
   "outputs": [],
   "source": [
    "class check_val_metrics:\n",
    "    def __init__(self, patience=5, min_delta=0, best_loss = torch.inf,early_stop = False):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = best_loss\n",
    "        self.early_stop = early_stop\n",
    "        \n",
    "\n",
    "    def __call__(self, loss, epoch , model, tokenizer):\n",
    "        if self.best_loss - loss > self.min_delta:\n",
    "            print(f\"Val loss has decreased -->reducing the global validation loss from {self.best_loss:.2f} to {loss:.2f}\")\n",
    "            s1 = (f\"Val loss has decreased -->reducing the global validation loss from {self.best_loss:.2f} to {loss:.2f}\")\n",
    "            print(f\" validation loss for epoch = {epoch} is {loss:.4f}\")\n",
    "            self.best_loss = loss\n",
    "            s2 = f\" validation loss for epoch = {epoch} is {loss:.4f}\"\n",
    "            print(f\" epoch= {epoch} :  val loss is {loss:.4f} \")\n",
    "            s3 = f\" epoch= {epoch} :  val loss is {loss:.4f} \"\n",
    "            #save the model\n",
    "            # Get the current date and time\n",
    "            current_datetime = datetime.datetime.now()\n",
    "            # Extract date and time components\n",
    "            current_date = str(current_datetime.date())\n",
    "            current_time = str(current_datetime.time()).split('.')[0]\n",
    "            file_name = 'model'+ current_date+current_time+'.pth'\n",
    "            path = os.path.join(\"model\",file_name)\n",
    "            print(f\"saving the model {file_name}\")\n",
    "            s4 = f\"saving the model {file_name}\"\n",
    "            #torch.save(model.state_dict(), path)\n",
    "            model.save_pretrained(path)\n",
    "            tokenizer.save_pretrained(path)\n",
    "            log_message = s1+s2+s3+s4\n",
    "            write_file(log_message)\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            print(f\"No improvement in validation loss-->epoch= {epoch} and best val loss is {self.best_loss:.2f}|current_Val loss = {loss}|counter = {self.counter}\")\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "63b34c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_output = model(input_ids = inp ,attention_mask = att, labels = inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ed2da456",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad\n",
    "\n",
    "def eval_model(val_loader, model, epoch , device = device,tokenizer = tokenizer):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    e = epoch+1\n",
    "    embedding_layer = model.transformer.wte\n",
    "    val_loss_accum = 0.0\n",
    "    print(f\"inside validation data for epoch {e}\")\n",
    "    for ind,(input_embed,attention_mask,labels) in enumerate(val_loader):\n",
    "        emb = torch.squeeze(input_embed,dim = 0)\n",
    "        emb = emb.to(device=device, non_blocking=True)\n",
    "        att_mask = torch.squeeze(attention_mask,dim = 0)\n",
    "        att_mask = att_mask.to(device=device, non_blocking=True)\n",
    "        labels = torch.squeeze(labels,dim = 0)\n",
    "        labels = labels.to(device)\n",
    "        with autocast(dtype = torch.bfloat16):\n",
    "            model_output = model(inputs_embeds = emb, attention_mask = att_mask)\n",
    "            logits = model_output.logits\n",
    "            predictions = torch.argmax(logits, dim=-1)\n",
    "            prediction_embeddings = model.transformer.wte(predictions)\n",
    "            cos_sim = F.cosine_similarity(torch.squeeze(prediction_embeddings,dim = 0), torch.squeeze(emb,dim = 0), dim=1)\n",
    "            cos_loss = 1- cos_sim.mean()\n",
    "            \n",
    "    \n",
    "    \n",
    "    \n",
    "        val_loss_accum+= cos_loss.detach().item()\n",
    "        del emb,att_mask,labels,model_output\n",
    "    return val_loss_accum        \n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "82c1acb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_model(train_loader,val_loader,model,num_epoch = 30,device = device,tokenizer = tokenizer):\n",
    "    #model.train()\n",
    "    device = device\n",
    "    lr_custom = 2e-5\n",
    "    print(f\"inside train model. Device = {device}\")\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.AdamW(params =  model.parameters(), lr= lr_custom,fused = True ,weight_decay = .1)\n",
    "      \n",
    "    extra_train = .1*num_epoch\n",
    "    max_train_steps = int(num_epoch +extra_train )\n",
    "    import time\n",
    "    from transformers import get_linear_schedule_with_warmup\n",
    "    total_steps = len(train_loader) * num_epoch\n",
    "    scheduler_cos = transformers.get_cosine_schedule_with_warmup( optimizer= optimizer, num_warmup_steps =int(total_steps * 0.1) ,num_training_steps= total_steps )\n",
    "        \n",
    "    epoch_train_log = []\n",
    "    epoch_val_log = []\n",
    "    validate_val_metric = check_val_metrics()\n",
    "    validate_train_metric = check_train_metrics(patience=5, min_delta=0 , B = T, T = T)\n",
    "    for i in range (max_train_steps):\n",
    "        \n",
    "        epoch_start_time = time.time()\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        # we use 2 schedulers - the first LR scheduler uses a cosine decay for 100 epochs the second scheduler takes the last LR from cosine scheduler and then maintains that LR for the next 10 epochs\n",
    "        if i >= num_epoch:\n",
    "            optimizer_reduced_lr = torch.optim.AdamW(params =  model.parameters(), lr= current_lr ,fused = True , weight_decay=.1)\n",
    "            scheduler_constant = transformers.get_constant_schedule_with_warmup( optimizer = optimizer_reduced_lr ,num_warmup_steps = 0, last_epoch = -1 )\n",
    "        \n",
    "        epoch_train_loss = 0       \n",
    "        for ind,(input_embed,attention_mask,labels) in enumerate(train_loader):\n",
    "            if ind == int(len(train_loader)/2):\n",
    "                batch_time = time.time()\n",
    "                duration = batch_time - epoch_start_time\n",
    "                print(f\"executing epoch:{i+1}, it took {duration/60} mins from beginning of epoch till batch#{ind}\")\n",
    "            \n",
    "            emb = torch.squeeze(input_embed,dim = 0)\n",
    "            emb = emb.to(device=device, non_blocking=True)\n",
    "            att_mask = torch.squeeze(attention_mask,dim = 0)\n",
    "            att_mask = attention_mask.to(device=device, non_blocking=True)\n",
    "            labels = torch.squeeze(labels,dim = 0)\n",
    "            labels = labels.to(device)\n",
    "                        \n",
    "            with autocast(dtype = torch.bfloat16):\n",
    "                model_output = model(inputs_embeds = emb, attention_mask = att_mask)\n",
    "                logits = model_output.logits\n",
    "                predictions = torch.argmax(logits, dim=-1)\n",
    "                prediction_embeddings = model.transformer.wte(predictions)\n",
    "                cos_sim = F.cosine_similarity(torch.squeeze(prediction_embeddings,dim = 0), torch.squeeze(emb,dim = 0), dim=1)\n",
    "                cos_loss = 1- cos_sim.mean()\n",
    "                               \n",
    "#                 if np.isnan(model_output.loss.item()):\n",
    "#                     print(\"f nan values encountered..\")\n",
    "#                     decoded_texts = [tokenizer.decode(ids, skip_special_tokens=True) for ids in ids]\n",
    "#                     print(f\"*********$$$$$$$$$ decoded_texts = {decoded_texts}*************\")\n",
    "                total_loss =cos_loss \n",
    "                                \n",
    "            #assert not np.isnan(model_output.loss.item()), \"NaN value found\"\n",
    "                               \n",
    "            total_loss.backward()\n",
    "            epoch_train_loss += total_loss.detach().item()\n",
    "            norm = torch.nn.utils.clip_grad_norm(model.parameters() , 1.0)\n",
    "            if i <= num_epoch:\n",
    "                optimizer.step()\n",
    "                scheduler_cos.step()\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "            else:\n",
    "                optimizer_reduced_lr.step()\n",
    "                optimizer_reduced_lr.zero_grad(set_to_none=True)\n",
    "                scheduler_constant.step()\n",
    "                \n",
    "                         \n",
    "            del emb,att_mask,labels,model_output,logits,predictions,prediction_embeddings\n",
    "            \n",
    "        #batch processing complete \n",
    "        #print(f\"batch processing complete , lambda = {lambda_val} |total_loss for batch= {total_loss}\")\n",
    "        \n",
    "        if i <= num_epoch:\n",
    "            current_lr = scheduler_cos.get_last_lr()[0]\n",
    "        epoch_end_time = time.time()\n",
    "        epoch_durn = (epoch_end_time - epoch_start_time)\n",
    "        num_token = B*T*len(train_loader)\n",
    "        epoch_train_log.append(epoch_train_loss)\n",
    "        validate_train_metric(epoch_train_loss, i , epoch_durn, norm , current_lr, num_token)\n",
    "        \n",
    "        if validate_train_metric.improvement:\n",
    "            val_loss= eval_model(val_loader, model, epoch = i, device = device,tokenizer = tokenizer)\n",
    "            epoch_val_log.append(val_loss)\n",
    "            validate_val_metric(val_loss, i , model, tokenizer)\n",
    "            if validate_train_metric.early_stop or validate_val_metric.early_stop :\n",
    "                print(f\"early stopping trigerred either from training data or val data | train_counter = {validate_train_metric.counter}|val_counter = {validate_val_metric.counter}\")\n",
    "                break\n",
    "        else:\n",
    "            if validate_val_metric.early_stop:\n",
    "                print(f\"early stopping trigerred from validation data\")\n",
    "                break\n",
    "              \n",
    "    \n",
    "    return model,epoch_train_log,epoch_val_log\n",
    "        \n",
    "            \n",
    "            \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad20d4bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inside train model. Device = cuda:0\n",
      "executing epoch:1, it took 33.36480184396108 mins from beginning of epoch till batch#131\n",
      "training loss has decreased---> reducing the best loss from inf to 229.60 | throughput = 3763 tokens/second | norm = 0.0348 | learning rate = 6.66667e-06\n",
      "inside validation data for epoch 1\n",
      "Val loss has decreased -->reducing the global validation loss from inf to 231.71\n",
      " validation loss for epoch = 0 is 231.7143\n",
      " epoch= 0 :  val loss is 231.7143 \n",
      "saving the model model2024-07-1820:34:08.pth\n",
      "*****LOGGING INFO IN GPT2_COS_SIM_random_init_wts_True_2024-07-18.log*********\n",
      "executing epoch:2, it took 39.32638173103332 mins from beginning of epoch till batch#131\n",
      "training loss has decreased---> reducing the best loss from 229.60 to 228.04 | throughput = 3779 tokens/second | norm = 0.0771 | learning rate = 1.33333e-05\n",
      "inside validation data for epoch 2\n",
      "Val loss has decreased -->reducing the global validation loss from 231.71 to 229.36\n",
      " validation loss for epoch = 1 is 229.3571\n",
      " epoch= 1 :  val loss is 229.3571 \n",
      "saving the model model2024-07-1821:50:56.pth\n",
      "*****LOGGING INFO IN GPT2_COS_SIM_random_init_wts_True_2024-07-18.log*********\n",
      "executing epoch:3, it took 35.851384397347765 mins from beginning of epoch till batch#131\n",
      "training loss has decreased---> reducing the best loss from 228.04 to 224.86 | throughput = 3783 tokens/second | norm = 0.0455 | learning rate = 2.00000e-05\n",
      "inside validation data for epoch 3\n",
      "Val loss has decreased -->reducing the global validation loss from 229.36 to 225.54\n",
      " validation loss for epoch = 2 is 225.5381\n",
      " epoch= 2 :  val loss is 225.5381 \n",
      "saving the model model2024-07-1823:07:39.pth\n",
      "*****LOGGING INFO IN GPT2_COS_SIM_random_init_wts_True_2024-07-18.log*********\n",
      "executing epoch:4, it took 37.01156228383382 mins from beginning of epoch till batch#131\n",
      "training loss has decreased---> reducing the best loss from 224.86 to 220.51 | throughput = 3779 tokens/second | norm = 0.0642 | learning rate = 1.99324e-05\n",
      "inside validation data for epoch 4\n",
      "Val loss has decreased -->reducing the global validation loss from 225.54 to 221.09\n",
      " validation loss for epoch = 3 is 221.0927\n",
      " epoch= 3 :  val loss is 221.0927 \n",
      "saving the model model2024-07-1900:24:27.pth\n",
      "*****LOGGING INFO IN GPT2_COS_SIM_random_init_wts_True_2024-07-19.log*********\n",
      "executing epoch:5, it took 38.52421621481577 mins from beginning of epoch till batch#131\n",
      "training loss has decreased---> reducing the best loss from 220.51 to 216.05 | throughput = 3782 tokens/second | norm = 0.0261 | learning rate = 1.97304e-05\n",
      "inside validation data for epoch 5\n",
      "Val loss has decreased -->reducing the global validation loss from 221.09 to 216.76\n",
      " validation loss for epoch = 4 is 216.7560\n",
      " epoch= 4 :  val loss is 216.7560 \n",
      "saving the model model2024-07-1901:41:11.pth\n",
      "*****LOGGING INFO IN GPT2_COS_SIM_random_init_wts_True_2024-07-19.log*********\n",
      "executing epoch:6, it took 37.400019721190134 mins from beginning of epoch till batch#131\n",
      "training loss has decreased---> reducing the best loss from 216.05 to 211.66 | throughput = 3782 tokens/second | norm = 0.0786 | learning rate = 1.93969e-05\n",
      "inside validation data for epoch 6\n",
      "Val loss has decreased -->reducing the global validation loss from 216.76 to 212.46\n",
      " validation loss for epoch = 5 is 212.4650\n",
      " epoch= 5 :  val loss is 212.4650 \n",
      "saving the model model2024-07-1902:57:55.pth\n",
      "*****LOGGING INFO IN GPT2_COS_SIM_random_init_wts_True_2024-07-19.log*********\n",
      "executing epoch:7, it took 41.54517516692479 mins from beginning of epoch till batch#131\n",
      "training loss has decreased---> reducing the best loss from 211.66 to 207.36 | throughput = 3782 tokens/second | norm = 0.0784 | learning rate = 1.89363e-05\n",
      "inside validation data for epoch 7\n",
      "Val loss has decreased -->reducing the global validation loss from 212.46 to 208.34\n",
      " validation loss for epoch = 6 is 208.3407\n",
      " epoch= 6 :  val loss is 208.3407 \n",
      "saving the model model2024-07-1904:14:39.pth\n",
      "*****LOGGING INFO IN GPT2_COS_SIM_random_init_wts_True_2024-07-19.log*********\n",
      "executing epoch:8, it took 38.17312734524409 mins from beginning of epoch till batch#131\n",
      "training loss has decreased---> reducing the best loss from 207.36 to 203.23 | throughput = 3779 tokens/second | norm = 0.0270 | learning rate = 1.83549e-05\n",
      "inside validation data for epoch 8\n",
      "Val loss has decreased -->reducing the global validation loss from 208.34 to 204.28\n",
      " validation loss for epoch = 7 is 204.2757\n",
      " epoch= 7 :  val loss is 204.2757 \n",
      "saving the model model2024-07-1905:31:26.pth\n",
      "*****LOGGING INFO IN GPT2_COS_SIM_random_init_wts_True_2024-07-19.log*********\n",
      "executing epoch:9, it took 36.96751948197683 mins from beginning of epoch till batch#131\n",
      "training loss has decreased---> reducing the best loss from 203.23 to 199.22 | throughput = 3786 tokens/second | norm = 0.0271 | learning rate = 1.76604e-05\n",
      "inside validation data for epoch 9\n",
      "Val loss has decreased -->reducing the global validation loss from 204.28 to 200.39\n",
      " validation loss for epoch = 8 is 200.3884\n",
      " epoch= 8 :  val loss is 200.3884 \n",
      "saving the model model2024-07-1906:48:05.pth\n",
      "*****LOGGING INFO IN GPT2_COS_SIM_random_init_wts_True_2024-07-19.log*********\n",
      "executing epoch:10, it took 35.78276038169861 mins from beginning of epoch till batch#131\n",
      "training loss has decreased---> reducing the best loss from 199.22 to 195.38 | throughput = 3790 tokens/second | norm = 0.0573 | learning rate = 1.68624e-05\n",
      "inside validation data for epoch 10\n",
      "Val loss has decreased -->reducing the global validation loss from 200.39 to 196.69\n",
      " validation loss for epoch = 9 is 196.6861\n",
      " epoch= 9 :  val loss is 196.6861 \n",
      "saving the model model2024-07-1908:04:39.pth\n",
      "*****LOGGING INFO IN GPT2_COS_SIM_random_init_wts_True_2024-07-19.log*********\n",
      "executing epoch:11, it took 40.6817041516304 mins from beginning of epoch till batch#131\n",
      "training loss has decreased---> reducing the best loss from 195.38 to 191.77 | throughput = 3793 tokens/second | norm = 0.0543 | learning rate = 1.59716e-05\n",
      "inside validation data for epoch 11\n",
      "Val loss has decreased -->reducing the global validation loss from 196.69 to 193.12\n",
      " validation loss for epoch = 10 is 193.1243\n",
      " epoch= 10 :  val loss is 193.1243 \n",
      "saving the model model2024-07-1909:21:10.pth\n",
      "*****LOGGING INFO IN GPT2_COS_SIM_random_init_wts_True_2024-07-19.log*********\n",
      "executing epoch:12, it took 36.15063066482544 mins from beginning of epoch till batch#131\n",
      "training loss has decreased---> reducing the best loss from 191.77 to 188.33 | throughput = 3793 tokens/second | norm = 0.0501 | learning rate = 1.50000e-05\n",
      "inside validation data for epoch 12\n",
      "Val loss has decreased -->reducing the global validation loss from 193.12 to 189.81\n",
      " validation loss for epoch = 11 is 189.8091\n",
      " epoch= 11 :  val loss is 189.8091 \n",
      "saving the model model2024-07-1910:37:41.pth\n",
      "*****LOGGING INFO IN GPT2_COS_SIM_random_init_wts_True_2024-07-19.log*********\n",
      "executing epoch:13, it took 40.15525751113891 mins from beginning of epoch till batch#131\n",
      "training loss has decreased---> reducing the best loss from 188.33 to 185.14 | throughput = 3792 tokens/second | norm = 0.0514 | learning rate = 1.39608e-05\n",
      "inside validation data for epoch 13\n",
      "Val loss has decreased -->reducing the global validation loss from 189.81 to 186.68\n",
      " validation loss for epoch = 12 is 186.6813\n",
      " epoch= 12 :  val loss is 186.6813 \n",
      "saving the model model2024-07-1911:54:13.pth\n",
      "*****LOGGING INFO IN GPT2_COS_SIM_random_init_wts_True_2024-07-19.log*********\n",
      "executing epoch:14, it took 45.38423909743627 mins from beginning of epoch till batch#131\n",
      "training loss has decreased---> reducing the best loss from 185.14 to 182.14 | throughput = 3426 tokens/second | norm = 0.0206 | learning rate = 1.28680e-05\n",
      "inside validation data for epoch 14\n",
      "Val loss has decreased -->reducing the global validation loss from 186.68 to 183.79\n",
      " validation loss for epoch = 13 is 183.7947\n",
      " epoch= 13 :  val loss is 183.7947 \n",
      "saving the model model2024-07-1913:18:51.pth\n",
      "*****LOGGING INFO IN GPT2_COS_SIM_random_init_wts_True_2024-07-19.log*********\n",
      "executing epoch:15, it took 39.135313447316484 mins from beginning of epoch till batch#131\n",
      "training loss has decreased---> reducing the best loss from 182.14 to 179.40 | throughput = 3424 tokens/second | norm = 0.0640 | learning rate = 1.17365e-05\n",
      "inside validation data for epoch 15\n",
      "Val loss has decreased -->reducing the global validation loss from 183.79 to 181.09\n",
      " validation loss for epoch = 14 is 181.0919\n",
      " epoch= 14 :  val loss is 181.0919 \n",
      "saving the model model2024-07-1914:43:31.pth\n",
      "*****LOGGING INFO IN GPT2_COS_SIM_random_init_wts_True_2024-07-19.log*********\n",
      "executing epoch:16, it took 38.83456325133641 mins from beginning of epoch till batch#131\n",
      "training loss has decreased---> reducing the best loss from 179.40 to 176.80 | throughput = 3771 tokens/second | norm = 0.0279 | learning rate = 1.05814e-05\n",
      "inside validation data for epoch 16\n",
      "Val loss has decreased -->reducing the global validation loss from 181.09 to 178.68\n",
      " validation loss for epoch = 15 is 178.6783\n",
      " epoch= 15 :  val loss is 178.6783 \n",
      "saving the model model2024-07-1916:00:28.pth\n",
      "*****LOGGING INFO IN GPT2_COS_SIM_random_init_wts_True_2024-07-19.log*********\n",
      "executing epoch:17, it took 39.97865556875865 mins from beginning of epoch till batch#131\n",
      "training loss has decreased---> reducing the best loss from 176.80 to 174.55 | throughput = 3773 tokens/second | norm = 0.0250 | learning rate = 9.41855e-06\n",
      "inside validation data for epoch 17\n",
      "Val loss has decreased -->reducing the global validation loss from 178.68 to 176.51\n",
      " validation loss for epoch = 16 is 176.5144\n",
      " epoch= 16 :  val loss is 176.5144 \n",
      "saving the model model2024-07-1917:17:22.pth\n",
      "*****LOGGING INFO IN GPT2_COS_SIM_random_init_wts_True_2024-07-19.log*********\n",
      "executing epoch:18, it took 36.900296874841054 mins from beginning of epoch till batch#131\n"
     ]
    }
   ],
   "source": [
    "tr_model,epoch_train_log,epoch_val_log = train_model(train_loader, val_loader,model=tune_model,tokenizer = tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e344ce7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json , os\n",
    "path_var_train_log = os.path.join(\".\",\"Tokenizers_and_loss_vals\",\"epoch_train_plain_loss.json\")\n",
    "path_var_val_log = os.path.join(\".\",\"Tokenizers_and_loss_vals\",\"epoch_val_val_loss_.json\")\n",
    "\n",
    "#print(path_var)\n",
    "#Write the list to a JSON file\n",
    "with open(path_var_train_log, \"w\") as file:\n",
    "    json.dump(epoch_train_log, file)\n",
    "\n",
    "with open(path_var_val_log, \"w\") as file:\n",
    "    json.dump(epoch_val_log, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee44c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path_var_train_log, \"r\") as file:\n",
    "    train_loss = json.load(file)\n",
    "with open(path_var_val_log, \"r\") as file:\n",
    "    val_loss = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb38fa16",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_values = range(len(train_loss))\n",
    "plt.plot(x_values, train_loss, label='Train_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212a85ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_values_val = range(len(val_loss))\n",
    "plt.plot(x_values_val, val_loss, label='val_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d93fb1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_length = min(len(train_loss), len(val_loss))\n",
    "list1 = train_loss[:min_length]\n",
    "list2 = val_loss[:min_length]\n",
    "\n",
    "# Create x-axis values\n",
    "x_values = range(min_length)\n",
    "\n",
    "# Plot the lists\n",
    "plt.plot(x_values, list1, label='Train_loss')\n",
    "plt.plot(x_values, list2, label='Val_loss')\n",
    "\n",
    "# Adding labels and title\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Train Loss and Val_Loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868380cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ad26cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0513f431",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de49339c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3c48f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c323b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
