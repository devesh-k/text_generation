{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62010205",
   "metadata": {},
   "outputs": [],
   "source": [
    "## reference https://huggingface.co/learn/nlp-course/en/chapter7/6?fw=pt#training-a-causal-language-model-from-scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da70242d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79ae0c9d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: jupyter in /opt/conda/lib/python3.8/site-packages (1.0.0)\n",
      "Requirement already satisfied: ipywidgets in /opt/conda/lib/python3.8/site-packages (8.1.3)\n",
      "Requirement already satisfied: qtconsole in /opt/conda/lib/python3.8/site-packages (from jupyter) (5.5.2)\n",
      "Requirement already satisfied: ipykernel in /opt/conda/lib/python3.8/site-packages (from jupyter) (6.29.5)\n",
      "Requirement already satisfied: notebook in /opt/conda/lib/python3.8/site-packages (from jupyter) (6.4.1)\n",
      "Requirement already satisfied: nbconvert in /opt/conda/lib/python3.8/site-packages (from jupyter) (6.3.0)\n",
      "Requirement already satisfied: jupyter-console in /opt/conda/lib/python3.8/site-packages (from jupyter) (6.6.3)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.11 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (3.0.11)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (7.30.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (5.14.3)\n",
      "Requirement already satisfied: comm>=0.1.3 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.11 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (4.0.11)\n",
      "Requirement already satisfied: setuptools>=18.5 in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (59.4.0)\n",
      "Requirement already satisfied: decorator in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (5.1.0)\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (4.8.0)\n",
      "Requirement already satisfied: matplotlib-inline in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.3)\n",
      "Requirement already satisfied: pygments in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (2.10.0)\n",
      "Requirement already satisfied: pickleshare in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.7.5)\n",
      "Requirement already satisfied: backcall in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.47)\n",
      "Requirement already satisfied: jedi>=0.16 in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.18.1)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /opt/conda/lib/python3.8/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.8/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.8/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=6.1.0->ipywidgets) (0.2.5)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.8/site-packages (from ipykernel->jupyter) (21.3)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /opt/conda/lib/python3.8/site-packages (from ipykernel->jupyter) (5.7.2)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in /opt/conda/lib/python3.8/site-packages (from ipykernel->jupyter) (7.1.0)\n",
      "Requirement already satisfied: nest-asyncio in /opt/conda/lib/python3.8/site-packages (from ipykernel->jupyter) (1.5.4)\n",
      "Requirement already satisfied: pyzmq>=24 in /opt/conda/lib/python3.8/site-packages (from ipykernel->jupyter) (26.0.3)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.8/site-packages (from ipykernel->jupyter) (5.8.0)\n",
      "Requirement already satisfied: tornado>=6.1 in /opt/conda/lib/python3.8/site-packages (from ipykernel->jupyter) (6.1)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in /opt/conda/lib/python3.8/site-packages (from ipykernel->jupyter) (1.8.2)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /opt/conda/lib/python3.8/site-packages (from jupyter-client>=6.1.12->ipykernel->jupyter) (2.8.2)\n",
      "Requirement already satisfied: entrypoints in /opt/conda/lib/python3.8/site-packages (from jupyter-client>=6.1.12->ipykernel->jupyter) (0.3)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /opt/conda/lib/python3.8/site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel->jupyter) (4.2.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.8/site-packages (from python-dateutil>=2.1->jupyter-client>=6.1.12->ipykernel->jupyter) (1.16.0)\n",
      "Requirement already satisfied: mistune<2,>=0.8.1 in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (0.8.4)\n",
      "Requirement already satisfied: nbformat>=4.4 in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (5.1.3)\n",
      "Requirement already satisfied: jinja2>=2.4 in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (3.0.3)\n",
      "Requirement already satisfied: bleach in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (4.1.0)\n",
      "Requirement already satisfied: testpath in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (0.5.0)\n",
      "Requirement already satisfied: nbclient<0.6.0,>=0.5.0 in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (0.5.9)\n",
      "Requirement already satisfied: defusedxml in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (0.7.1)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (1.5.0)\n",
      "Requirement already satisfied: jupyterlab-pygments in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (0.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.8/site-packages (from jinja2>=2.4->nbconvert->jupyter) (2.0.1)\n",
      "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /opt/conda/lib/python3.8/site-packages (from nbformat>=4.4->nbconvert->jupyter) (4.2.1)\n",
      "Requirement already satisfied: ipython-genutils in /opt/conda/lib/python3.8/site-packages (from nbformat>=4.4->nbconvert->jupyter) (0.2.0)\n",
      "Requirement already satisfied: importlib-resources>=1.4.0 in /opt/conda/lib/python3.8/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.4->nbconvert->jupyter) (5.4.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /opt/conda/lib/python3.8/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.4->nbconvert->jupyter) (21.2.0)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /opt/conda/lib/python3.8/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.4->nbconvert->jupyter) (0.18.0)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /opt/conda/lib/python3.8/site-packages (from importlib-resources>=1.4.0->jsonschema!=2.5.0,>=2.4->nbformat>=4.4->nbconvert->jupyter) (3.6.0)\n",
      "Requirement already satisfied: webencodings in /opt/conda/lib/python3.8/site-packages (from bleach->nbconvert->jupyter) (0.5.1)\n",
      "Requirement already satisfied: terminado>=0.8.3 in /opt/conda/lib/python3.8/site-packages (from notebook->jupyter) (0.12.1)\n",
      "Requirement already satisfied: prometheus-client in /opt/conda/lib/python3.8/site-packages (from notebook->jupyter) (0.12.0)\n",
      "Requirement already satisfied: argon2-cffi in /opt/conda/lib/python3.8/site-packages (from notebook->jupyter) (21.1.0)\n",
      "Requirement already satisfied: Send2Trash>=1.5.0 in /opt/conda/lib/python3.8/site-packages (from notebook->jupyter) (1.8.0)\n",
      "Requirement already satisfied: cffi>=1.0.0 in /opt/conda/lib/python3.8/site-packages (from argon2-cffi->notebook->jupyter) (1.15.0)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.8/site-packages (from cffi>=1.0.0->argon2-cffi->notebook->jupyter) (2.21)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging->ipykernel->jupyter) (3.0.6)\n",
      "Requirement already satisfied: qtpy>=2.4.0 in /opt/conda/lib/python3.8/site-packages (from qtconsole->jupyter) (2.4.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#installing some libraries\n",
    "#!pip install datasets\n",
    "!pip install --upgrade jupyter ipywidgets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "efda7204",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch, transformers\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import PreTrainedModel, PretrainedConfig\n",
    "from transformers import AutoModel, AutoConfig,AutoModelForCausalLM, AutoConfig,GPT2Config,GPT2Tokenizer\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "#from datasets import load_dataset\n",
    "import pandas as pd, numpy as np\n",
    "from torch.cuda.amp import autocast\n",
    "from torch import cuda\n",
    "import datetime\n",
    "import warnings,itertools\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "import json\n",
    "# Ignore all warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "#pip install transformers bitsandbytes>=0.39.0 -q\n",
    "import zipfile,logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2fda88d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "import random\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc557e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "282fa4cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "model\n"
     ]
    }
   ],
   "source": [
    "#global params for training\n",
    "\n",
    "B,T = 64,1024\n",
    "epoch = 100\n",
    "random_init_wts = True\n",
    "min_text_len = 0\n",
    "# hard coded com\n",
    "comp_ratio = 3\n",
    "# train_loss_list = []\n",
    "# val_loss_list =[]\n",
    "if cuda.is_available():\n",
    "    device = torch.device('cuda:0')\n",
    "    print(device)\n",
    "else:\n",
    "    device = 'cpu'\n",
    "#print(device)\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "#os.environ[\"MKL_DEBUG_CPU_TYPE\"] = \"5\"\n",
    "\n",
    "#print(global_tr_loss)\n",
    "model_path = os.path.join(\"model\")\n",
    "print(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5182202a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./data/unzip_text_10M'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "directory = os.path.join('.','data','unzip_text_10M')  # Replace with your directory path\n",
    "directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a7a0e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_text(directory):\n",
    "    directory = os.path.join('.','data','unzip_text_10M',str(directory))  # Replace with your directory path\n",
    "    print(f\"directory :{directory}\")\n",
    "    # List all files in the directory\n",
    "    files = [f for f in os.listdir(directory) if os.path.isfile(os.path.join(directory, f))]\n",
    "    print(f\"files:{files}\")\n",
    "    text_content = []\n",
    "    # Read each file\n",
    "    total_lines = 0\n",
    "    for filenum,filename in enumerate(files):\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            #first_line = file.read()\n",
    "            #print(f\"filename :{filename}->first few lines {first_line}\")\n",
    "            #continue\n",
    "            #lines_list = [line.strip() for line in open(file_path, 'r')]\n",
    "            text = file.read()\n",
    "            text_content.append(text)\n",
    "            print(f\"the file:{filename} has been appeneded to the uber list and its length is {len(text_content)} \")\n",
    "            #total_lines+=len(lines_list)\n",
    "            #text_content.append(lines_list)\n",
    "    \n",
    "    flattened_list = ''.join(text_content)\n",
    "    assert (len(flattened_list) == total_lines , f\"Expected {len(flattened_list)} to be equal to {total_lines}\" )\n",
    "    \n",
    "    return flattened_list\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2dbccf06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "directory :./data/unzip_text_10M/train_10M\n",
      "files:['switchboard.train', 'simple_wiki.train', 'open_subtitles.train', 'gutenberg.train', 'childes.train', 'bnc_spoken.train']\n",
      "the file:switchboard.train has been appeneded to the uber list and its length is 1 \n",
      "the file:simple_wiki.train has been appeneded to the uber list and its length is 2 \n",
      "the file:open_subtitles.train has been appeneded to the uber list and its length is 3 \n",
      "the file:gutenberg.train has been appeneded to the uber list and its length is 4 \n",
      "the file:childes.train has been appeneded to the uber list and its length is 5 \n",
      "the file:bnc_spoken.train has been appeneded to the uber list and its length is 6 \n"
     ]
    }
   ],
   "source": [
    "train_list = read_text(\"train_10M\")\n",
    "#print(train_dict)\n",
    "#val_list = read_text(\"dev\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7014a882",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54215049"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2daa3460",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "827\n"
     ]
    }
   ],
   "source": [
    "chunks = len(train_list)//(B*T)\n",
    "print(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3b839516",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(\"distilgpt2\")\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bfe24764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50257\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.pad_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9424783d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "directory :./data/unzip_text_10M/dev\n",
      "files:['switchboard.dev', 'simple_wiki.dev', 'open_subtitles.dev', 'gutenberg.dev', 'childes.dev', 'bnc_spoken.dev']\n",
      "the file:switchboard.dev has been appeneded to the uber list and its length is 1 \n",
      "the file:simple_wiki.dev has been appeneded to the uber list and its length is 2 \n",
      "the file:open_subtitles.dev has been appeneded to the uber list and its length is 3 \n",
      "the file:gutenberg.dev has been appeneded to the uber list and its length is 4 \n",
      "the file:childes.dev has been appeneded to the uber list and its length is 5 \n",
      "the file:bnc_spoken.dev has been appeneded to the uber list and its length is 6 \n"
     ]
    }
   ],
   "source": [
    "val_list = read_text(\"dev\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7f86083a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50256\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2d3e5d1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4800672319849466"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.random()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b3c33630",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "30a6d456",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ranked_synonyms(word, pos):\n",
    "    synonyms = []\n",
    "    for syn in wordnet.synsets(word, pos=pos):\n",
    "        for lemma in syn.lemmas():\n",
    "            if lemma.name() != word:\n",
    "                synonyms.append((lemma.name(), syn.wup_similarity(syn)))\n",
    "    \n",
    "    # Sort synonyms by similarity score in descending order\n",
    "    ranked_synonyms = sorted(set(synonyms), key=lambda x: x[1] if x[1] is not None else 0, reverse=True)\n",
    "    return [syn for syn, _ in ranked_synonyms]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1a6c8770",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def replace_adj_with_synonyms(text):\n",
    "    words = word_tokenize(text)\n",
    "    tagged = pos_tag(words)\n",
    "    result = []\n",
    "    for word, pos in tagged:\n",
    "        if pos.startswith('J'):\n",
    "            wordnet_pos = get_wordnet_pos(pos)\n",
    "            lemma = lemmatizer.lemmatize(word, wordnet_pos)\n",
    "            synonyms = get_ranked_synonyms(lemma, wordnet_pos)\n",
    "            if synonyms:\n",
    "                replacement = random.choice(synonyms)  # Choose from top 3 synonyms\n",
    "                #print(f\"word = {word}|replacement = {replacement}\")\n",
    "                result.append(replacement)\n",
    "            else:\n",
    "                result.append(word)\n",
    "        else:\n",
    "            result.append(word)\n",
    "    return \" \".join(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9e2aae73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_text = train_list[147888:152221]\n",
    "# repl = replace_verbs_with_synonyms(sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "31997a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(repl)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ad0a8364",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "00fbec61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequences(token_ids_list, max_length = B*T, tokenizer = tokenizer):\n",
    "    padded_sequences = tokenizer.pad(\n",
    "        {\"input_ids\": token_ids_list},\n",
    "        padding='max_length',\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    return padded_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ddcd3e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text (text,tokenizer = tokenizer,max_length = B*T):\n",
    "    #print(f\"inside tokenize_text\")\n",
    "    enc = tokenizer(text,padding='max_length',truncation=True,max_length=max_length,return_tensors=\"pt\",return_attention_mask=True)\n",
    "    input_id = enc['input_ids']\n",
    "    att_mask = enc['attention_mask']\n",
    "    \n",
    "    # now concatenate these lists to B*T\n",
    "    input_id = torch.squeeze(input_id, dim = 0).to(dtype = torch.long)\n",
    "    att_mask = torch.squeeze(att_mask, dim = 0).to(dtype = torch.bool)\n",
    "\n",
    "    return input_id,att_mask\n",
    "    \n",
    "    \n",
    "    \n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f7865acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inp,att = tokenize_text(repl)\n",
    "# #a = inp['input_ids']\n",
    "# print(inp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0590d968",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test the tokenizer:\n",
    "model_name = 'distilgpt2'\n",
    "if random_init_wts:\n",
    "    config = AutoConfig.from_pretrained(model_name, vocab_size = 50304)\n",
    "    # Initialize the model with random weights\n",
    "    model = AutoModelForCausalLM.from_config(config)\n",
    "else:\n",
    "    #model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "    \n",
    "#model = torch.compile(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ff3e43c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50257"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b115f2",
   "metadata": {},
   "source": [
    "### Data loaders and Dataset for batched training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4e76532b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataset_pyt_train(Dataset):\n",
    "    def __init__(self, text_list, B = B, T = T, tokenizer = tokenizer, comp_ratio = comp_ratio,prob = .20):\n",
    "        self.text_list = text_list\n",
    "        #print(f\"Value of B {B}\")\n",
    "        self.prob = prob\n",
    "                                        \n",
    "    def __getitem__(self, idx):\n",
    "        #words_list[start_index:start_index + n]\n",
    "        chunk = self.text_list[idx:idx + B*T*comp_ratio]\n",
    "        #print(f\"length of chunk = {len(chunk)}\")\n",
    "        random_num = random.random()\n",
    "        #print(f\"The random number generated = {random_num}\")\n",
    "        if random_num > self.prob:\n",
    "            chunk = replace_adj_with_synonyms(chunk)\n",
    "        inp,att = tokenize_text(chunk, tokenizer)\n",
    "        input_id = inp.view(B,T)\n",
    "        attention_mask = att.view(B,T)\n",
    "        #print(f\"shape of input_id = {input_id.shape}| shape of attention = {attention_mask.shape}\")\n",
    "        \n",
    "        return input_id,attention_mask\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        #return the length of the dataframe\n",
    "        num_chunks = comp_ratio*B*T\n",
    "        return len(self.text_list)//num_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "00119e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataset_pyt_val(Dataset):\n",
    "    def __init__(self, text_list, B = B, T = T, tokenizer = tokenizer, comp_ratio = comp_ratio):\n",
    "        self.text_list = text_list\n",
    "        #print(f\"Value of B {B}\")\n",
    "                                                \n",
    "    def __getitem__(self, idx):\n",
    "        #words_list[start_index:start_index + n]\n",
    "        chunk = self.text_list[idx:idx + B*T*comp_ratio]\n",
    "        #print(f\"length of chunk = {len()}\")\n",
    "        inp,att = tokenize_text(chunk, tokenizer)\n",
    "        input_id = inp.view(B,T)\n",
    "        attention_mask = att.view(B,T)\n",
    "        #print(f\"shape of input_id = {input_id.shape}| shape of attention = {attention_mask.shape}\")\n",
    "        \n",
    "        return input_id,attention_mask\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        #return the length of the dataframe\n",
    "        num_chunks = comp_ratio*B*T\n",
    "        return len(self.text_list)//num_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "77dc3ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_dataset = dataset_pyt(filtered_df,tokenizer = tokenizer)\n",
    "train_dataset = dataset_pyt_train(train_list)\n",
    "val_dataset = dataset_pyt_val(val_list)\n",
    "\n",
    "train_loader = DataLoader(train_dataset,batch_size = 1, shuffle = True , num_workers = 4, pin_memory = True)\n",
    "val_loader = DataLoader(val_dataset,batch_size = 1, shuffle = True , num_workers = 4, pin_memory = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f6cc6e96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer vocabulary size: 50258\n",
      "EOS token ID: 50256\n",
      "PAD token ID: 50257\n"
     ]
    }
   ],
   "source": [
    "print(f\"Tokenizer vocabulary size: {len(tokenizer)}\")\n",
    "print(f\"EOS token ID: {tokenizer.eos_token_id}\")\n",
    "print(f\"PAD token ID: {tokenizer.pad_token_id}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cdba7685",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "275"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_list)//(3*B*T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e56f8b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_file(log_message, model_name = \"GPT2\" ,random_init_wts = random_init_wts ):\n",
    "    current_datetime = datetime.datetime.now()\n",
    "    # Extract date and time components\n",
    "    current_date = str(current_datetime.date())\n",
    "    log_file = model_name +'_nll_loss_adj_replacement'+'random_init_wts'+ '_'+str(random_init_wts)+'_' +current_date+'.log'\n",
    "    print(f\"*****LOGGING INFO IN {log_file}*********\")\n",
    "    filepath = os.path.join(\"model\",log_file)\n",
    "    logging.basicConfig(filename=filepath, \n",
    "                    filemode='a',  # Overwrite the log file each time\n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s', \n",
    "                    level=logging.DEBUG)\n",
    "    logger = logging.getLogger()\n",
    "    logger.info(log_message)\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "00ba514f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the train loader is 275\n",
      "Length of the val loader is 287\n",
      "num_tokens= 18022400\n"
     ]
    }
   ],
   "source": [
    "print(f\"Length of the train loader is {len(train_loader)}\")\n",
    "print(f\"Length of the val loader is {len(val_loader)}\")\n",
    "print(f\"num_tokens= {B*T*len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "83af4a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#emb,att,inp = train_dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b4a5485c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class check_train_metrics:\n",
    "    def __init__(self, patience=25, min_delta=0 , B = T, T = T,best_loss = torch.inf,early_stop = False):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = best_loss\n",
    "        self.early_stop = early_stop\n",
    "        self.B = B\n",
    "        self.T = T\n",
    "        self.improvement = None\n",
    "\n",
    "    def __call__(self, loss, epoch , epoch_durn, norm , current_lr, num_token):\n",
    "        if self.best_loss - loss > self.min_delta:\n",
    "            \n",
    "            print(f\"training loss has decreased---> reducing the best loss from {self.best_loss:.2f} to {loss:.2f} | throughput = {int(num_token/epoch_durn)} tokens/second | norm = {norm:.4f} | learning rate = {current_lr:.5e}\")\n",
    "            self.best_loss = loss\n",
    "            self.counter = 0\n",
    "            self.improvement = True\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            self.improvement = False\n",
    "            print(f\"No improvement in training  loss-->epoch= {epoch} and best loss is {self.best_loss:.2f}|current_loss = {loss}|counter = {self.counter}\")\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "72282eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class check_val_metrics:\n",
    "    def __init__(self, patience=25, min_delta=0, best_loss = torch.inf,early_stop = False):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = best_loss\n",
    "        self.early_stop = early_stop\n",
    "        \n",
    "\n",
    "    def __call__(self, loss, epoch , model, tokenizer):\n",
    "        if self.best_loss - loss > self.min_delta:\n",
    "            print(f\"Val loss has decreased -->reducing the global validation loss from {self.best_loss:.2f} to {loss:.2f}\")\n",
    "            s1 = (f\"Val loss has decreased -->reducing the global validation loss from {self.best_loss:.2f} to {loss:.2f}\")\n",
    "            print(f\" validation loss for epoch = {epoch} is {loss:.4f}\")\n",
    "            self.best_loss = loss\n",
    "            s2 = f\" validation loss for epoch = {epoch} is {loss:.4f}\"\n",
    "            print(f\" epoch= {epoch} :  val loss is {loss:.4f} \")\n",
    "            s3 = f\" epoch= {epoch} :  val loss is {loss:.4f} \"\n",
    "            #save the model\n",
    "            # Get the current date and time\n",
    "            current_datetime = datetime.datetime.now()\n",
    "            # Extract date and time components\n",
    "            current_date = str(current_datetime.date())\n",
    "            current_time = str(current_datetime.time()).split('.')[0]\n",
    "            file_name = 'model'+ current_date+current_time+'.pth'\n",
    "            path = os.path.join(\"model\",file_name)\n",
    "            print(f\"saving the model {file_name}\")\n",
    "            s4 = f\"saving the model {file_name}\"\n",
    "            #torch.save(model.state_dict(), path)\n",
    "            model.save_pretrained(path)\n",
    "            tokenizer.save_pretrained(path)\n",
    "            log_message = s1+s2+s3+s4\n",
    "            write_file(log_message)\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            print(f\"No improvement in validation loss-->epoch= {epoch} and best val loss is {self.best_loss:.2f}|current_Val loss = {loss}|counter = {self.counter}\")\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "78de17c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_output = model(input_ids = inp ,attention_mask = att, labels = inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "130cd37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad\n",
    "\n",
    "def eval_model(val_loader, model, epoch , device = device,tokenizer = tokenizer):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    e = epoch+1\n",
    "    val_loss_accum = 0.0\n",
    "    print(f\"inside validation data for epoch {e}\")\n",
    "    for ind,(input_id,attention_mask) in enumerate(val_loader):\n",
    "        ids = input_id.to(device=device, non_blocking=True)\n",
    "        ids = torch.squeeze(ids, dim = 0)\n",
    "        att_mask = attention_mask.to(device=device, non_blocking=True)\n",
    "        att_mask =  torch.squeeze(att_mask, dim = 0)\n",
    "        labels = ids.clone().to(device)\n",
    "        with autocast(dtype = torch.bfloat16):\n",
    "            model_output = model(input_ids = ids ,attention_mask = att_mask, labels = labels)\n",
    "            #print(f\"model_output loss = {model_output.loss}\")\n",
    "            total_loss = model_output.loss  \n",
    "    \n",
    "    \n",
    "    \n",
    "        val_loss_accum+= total_loss.detach().item()\n",
    "        del att_mask,labels,model_output,total_loss,ids\n",
    "    return val_loss_accum        \n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ab1f72f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_model(train_loader,val_loader,model,num_epoch = 100,device = device,tokenizer = tokenizer):\n",
    "    #model.train()\n",
    "    device = device\n",
    "    lr_custom = 5e-5\n",
    "    print(f\"inside train model. Device = {device}\")\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.AdamW(params =  model.parameters(), lr= lr_custom,fused = True ,weight_decay = .1)\n",
    "      \n",
    "    extra_train = .1*num_epoch\n",
    "    max_train_steps = int(num_epoch +extra_train )\n",
    "    import time\n",
    "    from transformers import get_linear_schedule_with_warmup\n",
    "    total_steps = len(train_loader) * num_epoch\n",
    "    scheduler_cos = transformers.get_cosine_schedule_with_warmup( optimizer= optimizer, num_warmup_steps =int(total_steps * 0.1) ,num_training_steps= total_steps )\n",
    "        \n",
    "    epoch_train_log = []\n",
    "    epoch_val_log = []\n",
    "    validate_val_metric = check_val_metrics()\n",
    "    validate_train_metric = check_train_metrics()\n",
    "    for i in range (max_train_steps):\n",
    "        \n",
    "        epoch_start_time = time.time()\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        # we use 2 schedulers - the first LR scheduler uses a cosine decay for 100 epochs the second scheduler takes the last LR from cosine scheduler and then maintains that LR for the next 10 epochs\n",
    "        if i >= num_epoch:\n",
    "            optimizer_reduced_lr = torch.optim.AdamW(params =  model.parameters(), lr= current_lr ,fused = True , weight_decay=.1)\n",
    "            scheduler_constant = transformers.get_constant_schedule_with_warmup( optimizer = optimizer_reduced_lr ,num_warmup_steps = 0, last_epoch = -1 )\n",
    "        \n",
    "        epoch_train_loss = 0       \n",
    "        for ind,(input_id,attention_mask) in enumerate(train_loader):\n",
    "            if ind == int(len(train_loader)/2):\n",
    "                batch_time = time.time()\n",
    "                duration = batch_time - epoch_start_time\n",
    "                print(f\"executing epoch:{i+1}, it took {duration/60} mins from beginning of epoch till batch#{ind}\")\n",
    "            \n",
    "            ids = input_id.to(device=device, non_blocking=True)\n",
    "            ids = torch.squeeze(ids, dim = 0)\n",
    "            att_mask = attention_mask.to(device=device, non_blocking=True)\n",
    "            att_mask =  torch.squeeze(att_mask, dim = 0)\n",
    "            labels = ids.clone().to(device)\n",
    "                        \n",
    "            with autocast(dtype = torch.bfloat16):\n",
    "                model_output = model(input_ids = ids ,attention_mask = att_mask, labels = labels)\n",
    "                total_loss = model_output.loss\n",
    "                if np.isnan(model_output.loss.item()):\n",
    "                    print(\"f nan values encountered..\")\n",
    "                    decoded_texts = [tokenizer.decode(ids, skip_special_tokens=True) for ids in ids]\n",
    "                    print(f\"*********$$$$$$$$$ decoded_texts = {decoded_texts}*************\")\n",
    "            assert not np.isnan(model_output.loss.item()), \"NaN value found\"\n",
    "                               \n",
    "            total_loss.backward()\n",
    "            epoch_train_loss += total_loss.detach().item()\n",
    "            norm = torch.nn.utils.clip_grad_norm(model.parameters() , 1.0)\n",
    "            if i <= num_epoch:\n",
    "                optimizer.step()\n",
    "                scheduler_cos.step()\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "            else:\n",
    "                optimizer_reduced_lr.step()\n",
    "                optimizer_reduced_lr.zero_grad(set_to_none=True)\n",
    "                scheduler_constant.step()\n",
    "                \n",
    "                         \n",
    "            del att_mask,labels,model_output,ids\n",
    "            \n",
    "        #batch processing complete \n",
    "        #print(f\"batch processing complete , lambda = {lambda_val} |total_loss for batch= {total_loss}\")\n",
    "        \n",
    "        if i <= num_epoch:\n",
    "            current_lr = scheduler_cos.get_last_lr()[0]\n",
    "        epoch_end_time = time.time()\n",
    "        epoch_durn = (epoch_end_time - epoch_start_time)\n",
    "        num_token = B*T*len(train_loader)\n",
    "        epoch_train_log.append(epoch_train_loss)\n",
    "        validate_train_metric(epoch_train_loss, i , epoch_durn, norm , current_lr, num_token)\n",
    "        \n",
    "        if validate_train_metric.improvement:\n",
    "            val_loss= eval_model(val_loader, model, epoch = i, device = device,tokenizer = tokenizer)\n",
    "            epoch_val_log.append(val_loss)\n",
    "            validate_val_metric(val_loss, i , model, tokenizer)\n",
    "            if validate_train_metric.early_stop or validate_val_metric.early_stop :\n",
    "                print(f\"early stopping trigerred either from training data or val data | train_counter = {validate_train_metric.counter}|val_counter = {validate_val_metric.counter}\")\n",
    "                break\n",
    "        else:\n",
    "            if validate_val_metric.early_stop:\n",
    "                print(f\"early stopping trigerred from validation data\")\n",
    "                break\n",
    "              \n",
    "    \n",
    "    return model,epoch_train_log,epoch_val_log\n",
    "        \n",
    "            \n",
    "            \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc3fdd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inside train model. Device = cuda:0\n",
      "executing epoch:1, it took 1.4281721274058023 mins from beginning of epoch till batch#137\n",
      "training loss has decreased---> reducing the best loss from inf to 2188.70 | throughput = 106162 tokens/second | norm = 1.9495 | learning rate = 5.00000e-06\n",
      "inside validation data for epoch 1\n",
      "Val loss has decreased -->reducing the global validation loss from inf to 1851.48\n",
      " validation loss for epoch = 0 is 1851.4790\n",
      " epoch= 0 :  val loss is 1851.4790 \n",
      "saving the model model2024-07-2803:16:31.pth\n",
      "[2024-07-28 03:16:33,101] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "df: /root/.triton/autotune: No such file or directory\n",
      "/opt/conda/compiler_compat/ld: cannot find -laio\n",
      "collect2: error: ld returned 1 exit status\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93m [WARNING] \u001b[0m async_io requires the dev libaio .so object and headers but these were not found.\n",
      "\u001b[93m [WARNING] \u001b[0m async_io: please install the libaio-dev package with apt\n",
      "\u001b[93m [WARNING] \u001b[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\n",
      "\u001b[93m [WARNING] \u001b[0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH\n",
      "\u001b[93m [WARNING] \u001b[0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3\n",
      "\u001b[93m [WARNING] \u001b[0m using untested triton version (2.3.1), only 1.0.0 is known to be compatible\n",
      "*****LOGGING INFO IN GPT2_nll_loss_adj_replacementrandom_init_wts_True_2024-07-28.log*********\n",
      "executing epoch:2, it took 1.3618555545806885 mins from beginning of epoch till batch#137\n",
      "training loss has decreased---> reducing the best loss from 2188.70 to 1339.69 | throughput = 111805 tokens/second | norm = 2.4176 | learning rate = 1.00000e-05\n",
      "inside validation data for epoch 2\n",
      "Val loss has decreased -->reducing the global validation loss from 1851.48 to 1350.92\n",
      " validation loss for epoch = 1 is 1350.9196\n",
      " epoch= 1 :  val loss is 1350.9196 \n",
      "saving the model model2024-07-2803:20:16.pth\n",
      "*****LOGGING INFO IN GPT2_nll_loss_adj_replacementrandom_init_wts_True_2024-07-28.log*********\n",
      "executing epoch:3, it took 1.355851141611735 mins from beginning of epoch till batch#137\n",
      "training loss has decreased---> reducing the best loss from 1339.69 to 1023.63 | throughput = 112204 tokens/second | norm = 2.8974 | learning rate = 1.50000e-05\n",
      "inside validation data for epoch 3\n",
      "Val loss has decreased -->reducing the global validation loss from 1350.92 to 1132.14\n",
      " validation loss for epoch = 2 is 1132.1389\n",
      " epoch= 2 :  val loss is 1132.1389 \n",
      "saving the model model2024-07-2803:23:54.pth\n",
      "*****LOGGING INFO IN GPT2_nll_loss_adj_replacementrandom_init_wts_True_2024-07-28.log*********\n",
      "executing epoch:4, it took 1.3636626283327737 mins from beginning of epoch till batch#137\n",
      "training loss has decreased---> reducing the best loss from 1023.63 to 837.57 | throughput = 111535 tokens/second | norm = 4.4187 | learning rate = 2.00000e-05\n",
      "inside validation data for epoch 4\n",
      "Val loss has decreased -->reducing the global validation loss from 1132.14 to 1088.15\n",
      " validation loss for epoch = 3 is 1088.1464\n",
      " epoch= 3 :  val loss is 1088.1464 \n",
      "saving the model model2024-07-2803:27:34.pth\n",
      "*****LOGGING INFO IN GPT2_nll_loss_adj_replacementrandom_init_wts_True_2024-07-28.log*********\n",
      "executing epoch:5, it took 1.3753790616989137 mins from beginning of epoch till batch#137\n",
      "training loss has decreased---> reducing the best loss from 837.57 to 705.79 | throughput = 110774 tokens/second | norm = 4.5187 | learning rate = 2.50000e-05\n",
      "inside validation data for epoch 5\n",
      "Val loss has decreased -->reducing the global validation loss from 1088.15 to 1059.48\n",
      " validation loss for epoch = 4 is 1059.4797\n",
      " epoch= 4 :  val loss is 1059.4797 \n",
      "saving the model model2024-07-2803:31:14.pth\n",
      "*****LOGGING INFO IN GPT2_nll_loss_adj_replacementrandom_init_wts_True_2024-07-28.log*********\n",
      "executing epoch:6, it took 1.3493556221326193 mins from beginning of epoch till batch#137\n",
      "training loss has decreased---> reducing the best loss from 705.79 to 596.95 | throughput = 112233 tokens/second | norm = 5.3178 | learning rate = 3.00000e-05\n",
      "inside validation data for epoch 6\n",
      "No improvement in validation loss-->epoch= 5 and best val loss is 1059.48|current_Val loss = 1094.0090305805206|counter = 1\n",
      "executing epoch:7, it took 1.3513104319572449 mins from beginning of epoch till batch#137\n",
      "training loss has decreased---> reducing the best loss from 596.95 to 470.51 | throughput = 112184 tokens/second | norm = 7.7166 | learning rate = 3.50000e-05\n",
      "inside validation data for epoch 7\n",
      "No improvement in validation loss-->epoch= 6 and best val loss is 1059.48|current_Val loss = 1130.2051005363464|counter = 2\n",
      "executing epoch:8, it took 1.3791650056838989 mins from beginning of epoch till batch#137\n",
      "training loss has decreased---> reducing the best loss from 470.51 to 316.06 | throughput = 111226 tokens/second | norm = 7.6944 | learning rate = 4.00000e-05\n",
      "inside validation data for epoch 8\n",
      "No improvement in validation loss-->epoch= 7 and best val loss is 1059.48|current_Val loss = 1199.4683909416199|counter = 3\n",
      "executing epoch:9, it took 1.3591336329778037 mins from beginning of epoch till batch#137\n",
      "training loss has decreased---> reducing the best loss from 316.06 to 183.57 | throughput = 111908 tokens/second | norm = 7.3333 | learning rate = 4.50000e-05\n",
      "inside validation data for epoch 9\n",
      "No improvement in validation loss-->epoch= 8 and best val loss is 1059.48|current_Val loss = 1278.532033443451|counter = 4\n",
      "executing epoch:10, it took 1.3648639957110087 mins from beginning of epoch till batch#137\n",
      "training loss has decreased---> reducing the best loss from 183.57 to 84.38 | throughput = 111575 tokens/second | norm = 8.8445 | learning rate = 5.00000e-05\n",
      "inside validation data for epoch 10\n",
      "No improvement in validation loss-->epoch= 9 and best val loss is 1059.48|current_Val loss = 1403.5309381484985|counter = 5\n",
      "executing epoch:11, it took 1.3785722931226094 mins from beginning of epoch till batch#137\n",
      "training loss has decreased---> reducing the best loss from 84.38 to 49.54 | throughput = 111500 tokens/second | norm = 2.0745 | learning rate = 4.99848e-05\n",
      "inside validation data for epoch 11\n",
      "No improvement in validation loss-->epoch= 10 and best val loss is 1059.48|current_Val loss = 1413.0610904693604|counter = 6\n",
      "executing epoch:12, it took 1.3698379079500833 mins from beginning of epoch till batch#137\n",
      "training loss has decreased---> reducing the best loss from 49.54 to 30.04 | throughput = 111341 tokens/second | norm = 0.6005 | learning rate = 4.99391e-05\n",
      "inside validation data for epoch 12\n",
      "No improvement in validation loss-->epoch= 11 and best val loss is 1059.48|current_Val loss = 1455.0661511421204|counter = 7\n",
      "executing epoch:13, it took 1.3736556092898051 mins from beginning of epoch till batch#137\n",
      "training loss has decreased---> reducing the best loss from 30.04 to 21.94 | throughput = 111503 tokens/second | norm = 0.4573 | learning rate = 4.98630e-05\n",
      "inside validation data for epoch 13\n",
      "No improvement in validation loss-->epoch= 12 and best val loss is 1059.48|current_Val loss = 1495.7268195152283|counter = 8\n",
      "executing epoch:14, it took 1.3312447468439739 mins from beginning of epoch till batch#137\n",
      "training loss has decreased---> reducing the best loss from 21.94 to 17.96 | throughput = 112926 tokens/second | norm = 0.2200 | learning rate = 4.97567e-05\n",
      "inside validation data for epoch 14\n",
      "No improvement in validation loss-->epoch= 13 and best val loss is 1059.48|current_Val loss = 1535.5318756103516|counter = 9\n",
      "executing epoch:15, it took 1.3695608178774517 mins from beginning of epoch till batch#137\n",
      "training loss has decreased---> reducing the best loss from 17.96 to 16.38 | throughput = 111355 tokens/second | norm = 0.3420 | learning rate = 4.96202e-05\n",
      "inside validation data for epoch 15\n",
      "No improvement in validation loss-->epoch= 14 and best val loss is 1059.48|current_Val loss = 1520.5558438301086|counter = 10\n",
      "executing epoch:16, it took 1.358621096611023 mins from beginning of epoch till batch#137\n",
      "training loss has decreased---> reducing the best loss from 16.38 to 15.17 | throughput = 112111 tokens/second | norm = 0.3205 | learning rate = 4.94537e-05\n",
      "inside validation data for epoch 16\n",
      "No improvement in validation loss-->epoch= 15 and best val loss is 1059.48|current_Val loss = 1581.1567754745483|counter = 11\n",
      "executing epoch:17, it took 1.3723572532335917 mins from beginning of epoch till batch#137\n",
      "training loss has decreased---> reducing the best loss from 15.17 to 13.43 | throughput = 111213 tokens/second | norm = 0.1665 | learning rate = 4.92574e-05\n",
      "inside validation data for epoch 17\n",
      "No improvement in validation loss-->epoch= 16 and best val loss is 1059.48|current_Val loss = 1604.337450504303|counter = 12\n",
      "executing epoch:18, it took 1.3687799056371053 mins from beginning of epoch till batch#137\n",
      "training loss has decreased---> reducing the best loss from 13.43 to 11.25 | throughput = 111747 tokens/second | norm = 0.1576 | learning rate = 4.90315e-05\n",
      "inside validation data for epoch 18\n",
      "No improvement in validation loss-->epoch= 17 and best val loss is 1059.48|current_Val loss = 1640.6189489364624|counter = 13\n",
      "executing epoch:19, it took 1.373717192808787 mins from beginning of epoch till batch#137\n",
      "No improvement in training  loss-->epoch= 18 and best loss is 11.25|current_loss = 11.357421129010618|counter = 1\n",
      "executing epoch:20, it took 1.3563971559206645 mins from beginning of epoch till batch#137\n",
      "No improvement in training  loss-->epoch= 19 and best loss is 11.25|current_loss = 12.504079081118107|counter = 2\n",
      "executing epoch:21, it took 1.3648392995198568 mins from beginning of epoch till batch#137\n",
      "No improvement in training  loss-->epoch= 20 and best loss is 11.25|current_loss = 11.288620005827397|counter = 3\n",
      "executing epoch:22, it took 1.36126758257548 mins from beginning of epoch till batch#137\n",
      "training loss has decreased---> reducing the best loss from 11.25 to 11.07 | throughput = 111524 tokens/second | norm = 0.1421 | learning rate = 4.78386e-05\n",
      "inside validation data for epoch 22\n",
      "No improvement in validation loss-->epoch= 21 and best val loss is 1059.48|current_Val loss = 1673.1082000732422|counter = 14\n",
      "executing epoch:23, it took 1.3595247666041057 mins from beginning of epoch till batch#137\n",
      "No improvement in training  loss-->epoch= 22 and best loss is 11.07|current_loss = 11.14313604310155|counter = 1\n",
      "executing epoch:24, it took 1.3665498892466228 mins from beginning of epoch till batch#137\n",
      "No improvement in training  loss-->epoch= 23 and best loss is 11.07|current_loss = 14.44882491696626|counter = 2\n",
      "executing epoch:25, it took 1.3674842834472656 mins from beginning of epoch till batch#137\n",
      "training loss has decreased---> reducing the best loss from 11.07 to 11.06 | throughput = 111426 tokens/second | norm = 0.1229 | learning rate = 4.66506e-05\n",
      "inside validation data for epoch 25\n",
      "No improvement in validation loss-->epoch= 24 and best val loss is 1059.48|current_Val loss = 1671.973270893097|counter = 15\n",
      "executing epoch:26, it took 1.3578672885894776 mins from beginning of epoch till batch#137\n",
      "training loss has decreased---> reducing the best loss from 11.06 to 10.62 | throughput = 111838 tokens/second | norm = 0.0885 | learning rate = 4.62012e-05\n",
      "inside validation data for epoch 26\n",
      "No improvement in validation loss-->epoch= 25 and best val loss is 1059.48|current_Val loss = 1689.361840724945|counter = 16\n",
      "executing epoch:27, it took 1.3677284916241963 mins from beginning of epoch till batch#137\n",
      "No improvement in training  loss-->epoch= 26 and best loss is 10.62|current_loss = 10.753668039571494|counter = 1\n",
      "executing epoch:28, it took 1.3757443984349569 mins from beginning of epoch till batch#137\n",
      "No improvement in training  loss-->epoch= 27 and best loss is 10.62|current_loss = 13.13146165246144|counter = 2\n",
      "executing epoch:29, it took 1.3628224889437357 mins from beginning of epoch till batch#137\n",
      "No improvement in training  loss-->epoch= 28 and best loss is 10.62|current_loss = 11.775286242365837|counter = 3\n",
      "executing epoch:30, it took 1.3656381328900655 mins from beginning of epoch till batch#137\n",
      "No improvement in training  loss-->epoch= 29 and best loss is 10.62|current_loss = 10.697711823973805|counter = 4\n",
      "executing epoch:31, it took 1.362318209807078 mins from beginning of epoch till batch#137\n",
      "training loss has decreased---> reducing the best loss from 10.62 to 10.23 | throughput = 112089 tokens/second | norm = 0.0759 | learning rate = 4.35786e-05\n",
      "inside validation data for epoch 31\n",
      "No improvement in validation loss-->epoch= 30 and best val loss is 1059.48|current_Val loss = 1707.7170062065125|counter = 17\n",
      "executing epoch:32, it took 1.3583479285240174 mins from beginning of epoch till batch#137\n",
      "No improvement in training  loss-->epoch= 31 and best loss is 10.23|current_loss = 10.36737445415929|counter = 1\n",
      "executing epoch:33, it took 1.75385373433431 mins from beginning of epoch till batch#137\n",
      "No improvement in training  loss-->epoch= 32 and best loss is 10.23|current_loss = 11.168096330482513|counter = 2\n",
      "executing epoch:34, it took 1.7519400755564372 mins from beginning of epoch till batch#137\n",
      "No improvement in training  loss-->epoch= 33 and best loss is 10.23|current_loss = 10.960783390328288|counter = 3\n",
      "executing epoch:35, it took 1.7528584043184916 mins from beginning of epoch till batch#137\n",
      "training loss has decreased---> reducing the best loss from 10.23 to 10.21 | throughput = 87165 tokens/second | norm = 0.0665 | learning rate = 4.10697e-05\n",
      "inside validation data for epoch 35\n",
      "No improvement in validation loss-->epoch= 34 and best val loss is 1059.48|current_Val loss = 1726.5587468147278|counter = 18\n",
      "executing epoch:36, it took 1.7476202209790548 mins from beginning of epoch till batch#137\n",
      "No improvement in training  loss-->epoch= 35 and best loss is 10.21|current_loss = 10.874402769841254|counter = 1\n",
      "executing epoch:37, it took 1.7461630900700886 mins from beginning of epoch till batch#137\n",
      "No improvement in training  loss-->epoch= 36 and best loss is 10.21|current_loss = 10.586552652530372|counter = 2\n",
      "executing epoch:38, it took 1.726036516825358 mins from beginning of epoch till batch#137\n",
      "No improvement in training  loss-->epoch= 37 and best loss is 10.21|current_loss = 12.425568727310747|counter = 3\n",
      "executing epoch:39, it took 1.7507003982861837 mins from beginning of epoch till batch#137\n",
      "No improvement in training  loss-->epoch= 38 and best loss is 10.21|current_loss = 10.435832494869828|counter = 4\n",
      "executing epoch:40, it took 1.683652937412262 mins from beginning of epoch till batch#137\n",
      "No improvement in training  loss-->epoch= 39 and best loss is 10.21|current_loss = 10.305822278838605|counter = 5\n",
      "executing epoch:41, it took 1.3684948801994323 mins from beginning of epoch till batch#137\n",
      "No improvement in training  loss-->epoch= 40 and best loss is 10.21|current_loss = 10.568321109749377|counter = 6\n",
      "executing epoch:42, it took 1.3460063497225443 mins from beginning of epoch till batch#137\n",
      "No improvement in training  loss-->epoch= 41 and best loss is 10.21|current_loss = 10.454101079609245|counter = 7\n",
      "executing epoch:43, it took 1.3690025289853414 mins from beginning of epoch till batch#137\n",
      "No improvement in training  loss-->epoch= 42 and best loss is 10.21|current_loss = 10.385587363969535|counter = 8\n",
      "executing epoch:44, it took 1.3692255695660909 mins from beginning of epoch till batch#137\n",
      "No improvement in training  loss-->epoch= 43 and best loss is 10.21|current_loss = 10.301191651262343|counter = 9\n",
      "executing epoch:45, it took 1.3703263958295187 mins from beginning of epoch till batch#137\n",
      "No improvement in training  loss-->epoch= 44 and best loss is 10.21|current_loss = 10.694597624475136|counter = 10\n",
      "executing epoch:46, it took 1.3703455964724223 mins from beginning of epoch till batch#137\n",
      "No improvement in training  loss-->epoch= 45 and best loss is 10.21|current_loss = 11.799872370436788|counter = 11\n",
      "executing epoch:47, it took 1.3774386564890544 mins from beginning of epoch till batch#137\n",
      "No improvement in training  loss-->epoch= 46 and best loss is 10.21|current_loss = 11.014010913204402|counter = 12\n",
      "executing epoch:48, it took 1.3680183211962382 mins from beginning of epoch till batch#137\n",
      "No improvement in training  loss-->epoch= 47 and best loss is 10.21|current_loss = 10.374038320966065|counter = 13\n",
      "executing epoch:49, it took 1.3745777289072671 mins from beginning of epoch till batch#137\n",
      "No improvement in training  loss-->epoch= 48 and best loss is 10.21|current_loss = 11.009454624261707|counter = 14\n",
      "executing epoch:50, it took 1.3588770151138305 mins from beginning of epoch till batch#137\n",
      "training loss has decreased---> reducing the best loss from 10.21 to 10.18 | throughput = 111946 tokens/second | norm = 0.0529 | learning rate = 2.93412e-05\n",
      "inside validation data for epoch 50\n",
      "No improvement in validation loss-->epoch= 49 and best val loss is 1059.48|current_Val loss = 1749.3930320739746|counter = 19\n",
      "executing epoch:51, it took 1.35477557182312 mins from beginning of epoch till batch#137\n",
      "training loss has decreased---> reducing the best loss from 10.18 to 9.89 | throughput = 112153 tokens/second | norm = 0.0563 | learning rate = 2.84793e-05\n",
      "inside validation data for epoch 51\n",
      "No improvement in validation loss-->epoch= 50 and best val loss is 1059.48|current_Val loss = 1768.864164352417|counter = 20\n",
      "executing epoch:52, it took 1.374842902024587 mins from beginning of epoch till batch#137\n",
      "No improvement in training  loss-->epoch= 51 and best loss is 9.89|current_loss = 10.72682808386162|counter = 1\n",
      "executing epoch:53, it took 1.3643339157104493 mins from beginning of epoch till batch#137\n",
      "No improvement in training  loss-->epoch= 52 and best loss is 9.89|current_loss = 10.561074008932337|counter = 2\n",
      "executing epoch:54, it took 1.3672384063402812 mins from beginning of epoch till batch#137\n",
      "No improvement in training  loss-->epoch= 53 and best loss is 9.89|current_loss = 10.401689472841099|counter = 3\n",
      "executing epoch:55, it took 1.3620742042859395 mins from beginning of epoch till batch#137\n",
      "No improvement in training  loss-->epoch= 54 and best loss is 9.89|current_loss = 10.295015631709248|counter = 4\n",
      "executing epoch:56, it took 1.3737421035766602 mins from beginning of epoch till batch#137\n",
      "No improvement in training  loss-->epoch= 55 and best loss is 9.89|current_loss = 10.587990838568658|counter = 5\n",
      "executing epoch:57, it took 1.3711073795954387 mins from beginning of epoch till batch#137\n",
      "No improvement in training  loss-->epoch= 56 and best loss is 9.89|current_loss = 10.186304077273235|counter = 6\n",
      "executing epoch:58, it took 1.3770965417226155 mins from beginning of epoch till batch#137\n",
      "No improvement in training  loss-->epoch= 57 and best loss is 9.89|current_loss = 10.710940121207386|counter = 7\n",
      "executing epoch:59, it took 1.3687646706899008 mins from beginning of epoch till batch#137\n",
      "No improvement in training  loss-->epoch= 58 and best loss is 9.89|current_loss = 10.252695573493838|counter = 8\n",
      "executing epoch:60, it took 1.3668861587842305 mins from beginning of epoch till batch#137\n",
      "No improvement in training  loss-->epoch= 59 and best loss is 9.89|current_loss = 10.488742590416223|counter = 9\n",
      "executing epoch:61, it took 1.360216776529948 mins from beginning of epoch till batch#137\n",
      "No improvement in training  loss-->epoch= 60 and best loss is 9.89|current_loss = 10.298870358150452|counter = 10\n",
      "executing epoch:62, it took 1.364941934744517 mins from beginning of epoch till batch#137\n",
      "training loss has decreased---> reducing the best loss from 9.89 to 9.89 | throughput = 111849 tokens/second | norm = 0.0473 | learning rate = 1.89520e-05\n",
      "inside validation data for epoch 62\n",
      "No improvement in validation loss-->epoch= 61 and best val loss is 1059.48|current_Val loss = 1776.8502316474915|counter = 21\n",
      "executing epoch:63, it took 1.3480558077494302 mins from beginning of epoch till batch#137\n",
      "training loss has decreased---> reducing the best loss from 9.89 to 9.67 | throughput = 112668 tokens/second | norm = 0.0420 | learning rate = 1.81091e-05\n",
      "inside validation data for epoch 63\n",
      "No improvement in validation loss-->epoch= 62 and best val loss is 1059.48|current_Val loss = 1784.2611436843872|counter = 22\n",
      "executing epoch:64, it took 1.3687937021255494 mins from beginning of epoch till batch#137\n",
      "No improvement in training  loss-->epoch= 63 and best loss is 9.67|current_loss = 10.396680843783543|counter = 1\n",
      "executing epoch:65, it took 1.3611910303433736 mins from beginning of epoch till batch#137\n",
      "No improvement in training  loss-->epoch= 64 and best loss is 9.67|current_loss = 9.92401533992961|counter = 2\n",
      "executing epoch:66, it took 1.3616307894388835 mins from beginning of epoch till batch#137\n",
      "No improvement in training  loss-->epoch= 65 and best loss is 9.67|current_loss = 10.22279700357467|counter = 3\n",
      "executing epoch:67, it took 1.359007438023885 mins from beginning of epoch till batch#137\n",
      "No improvement in training  loss-->epoch= 66 and best loss is 9.67|current_loss = 10.247984998160973|counter = 4\n",
      "executing epoch:68, it took 1.379648792743683 mins from beginning of epoch till batch#137\n",
      "No improvement in training  loss-->epoch= 67 and best loss is 9.67|current_loss = 10.351291126571596|counter = 5\n",
      "executing epoch:69, it took 1.3706178784370422 mins from beginning of epoch till batch#137\n",
      "No improvement in training  loss-->epoch= 68 and best loss is 9.67|current_loss = 10.46235916740261|counter = 6\n",
      "executing epoch:70, it took 1.3717981100082397 mins from beginning of epoch till batch#137\n",
      "No improvement in training  loss-->epoch= 69 and best loss is 9.67|current_loss = 10.456301770871505|counter = 7\n",
      "executing epoch:71, it took 1.3613481561342875 mins from beginning of epoch till batch#137\n",
      "No improvement in training  loss-->epoch= 70 and best loss is 9.67|current_loss = 9.970576622290537|counter = 8\n",
      "executing epoch:72, it took 1.3657790859540304 mins from beginning of epoch till batch#137\n",
      "No improvement in training  loss-->epoch= 71 and best loss is 9.67|current_loss = 10.061513364082202|counter = 9\n",
      "executing epoch:73, it took 1.374712101618449 mins from beginning of epoch till batch#137\n",
      "No improvement in training  loss-->epoch= 72 and best loss is 9.67|current_loss = 10.919608655152842|counter = 10\n",
      "executing epoch:74, it took 1.371454616387685 mins from beginning of epoch till batch#137\n",
      "No improvement in training  loss-->epoch= 73 and best loss is 9.67|current_loss = 10.093061715830117|counter = 11\n",
      "executing epoch:75, it took 1.375286372502645 mins from beginning of epoch till batch#137\n",
      "No improvement in training  loss-->epoch= 74 and best loss is 9.67|current_loss = 10.292836878914386|counter = 12\n",
      "executing epoch:76, it took 1.36593048175176 mins from beginning of epoch till batch#137\n",
      "No improvement in training  loss-->epoch= 75 and best loss is 9.67|current_loss = 10.156401311978698|counter = 13\n",
      "executing epoch:77, it took 1.355815068880717 mins from beginning of epoch till batch#137\n",
      "No improvement in training  loss-->epoch= 76 and best loss is 9.67|current_loss = 10.073795433389023|counter = 14\n",
      "executing epoch:78, it took 1.375778647263845 mins from beginning of epoch till batch#137\n",
      "No improvement in training  loss-->epoch= 77 and best loss is 9.67|current_loss = 10.012990471208468|counter = 15\n",
      "executing epoch:79, it took 1.3578879634539287 mins from beginning of epoch till batch#137\n",
      "No improvement in training  loss-->epoch= 78 and best loss is 9.67|current_loss = 9.890121839474887|counter = 16\n",
      "executing epoch:80, it took 1.3680954416592916 mins from beginning of epoch till batch#137\n",
      "No improvement in training  loss-->epoch= 79 and best loss is 9.67|current_loss = 10.325476283906028|counter = 17\n",
      "executing epoch:81, it took 1.3671324928601583 mins from beginning of epoch till batch#137\n",
      "No improvement in training  loss-->epoch= 80 and best loss is 9.67|current_loss = 10.041931460145861|counter = 18\n",
      "executing epoch:82, it took 1.3781354983647665 mins from beginning of epoch till batch#137\n",
      "No improvement in training  loss-->epoch= 81 and best loss is 9.67|current_loss = 10.565658238716424|counter = 19\n",
      "executing epoch:83, it took 1.3727250377337137 mins from beginning of epoch till batch#137\n",
      "No improvement in training  loss-->epoch= 82 and best loss is 9.67|current_loss = 10.462514718063176|counter = 20\n",
      "executing epoch:84, it took 1.368800679842631 mins from beginning of epoch till batch#137\n",
      "No improvement in training  loss-->epoch= 83 and best loss is 9.67|current_loss = 10.068536963313818|counter = 21\n",
      "executing epoch:85, it took 1.3645015199979147 mins from beginning of epoch till batch#137\n",
      "No improvement in training  loss-->epoch= 84 and best loss is 9.67|current_loss = 9.9387135154102|counter = 22\n",
      "executing epoch:86, it took 1.3630313158035279 mins from beginning of epoch till batch#137\n",
      "No improvement in training  loss-->epoch= 85 and best loss is 9.67|current_loss = 10.088949643075466|counter = 23\n",
      "executing epoch:87, it took 1.3663397471110026 mins from beginning of epoch till batch#137\n",
      "No improvement in training  loss-->epoch= 86 and best loss is 9.67|current_loss = 9.888009342830628|counter = 24\n",
      "executing epoch:88, it took 1.3681911110877991 mins from beginning of epoch till batch#137\n",
      "No improvement in training  loss-->epoch= 87 and best loss is 9.67|current_loss = 10.431368885096163|counter = 25\n",
      "executing epoch:89, it took 1.3777928471565246 mins from beginning of epoch till batch#137\n",
      "No improvement in training  loss-->epoch= 88 and best loss is 9.67|current_loss = 10.41031778161414|counter = 26\n",
      "executing epoch:90, it took 1.3625816543896994 mins from beginning of epoch till batch#137\n",
      "No improvement in training  loss-->epoch= 89 and best loss is 9.67|current_loss = 9.975129889557138|counter = 27\n",
      "executing epoch:91, it took 1.3566840489705403 mins from beginning of epoch till batch#137\n",
      "No improvement in training  loss-->epoch= 90 and best loss is 9.67|current_loss = 10.1051912503317|counter = 28\n",
      "executing epoch:92, it took 1.3610900600751241 mins from beginning of epoch till batch#137\n",
      "No improvement in training  loss-->epoch= 91 and best loss is 9.67|current_loss = 9.776788505492732|counter = 29\n",
      "executing epoch:93, it took 1.3599499901135763 mins from beginning of epoch till batch#137\n",
      "No improvement in training  loss-->epoch= 92 and best loss is 9.67|current_loss = 9.932429447071627|counter = 30\n",
      "executing epoch:94, it took 1.362394428253174 mins from beginning of epoch till batch#137\n",
      "No improvement in training  loss-->epoch= 93 and best loss is 9.67|current_loss = 9.982960772234946|counter = 31\n",
      "executing epoch:95, it took 1.372106424967448 mins from beginning of epoch till batch#137\n",
      "No improvement in training  loss-->epoch= 94 and best loss is 9.67|current_loss = 9.987045128131285|counter = 32\n",
      "executing epoch:96, it took 1.3453035434087117 mins from beginning of epoch till batch#137\n",
      "No improvement in training  loss-->epoch= 95 and best loss is 9.67|current_loss = 10.219942569034174|counter = 33\n",
      "executing epoch:97, it took 1.365799327691396 mins from beginning of epoch till batch#137\n",
      "No improvement in training  loss-->epoch= 96 and best loss is 9.67|current_loss = 9.969478288432583|counter = 34\n",
      "executing epoch:98, it took 1.3579152266184489 mins from beginning of epoch till batch#137\n",
      "No improvement in training  loss-->epoch= 97 and best loss is 9.67|current_loss = 9.867461621994153|counter = 35\n",
      "executing epoch:99, it took 1.3567045370737711 mins from beginning of epoch till batch#137\n",
      "No improvement in training  loss-->epoch= 98 and best loss is 9.67|current_loss = 10.186781413387507|counter = 36\n",
      "executing epoch:100, it took 1.3652273217837017 mins from beginning of epoch till batch#137\n",
      "No improvement in training  loss-->epoch= 99 and best loss is 9.67|current_loss = 10.12118898704648|counter = 37\n",
      "executing epoch:101, it took 1.3556508700052896 mins from beginning of epoch till batch#137\n",
      "No improvement in training  loss-->epoch= 100 and best loss is 9.67|current_loss = 9.967544036684558|counter = 38\n",
      "executing epoch:102, it took 1.366516375541687 mins from beginning of epoch till batch#137\n",
      "No improvement in training  loss-->epoch= 101 and best loss is 9.67|current_loss = 9.848432541824877|counter = 39\n",
      "executing epoch:103, it took 1.371057359377543 mins from beginning of epoch till batch#137\n",
      "No improvement in training  loss-->epoch= 102 and best loss is 9.67|current_loss = 10.364706343272701|counter = 40\n",
      "executing epoch:104, it took 1.3648437102635702 mins from beginning of epoch till batch#137\n",
      "No improvement in training  loss-->epoch= 103 and best loss is 9.67|current_loss = 10.059450075728819|counter = 41\n",
      "executing epoch:105, it took 1.369418168067932 mins from beginning of epoch till batch#137\n",
      "No improvement in training  loss-->epoch= 104 and best loss is 9.67|current_loss = 10.302517269505188|counter = 42\n",
      "executing epoch:106, it took 1.3630021492640176 mins from beginning of epoch till batch#137\n",
      "No improvement in training  loss-->epoch= 105 and best loss is 9.67|current_loss = 10.036534731741995|counter = 43\n",
      "executing epoch:107, it took 1.3560032486915587 mins from beginning of epoch till batch#137\n",
      "No improvement in training  loss-->epoch= 106 and best loss is 9.67|current_loss = 10.052011854946613|counter = 44\n",
      "executing epoch:108, it took 1.3663304686546325 mins from beginning of epoch till batch#137\n",
      "No improvement in training  loss-->epoch= 107 and best loss is 9.67|current_loss = 10.382975991349667|counter = 45\n",
      "executing epoch:109, it took 1.3637938380241394 mins from beginning of epoch till batch#137\n",
      "No improvement in training  loss-->epoch= 108 and best loss is 9.67|current_loss = 10.189869658788666|counter = 46\n"
     ]
    }
   ],
   "source": [
    "tr_model,epoch_train_log,epoch_val_log = train_model(train_loader, val_loader,model=model,tokenizer = tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c87f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(epoch_train_log)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d13c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json , os\n",
    "# path_var_train_log = os.path.join(\".\",\"Tokenizers_and_loss_vals\",\"epoch_train_plain_loss.json\")\n",
    "# path_var_val_log = os.path.join(\".\",\"Tokenizers_and_loss_vals\",\"epoch_val_val_loss_.json\")\n",
    "\n",
    "# #print(path_var)\n",
    "# #Write the list to a JSON file\n",
    "# with open(path_var_train_log, \"w\") as file:\n",
    "#     json.dump(epoch_train_log, file)\n",
    "\n",
    "# with open(path_var_val_log, \"w\") as file:\n",
    "#     json.dump(epoch_val_log, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18b050a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(path_var_train_log, \"r\") as file:\n",
    "#     train_loss = json.load(file)\n",
    "# with open(path_var_val_log, \"r\") as file:\n",
    "#     val_loss = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668f2153",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_values = range(len(epoch_train_log))\n",
    "plt.plot(x_values, epoch_train_log, label='Train_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6398ef09",
   "metadata": {},
   "outputs": [],
   "source": [
    "#epoch_val_log= [t.detach().cpu().numpy() for t in epoch_val_log]\n",
    "x_values_val = range(len(epoch_val_log))\n",
    "plt.plot(x_values_val, epoch_val_log, label='val_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ab19f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_length = min(len(epoch_train_log), len(epoch_val_log))\n",
    "list1 = epoch_train_log[:min_length]\n",
    "list2 = epoch_val_log[:min_length]\n",
    "\n",
    "# Create x-axis values\n",
    "x_values = range(min_length)\n",
    "\n",
    "# Plot the lists\n",
    "plt.plot(x_values, list1, label='Train_loss')\n",
    "plt.plot(x_values, list2, label='Val_loss')\n",
    "\n",
    "# Adding labels and title\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Train Loss and Val_Loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80cbc2ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2432725",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae00376e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29b3922",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d81dfa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b60252",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
