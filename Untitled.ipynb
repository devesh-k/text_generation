{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "368809ad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting datasets\n",
      "  Downloading datasets-2.19.2-py3-none-any.whl (542 kB)\n",
      "\u001b[K     |████████████████████████████████| 542 kB 10.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pyarrow-hotfix\n",
      "  Downloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
      "Collecting multiprocess\n",
      "  Downloading multiprocess-0.70.16-py38-none-any.whl (132 kB)\n",
      "\u001b[K     |████████████████████████████████| 132 kB 103.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting requests>=2.32.1\n",
      "  Downloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "\u001b[K     |████████████████████████████████| 64 kB 118.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: packaging in /opt/conda/lib/python3.8/site-packages (from datasets) (21.3)\n",
      "Collecting fsspec[http]<=2024.3.1,>=2023.1.0\n",
      "  Downloading fsspec-2024.3.1-py3-none-any.whl (171 kB)\n",
      "\u001b[K     |████████████████████████████████| 171 kB 105.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.8/site-packages (from datasets) (3.4.0)\n",
      "Collecting pyarrow>=12.0.0\n",
      "  Downloading pyarrow-16.1.0-cp38-cp38-manylinux_2_28_x86_64.whl (40.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 40.9 MB 127.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting xxhash\n",
      "  Downloading xxhash-3.4.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "\u001b[K     |████████████████████████████████| 194 kB 117.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting aiohttp\n",
      "  Downloading aiohttp-3.9.5-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.3 MB 109.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: huggingface-hub>=0.21.2 in /opt/conda/lib/python3.8/site-packages (from datasets) (0.23.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.8/site-packages (from datasets) (1.21.4)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.8/site-packages (from datasets) (6.0)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.8/site-packages (from datasets) (1.3.4)\n",
      "Collecting dill<0.3.9,>=0.3.0\n",
      "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "\u001b[K     |████████████████████████████████| 116 kB 101.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.8/site-packages (from datasets) (4.62.3)\n",
      "Collecting aiosignal>=1.1.2\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Collecting async-timeout<5.0,>=4.0\n",
      "  Downloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Collecting multidict<7.0,>=4.5\n",
      "  Downloading multidict-6.0.5-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
      "\u001b[K     |████████████████████████████████| 129 kB 120.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting yarl<2.0,>=1.0\n",
      "  Downloading yarl-1.9.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (308 kB)\n",
      "\u001b[K     |████████████████████████████████| 308 kB 121.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets) (21.2.0)\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Downloading frozenlist-1.4.1-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (240 kB)\n",
      "\u001b[K     |████████████████████████████████| 240 kB 138.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.8/site-packages (from huggingface-hub>=0.21.2->datasets) (4.11.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging->datasets) (3.0.6)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests>=2.32.1->datasets) (3.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests>=2.32.1->datasets) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.8/site-packages (from requests>=2.32.1->datasets) (2.0.8)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests>=2.32.1->datasets) (1.26.7)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.8/site-packages (from pandas->datasets) (2021.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.8/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.16.0)\n",
      "Installing collected packages: multidict, frozenlist, yarl, async-timeout, aiosignal, requests, fsspec, dill, aiohttp, xxhash, pyarrow-hotfix, pyarrow, multiprocess, datasets\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.26.0\n",
      "    Uninstalling requests-2.26.0:\n",
      "      Successfully uninstalled requests-2.26.0\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2024.5.0\n",
      "    Uninstalling fsspec-2024.5.0:\n",
      "      Successfully uninstalled fsspec-2024.5.0\n",
      "  Attempting uninstall: pyarrow\n",
      "    Found existing installation: pyarrow 5.0.0\n",
      "    Uninstalling pyarrow-5.0.0:\n",
      "      Successfully uninstalled pyarrow-5.0.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchtext 0.12.0a0 requires torch==1.11.0a0+b6df043, but you have torch 2.3.0 which is incompatible.\u001b[0m\n",
      "Successfully installed aiohttp-3.9.5 aiosignal-1.3.1 async-timeout-4.0.3 datasets-2.19.2 dill-0.3.8 frozenlist-1.4.1 fsspec-2024.3.1 multidict-6.0.5 multiprocess-0.70.16 pyarrow-16.1.0 pyarrow-hotfix-0.6 requests-2.32.3 xxhash-3.4.1 yarl-1.9.4\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting jupyter\n",
      "  Downloading jupyter-1.0.0-py2.py3-none-any.whl (2.7 kB)\n",
      "Collecting ipywidgets\n",
      "  Downloading ipywidgets-8.1.3-py3-none-any.whl (139 kB)\n",
      "\u001b[K     |████████████████████████████████| 139 kB 10.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: nbconvert in /opt/conda/lib/python3.8/site-packages (from jupyter) (6.3.0)\n",
      "Requirement already satisfied: ipykernel in /opt/conda/lib/python3.8/site-packages (from jupyter) (6.6.0)\n",
      "Collecting qtconsole\n",
      "  Downloading qtconsole-5.5.2-py3-none-any.whl (123 kB)\n",
      "\u001b[K     |████████████████████████████████| 123 kB 98.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: notebook in /opt/conda/lib/python3.8/site-packages (from jupyter) (6.4.1)\n",
      "Collecting jupyter-console\n",
      "  Downloading jupyter_console-6.6.3-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (5.1.1)\n",
      "Collecting widgetsnbextension~=4.0.11\n",
      "  Downloading widgetsnbextension-4.0.11-py3-none-any.whl (2.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.3 MB 98.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting jupyterlab-widgets~=3.0.11\n",
      "  Downloading jupyterlab_widgets-3.0.11-py3-none-any.whl (214 kB)\n",
      "\u001b[K     |████████████████████████████████| 214 kB 152.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting comm>=0.1.3\n",
      "  Downloading comm-0.2.2-py3-none-any.whl (7.2 kB)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (7.30.0)\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (4.8.0)\n",
      "Requirement already satisfied: matplotlib-inline in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.3)\n",
      "Requirement already satisfied: backcall in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: pygments in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (2.10.0)\n",
      "Requirement already satisfied: pickleshare in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.7.5)\n",
      "Requirement already satisfied: decorator in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (5.1.0)\n",
      "Requirement already satisfied: setuptools>=18.5 in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (59.4.0)\n",
      "Requirement already satisfied: jedi>=0.16 in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.18.1)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.22)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /opt/conda/lib/python3.8/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.8/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.8/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=6.1.0->ipywidgets) (0.2.5)\n",
      "Requirement already satisfied: debugpy<2.0,>=1.0.0 in /opt/conda/lib/python3.8/site-packages (from ipykernel->jupyter) (1.5.1)\n",
      "Requirement already satisfied: tornado<7.0,>=4.2 in /opt/conda/lib/python3.8/site-packages (from ipykernel->jupyter) (6.1)\n",
      "Requirement already satisfied: jupyter-client<8.0 in /opt/conda/lib/python3.8/site-packages (from ipykernel->jupyter) (7.1.0)\n",
      "Requirement already satisfied: pyzmq>=13 in /opt/conda/lib/python3.8/site-packages (from jupyter-client<8.0->ipykernel->jupyter) (22.3.0)\n",
      "Requirement already satisfied: entrypoints in /opt/conda/lib/python3.8/site-packages (from jupyter-client<8.0->ipykernel->jupyter) (0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /opt/conda/lib/python3.8/site-packages (from jupyter-client<8.0->ipykernel->jupyter) (2.8.2)\n",
      "Requirement already satisfied: nest-asyncio>=1.5 in /opt/conda/lib/python3.8/site-packages (from jupyter-client<8.0->ipykernel->jupyter) (1.5.4)\n",
      "Requirement already satisfied: jupyter-core>=4.6.0 in /opt/conda/lib/python3.8/site-packages (from jupyter-client<8.0->ipykernel->jupyter) (4.9.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.8/site-packages (from python-dateutil>=2.1->jupyter-client<8.0->ipykernel->jupyter) (1.16.0)\n",
      "Collecting ipykernel\n",
      "  Downloading ipykernel-6.29.4-py3-none-any.whl (117 kB)\n",
      "\u001b[K     |████████████████████████████████| 117 kB 123.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0\n",
      "  Downloading prompt_toolkit-3.0.47-py3-none-any.whl (386 kB)\n",
      "\u001b[K     |████████████████████████████████| 386 kB 143.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting traitlets>=4.3.1\n",
      "  Downloading traitlets-5.14.3-py3-none-any.whl (85 kB)\n",
      "\u001b[K     |████████████████████████████████| 85 kB 124.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting jupyter-core>=4.6.0\n",
      "  Downloading jupyter_core-5.7.2-py3-none-any.whl (28 kB)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.8/site-packages (from ipykernel->jupyter) (21.3)\n",
      "Collecting pyzmq>=13\n",
      "  Downloading pyzmq-26.0.3-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (912 kB)\n",
      "\u001b[K     |████████████████████████████████| 912 kB 129.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting debugpy<2.0,>=1.0.0\n",
      "  Downloading debugpy-1.8.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.1 MB 96.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: psutil in /opt/conda/lib/python3.8/site-packages (from ipykernel->jupyter) (5.8.0)\n",
      "Collecting platformdirs>=2.5\n",
      "  Downloading platformdirs-4.2.2-py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: jinja2>=2.4 in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (3.0.3)\n",
      "Requirement already satisfied: jupyterlab-pygments in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (0.1.2)\n",
      "Requirement already satisfied: testpath in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (0.5.0)\n",
      "Requirement already satisfied: mistune<2,>=0.8.1 in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (0.8.4)\n",
      "Requirement already satisfied: defusedxml in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (0.7.1)\n",
      "Requirement already satisfied: nbclient<0.6.0,>=0.5.0 in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (0.5.9)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (1.5.0)\n",
      "Requirement already satisfied: bleach in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (4.1.0)\n",
      "Requirement already satisfied: nbformat>=4.4 in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (5.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.8/site-packages (from jinja2>=2.4->nbconvert->jupyter) (2.0.1)\n",
      "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /opt/conda/lib/python3.8/site-packages (from nbformat>=4.4->nbconvert->jupyter) (4.2.1)\n",
      "Requirement already satisfied: ipython-genutils in /opt/conda/lib/python3.8/site-packages (from nbformat>=4.4->nbconvert->jupyter) (0.2.0)\n",
      "Requirement already satisfied: importlib-resources>=1.4.0 in /opt/conda/lib/python3.8/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.4->nbconvert->jupyter) (5.4.0)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /opt/conda/lib/python3.8/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.4->nbconvert->jupyter) (0.18.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /opt/conda/lib/python3.8/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.4->nbconvert->jupyter) (21.2.0)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /opt/conda/lib/python3.8/site-packages (from importlib-resources>=1.4.0->jsonschema!=2.5.0,>=2.4->nbformat>=4.4->nbconvert->jupyter) (3.6.0)\n",
      "Requirement already satisfied: webencodings in /opt/conda/lib/python3.8/site-packages (from bleach->nbconvert->jupyter) (0.5.1)\n",
      "Requirement already satisfied: argon2-cffi in /opt/conda/lib/python3.8/site-packages (from notebook->jupyter) (21.1.0)\n",
      "Requirement already satisfied: terminado>=0.8.3 in /opt/conda/lib/python3.8/site-packages (from notebook->jupyter) (0.12.1)\n",
      "Requirement already satisfied: prometheus-client in /opt/conda/lib/python3.8/site-packages (from notebook->jupyter) (0.12.0)\n",
      "Requirement already satisfied: Send2Trash>=1.5.0 in /opt/conda/lib/python3.8/site-packages (from notebook->jupyter) (1.8.0)\n",
      "Requirement already satisfied: cffi>=1.0.0 in /opt/conda/lib/python3.8/site-packages (from argon2-cffi->notebook->jupyter) (1.15.0)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.8/site-packages (from cffi>=1.0.0->argon2-cffi->notebook->jupyter) (2.21)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging->ipykernel->jupyter) (3.0.6)\n",
      "Collecting qtpy>=2.4.0\n",
      "  Downloading QtPy-2.4.1-py3-none-any.whl (93 kB)\n",
      "\u001b[K     |████████████████████████████████| 93 kB 87.0 MB/s  eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: traitlets, platformdirs, pyzmq, jupyter-core, prompt-toolkit, debugpy, comm, widgetsnbextension, qtpy, jupyterlab-widgets, ipykernel, qtconsole, jupyter-console, ipywidgets, jupyter\n",
      "  Attempting uninstall: traitlets\n",
      "    Found existing installation: traitlets 5.1.1\n",
      "    Uninstalling traitlets-5.1.1:\n",
      "      Successfully uninstalled traitlets-5.1.1\n",
      "  Attempting uninstall: pyzmq\n",
      "    Found existing installation: pyzmq 22.3.0\n",
      "    Uninstalling pyzmq-22.3.0:\n",
      "      Successfully uninstalled pyzmq-22.3.0\n",
      "  Attempting uninstall: jupyter-core\n",
      "    Found existing installation: jupyter-core 4.9.1\n",
      "    Uninstalling jupyter-core-4.9.1:\n",
      "      Successfully uninstalled jupyter-core-4.9.1\n",
      "  Attempting uninstall: prompt-toolkit\n",
      "    Found existing installation: prompt-toolkit 3.0.22\n",
      "    Uninstalling prompt-toolkit-3.0.22:\n",
      "      Successfully uninstalled prompt-toolkit-3.0.22\n",
      "  Attempting uninstall: debugpy\n",
      "    Found existing installation: debugpy 1.5.1\n",
      "    Uninstalling debugpy-1.5.1:\n",
      "      Successfully uninstalled debugpy-1.5.1\n",
      "  Attempting uninstall: ipykernel\n",
      "    Found existing installation: ipykernel 6.6.0\n",
      "    Uninstalling ipykernel-6.6.0:\n",
      "      Successfully uninstalled ipykernel-6.6.0\n",
      "Successfully installed comm-0.2.2 debugpy-1.8.1 ipykernel-6.29.4 ipywidgets-8.1.3 jupyter-1.0.0 jupyter-console-6.6.3 jupyter-core-5.7.2 jupyterlab-widgets-3.0.11 platformdirs-4.2.2 prompt-toolkit-3.0.47 pyzmq-26.0.3 qtconsole-5.5.2 qtpy-2.4.1 traitlets-5.14.3 widgetsnbextension-4.0.11\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!pip install datasets\n",
    "!pip install --upgrade jupyter ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "779b7bd0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())\n",
    "#installing some libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd81de60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, GPT2LMHeadModel\n",
    "import torch.nn as nn\n",
    "# Assume model_name is the name of the Hugging Face model\n",
    "model_hf = AutoModelForCausalLM.from_pretrained(\"distilgpt2\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d23fb4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.compile(model_hf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5cb6178c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OptimizedModule(\n",
      "  (_orig_mod): GPT2LMHeadModel(\n",
      "    (transformer): GPT2Model(\n",
      "      (wte): Embedding(50257, 768)\n",
      "      (wpe): Embedding(1024, 768)\n",
      "      (drop): Dropout(p=0.1, inplace=False)\n",
      "      (h): ModuleList(\n",
      "        (0-5): 6 x GPT2Block(\n",
      "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): GPT2Attention(\n",
      "            (c_attn): Conv1D()\n",
      "            (c_proj): Conv1D()\n",
      "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): GPT2MLP(\n",
      "            (c_fc): Conv1D()\n",
      "            (c_proj): Conv1D()\n",
      "            (act): NewGELUActivation()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de6200c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OptimizedModule(\n",
       "  (_orig_mod): GPT2LMHeadModel(\n",
       "    (transformer): GPT2Model(\n",
       "      (wte): Embedding(50257, 768)\n",
       "      (wpe): Embedding(1024, 768)\n",
       "      (drop): Dropout(p=0.1, inplace=False)\n",
       "      (h): ModuleList(\n",
       "        (0-5): 6 x GPT2Block(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): GPT2Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): GPT2MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (act): NewGELUActivation()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4319c01b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformer.wte.weight torch.Size([50257, 768])\n",
      "transformer.wpe.weight torch.Size([1024, 768])\n",
      "transformer.h.0.ln_1.weight torch.Size([768])\n",
      "transformer.h.0.ln_1.bias torch.Size([768])\n",
      "transformer.h.0.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.0.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.0.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.0.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.0.ln_2.weight torch.Size([768])\n",
      "transformer.h.0.ln_2.bias torch.Size([768])\n",
      "transformer.h.0.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.0.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.0.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.0.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.1.ln_1.weight torch.Size([768])\n",
      "transformer.h.1.ln_1.bias torch.Size([768])\n",
      "transformer.h.1.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.1.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.1.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.1.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.1.ln_2.weight torch.Size([768])\n",
      "transformer.h.1.ln_2.bias torch.Size([768])\n",
      "transformer.h.1.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.1.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.1.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.1.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.2.ln_1.weight torch.Size([768])\n",
      "transformer.h.2.ln_1.bias torch.Size([768])\n",
      "transformer.h.2.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.2.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.2.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.2.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.2.ln_2.weight torch.Size([768])\n",
      "transformer.h.2.ln_2.bias torch.Size([768])\n",
      "transformer.h.2.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.2.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.2.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.2.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.3.ln_1.weight torch.Size([768])\n",
      "transformer.h.3.ln_1.bias torch.Size([768])\n",
      "transformer.h.3.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.3.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.3.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.3.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.3.ln_2.weight torch.Size([768])\n",
      "transformer.h.3.ln_2.bias torch.Size([768])\n",
      "transformer.h.3.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.3.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.3.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.3.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.4.ln_1.weight torch.Size([768])\n",
      "transformer.h.4.ln_1.bias torch.Size([768])\n",
      "transformer.h.4.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.4.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.4.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.4.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.4.ln_2.weight torch.Size([768])\n",
      "transformer.h.4.ln_2.bias torch.Size([768])\n",
      "transformer.h.4.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.4.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.4.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.4.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.5.ln_1.weight torch.Size([768])\n",
      "transformer.h.5.ln_1.bias torch.Size([768])\n",
      "transformer.h.5.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.5.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.5.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.5.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.5.ln_2.weight torch.Size([768])\n",
      "transformer.h.5.ln_2.bias torch.Size([768])\n",
      "transformer.h.5.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.5.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.5.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.5.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.ln_f.weight torch.Size([768])\n",
      "transformer.ln_f.bias torch.Size([768])\n",
      "lm_head.weight torch.Size([50257, 768])\n"
     ]
    }
   ],
   "source": [
    "sd_hf = model_hf.state_dict()\n",
    "for k,v in sd_hf.items():\n",
    "    print(k,v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ed88631",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-06-09 14:54:52,113] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "df: /root/.triton/autotune: No such file or directory\n",
      "/opt/conda/compiler_compat/ld: cannot find -laio\n",
      "collect2: error: ld returned 1 exit status\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93m [WARNING] \u001b[0m async_io requires the dev libaio .so object and headers but these were not found.\n",
      "\u001b[93m [WARNING] \u001b[0m async_io: please install the libaio-dev package with apt\n",
      "\u001b[93m [WARNING] \u001b[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\n",
      "\u001b[93m [WARNING] \u001b[0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH\n",
      "\u001b[93m [WARNING] \u001b[0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3\n",
      "\u001b[93m [WARNING] \u001b[0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible\n"
     ]
    }
   ],
   "source": [
    "from deepspeed.runtime.zero.stage3 import estimate_zero3_model_states_mem_needs_all_live"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6757aa93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated memory needed for params, optim states and gradients for a:\n",
      "HW: Setup with 1 node, 1 GPU per node.\n",
      "SW: Model with 81M total params, 38M largest layer params.\n",
      "  per CPU  |  per GPU |   Options\n",
      "    2.06GB |   0.14GB | offload_param=cpu , offload_optimizer=cpu , zero_init=1\n",
      "    2.06GB |   0.14GB | offload_param=cpu , offload_optimizer=cpu , zero_init=0\n",
      "    1.83GB |   0.30GB | offload_param=none, offload_optimizer=cpu , zero_init=1\n",
      "    1.83GB |   0.30GB | offload_param=none, offload_optimizer=cpu , zero_init=0\n",
      "    0.22GB |   1.52GB | offload_param=none, offload_optimizer=none, zero_init=1\n",
      "    0.46GB |   1.52GB | offload_param=none, offload_optimizer=none, zero_init=0\n"
     ]
    }
   ],
   "source": [
    "estimate_zero3_model_states_mem_needs_all_live(model, num_gpus_per_node=1, num_nodes=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15b02061",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-06-09 19:01:54,338] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "\u001b[93m [WARNING] \u001b[0m async_io requires the dev libaio .so object and headers but these were not found.\n",
      "\u001b[93m [WARNING] \u001b[0m async_io: please install the libaio-dev package with apt\n",
      "\u001b[93m [WARNING] \u001b[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\n",
      "\u001b[93m [WARNING] \u001b[0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/compiler_compat/ld: cannot find -laio\n",
      "collect2: error: ld returned 1 exit status\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93m [WARNING] \u001b[0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3\n",
      "\u001b[93m [WARNING] \u001b[0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible\n"
     ]
    }
   ],
   "source": [
    "import deepspeed\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c7dd1d0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\u001b[4mGPU0\tCPU Affinity\tNUMA Affinity\tGPU NUMA ID\u001b[0m\n",
      "GPU0\t X \t0-23\t\tN/A\t\tN/A\n",
      "\n",
      "Legend:\n",
      "\n",
      "  X    = Self\n",
      "  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n",
      "  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n",
      "  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n",
      "  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n",
      "  PIX  = Connection traversing at most a single PCIe bridge\n",
      "  NV#  = Connection traversing a bonded set of # NVLinks\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi topo --matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f975b23",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error initializing torch.distributed using env:// rendezvous: environment variable RANK expected, but not set",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_129/1588463775.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Initialize process group for distributed training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mdist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_process_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'nccl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Define your model, dataset, dataloader, optimizer, and criterion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/distributed/c10d_logger.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_P\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_P\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0m_T\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0mmsg_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_msg_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/distributed/c10d_logger.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_P\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_P\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0m_T\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0mt1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime_ns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         \u001b[0mfunc_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m         \u001b[0mtime_spent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime_ns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py\u001b[0m in \u001b[0;36minit_process_group\u001b[0;34m(backend, init_method, timeout, world_size, rank, store, group_name, pg_options, device_id)\u001b[0m\n\u001b[1;32m   1303\u001b[0m                 \u001b[0mnot_none\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_method\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrank\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworld_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m             )\n\u001b[0;32m-> 1305\u001b[0;31m             \u001b[0mstore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrank\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworld_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrendezvous_iterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1306\u001b[0m             \u001b[0mstore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_timeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/distributed/rendezvous.py\u001b[0m in \u001b[0;36m_env_rendezvous_handler\u001b[0;34m(url, timeout, **kwargs)\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0mrank\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"rank\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m         \u001b[0mrank\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_get_env_or_raise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"RANK\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m\"world_size\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mquery_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/distributed/rendezvous.py\u001b[0m in \u001b[0;36m_get_env_or_raise\u001b[0;34m(env_var)\u001b[0m\n\u001b[1;32m    217\u001b[0m         \u001b[0menv_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0menv_val\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 219\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0m_env_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv_var\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    220\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0menv_val\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error initializing torch.distributed using env:// rendezvous: environment variable RANK expected, but not set"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import deepspeed\n",
    "from torch.utils.data import DataLoader, DistributedSampler\n",
    "from torchvision import datasets, transforms\n",
    "import torch.distributed as dist\n",
    "\n",
    "# Initialize process group for distributed training\n",
    "dist.init_process_group(backend='nccl')\n",
    "\n",
    "# Define your model, dataset, dataloader, optimizer, and criterion\n",
    "class MyModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.layer = torch.nn.Linear(10, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layer(x)\n",
    "\n",
    "model = MyModel()\n",
    "\n",
    "# Define optimizer and loss function\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Define DeepSpeed configuration\n",
    "ds_config = {\n",
    "    \"train_batch_size\": 32 * 4 * dist.get_world_size(),  # batch_size * gradient_accumulation_steps * num_GPUs\n",
    "    \"gradient_accumulation_steps\": 4,\n",
    "    \"fp16\": {\n",
    "        \"enabled\": True\n",
    "    },\n",
    "    \"zero_optimization\": {\n",
    "        \"stage\": 1\n",
    "    },\n",
    "}\n",
    "\n",
    "# Initialize DeepSpeed\n",
    "model, optimizer, _, _ = deepspeed.initialize(model=model, optimizer=optimizer, config=ds_config)\n",
    "\n",
    "# Create dataset and dataloader\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "dataset = datasets.CIFAR10(root='./data', train=True, transform=transform, download=True)\n",
    "sampler = DistributedSampler(dataset)\n",
    "dataloader = DataLoader(dataset, batch_size=32, sampler=sampler, num_workers=8, pin_memory=True)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(dataloader):\n",
    "        inputs, labels = data[0].to(model.local_rank), data[1].to(model.local_rank)\n",
    "\n",
    "        # Zero gradients\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss = loss / 4  # Normalize the loss for gradient accumulation\n",
    "\n",
    "        # Backward pass with DeepSpeed\n",
    "        model.backward(loss)\n",
    "\n",
    "        # Step the optimizer with DeepSpeed\n",
    "        model.step()\n",
    "\n",
    "        running_loss += loss.item() * 4\n",
    "\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {running_loss / len(dataloader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55a05aee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-06-09 19:02:32,805] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.14.2, git-hash=unknown, git-branch=unknown\n",
      "[2024-06-09 19:02:32,807] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-06-09 19:02:32,807] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...\n",
      "[2024-06-09 19:02:34,125] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=172.18.0.3, master_port=29500\n",
      "[2024-06-09 19:02:34,127] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "[2024-06-09 19:02:35,153] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2024-06-09 19:02:35,154] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n",
      "[2024-06-09 19:02:35,155] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2024-06-09 19:02:35,156] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = Adam\n",
      "[2024-06-09 19:02:35,157] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=Adam type=<class 'torch.optim.adam.Adam'>\n",
      "[2024-06-09 19:02:35,157] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 1 optimizer\n",
      "[2024-06-09 19:02:35,158] [INFO] [stage_1_and_2.py:148:__init__] Reduce bucket size 500,000,000\n",
      "[2024-06-09 19:02:35,158] [INFO] [stage_1_and_2.py:149:__init__] Allgather bucket size 500,000,000\n",
      "[2024-06-09 19:02:35,159] [INFO] [stage_1_and_2.py:150:__init__] CPU Offload: False\n",
      "[2024-06-09 19:02:35,159] [INFO] [stage_1_and_2.py:151:__init__] Round robin gradient partitioning: False\n",
      "[2024-06-09 19:02:35,522] [INFO] [utils.py:779:see_memory_usage] Before initializing optimizer states\n",
      "[2024-06-09 19:02:35,523] [INFO] [utils.py:780:see_memory_usage] MA 0.46 GB         Max_MA 0.62 GB         CA 0.62 GB         Max_CA 1 GB \n",
      "[2024-06-09 19:02:35,524] [INFO] [utils.py:787:see_memory_usage] CPU Virtual Memory:  used = 9.37 GB, percent = 1.1%\n",
      "[2024-06-09 19:02:35,691] [INFO] [utils.py:779:see_memory_usage] After initializing optimizer states\n",
      "[2024-06-09 19:02:35,692] [INFO] [utils.py:780:see_memory_usage] MA 0.46 GB         Max_MA 0.77 GB         CA 0.93 GB         Max_CA 1 GB \n",
      "[2024-06-09 19:02:35,693] [INFO] [utils.py:787:see_memory_usage] CPU Virtual Memory:  used = 9.37 GB, percent = 1.1%\n",
      "[2024-06-09 19:02:35,693] [INFO] [stage_1_and_2.py:543:__init__] optimizer state initialized\n",
      "[2024-06-09 19:02:35,804] [INFO] [utils.py:779:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2024-06-09 19:02:35,806] [INFO] [utils.py:780:see_memory_usage] MA 0.46 GB         Max_MA 0.46 GB         CA 0.93 GB         Max_CA 1 GB \n",
      "[2024-06-09 19:02:35,807] [INFO] [utils.py:787:see_memory_usage] CPU Virtual Memory:  used = 9.37 GB, percent = 1.1%\n",
      "[2024-06-09 19:02:35,814] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = Adam\n",
      "[2024-06-09 19:02:35,815] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler\n",
      "[2024-06-09 19:02:35,815] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n",
      "[2024-06-09 19:02:35,816] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.001], mom=[(0.9, 0.999)]\n",
      "[2024-06-09 19:02:35,816] [INFO] [config.py:996:print] DeepSpeedEngine configuration:\n",
      "[2024-06-09 19:02:35,817] [INFO] [config.py:1000:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2024-06-09 19:02:35,817] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2024-06-09 19:02:35,818] [INFO] [config.py:1000:print]   amp_enabled .................. False\n",
      "[2024-06-09 19:02:35,818] [INFO] [config.py:1000:print]   amp_params ................... False\n",
      "[2024-06-09 19:02:35,818] [INFO] [config.py:1000:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2024-06-09 19:02:35,819] [INFO] [config.py:1000:print]   bfloat16_enabled ............. False\n",
      "[2024-06-09 19:02:35,819] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False\n",
      "[2024-06-09 19:02:35,820] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2024-06-09 19:02:35,820] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True\n",
      "[2024-06-09 19:02:35,821] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False\n",
      "[2024-06-09 19:02:35,821] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f734bf72130>\n",
      "[2024-06-09 19:02:35,821] [INFO] [config.py:1000:print]   communication_data_type ...... None\n",
      "[2024-06-09 19:02:35,822] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}\n",
      "[2024-06-09 19:02:35,822] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2024-06-09 19:02:35,823] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False\n",
      "[2024-06-09 19:02:35,823] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False\n",
      "[2024-06-09 19:02:35,823] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2024-06-09 19:02:35,824] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False\n",
      "[2024-06-09 19:02:35,824] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False\n",
      "[2024-06-09 19:02:35,824] [INFO] [config.py:1000:print]   disable_allgather ............ False\n",
      "[2024-06-09 19:02:35,825] [INFO] [config.py:1000:print]   dump_state ................... False\n",
      "[2024-06-09 19:02:35,825] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None\n",
      "[2024-06-09 19:02:35,825] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False\n",
      "[2024-06-09 19:02:35,826] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2024-06-09 19:02:35,826] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2024-06-09 19:02:35,827] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0\n",
      "[2024-06-09 19:02:35,827] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100\n",
      "[2024-06-09 19:02:35,827] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06\n",
      "[2024-06-09 19:02:35,828] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01\n",
      "[2024-06-09 19:02:35,828] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False\n",
      "[2024-06-09 19:02:35,829] [INFO] [config.py:1000:print]   elasticity_enabled ........... False\n",
      "[2024-06-09 19:02:35,829] [INFO] [config.py:1000:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2024-06-09 19:02:35,829] [INFO] [config.py:1000:print]   fp16_auto_cast ............... False\n",
      "[2024-06-09 19:02:35,830] [INFO] [config.py:1000:print]   fp16_enabled ................. True\n",
      "[2024-06-09 19:02:35,830] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False\n",
      "[2024-06-09 19:02:35,831] [INFO] [config.py:1000:print]   global_rank .................. 0\n",
      "[2024-06-09 19:02:35,831] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None\n",
      "[2024-06-09 19:02:35,831] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 4\n",
      "[2024-06-09 19:02:35,832] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0\n",
      "[2024-06-09 19:02:35,832] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0\n",
      "[2024-06-09 19:02:35,832] [INFO] [config.py:1000:print]   graph_harvesting ............. False\n",
      "[2024-06-09 19:02:35,833] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2024-06-09 19:02:35,833] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 65536\n",
      "[2024-06-09 19:02:35,833] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False\n",
      "[2024-06-09 19:02:35,834] [INFO] [config.py:1000:print]   loss_scale ................... 0\n",
      "[2024-06-09 19:02:35,834] [INFO] [config.py:1000:print]   memory_breakdown ............. False\n",
      "[2024-06-09 19:02:35,835] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False\n",
      "[2024-06-09 19:02:35,835] [INFO] [config.py:1000:print]   mics_shard_size .............. -1\n",
      "[2024-06-09 19:02:35,835] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
      "[2024-06-09 19:02:35,836] [INFO] [config.py:1000:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2024-06-09 19:02:35,836] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False\n",
      "[2024-06-09 19:02:35,836] [INFO] [config.py:1000:print]   optimizer_name ............... None\n",
      "[2024-06-09 19:02:35,837] [INFO] [config.py:1000:print]   optimizer_params ............. None\n",
      "[2024-06-09 19:02:35,837] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2024-06-09 19:02:35,837] [INFO] [config.py:1000:print]   pld_enabled .................. False\n",
      "[2024-06-09 19:02:35,838] [INFO] [config.py:1000:print]   pld_params ................... False\n",
      "[2024-06-09 19:02:35,838] [INFO] [config.py:1000:print]   prescale_gradients ........... False\n",
      "[2024-06-09 19:02:35,838] [INFO] [config.py:1000:print]   scheduler_name ............... None\n",
      "[2024-06-09 19:02:35,839] [INFO] [config.py:1000:print]   scheduler_params ............. None\n",
      "[2024-06-09 19:02:35,839] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2024-06-09 19:02:35,839] [INFO] [config.py:1000:print]   sparse_attention ............. None\n",
      "[2024-06-09 19:02:35,840] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False\n",
      "[2024-06-09 19:02:35,840] [INFO] [config.py:1000:print]   steps_per_print .............. 10\n",
      "[2024-06-09 19:02:35,841] [INFO] [config.py:1000:print]   train_batch_size ............. 512\n",
      "[2024-06-09 19:02:35,841] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  128\n",
      "[2024-06-09 19:02:35,841] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False\n",
      "[2024-06-09 19:02:35,842] [INFO] [config.py:1000:print]   use_node_local_storage ....... False\n",
      "[2024-06-09 19:02:35,842] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False\n",
      "[2024-06-09 19:02:35,843] [INFO] [config.py:1000:print]   weight_quantization_config ... None\n",
      "[2024-06-09 19:02:35,843] [INFO] [config.py:1000:print]   world_size ................... 1\n",
      "[2024-06-09 19:02:35,843] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  False\n",
      "[2024-06-09 19:02:35,844] [INFO] [config.py:1000:print]   zero_config .................. stage=1 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
      "[2024-06-09 19:02:35,844] [INFO] [config.py:1000:print]   zero_enabled ................. True\n",
      "[2024-06-09 19:02:35,844] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2024-06-09 19:02:35,845] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 1\n",
      "[2024-06-09 19:02:35,845] [INFO] [config.py:986:print_user_config]   json = {\n",
      "    \"train_batch_size\": 512, \n",
      "    \"gradient_accumulation_steps\": 4, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": true\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 1\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "ds_config = {\n",
    "    \"train_batch_size\": 32 * 4 * 4,  # batch_size * gradient_accumulation_steps * num_GPUs\n",
    "    \"gradient_accumulation_steps\": 4,\n",
    "    \"fp16\": {\n",
    "        \"enabled\": True\n",
    "    },\n",
    "    \"zero_optimization\": {\n",
    "        \"stage\": 1\n",
    "    },\n",
    "}\n",
    "model = AutoModelForCausalLM.from_pretrained(\"distilgpt2\")\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Initialize DeepSpeed\n",
    "model, optimizer, _, _ = deepspeed.initialize(model=model, optimizer=optimizer, config=ds_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3046dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(params =  model.parameters(), lr= 5e-5)\n",
    "#model.to(device)\n",
    "#m = nn.Softmax()\n",
    "#import time\n",
    "#scheduler = transformers.get_cosine_schedule_with_warmup( optimizer= optimizer, num_warmup_steps =len(train_loader)*num_epoch*.1 ,num_training_steps= len(train_loader)*num_epoch,last_epoch = -1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dcd094d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import deepspeed.runtime.lr_schedules as sch\n",
    "scheduler = sch.WarmupCosineLR(optimizer= optimizer, warmup_num_steps = 90000*15*.1 ,total_num_steps = 90000*15 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21558b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"scheduler\": {\n",
    "      \"type\": \"WarmupCosineLR\",\n",
    "      \"params\": {\n",
    "          \"warmup_num_steps\": len(train_loader)*num_epoch*.1,   \n",
    "          \"total_num_steps\": len(train_loader)*num_epoch\n",
    "      }\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "577b52be",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_355/4200220046.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;34m\"optimizer\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"type\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"Adam\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"params\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"lr\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m5e-5\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \"scheduler\": {\"type\": \"WarmupCosineLR\",\n\u001b[0;32m----> 8\u001b[0;31m                   \u001b[0;34m\"params\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"warmup_num_steps\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnum_epoch\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"total_num_steps\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnum_epoch\u001b[0m \u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m                  },\n\u001b[1;32m     10\u001b[0m     \"fp16\": {\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_loader' is not defined"
     ]
    }
   ],
   "source": [
    "ds_config = {\n",
    "    \"train_batch_size\": 64 ,  # batch_size * gradient_accumulation_steps * num_GPUs\n",
    "    \"gradient_accumulation_steps\": 4,\n",
    "    \"fp16\": {\"enabled\": True},\n",
    "    \"zero_optimization\": {\"stage\": 1},\n",
    "    \"optimizer\": {\"type\": \"Adam\",\"params\": {\"lr\": 5e-5} },\n",
    "    \"scheduler\": {\"type\": \"WarmupCosineLR\",\n",
    "                  \"params\": {\"warmup_num_steps\": len(train_loader)*num_epoch*.1, \"total_num_steps\": len(train_loader)*num_epoch }\n",
    "                 },\n",
    "    \"fp16\": {\n",
    "    \"enabled\": true,\n",
    "    \"auto_cast\": false,\n",
    "    \"loss_scale\": 0,\n",
    "    \"initial_scale_power\": 16,\n",
    "    \"loss_scale_window\": 1000,\n",
    "    \"hysteresis\": 2,\n",
    "    \"consecutive_hysteresis\": false,\n",
    "    \"min_loss_scale\": 1\n",
    "},\n",
    "    \"zero_optimization\": {\n",
    "    \"stage\": 2\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c92852",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
