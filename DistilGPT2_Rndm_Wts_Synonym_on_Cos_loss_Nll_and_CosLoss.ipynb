{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c1ecf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## reference https://huggingface.co/learn/nlp-course/en/chapter7/6?fw=pt#training-a-causal-language-model-from-scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719e61cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "acb5b452",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting jupyter\n",
      "  Downloading jupyter-1.0.0-py2.py3-none-any.whl (2.7 kB)\n",
      "Collecting ipywidgets\n",
      "  Downloading ipywidgets-8.1.3-py3-none-any.whl (139 kB)\n",
      "\u001b[K     |████████████████████████████████| 139 kB 9.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting qtconsole\n",
      "  Downloading qtconsole-5.5.2-py3-none-any.whl (123 kB)\n",
      "\u001b[K     |████████████████████████████████| 123 kB 68.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting jupyter-console\n",
      "  Downloading jupyter_console-6.6.3-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: notebook in /opt/conda/lib/python3.8/site-packages (from jupyter) (6.4.1)\n",
      "Requirement already satisfied: nbconvert in /opt/conda/lib/python3.8/site-packages (from jupyter) (6.3.0)\n",
      "Requirement already satisfied: ipykernel in /opt/conda/lib/python3.8/site-packages (from jupyter) (6.6.0)\n",
      "Collecting comm>=0.1.3\n",
      "  Downloading comm-0.2.2-py3-none-any.whl (7.2 kB)\n",
      "Collecting widgetsnbextension~=4.0.11\n",
      "  Downloading widgetsnbextension-4.0.11-py3-none-any.whl (2.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.3 MB 161.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: ipython>=6.1.0 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (7.30.0)\n",
      "Collecting jupyterlab-widgets~=3.0.11\n",
      "  Downloading jupyterlab_widgets-3.0.11-py3-none-any.whl (214 kB)\n",
      "\u001b[K     |████████████████████████████████| 214 kB 148.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: traitlets>=4.3.1 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.18.1)\n",
      "Requirement already satisfied: decorator in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (5.1.0)\n",
      "Requirement already satisfied: backcall in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: pygments in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (2.10.0)\n",
      "Requirement already satisfied: matplotlib-inline in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.3)\n",
      "Requirement already satisfied: setuptools>=18.5 in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (59.4.0)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.22)\n",
      "Requirement already satisfied: pickleshare in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.7.5)\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (4.8.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /opt/conda/lib/python3.8/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.8/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.8/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=6.1.0->ipywidgets) (0.2.5)\n",
      "Requirement already satisfied: debugpy<2.0,>=1.0.0 in /opt/conda/lib/python3.8/site-packages (from ipykernel->jupyter) (1.5.1)\n",
      "Requirement already satisfied: jupyter-client<8.0 in /opt/conda/lib/python3.8/site-packages (from ipykernel->jupyter) (7.1.0)\n",
      "Requirement already satisfied: tornado<7.0,>=4.2 in /opt/conda/lib/python3.8/site-packages (from ipykernel->jupyter) (6.1)\n",
      "Requirement already satisfied: entrypoints in /opt/conda/lib/python3.8/site-packages (from jupyter-client<8.0->ipykernel->jupyter) (0.3)\n",
      "Requirement already satisfied: pyzmq>=13 in /opt/conda/lib/python3.8/site-packages (from jupyter-client<8.0->ipykernel->jupyter) (22.3.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /opt/conda/lib/python3.8/site-packages (from jupyter-client<8.0->ipykernel->jupyter) (2.8.2)\n",
      "Requirement already satisfied: jupyter-core>=4.6.0 in /opt/conda/lib/python3.8/site-packages (from jupyter-client<8.0->ipykernel->jupyter) (4.9.1)\n",
      "Requirement already satisfied: nest-asyncio>=1.5 in /opt/conda/lib/python3.8/site-packages (from jupyter-client<8.0->ipykernel->jupyter) (1.5.4)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.8/site-packages (from python-dateutil>=2.1->jupyter-client<8.0->ipykernel->jupyter) (1.16.0)\n",
      "Collecting prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0\n",
      "  Downloading prompt_toolkit-3.0.47-py3-none-any.whl (386 kB)\n",
      "\u001b[K     |████████████████████████████████| 386 kB 98.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting ipykernel\n",
      "  Downloading ipykernel-6.29.5-py3-none-any.whl (117 kB)\n",
      "\u001b[K     |████████████████████████████████| 117 kB 112.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting traitlets>=4.3.1\n",
      "  Downloading traitlets-5.14.3-py3-none-any.whl (85 kB)\n",
      "\u001b[K     |████████████████████████████████| 85 kB 107.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting jupyter-core>=4.6.0\n",
      "  Downloading jupyter_core-5.7.2-py3-none-any.whl (28 kB)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.8/site-packages (from ipykernel->jupyter) (5.8.0)\n",
      "Collecting debugpy<2.0,>=1.0.0\n",
      "  Downloading debugpy-1.8.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.1 MB 111.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: packaging in /opt/conda/lib/python3.8/site-packages (from ipykernel->jupyter) (21.3)\n",
      "Collecting pyzmq>=13\n",
      "  Downloading pyzmq-26.0.3-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (912 kB)\n",
      "\u001b[K     |████████████████████████████████| 912 kB 106.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting platformdirs>=2.5\n",
      "  Downloading platformdirs-4.2.2-py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: jupyterlab-pygments in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (0.1.2)\n",
      "Requirement already satisfied: nbclient<0.6.0,>=0.5.0 in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (0.5.9)\n",
      "Requirement already satisfied: mistune<2,>=0.8.1 in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (0.8.4)\n",
      "Requirement already satisfied: bleach in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (4.1.0)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (1.5.0)\n",
      "Requirement already satisfied: testpath in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (0.5.0)\n",
      "Requirement already satisfied: jinja2>=2.4 in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (3.0.3)\n",
      "Requirement already satisfied: nbformat>=4.4 in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (5.1.3)\n",
      "Requirement already satisfied: defusedxml in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (0.7.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.8/site-packages (from jinja2>=2.4->nbconvert->jupyter) (2.0.1)\n",
      "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /opt/conda/lib/python3.8/site-packages (from nbformat>=4.4->nbconvert->jupyter) (4.2.1)\n",
      "Requirement already satisfied: ipython-genutils in /opt/conda/lib/python3.8/site-packages (from nbformat>=4.4->nbconvert->jupyter) (0.2.0)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /opt/conda/lib/python3.8/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.4->nbconvert->jupyter) (0.18.0)\n",
      "Requirement already satisfied: importlib-resources>=1.4.0 in /opt/conda/lib/python3.8/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.4->nbconvert->jupyter) (5.4.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /opt/conda/lib/python3.8/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.4->nbconvert->jupyter) (21.2.0)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /opt/conda/lib/python3.8/site-packages (from importlib-resources>=1.4.0->jsonschema!=2.5.0,>=2.4->nbformat>=4.4->nbconvert->jupyter) (3.6.0)\n",
      "Requirement already satisfied: webencodings in /opt/conda/lib/python3.8/site-packages (from bleach->nbconvert->jupyter) (0.5.1)\n",
      "Requirement already satisfied: Send2Trash>=1.5.0 in /opt/conda/lib/python3.8/site-packages (from notebook->jupyter) (1.8.0)\n",
      "Requirement already satisfied: prometheus-client in /opt/conda/lib/python3.8/site-packages (from notebook->jupyter) (0.12.0)\n",
      "Requirement already satisfied: terminado>=0.8.3 in /opt/conda/lib/python3.8/site-packages (from notebook->jupyter) (0.12.1)\n",
      "Requirement already satisfied: argon2-cffi in /opt/conda/lib/python3.8/site-packages (from notebook->jupyter) (21.1.0)\n",
      "Requirement already satisfied: cffi>=1.0.0 in /opt/conda/lib/python3.8/site-packages (from argon2-cffi->notebook->jupyter) (1.15.0)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.8/site-packages (from cffi>=1.0.0->argon2-cffi->notebook->jupyter) (2.21)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging->ipykernel->jupyter) (3.0.6)\n",
      "Collecting qtpy>=2.4.0\n",
      "  Downloading QtPy-2.4.1-py3-none-any.whl (93 kB)\n",
      "\u001b[K     |████████████████████████████████| 93 kB 80.1 MB/s  eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: traitlets, platformdirs, pyzmq, jupyter-core, prompt-toolkit, debugpy, comm, widgetsnbextension, qtpy, jupyterlab-widgets, ipykernel, qtconsole, jupyter-console, ipywidgets, jupyter\n",
      "  Attempting uninstall: traitlets\n",
      "    Found existing installation: traitlets 5.1.1\n",
      "    Uninstalling traitlets-5.1.1:\n",
      "      Successfully uninstalled traitlets-5.1.1\n",
      "  Attempting uninstall: pyzmq\n",
      "    Found existing installation: pyzmq 22.3.0\n",
      "    Uninstalling pyzmq-22.3.0:\n",
      "      Successfully uninstalled pyzmq-22.3.0\n",
      "  Attempting uninstall: jupyter-core\n",
      "    Found existing installation: jupyter-core 4.9.1\n",
      "    Uninstalling jupyter-core-4.9.1:\n",
      "      Successfully uninstalled jupyter-core-4.9.1\n",
      "  Attempting uninstall: prompt-toolkit\n",
      "    Found existing installation: prompt-toolkit 3.0.22\n",
      "    Uninstalling prompt-toolkit-3.0.22:\n",
      "      Successfully uninstalled prompt-toolkit-3.0.22\n",
      "  Attempting uninstall: debugpy\n",
      "    Found existing installation: debugpy 1.5.1\n",
      "    Uninstalling debugpy-1.5.1:\n",
      "      Successfully uninstalled debugpy-1.5.1\n",
      "  Attempting uninstall: ipykernel\n",
      "    Found existing installation: ipykernel 6.6.0\n",
      "    Uninstalling ipykernel-6.6.0:\n",
      "      Successfully uninstalled ipykernel-6.6.0\n",
      "Successfully installed comm-0.2.2 debugpy-1.8.2 ipykernel-6.29.5 ipywidgets-8.1.3 jupyter-1.0.0 jupyter-console-6.6.3 jupyter-core-5.7.2 jupyterlab-widgets-3.0.11 platformdirs-4.2.2 prompt-toolkit-3.0.47 pyzmq-26.0.3 qtconsole-5.5.2 qtpy-2.4.1 traitlets-5.14.3 widgetsnbextension-4.0.11\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#installing some libraries\n",
    "#!pip install datasets\n",
    "!pip install --upgrade jupyter ipywidgets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1f42b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch, transformers\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import PreTrainedModel, PretrainedConfig\n",
    "from transformers import AutoModel, AutoConfig,AutoModelForCausalLM, AutoConfig,GPT2Config,GPT2Tokenizer\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "#from datasets import load_dataset\n",
    "import pandas as pd, numpy as np\n",
    "from torch.cuda.amp import autocast\n",
    "from torch import cuda\n",
    "import datetime\n",
    "import warnings,itertools\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "import json\n",
    "# Ignore all warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "#pip install transformers bitsandbytes>=0.39.0 -q\n",
    "import zipfile,logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bdec7962",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "import random\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24aa9c26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9498681",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "model\n"
     ]
    }
   ],
   "source": [
    "#global params for training\n",
    "\n",
    "B,T = 64,1024\n",
    "epoch = 100\n",
    "random_init_wts = True\n",
    "min_text_len = 0\n",
    "# hard coded com\n",
    "comp_ratio = 3\n",
    "# train_loss_list = []\n",
    "# val_loss_list =[]\n",
    "if cuda.is_available():\n",
    "    device = torch.device('cuda:0')\n",
    "    print(device)\n",
    "else:\n",
    "    device = 'cpu'\n",
    "#print(device)\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "#os.environ[\"MKL_DEBUG_CPU_TYPE\"] = \"5\"\n",
    "\n",
    "#print(global_tr_loss)\n",
    "model_path = os.path.join(\"model\")\n",
    "print(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af3e2ad6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./data/unzip_text_10M'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "directory = os.path.join('.','data','unzip_text_10M')  # Replace with your directory path\n",
    "directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b247f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_text(directory):\n",
    "    directory = os.path.join('.','data','unzip_text_10M',str(directory))  # Replace with your directory path\n",
    "    print(f\"directory :{directory}\")\n",
    "    # List all files in the directory\n",
    "    files = [f for f in os.listdir(directory) if os.path.isfile(os.path.join(directory, f))]\n",
    "    print(f\"files:{files}\")\n",
    "    text_content = []\n",
    "    # Read each file\n",
    "    total_lines = 0\n",
    "    for filenum,filename in enumerate(files):\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            #first_line = file.read()\n",
    "            #print(f\"filename :{filename}->first few lines {first_line}\")\n",
    "            #continue\n",
    "            #lines_list = [line.strip() for line in open(file_path, 'r')]\n",
    "            text = file.read()\n",
    "            text_content.append(text)\n",
    "            print(f\"the file:{filename} has been appeneded to the uber list and its length is {len(text_content)} \")\n",
    "            #total_lines+=len(lines_list)\n",
    "            #text_content.append(lines_list)\n",
    "    \n",
    "    flattened_list = ''.join(text_content)\n",
    "    assert (len(flattened_list) == total_lines , f\"Expected {len(flattened_list)} to be equal to {total_lines}\" )\n",
    "    \n",
    "    return flattened_list\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "81a01d4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "directory :./data/unzip_text_10M/train_10M\n",
      "files:['switchboard.train', 'simple_wiki.train', 'open_subtitles.train', 'gutenberg.train', 'childes.train', 'bnc_spoken.train']\n",
      "the file:switchboard.train has been appeneded to the uber list and its length is 1 \n",
      "the file:simple_wiki.train has been appeneded to the uber list and its length is 2 \n",
      "the file:open_subtitles.train has been appeneded to the uber list and its length is 3 \n",
      "the file:gutenberg.train has been appeneded to the uber list and its length is 4 \n",
      "the file:childes.train has been appeneded to the uber list and its length is 5 \n",
      "the file:bnc_spoken.train has been appeneded to the uber list and its length is 6 \n"
     ]
    }
   ],
   "source": [
    "train_list = read_text(\"train_10M\")\n",
    "#print(train_dict)\n",
    "#val_list = read_text(\"dev\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ad1fc59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54215049"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "597387c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "827\n"
     ]
    }
   ],
   "source": [
    "chunks = len(train_list)//(B*T)\n",
    "print(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9f5f7cf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.010635852813720703,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "tokenizer_config.json",
       "rate": null,
       "total": 26,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72bd2e1e4ad64199af0a310e5cd75448",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.010523080825805664,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "vocab.json",
       "rate": null,
       "total": 1042301,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "316e2f615b3e4e85aba309b92ffb0995",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.010157346725463867,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "merges.txt",
       "rate": null,
       "total": 456318,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68469f4f414d488791710f0efc58c73f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.011482954025268555,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "tokenizer.json",
       "rate": null,
       "total": 1355256,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc8cf3e00dcd4cf4b17db004026aaceb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.010059833526611328,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "config.json",
       "rate": null,
       "total": 762,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f419f11135fc4c14bbd2c5ab3b7ca6a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/762 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(\"distilgpt2\")\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "250dde93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50257\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.pad_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e26ce9ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "directory :./data/unzip_text_10M/dev\n",
      "files:['switchboard.dev', 'simple_wiki.dev', 'open_subtitles.dev', 'gutenberg.dev', 'childes.dev', 'bnc_spoken.dev']\n",
      "the file:switchboard.dev has been appeneded to the uber list and its length is 1 \n",
      "the file:simple_wiki.dev has been appeneded to the uber list and its length is 2 \n",
      "the file:open_subtitles.dev has been appeneded to the uber list and its length is 3 \n",
      "the file:gutenberg.dev has been appeneded to the uber list and its length is 4 \n",
      "the file:childes.dev has been appeneded to the uber list and its length is 5 \n",
      "the file:bnc_spoken.dev has been appeneded to the uber list and its length is 6 \n"
     ]
    }
   ],
   "source": [
    "val_list = read_text(\"dev\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0d675b4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50256\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f5d94979",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9338524209088745"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.random()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bc86d71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d1affb2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ranked_synonyms(word, pos):\n",
    "    synonyms = []\n",
    "    for syn in wordnet.synsets(word, pos=pos):\n",
    "        for lemma in syn.lemmas():\n",
    "            if lemma.name() != word:\n",
    "                synonyms.append((lemma.name(), syn.wup_similarity(syn)))\n",
    "    \n",
    "    # Sort synonyms by similarity score in descending order\n",
    "    ranked_synonyms = sorted(set(synonyms), key=lambda x: x[1] if x[1] is not None else 0, reverse=True)\n",
    "    return [syn for syn, _ in ranked_synonyms]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3c400d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def replace_verbs_with_synonyms(text):\n",
    "    words = word_tokenize(text)\n",
    "    tagged = pos_tag(words)\n",
    "    result = []\n",
    "    for word, pos in tagged:\n",
    "        if pos =='VB':\n",
    "            wordnet_pos = get_wordnet_pos(pos)\n",
    "            lemma = lemmatizer.lemmatize(word, wordnet_pos)\n",
    "            synonyms = get_ranked_synonyms(lemma, wordnet_pos)\n",
    "            if synonyms:\n",
    "                replacement = random.choice(synonyms)  # Choose from top 3 synonyms\n",
    "                #print(f\"word = {word}|replacement = {replacement}\")\n",
    "                result.append(replacement)\n",
    "            else:\n",
    "                result.append(word)\n",
    "        else:\n",
    "            result.append(word)\n",
    "    return \" \".join(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "77040ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = train_list[584:1000]\n",
    "repl = replace_verbs_with_synonyms(sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c7316970",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "like Thumper that , boy , I could just plump out and sustain all kinds of game B : Did it figure_out ? A : Yeah , A : except we live in Plano , Texas now B : No , B : right . A : so B : I , um , I had a , for many years I had a dog that was part Springer Spaniel . B : I just have_a_go_at_it them . B : Her name was Molly , B : but she is n't alive any more B : We had her for , um , fifteen years , I think , my family did , and just loved her . B : She was the greatest ,\n"
     ]
    }
   ],
   "source": [
    "print(repl)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6ccf7098",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "like Thumper that, boy, I could just go out and get all kinds of game\n",
      "B:\tDid it work?\n",
      "A:\tYeah,\n",
      "A:\texcept we live in Plano, Texas now\n",
      "B:\tNo,\n",
      "B:\tright.\n",
      "A:\tso\n",
      "B:\tI, um, I had a, for many years I had a dog that was part Springer Spaniel.\n",
      "B:\tI just love them.\n",
      "B:\tHer name was Molly,\n",
      "B:\tbut she isn't alive any more\n",
      "B:\tWe had her for, um, fifteen years, I think, my family did, and just loved her.\n",
      "B:\tShe was the greatest,\n"
     ]
    }
   ],
   "source": [
    "print(sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c527dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc354140",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e10d7852",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequences(token_ids_list, max_length = B*T, tokenizer = tokenizer):\n",
    "    padded_sequences = tokenizer.pad(\n",
    "        {\"input_ids\": token_ids_list},\n",
    "        padding='max_length',\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    return padded_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e78e7afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text (text,tokenizer = tokenizer,max_length = B*T):\n",
    "    #print(f\"inside tokenize_text\")\n",
    "    enc = tokenizer(text,padding='max_length',truncation=True,max_length=max_length,return_tensors=\"pt\",return_attention_mask=True)\n",
    "    input_id = enc['input_ids']\n",
    "    att_mask = enc['attention_mask']\n",
    "    \n",
    "    # now concatenate these lists to B*T\n",
    "    input_id = torch.squeeze(input_id, dim = 0).to(dtype = torch.long)\n",
    "    att_mask = torch.squeeze(att_mask, dim = 0).to(dtype = torch.bool)\n",
    "    return input_id,att_mask\n",
    "    \n",
    "    \n",
    "    \n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7a0e9560",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test the tokenizer:\n",
    "model_name = 'distilgpt2'\n",
    "if random_init_wts:\n",
    "    config = AutoConfig.from_pretrained(model_name, vocab_size = 50304)\n",
    "    # Initialize the model with random weights\n",
    "    model = AutoModelForCausalLM.from_config(config)\n",
    "else:\n",
    "    #model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "    \n",
    "model = torch.compile(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0553f504",
   "metadata": {},
   "source": [
    "### Data loaders and Dataset for batched training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0791f2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataset_pyt_train(Dataset):\n",
    "    def __init__(self, text_list, B = B, T = T, tokenizer = tokenizer, prob = .3,synonym_flag = False):\n",
    "        self.text_list = text_list\n",
    "        #print(f\"Value of B {B}\")\n",
    "        self.prob = prob\n",
    "        self.synonym_flag = synonym_flag\n",
    "        \n",
    "                                        \n",
    "    def __getitem__(self, idx):\n",
    "        #words_list[start_index:start_index + n]\n",
    "        chunk = self.text_list[idx:idx + B*T*comp_ratio]\n",
    "        #print(f\"length of chunk = {len(chunk)}\")\n",
    "        random_num = random.random()\n",
    "        #print(f\"The random number generated = {random_num}\")\n",
    "        if random_num > self.prob:\n",
    "            chunk = replace_verbs_with_synonyms(chunk)\n",
    "            self.synonym_flag = True\n",
    "        inp,att = tokenize_text(chunk, tokenizer)\n",
    "        input_id = inp.view(B,T)\n",
    "        attention_mask = att.view(B,T)\n",
    "        #print(f\"shape of input_id = {input_id.shape}| shape of attention = {attention_mask.shape}\")\n",
    "        \n",
    "        return input_id,attention_mask,self.synonym_flag\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        #return the length of the dataframe\n",
    "        num_chunks = comp_ratio*B*T\n",
    "        return len(self.text_list)//num_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5e27f6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataset_pyt_val(Dataset):\n",
    "    def __init__(self, text_list, B = B, T = T, tokenizer = tokenizer, comp_ratio = comp_ratio):\n",
    "        self.text_list = text_list\n",
    "        #print(f\"Value of B {B}\")\n",
    "                                                \n",
    "    def __getitem__(self, idx):\n",
    "        #words_list[start_index:start_index + n]\n",
    "        chunk = self.text_list[idx:idx + B*T*comp_ratio]\n",
    "        #print(f\"length of chunk = {len()}\")\n",
    "        inp,att = tokenize_text(chunk, tokenizer)\n",
    "        input_id = inp.view(B,T)\n",
    "        attention_mask = att.view(B,T)\n",
    "        #print(f\"shape of input_id = {input_id.shape}| shape of attention = {attention_mask.shape}\")\n",
    "        \n",
    "        return input_id,attention_mask\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        #return the length of the dataframe\n",
    "        num_chunks = comp_ratio*B*T\n",
    "        return len(self.text_list)//num_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "13d87875",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_dataset = dataset_pyt(filtered_df,tokenizer = tokenizer)\n",
    "train_dataset = dataset_pyt_train(train_list)\n",
    "val_dataset = dataset_pyt_val(val_list)\n",
    "\n",
    "train_loader = DataLoader(train_dataset,batch_size = 1, shuffle = True , num_workers = 4, pin_memory = True)\n",
    "val_loader = DataLoader(val_dataset,batch_size = 1, shuffle = True , num_workers = 4, pin_memory = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "416e500f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x,y,z = train_dataset[0]\n",
    "# print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7c13a5da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer vocabulary size: 50258\n",
      "EOS token ID: 50256\n",
      "PAD token ID: 50257\n"
     ]
    }
   ],
   "source": [
    "print(f\"Tokenizer vocabulary size: {len(tokenizer)}\")\n",
    "print(f\"EOS token ID: {tokenizer.eos_token_id}\")\n",
    "print(f\"PAD token ID: {tokenizer.pad_token_id}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "62a0fa7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_file(log_message, model_name = \"GPT2\" ,random_init_wts = random_init_wts ):\n",
    "    current_datetime = datetime.datetime.now()\n",
    "    # Extract date and time components\n",
    "    current_date = str(current_datetime.date())\n",
    "    log_file = model_name +'_COS_SIM_'+'random_init_wts'+ '_'+str(random_init_wts)+'_' +current_date+'.log'\n",
    "    print(f\"*****LOGGING INFO IN {log_file}*********\")\n",
    "    filepath = os.path.join(\"model\",log_file)\n",
    "    logging.basicConfig(filename=filepath, \n",
    "                    filemode='a',  # Overwrite the log file each time\n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s', \n",
    "                    level=logging.DEBUG)\n",
    "    logger = logging.getLogger()\n",
    "    logger.info(log_message)\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8c3b90ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the train loader is 275\n",
      "Length of the val loader is 287\n",
      "num_tokens= 18022400\n"
     ]
    }
   ],
   "source": [
    "print(f\"Length of the train loader is {len(train_loader)}\")\n",
    "print(f\"Length of the val loader is {len(val_loader)}\")\n",
    "print(f\"num_tokens= {B*T*len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f29c1752",
   "metadata": {},
   "outputs": [],
   "source": [
    "#emb,att,inp = train_dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e6fbc4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class check_train_metrics:\n",
    "    def __init__(self, patience=20, min_delta=0 , B = T, T = T,best_loss = torch.inf,early_stop = False):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = best_loss\n",
    "        self.early_stop = early_stop\n",
    "        self.B = B\n",
    "        self.T = T\n",
    "        self.improvement = None\n",
    "\n",
    "    def __call__(self, loss, epoch , epoch_durn, norm , current_lr, num_token):\n",
    "        if self.best_loss - loss > self.min_delta:\n",
    "            \n",
    "            print(f\"training loss has decreased---> reducing the best loss from {self.best_loss:.2f} to {loss:.2f} | throughput = {int(num_token/epoch_durn)} tokens/second | norm = {norm:.4f} | learning rate = {current_lr:.5e}\")\n",
    "            self.best_loss = loss\n",
    "            self.counter = 0\n",
    "            self.improvement = True\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            self.improvement = False\n",
    "            print(f\"No improvement in training  loss-->epoch= {epoch} and best loss is {self.best_loss:.2f}|current_loss = {loss}|counter = {self.counter}\")\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e3ef8381",
   "metadata": {},
   "outputs": [],
   "source": [
    "class check_val_metrics:\n",
    "    def __init__(self, patience=25, min_delta=0, best_loss = torch.inf,early_stop = False):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = best_loss\n",
    "        self.early_stop = early_stop\n",
    "        \n",
    "\n",
    "    def __call__(self, loss, epoch , model, tokenizer):\n",
    "        if self.best_loss - loss > self.min_delta:\n",
    "            print(f\"Val loss has decreased -->reducing the global validation loss from {self.best_loss:.2f} to {loss:.2f}\")\n",
    "            s1 = (f\"Val loss has decreased -->reducing the global validation loss from {self.best_loss:.2f} to {loss:.2f}\")\n",
    "            print(f\" validation loss for epoch = {epoch} is {loss:.4f}\")\n",
    "            self.best_loss = loss\n",
    "            s2 = f\" validation loss for epoch = {epoch} is {loss:.4f}\"\n",
    "            print(f\" epoch= {epoch} :  val loss is {loss:.4f} \")\n",
    "            s3 = f\" epoch= {epoch} :  val loss is {loss:.4f} \"\n",
    "            #save the model\n",
    "            # Get the current date and time\n",
    "            current_datetime = datetime.datetime.now()\n",
    "            # Extract date and time components\n",
    "            current_date = str(current_datetime.date())\n",
    "            current_time = str(current_datetime.time()).split('.')[0]\n",
    "            file_name = 'model'+ current_date+current_time+'.pth'\n",
    "            path = os.path.join(\"model\",file_name)\n",
    "            print(f\"saving the model {file_name}\")\n",
    "            s4 = f\"saving the model {file_name}\"\n",
    "            #torch.save(model.state_dict(), path)\n",
    "            model.save_pretrained(path)\n",
    "            tokenizer.save_pretrained(path)\n",
    "            log_message = s1+s2+s3+s4\n",
    "            write_file(log_message)\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            print(f\"No improvement in validation loss-->epoch= {epoch} and best val loss is {self.best_loss:.2f}|current_Val loss = {loss}|counter = {self.counter}\")\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f94782ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_output = model(input_ids = inp ,attention_mask = att, labels = inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "73a5594a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad\n",
    "\n",
    "def eval_model(val_loader, model, epoch , device = device,tokenizer = tokenizer):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    e = epoch+1\n",
    "    val_loss_accum = 0.0\n",
    "    embedding_layer = model.transformer.wte\n",
    "    print(f\"inside validation data for epoch {e}\")\n",
    "    for ind,(input_id,attention_mask) in enumerate(val_loader):\n",
    "        ids = input_id.to(device=device, non_blocking=True)\n",
    "        ids = torch.squeeze(ids, dim = 0)\n",
    "        att_mask = attention_mask.to(device=device, non_blocking=True)\n",
    "        att_mask =  torch.squeeze(att_mask, dim = 0)\n",
    "        labels = ids.clone().to(device)\n",
    "        with autocast(dtype = torch.bfloat16):\n",
    "            model_output = model(input_ids = ids ,attention_mask = att_mask, labels = labels)\n",
    "            total_loss = model_output.loss\n",
    "            \n",
    "    \n",
    "    \n",
    "        val_loss_accum+= total_loss.detach().item()\n",
    "        del att_mask,labels,model_output,total_loss,ids\n",
    "    return val_loss_accum        \n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "db7ef5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_model(train_loader,val_loader,model,num_epoch = 100,device = device,tokenizer = tokenizer):\n",
    "    #model.train()\n",
    "    device = device\n",
    "    lr_custom = 1e-5\n",
    "    print(f\"inside train model. Device = {device}\")\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.AdamW(params =  model.parameters(), lr= lr_custom,fused = True ,weight_decay = .1)\n",
    "      \n",
    "    extra_train = .1*num_epoch\n",
    "    max_train_steps = int(num_epoch +extra_train )\n",
    "    import time\n",
    "    from transformers import get_linear_schedule_with_warmup\n",
    "    total_steps = len(train_loader) * num_epoch\n",
    "    scheduler_cos = transformers.get_cosine_schedule_with_warmup( optimizer= optimizer, num_warmup_steps =int(total_steps * 0.1) ,num_training_steps= total_steps )\n",
    "        \n",
    "    epoch_train_log = []\n",
    "    epoch_val_log = []\n",
    "    validate_val_metric = check_val_metrics()\n",
    "    validate_train_metric = check_train_metrics()\n",
    "    embedding_layer = model.transformer.wte\n",
    "    for i in range (max_train_steps):\n",
    "        \n",
    "        epoch_start_time = time.time()\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        # we use 2 schedulers - the first LR scheduler uses a cosine decay for 100 epochs the second scheduler takes the last LR from cosine scheduler and then maintains that LR for the next 10 epochs\n",
    "        if i >= num_epoch:\n",
    "            optimizer_reduced_lr = torch.optim.AdamW(params =  model.parameters(), lr= current_lr ,fused = True , weight_decay=.1)\n",
    "            scheduler_constant = transformers.get_constant_schedule_with_warmup( optimizer = optimizer_reduced_lr ,num_warmup_steps = 0, last_epoch = -1 )\n",
    "        \n",
    "        epoch_train_loss = 0       \n",
    "        for ind,(input_id,attention_mask,synonym_flag) in enumerate(train_loader):\n",
    "            if ind == int(len(train_loader)/2):\n",
    "                batch_time = time.time()\n",
    "                duration = batch_time - epoch_start_time\n",
    "                print(f\"executing epoch:{i+1}, it took {duration/60} mins from beginning of epoch till batch#{ind}\")\n",
    "            \n",
    "            ids = input_id.to(device=device, non_blocking=True)\n",
    "            ids = torch.squeeze(ids, dim = 0)\n",
    "            att_mask = attention_mask.to(device=device, non_blocking=True)\n",
    "            att_mask =  torch.squeeze(att_mask, dim = 0)\n",
    "            labels = ids.clone().to(device)\n",
    "            synonym_flag = synonym_flag\n",
    "            if synonym_flag:\n",
    "                with autocast(dtype = torch.bfloat16):\n",
    "                    input_emb = embedding_layer(ids)\n",
    "                    model_output = model(input_ids = ids ,attention_mask = att_mask, labels = labels)\n",
    "                    logits = model_output.logits\n",
    "                    predictions = torch.argmax(logits, dim=-1)\n",
    "                    #print(f\"predictions = {predictions}\")\n",
    "                    prediction_embeddings = embedding_layer(predictions)\n",
    "                    cos_sim = F.cosine_similarity(torch.squeeze(prediction_embeddings,dim = 0), torch.squeeze(input_emb,dim = 0), dim=1)\n",
    "                    cos_loss = 1- cos_sim.mean()\n",
    "                    total_loss = cos_loss\n",
    "                \n",
    "                del input_emb,logits,predictions,prediction_embeddings,cos_sim,cos_loss\n",
    "                \n",
    "            else:\n",
    "                with autocast(dtype = torch.bfloat16):\n",
    "                    model_output = model(input_ids = ids ,attention_mask = att_mask, labels = labels)\n",
    "                    total_loss = model_output.loss\n",
    "                \n",
    "                          \n",
    "            total_loss.backward()\n",
    "            epoch_train_loss += total_loss.detach().item()\n",
    "            norm = torch.nn.utils.clip_grad_norm(model.parameters() , 1.0)\n",
    "            if i <= num_epoch:\n",
    "                optimizer.step()\n",
    "                scheduler_cos.step()\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "            else:\n",
    "                optimizer_reduced_lr.step()\n",
    "                optimizer_reduced_lr.zero_grad(set_to_none=True)\n",
    "                scheduler_constant.step()\n",
    "                \n",
    "                         \n",
    "            del att_mask,labels,model_output,ids\n",
    "            \n",
    "        #batch processing complete \n",
    "        #print(f\"batch processing complete , lambda = {lambda_val} |total_loss for batch= {total_loss}\")\n",
    "        \n",
    "        if i <= num_epoch:\n",
    "            current_lr = scheduler_cos.get_last_lr()[0]\n",
    "        epoch_end_time = time.time()\n",
    "        epoch_durn = (epoch_end_time - epoch_start_time)\n",
    "        num_token = B*T*len(train_loader)\n",
    "        epoch_train_log.append(epoch_train_loss)\n",
    "        validate_train_metric(epoch_train_loss, i , epoch_durn, norm , current_lr, num_token)\n",
    "        \n",
    "        if validate_train_metric.improvement:\n",
    "            val_loss= eval_model(val_loader, model, epoch = i, device = device,tokenizer = tokenizer)\n",
    "            epoch_val_log.append(val_loss)\n",
    "            validate_val_metric(val_loss, i , model, tokenizer)\n",
    "            if validate_train_metric.early_stop or validate_val_metric.early_stop :\n",
    "                print(f\"early stopping trigerred either from training data or val data | train_counter = {validate_train_metric.counter}|val_counter = {validate_val_metric.counter}\")\n",
    "                break\n",
    "        else:\n",
    "            if validate_val_metric.early_stop:\n",
    "                print(f\"early stopping trigerred from validation data\")\n",
    "                break\n",
    "              \n",
    "    \n",
    "    return model,epoch_train_log,epoch_val_log\n",
    "        \n",
    "            \n",
    "            \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "265f195d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inside train model. Device = cuda:0\n",
      "executing epoch:1, it took 1.9446785648663838 mins from beginning of epoch till batch#137\n",
      "training loss has decreased---> reducing the best loss from inf to 280.15 | throughput = 97001 tokens/second | norm = 0.0583 | learning rate = 1.00000e-06\n",
      "inside validation data for epoch 1\n",
      "Val loss has decreased -->reducing the global validation loss from inf to 3159.47\n",
      " validation loss for epoch = 0 is 3159.4712\n",
      " epoch= 0 :  val loss is 3159.4712 \n",
      "saving the model model2024-07-2700:49:30.pth\n",
      "*****LOGGING INFO IN GPT2_COS_SIM_random_init_wts_True_2024-07-27.log*********\n",
      "executing epoch:2, it took 1.374309480190277 mins from beginning of epoch till batch#137\n",
      "training loss has decreased---> reducing the best loss from 280.15 to 267.56 | throughput = 117740 tokens/second | norm = 0.0803 | learning rate = 2.00000e-06\n",
      "inside validation data for epoch 2\n",
      "Val loss has decreased -->reducing the global validation loss from 3159.47 to 3147.75\n",
      " validation loss for epoch = 1 is 3147.7453\n",
      " epoch= 1 :  val loss is 3147.7453 \n",
      "saving the model model2024-07-2700:52:36.pth\n",
      "*****LOGGING INFO IN GPT2_COS_SIM_random_init_wts_True_2024-07-27.log*********\n",
      "executing epoch:3, it took 1.2268045663833618 mins from beginning of epoch till batch#137\n",
      "No improvement in training  loss-->epoch= 2 and best loss is 267.56|current_loss = 286.996879696846|counter = 1\n",
      "executing epoch:4, it took 1.191362158457438 mins from beginning of epoch till batch#137\n",
      "training loss has decreased---> reducing the best loss from 267.56 to 265.07 | throughput = 123015 tokens/second | norm = 0.1777 | learning rate = 4.00000e-06\n",
      "inside validation data for epoch 4\n",
      "Val loss has decreased -->reducing the global validation loss from 3147.75 to 2768.26\n",
      " validation loss for epoch = 3 is 2768.2619\n",
      " epoch= 3 :  val loss is 2768.2619 \n",
      "saving the model model2024-07-2700:57:55.pth\n",
      "*****LOGGING INFO IN GPT2_COS_SIM_random_init_wts_True_2024-07-27.log*********\n",
      "executing epoch:5, it took 1.265704909960429 mins from beginning of epoch till batch#137\n",
      "training loss has decreased---> reducing the best loss from 265.07 to 241.02 | throughput = 118630 tokens/second | norm = 0.2740 | learning rate = 5.00000e-06\n",
      "inside validation data for epoch 5\n",
      "Val loss has decreased -->reducing the global validation loss from 2768.26 to 2647.72\n",
      " validation loss for epoch = 4 is 2647.7184\n",
      " epoch= 4 :  val loss is 2647.7184 \n",
      "saving the model model2024-07-2701:01:00.pth\n",
      "*****LOGGING INFO IN GPT2_COS_SIM_random_init_wts_True_2024-07-27.log*********\n",
      "executing epoch:6, it took 1.2562864979108175 mins from beginning of epoch till batch#137\n",
      "No improvement in training  loss-->epoch= 5 and best loss is 241.02|current_loss = 246.06773036718369|counter = 1\n",
      "executing epoch:7, it took 1.295466140906016 mins from beginning of epoch till batch#137\n",
      "training loss has decreased---> reducing the best loss from 241.02 to 226.13 | throughput = 120770 tokens/second | norm = 0.3654 | learning rate = 7.00000e-06\n",
      "inside validation data for epoch 7\n",
      "Val loss has decreased -->reducing the global validation loss from 2647.72 to 2150.98\n",
      " validation loss for epoch = 6 is 2150.9801\n",
      " epoch= 6 :  val loss is 2150.9801 \n",
      "saving the model model2024-07-2701:06:26.pth\n",
      "*****LOGGING INFO IN GPT2_COS_SIM_random_init_wts_True_2024-07-27.log*********\n",
      "executing epoch:8, it took 1.2115471959114075 mins from beginning of epoch till batch#137\n",
      "training loss has decreased---> reducing the best loss from 226.13 to 195.51 | throughput = 123465 tokens/second | norm = 0.2351 | learning rate = 8.00000e-06\n",
      "inside validation data for epoch 8\n",
      "Val loss has decreased -->reducing the global validation loss from 2150.98 to 2034.36\n",
      " validation loss for epoch = 7 is 2034.3630\n",
      " epoch= 7 :  val loss is 2034.3630 \n",
      "saving the model model2024-07-2701:09:26.pth\n",
      "*****LOGGING INFO IN GPT2_COS_SIM_random_init_wts_True_2024-07-27.log*********\n",
      "executing epoch:9, it took 1.2263822913169862 mins from beginning of epoch till batch#137\n",
      "training loss has decreased---> reducing the best loss from 195.51 to 178.51 | throughput = 123771 tokens/second | norm = 0.2200 | learning rate = 9.00000e-06\n",
      "inside validation data for epoch 9\n",
      "Val loss has decreased -->reducing the global validation loss from 2034.36 to 1934.52\n",
      " validation loss for epoch = 8 is 1934.5153\n",
      " epoch= 8 :  val loss is 1934.5153 \n",
      "saving the model model2024-07-2701:12:25.pth\n",
      "*****LOGGING INFO IN GPT2_COS_SIM_random_init_wts_True_2024-07-27.log*********\n",
      "executing epoch:10, it took 1.1420945843060812 mins from beginning of epoch till batch#137\n",
      "training loss has decreased---> reducing the best loss from 178.51 to 155.59 | throughput = 129193 tokens/second | norm = 0.2050 | learning rate = 1.00000e-05\n",
      "inside validation data for epoch 10\n",
      "Val loss has decreased -->reducing the global validation loss from 1934.52 to 1866.70\n",
      " validation loss for epoch = 9 is 1866.7044\n",
      " epoch= 9 :  val loss is 1866.7044 \n",
      "saving the model model2024-07-2701:15:17.pth\n",
      "*****LOGGING INFO IN GPT2_COS_SIM_random_init_wts_True_2024-07-27.log*********\n",
      "executing epoch:11, it took 1.2499058842658997 mins from beginning of epoch till batch#137\n",
      "training loss has decreased---> reducing the best loss from 155.59 to 134.68 | throughput = 120575 tokens/second | norm = 0.2426 | learning rate = 9.99695e-06\n",
      "inside validation data for epoch 11\n",
      "Val loss has decreased -->reducing the global validation loss from 1866.70 to 1815.04\n",
      " validation loss for epoch = 10 is 1815.0360\n",
      " epoch= 10 :  val loss is 1815.0360 \n",
      "saving the model model2024-07-2701:18:20.pth\n",
      "*****LOGGING INFO IN GPT2_COS_SIM_random_init_wts_True_2024-07-27.log*********\n",
      "executing epoch:12, it took 1.2163222670555114 mins from beginning of epoch till batch#137\n",
      "No improvement in training  loss-->epoch= 11 and best loss is 134.68|current_loss = 183.4186648130417|counter = 1\n",
      "executing epoch:13, it took 1.2916736006736755 mins from beginning of epoch till batch#137\n",
      "No improvement in training  loss-->epoch= 12 and best loss is 134.68|current_loss = 150.71807885169983|counter = 2\n",
      "executing epoch:14, it took 1.2565598766009012 mins from beginning of epoch till batch#137\n",
      "training loss has decreased---> reducing the best loss from 134.68 to 105.06 | throughput = 124782 tokens/second | norm = 0.1781 | learning rate = 9.95134e-06\n",
      "inside validation data for epoch 14\n",
      "Val loss has decreased -->reducing the global validation loss from 1815.04 to 1713.53\n",
      " validation loss for epoch = 13 is 1713.5303\n",
      " epoch= 13 :  val loss is 1713.5303 \n",
      "saving the model model2024-07-2701:26:07.pth\n",
      "*****LOGGING INFO IN GPT2_COS_SIM_random_init_wts_True_2024-07-27.log*********\n",
      "executing epoch:15, it took 1.2167463819185893 mins from beginning of epoch till batch#137\n",
      "training loss has decreased---> reducing the best loss from 105.06 to 80.75 | throughput = 121300 tokens/second | norm = 0.1547 | learning rate = 9.92404e-06\n",
      "inside validation data for epoch 15\n",
      "Val loss has decreased -->reducing the global validation loss from 1713.53 to 1713.39\n",
      " validation loss for epoch = 14 is 1713.3889\n",
      " epoch= 14 :  val loss is 1713.3889 \n",
      "saving the model model2024-07-2701:29:09.pth\n",
      "*****LOGGING INFO IN GPT2_COS_SIM_random_init_wts_True_2024-07-27.log*********\n",
      "executing epoch:16, it took 1.2923261205355325 mins from beginning of epoch till batch#137\n",
      "No improvement in training  loss-->epoch= 15 and best loss is 80.75|current_loss = 88.04367518424988|counter = 1\n",
      "executing epoch:17, it took 1.1872136433919271 mins from beginning of epoch till batch#137\n",
      "training loss has decreased---> reducing the best loss from 80.75 to 60.78 | throughput = 130284 tokens/second | norm = 0.1251 | learning rate = 9.85148e-06\n",
      "inside validation data for epoch 17\n",
      "Val loss has decreased -->reducing the global validation loss from 1713.39 to 1661.58\n",
      " validation loss for epoch = 16 is 1661.5847\n",
      " epoch= 16 :  val loss is 1661.5847 \n",
      "saving the model model2024-07-2701:34:32.pth\n",
      "*****LOGGING INFO IN GPT2_COS_SIM_random_init_wts_True_2024-07-27.log*********\n",
      "executing epoch:18, it took 1.2254018704096477 mins from beginning of epoch till batch#137\n",
      "No improvement in training  loss-->epoch= 17 and best loss is 60.78|current_loss = 62.493278205394745|counter = 1\n",
      "executing epoch:19, it took 1.2176064570744833 mins from beginning of epoch till batch#137\n",
      "training loss has decreased---> reducing the best loss from 60.78 to 38.66 | throughput = 124136 tokens/second | norm = 0.1018 | learning rate = 9.75528e-06\n",
      "inside validation data for epoch 19\n",
      "No improvement in validation loss-->epoch= 18 and best val loss is 1661.58|current_Val loss = 1687.3775935173035|counter = 1\n",
      "executing epoch:20, it took 1.2535952130953472 mins from beginning of epoch till batch#137\n",
      "training loss has decreased---> reducing the best loss from 38.66 to 38.13 | throughput = 120843 tokens/second | norm = 0.1002 | learning rate = 9.69846e-06\n",
      "inside validation data for epoch 20\n",
      "No improvement in validation loss-->epoch= 19 and best val loss is 1661.58|current_Val loss = 1714.5430178642273|counter = 2\n",
      "executing epoch:21, it took 1.2547154744466147 mins from beginning of epoch till batch#137\n",
      "training loss has decreased---> reducing the best loss from 38.13 to 27.48 | throughput = 121121 tokens/second | norm = 0.1571 | learning rate = 9.63592e-06\n",
      "inside validation data for epoch 21\n",
      "No improvement in validation loss-->epoch= 20 and best val loss is 1661.58|current_Val loss = 1764.2576656341553|counter = 3\n",
      "executing epoch:22, it took 1.2142058809598286 mins from beginning of epoch till batch#137\n",
      "training loss has decreased---> reducing the best loss from 27.48 to 23.30 | throughput = 126000 tokens/second | norm = 0.0777 | learning rate = 9.56773e-06\n",
      "inside validation data for epoch 22\n",
      "No improvement in validation loss-->epoch= 21 and best val loss is 1661.58|current_Val loss = 1815.8344583511353|counter = 4\n",
      "executing epoch:23, it took 1.2621482173601786 mins from beginning of epoch till batch#137\n",
      "No improvement in training  loss-->epoch= 22 and best loss is 23.30|current_loss = 38.243916273117065|counter = 1\n",
      "executing epoch:24, it took 1.2508958339691163 mins from beginning of epoch till batch#137\n",
      "No improvement in training  loss-->epoch= 23 and best loss is 23.30|current_loss = 32.42313814163208|counter = 2\n",
      "executing epoch:25, it took 1.2585068742434184 mins from beginning of epoch till batch#137\n",
      "No improvement in training  loss-->epoch= 24 and best loss is 23.30|current_loss = 27.484153151512146|counter = 3\n",
      "executing epoch:26, it took 1.1871918519337972 mins from beginning of epoch till batch#137\n",
      "No improvement in training  loss-->epoch= 25 and best loss is 23.30|current_loss = 26.111683249473572|counter = 4\n",
      "executing epoch:27, it took 1.2858925422032674 mins from beginning of epoch till batch#137\n",
      "training loss has decreased---> reducing the best loss from 23.30 to 18.03 | throughput = 122563 tokens/second | norm = 0.1159 | learning rate = 9.14519e-06\n",
      "inside validation data for epoch 27\n",
      "No improvement in validation loss-->epoch= 26 and best val loss is 1661.58|current_Val loss = 1875.1431574821472|counter = 5\n",
      "executing epoch:28, it took 1.221977166334788 mins from beginning of epoch till batch#137\n",
      "training loss has decreased---> reducing the best loss from 18.03 to 16.52 | throughput = 122670 tokens/second | norm = 0.0385 | learning rate = 9.04508e-06\n",
      "inside validation data for epoch 28\n",
      "No improvement in validation loss-->epoch= 27 and best val loss is 1661.58|current_Val loss = 1902.98117685318|counter = 6\n",
      "executing epoch:29, it took 1.2209847927093507 mins from beginning of epoch till batch#137\n",
      "No improvement in training  loss-->epoch= 28 and best loss is 16.52|current_loss = 27.98242086172104|counter = 1\n",
      "executing epoch:30, it took 1.290769668420156 mins from beginning of epoch till batch#137\n",
      "No improvement in training  loss-->epoch= 29 and best loss is 16.52|current_loss = 33.11701601743698|counter = 2\n",
      "executing epoch:31, it took 1.2642309109369914 mins from beginning of epoch till batch#137\n",
      "training loss has decreased---> reducing the best loss from 16.52 to 7.42 | throughput = 119336 tokens/second | norm = 0.0252 | learning rate = 8.71572e-06\n",
      "inside validation data for epoch 31\n",
      "No improvement in validation loss-->epoch= 30 and best val loss is 1661.58|current_Val loss = 1880.7630367279053|counter = 7\n",
      "executing epoch:32, it took 1.2268190105756125 mins from beginning of epoch till batch#137\n",
      "No improvement in training  loss-->epoch= 31 and best loss is 7.42|current_loss = 41.380823254585266|counter = 1\n",
      "executing epoch:33, it took 1.2488764564196269 mins from beginning of epoch till batch#137\n",
      "No improvement in training  loss-->epoch= 32 and best loss is 7.42|current_loss = 66.35430419445038|counter = 2\n",
      "executing epoch:34, it took 1.2989359855651856 mins from beginning of epoch till batch#137\n",
      "No improvement in training  loss-->epoch= 33 and best loss is 7.42|current_loss = 39.88920283317566|counter = 3\n",
      "executing epoch:35, it took 1.220282773176829 mins from beginning of epoch till batch#137\n",
      "No improvement in training  loss-->epoch= 34 and best loss is 7.42|current_loss = 56.42492914199829|counter = 4\n",
      "executing epoch:36, it took 1.2853916883468628 mins from beginning of epoch till batch#137\n",
      "No improvement in training  loss-->epoch= 35 and best loss is 7.42|current_loss = 39.33103150129318|counter = 5\n",
      "executing epoch:37, it took 1.2594847599665324 mins from beginning of epoch till batch#137\n",
      "No improvement in training  loss-->epoch= 36 and best loss is 7.42|current_loss = 36.38583981990814|counter = 6\n",
      "executing epoch:38, it took 1.2951748887697856 mins from beginning of epoch till batch#137\n",
      "No improvement in training  loss-->epoch= 37 and best loss is 7.42|current_loss = 34.53302443027496|counter = 7\n",
      "executing epoch:39, it took 1.2146754225095113 mins from beginning of epoch till batch#137\n",
      "No improvement in training  loss-->epoch= 38 and best loss is 7.42|current_loss = 29.694562911987305|counter = 8\n",
      "executing epoch:40, it took 1.1958149433135987 mins from beginning of epoch till batch#137\n",
      "No improvement in training  loss-->epoch= 39 and best loss is 7.42|current_loss = 26.59489196538925|counter = 9\n",
      "executing epoch:41, it took 1.2230172038078309 mins from beginning of epoch till batch#137\n",
      "No improvement in training  loss-->epoch= 40 and best loss is 7.42|current_loss = 42.653824746608734|counter = 10\n",
      "executing epoch:42, it took 1.3273182471593221 mins from beginning of epoch till batch#137\n",
      "No improvement in training  loss-->epoch= 41 and best loss is 7.42|current_loss = 21.049584209918976|counter = 11\n",
      "executing epoch:43, it took 1.2546746134757996 mins from beginning of epoch till batch#137\n",
      "No improvement in training  loss-->epoch= 42 and best loss is 7.42|current_loss = 31.912907123565674|counter = 12\n",
      "executing epoch:44, it took 1.2494078159332276 mins from beginning of epoch till batch#137\n",
      "No improvement in training  loss-->epoch= 43 and best loss is 7.42|current_loss = 43.08548718690872|counter = 13\n",
      "executing epoch:45, it took 1.2171501676241556 mins from beginning of epoch till batch#137\n",
      "No improvement in training  loss-->epoch= 44 and best loss is 7.42|current_loss = 41.40955811738968|counter = 14\n",
      "executing epoch:46, it took 1.2938221414883933 mins from beginning of epoch till batch#137\n",
      "No improvement in training  loss-->epoch= 45 and best loss is 7.42|current_loss = 21.529473662376404|counter = 15\n",
      "executing epoch:47, it took 1.2535633246103923 mins from beginning of epoch till batch#137\n",
      "No improvement in training  loss-->epoch= 46 and best loss is 7.42|current_loss = 14.572891592979431|counter = 16\n",
      "executing epoch:48, it took 1.2934442480405173 mins from beginning of epoch till batch#137\n",
      "No improvement in training  loss-->epoch= 47 and best loss is 7.42|current_loss = 69.2491102218628|counter = 17\n",
      "executing epoch:49, it took 1.222914695739746 mins from beginning of epoch till batch#137\n",
      "No improvement in training  loss-->epoch= 48 and best loss is 7.42|current_loss = 49.07943284511566|counter = 18\n",
      "executing epoch:50, it took 1.2508780439694722 mins from beginning of epoch till batch#137\n",
      "No improvement in training  loss-->epoch= 49 and best loss is 7.42|current_loss = 18.787886202335358|counter = 19\n",
      "executing epoch:51, it took 1.2484993815422059 mins from beginning of epoch till batch#137\n",
      "No improvement in training  loss-->epoch= 50 and best loss is 7.42|current_loss = 18.208578526973724|counter = 20\n",
      "executing epoch:52, it took 1.257591994603475 mins from beginning of epoch till batch#137\n",
      "training loss has decreased---> reducing the best loss from 7.42 to 5.78 | throughput = 125785 tokens/second | norm = 0.0268 | learning rate = 5.52264e-06\n",
      "inside validation data for epoch 52\n",
      "No improvement in validation loss-->epoch= 51 and best val loss is 1661.58|current_Val loss = 1845.1566739082336|counter = 8\n",
      "early stopping trigerred either from training data or val data | train_counter = 0|val_counter = 8\n"
     ]
    }
   ],
   "source": [
    "tr_model,epoch_train_log,epoch_val_log = train_model(train_loader, val_loader,model=model,tokenizer = tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0d768dc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[280.14505380392075, 267.5603966116905, 286.996879696846, 265.07401144504547, 241.0162172317505, 246.06773036718369, 226.12893688678741, 195.51309883594513, 178.50518614053726, 155.59245437383652, 134.67828905582428, 183.4186648130417, 150.71807885169983, 105.05882179737091, 80.75434875488281, 88.04367518424988, 60.78037965297699, 62.493278205394745, 38.65872371196747, 38.12629860639572, 27.47838819026947, 23.297076761722565, 38.243916273117065, 32.42313814163208, 27.484153151512146, 26.111683249473572, 18.025650024414062, 16.518497347831726, 27.98242086172104, 33.11701601743698, 7.421100497245789, 41.380823254585266, 66.35430419445038, 39.88920283317566, 56.42492914199829, 39.33103150129318, 36.38583981990814, 34.53302443027496, 29.694562911987305, 26.59489196538925, 42.653824746608734, 21.049584209918976, 31.912907123565674, 43.08548718690872, 41.40955811738968, 21.529473662376404, 14.572891592979431, 69.2491102218628, 49.07943284511566, 18.787886202335358, 18.208578526973724, 5.777827501296997]\n"
     ]
    }
   ],
   "source": [
    "print(epoch_train_log)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7c551c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json , os\n",
    "# path_var_train_log = os.path.join(\".\",\"Tokenizers_and_loss_vals\",\"epoch_train_plain_loss.json\")\n",
    "# path_var_val_log = os.path.join(\".\",\"Tokenizers_and_loss_vals\",\"epoch_val_val_loss_.json\")\n",
    "\n",
    "# #print(path_var)\n",
    "# #Write the list to a JSON file\n",
    "# with open(path_var_train_log, \"w\") as file:\n",
    "#     json.dump(epoch_train_log, file)\n",
    "\n",
    "# with open(path_var_val_log, \"w\") as file:\n",
    "#     json.dump(epoch_val_log, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "09a12938",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(path_var_train_log, \"r\") as file:\n",
    "#     train_loss = json.load(file)\n",
    "# with open(path_var_val_log, \"r\") as file:\n",
    "#     val_loss = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "162360a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f6972cfc040>]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_values = range(len(epoch_train_log))\n",
    "plt.plot(x_values, epoch_train_log, label='Train_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d1cbc7e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f6972b9d370>]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#epoch_val_log= [t.detach().cpu().numpy() for t in epoch_val_log]\n",
    "x_values_val = range(len(epoch_val_log))\n",
    "plt.plot(x_values_val, epoch_val_log, label='val_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "16670ef8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f698c581af0>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "min_length = min(len(epoch_train_log), len(epoch_val_log))\n",
    "list1 = epoch_train_log[:min_length]\n",
    "list2 = epoch_val_log[:min_length]\n",
    "\n",
    "# Create x-axis values\n",
    "x_values = range(min_length)\n",
    "\n",
    "# Plot the lists\n",
    "plt.plot(x_values, list1, label='Train_loss')\n",
    "plt.plot(x_values, list2, label='Val_loss')\n",
    "\n",
    "# Adding labels and title\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Train Loss and Val_Loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52f3b41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0528f81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54dfbe40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe482eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe57bb2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20728f73",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
