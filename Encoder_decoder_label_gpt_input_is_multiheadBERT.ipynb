{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bea3ab04",
   "metadata": {},
   "outputs": [],
   "source": [
    "## reference https://huggingface.co/learn/nlp-course/en/chapter7/6?fw=pt#training-a-causal-language-model-from-scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c247d610",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21c3612b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: jupyter in /opt/conda/lib/python3.8/site-packages (1.0.0)\n",
      "Requirement already satisfied: ipywidgets in /opt/conda/lib/python3.8/site-packages (8.1.3)\n",
      "Requirement already satisfied: ipykernel in /opt/conda/lib/python3.8/site-packages (from jupyter) (6.29.5)\n",
      "Requirement already satisfied: qtconsole in /opt/conda/lib/python3.8/site-packages (from jupyter) (5.5.2)\n",
      "Requirement already satisfied: jupyter-console in /opt/conda/lib/python3.8/site-packages (from jupyter) (6.6.3)\n",
      "Requirement already satisfied: notebook in /opt/conda/lib/python3.8/site-packages (from jupyter) (6.4.1)\n",
      "Requirement already satisfied: nbconvert in /opt/conda/lib/python3.8/site-packages (from jupyter) (6.3.0)\n",
      "Requirement already satisfied: comm>=0.1.3 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.11 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (3.0.11)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (5.14.3)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.11 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (4.0.11)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (7.30.0)\n",
      "Requirement already satisfied: setuptools>=18.5 in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (59.4.0)\n",
      "Requirement already satisfied: jedi>=0.16 in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.18.1)\n",
      "Requirement already satisfied: pickleshare in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.7.5)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.47)\n",
      "Requirement already satisfied: backcall in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (4.8.0)\n",
      "Requirement already satisfied: matplotlib-inline in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.3)\n",
      "Requirement already satisfied: decorator in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (5.1.0)\n",
      "Requirement already satisfied: pygments in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (2.10.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /opt/conda/lib/python3.8/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.8/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.8/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=6.1.0->ipywidgets) (0.2.5)\n",
      "Requirement already satisfied: nest-asyncio in /opt/conda/lib/python3.8/site-packages (from ipykernel->jupyter) (1.5.4)\n",
      "Requirement already satisfied: tornado>=6.1 in /opt/conda/lib/python3.8/site-packages (from ipykernel->jupyter) (6.1)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.8/site-packages (from ipykernel->jupyter) (5.8.0)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in /opt/conda/lib/python3.8/site-packages (from ipykernel->jupyter) (1.8.2)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.8/site-packages (from ipykernel->jupyter) (21.3)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in /opt/conda/lib/python3.8/site-packages (from ipykernel->jupyter) (7.1.0)\n",
      "Requirement already satisfied: pyzmq>=24 in /opt/conda/lib/python3.8/site-packages (from ipykernel->jupyter) (26.0.3)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /opt/conda/lib/python3.8/site-packages (from ipykernel->jupyter) (5.7.2)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /opt/conda/lib/python3.8/site-packages (from jupyter-client>=6.1.12->ipykernel->jupyter) (2.8.2)\n",
      "Requirement already satisfied: entrypoints in /opt/conda/lib/python3.8/site-packages (from jupyter-client>=6.1.12->ipykernel->jupyter) (0.3)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /opt/conda/lib/python3.8/site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel->jupyter) (4.2.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.8/site-packages (from python-dateutil>=2.1->jupyter-client>=6.1.12->ipykernel->jupyter) (1.16.0)\n",
      "Requirement already satisfied: bleach in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (4.1.0)\n",
      "Requirement already satisfied: jinja2>=2.4 in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (3.0.3)\n",
      "Requirement already satisfied: mistune<2,>=0.8.1 in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (0.8.4)\n",
      "Requirement already satisfied: testpath in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (0.5.0)\n",
      "Requirement already satisfied: nbclient<0.6.0,>=0.5.0 in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (0.5.9)\n",
      "Requirement already satisfied: jupyterlab-pygments in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (0.1.2)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (1.5.0)\n",
      "Requirement already satisfied: nbformat>=4.4 in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (5.1.3)\n",
      "Requirement already satisfied: defusedxml in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (0.7.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.8/site-packages (from jinja2>=2.4->nbconvert->jupyter) (2.0.1)\n",
      "Requirement already satisfied: ipython-genutils in /opt/conda/lib/python3.8/site-packages (from nbformat>=4.4->nbconvert->jupyter) (0.2.0)\n",
      "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /opt/conda/lib/python3.8/site-packages (from nbformat>=4.4->nbconvert->jupyter) (4.2.1)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /opt/conda/lib/python3.8/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.4->nbconvert->jupyter) (0.18.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /opt/conda/lib/python3.8/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.4->nbconvert->jupyter) (21.2.0)\n",
      "Requirement already satisfied: importlib-resources>=1.4.0 in /opt/conda/lib/python3.8/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.4->nbconvert->jupyter) (5.4.0)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /opt/conda/lib/python3.8/site-packages (from importlib-resources>=1.4.0->jsonschema!=2.5.0,>=2.4->nbformat>=4.4->nbconvert->jupyter) (3.6.0)\n",
      "Requirement already satisfied: webencodings in /opt/conda/lib/python3.8/site-packages (from bleach->nbconvert->jupyter) (0.5.1)\n",
      "Requirement already satisfied: terminado>=0.8.3 in /opt/conda/lib/python3.8/site-packages (from notebook->jupyter) (0.12.1)\n",
      "Requirement already satisfied: prometheus-client in /opt/conda/lib/python3.8/site-packages (from notebook->jupyter) (0.12.0)\n",
      "Requirement already satisfied: argon2-cffi in /opt/conda/lib/python3.8/site-packages (from notebook->jupyter) (21.1.0)\n",
      "Requirement already satisfied: Send2Trash>=1.5.0 in /opt/conda/lib/python3.8/site-packages (from notebook->jupyter) (1.8.0)\n",
      "Requirement already satisfied: cffi>=1.0.0 in /opt/conda/lib/python3.8/site-packages (from argon2-cffi->notebook->jupyter) (1.15.0)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.8/site-packages (from cffi>=1.0.0->argon2-cffi->notebook->jupyter) (2.21)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging->ipykernel->jupyter) (3.0.6)\n",
      "Requirement already satisfied: qtpy>=2.4.0 in /opt/conda/lib/python3.8/site-packages (from qtconsole->jupyter) (2.4.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#installing some libraries\n",
    "#!pip install datasets\n",
    "!pip install --upgrade jupyter ipywidgets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a4f4be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch, transformers\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import PreTrainedModel, PretrainedConfig\n",
    "from transformers import AutoModel, AutoConfig,AutoModelForCausalLM, GPT2Config,GPT2Tokenizer,GPT2LMHeadModel\n",
    "from transformers import BertModel, BertTokenizer, GPT2LMHeadModel,BertConfig,DistilBertModel, DistilBertTokenizer,DistilBertConfig\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "#from datasets import load_dataset\n",
    "import pandas as pd, numpy as np\n",
    "from torch.cuda.amp import autocast\n",
    "from torch import cuda\n",
    "import datetime\n",
    "import warnings,itertools\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "import json\n",
    "import random\n",
    "# Ignore all warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "#pip install transformers bitsandbytes>=0.39.0 -q\n",
    "import zipfile,logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "956005a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "model\n"
     ]
    }
   ],
   "source": [
    "#global params for training\n",
    "\n",
    "B =  32\n",
    "T_g = 1024\n",
    "T_b = 512\n",
    "epoch = 100\n",
    "random_init_wts = True\n",
    "# hard coded compression ratio. This is the ratio of text_len to the # of tokens produced by the tokeinzer\n",
    "comp_ratio = 3\n",
    "\n",
    "if cuda.is_available():\n",
    "    device = torch.device('cuda:0')\n",
    "    print(device)\n",
    "else:\n",
    "    device = 'cpu'\n",
    "#print(device)\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "#os.environ[\"MKL_DEBUG_CPU_TYPE\"] = \"5\"\n",
    "\n",
    "#print(global_tr_loss)\n",
    "model_path = os.path.join(\"model\")\n",
    "print(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55cac234",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./data/unzip_text_10M'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "directory = os.path.join('.','data','unzip_text_10M')  # Replace with your directory path\n",
    "directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "03804f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_text(directory):\n",
    "    directory = os.path.join('.','data','unzip_text_10M',str(directory))  # Replace with your directory path\n",
    "    print(f\"directory :{directory}\")\n",
    "    # List all files in the directory\n",
    "    files = [f for f in os.listdir(directory) if os.path.isfile(os.path.join(directory, f))]\n",
    "    print(f\"files:{files}\")\n",
    "    text_content = []\n",
    "    # Read each file\n",
    "    total_lines = 0\n",
    "    for filenum,filename in enumerate(files):\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            #first_line = file.read()\n",
    "            #print(f\"filename :{filename}->first few lines {first_line}\")\n",
    "            #continue\n",
    "            #lines_list = [line.strip() for line in open(file_path, 'r')]\n",
    "            text = file.read()\n",
    "            text_content.append(text)\n",
    "            print(f\"the file:{filename} has been appeneded to the uber list and its length is {len(text_content)} \")\n",
    "            #total_lines+=len(lines_list)\n",
    "            #text_content.append(lines_list)\n",
    "    \n",
    "    flattened_list = ''.join(text_content)\n",
    "    assert (len(flattened_list) == total_lines , f\"Expected {len(flattened_list)} to be equal to {total_lines}\" )\n",
    "    \n",
    "    return flattened_list\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a87bb0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "directory :./data/unzip_text_10M/train_10M\n",
      "files:['switchboard.train', 'simple_wiki.train', 'open_subtitles.train', 'gutenberg.train', 'childes.train', 'bnc_spoken.train']\n",
      "the file:switchboard.train has been appeneded to the uber list and its length is 1 \n",
      "the file:simple_wiki.train has been appeneded to the uber list and its length is 2 \n",
      "the file:open_subtitles.train has been appeneded to the uber list and its length is 3 \n",
      "the file:gutenberg.train has been appeneded to the uber list and its length is 4 \n",
      "the file:childes.train has been appeneded to the uber list and its length is 5 \n",
      "the file:bnc_spoken.train has been appeneded to the uber list and its length is 6 \n"
     ]
    }
   ],
   "source": [
    "train_list = read_text(\"train_10M\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b257ebb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54215049"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ba22841f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "directory :./data/unzip_text_10M/dev\n",
      "files:['switchboard.dev', 'simple_wiki.dev', 'open_subtitles.dev', 'gutenberg.dev', 'childes.dev', 'bnc_spoken.dev']\n",
      "the file:switchboard.dev has been appeneded to the uber list and its length is 1 \n",
      "the file:simple_wiki.dev has been appeneded to the uber list and its length is 2 \n",
      "the file:open_subtitles.dev has been appeneded to the uber list and its length is 3 \n",
      "the file:gutenberg.dev has been appeneded to the uber list and its length is 4 \n",
      "the file:childes.dev has been appeneded to the uber list and its length is 5 \n",
      "the file:bnc_spoken.dev has been appeneded to the uber list and its length is 6 \n"
     ]
    }
   ],
   "source": [
    "val_list = read_text(\"dev\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2e204b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test the tokenizer:\n",
    "bert_model_name = 'distilbert-base-uncased'\n",
    "gpt2_model_name = 'distilgpt2'\n",
    "bert_tokenizer = DistilBertTokenizer.from_pretrained(bert_model_name)\n",
    "bert_config =  DistilBertConfig()\n",
    "bert_model = DistilBertModel(bert_config)\n",
    "gpt2_config = AutoConfig.from_pretrained(gpt2_model_name, vocab_size = 50304)\n",
    "gpt_model = AutoModelForCausalLM.from_config(gpt2_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "616a9b71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'DistilBertTokenizer'. \n",
      "The class this function is called from is 'BertTokenizer'.\n"
     ]
    }
   ],
   "source": [
    "gpt_tokenizer = GPT2Tokenizer.from_pretrained(\"distilgpt2\")\n",
    "if gpt_tokenizer.pad_token is None:\n",
    "    gpt_tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(bert_model_name)\n",
    "if bert_tokenizer.pad_token is None:\n",
    "    bert_tokenizer.add_special_tokens({'pad_token': '[PAD]'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fa8ae829",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DistilBertTokenizer.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "65141fdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[PAD]'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_tokenizer.pad_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "087e9907",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class BertGPT2EncoderDecoder_bkp(torch.nn.Module):\n",
    "#     def __init__(self, bert_model, gpt_model):\n",
    "#         super(BertGPT2EncoderDecoder, self).__init__()\n",
    "#         self.bert_model = bert_model\n",
    "#         self.gpt_model = gpt_model\n",
    "        \n",
    "#         # Linear layer to match BERT output dim to GPT-2 input dim\n",
    "#         self.linear = torch.nn.Linear(bert_model.config.hidden_size, gpt_model.config.n_embd)\n",
    "\n",
    "#     def forward(self, bet_enc,gpt_enc, labels=None):\n",
    "#         # Encode with BERT\n",
    "#         bert_input_id = bet_enc['input_ids']\n",
    "#         bert_attention = bet_enc['attention_mask']\n",
    "        \n",
    "#         gpt_input_id = gpt_enc['input_ids']\n",
    "#         gpt_attention = gpt_enc['attention_mask']\n",
    "        \n",
    "#         bert_input_id = bert_input_id.view(B,T_b)\n",
    "#         bert_attention = bert_attention.view(B,T_b)\n",
    "        \n",
    "#         print(f\"shape that goes into the model | input_id = {bert_input_id.shape}|attention_mask = {bert_attention.shape}\")\n",
    "        \n",
    "#         encoder_outputs = self.bert_model(input_ids=bert_input_id, attention_mask=bert_attention)\n",
    "#         encoder_hidden_states = encoder_outputs.last_hidden_state  # Shape: [batch_size, seq_len, bert_hidden_size]\n",
    "#         print(f\"shape of Bert output = {encoder_hidden_states.shape}\")\n",
    "                \n",
    "#         # Project BERT output to GPT-2 input dimension\n",
    "#         projected_encoder_hidden_states = self.linear(encoder_hidden_states)  # Shape: [batch_size, seq_len, gpt2_hidden_size]\n",
    "#         print(f\"After linear layer shape = {projected_encoder_hidden_states.shape}\")\n",
    "#         # Decode with GPT-2\n",
    "#         extended_hidden_states = torch.cat([projected_encoder_hidden_states] * 2, dim=1)  # Shape: [batch_size, 1024, gpt2_hidden_size]\n",
    "#         # Extend attention mask\n",
    "#         extended_attention_mask = torch.cat([bert_attention] * 2, dim=1)  # Shape: [batch_size, 1024]\n",
    "#         print(f\"shape of extended_hidden_sate = {extended_hidden_states.shape}| extended attenion shape = {extended_attention_mask.shape}\")\n",
    "#         labels = torch.cat([bert_input_id] * 2, dim=1).view(B,T_g)\n",
    "        \n",
    "        \n",
    "#         decoder_outputs = self.gpt_model(inputs_embeds=extended_hidden_states, attention_mask = extended_attention_mask,\n",
    "#                                           labels=labels)\n",
    "#         #print(f\"decoder_outputs = {decoder_outputs}\")\n",
    "\n",
    "        \n",
    "\n",
    "#         return decoder_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "213c3366",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text (chunk, bert_tokenizer = bert_tokenizer,bert_max_length = B*T_b,gpt_tokenizer = gpt_tokenizer,gpt_max_length = B*T_g):\n",
    "    \n",
    "    \n",
    "    enc_bert = bert_tokenizer(chunk,padding='max_length',truncation=True,max_length=gpt_max_length,return_tensors=\"pt\",return_attention_mask=True)\n",
    "    #enc_gpt = gpt_tokenizer(chunk,padding='max_length',truncation=True,max_length=gpt_max_length,return_tensors=\"pt\",return_attention_mask=True)\n",
    "    \n",
    "    label_enc =gpt_tokenizer(chunk,padding='max_length',truncation=True,max_length=gpt_max_length,return_tensors=\"pt\",return_attention_mask=True)\n",
    "    labels = label_enc['input_ids']\n",
    "    #labels[labels == gpt_tokenizer.pad_token_id] = -100  # Ignore padding tokens in the loss\n",
    "    #print(f\"attention mask = {enc_bert['attention_mask'].shape}| bert_input_id_shape = {label_enc['input_ids'].shape}\")   \n",
    "    \n",
    "    return enc_bert,labels\n",
    "    \n",
    "    \n",
    "    \n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64baa15e",
   "metadata": {},
   "source": [
    "### Data loaders and Dataset for batched training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f9d4d897",
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataset_pyt_train(Dataset):\n",
    "    def __init__(self, text_list, B = B, bert_tokenizer = bert_tokenizer, T_b = T_b,comp_ratio = comp_ratio):\n",
    "        self.text_list = text_list\n",
    "        self.bert_tokeinzer = bert_tokenizer     \n",
    "        \n",
    "                                        \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        chunk = self.text_list[idx:idx + B*T_g*comp_ratio]\n",
    "        enc_bert,labels = tokenize_text(chunk)\n",
    "              \n",
    "        return enc_bert,labels\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        \n",
    "        num_chunks = comp_ratio*B*T_g\n",
    "        return len(self.text_list)//num_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a6a1733f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataset_pyt_val(Dataset):\n",
    "    def __init__(self, text_list, B = B, bert_tokenizer = bert_tokenizer, T_b = T_b,comp_ratio = comp_ratio):\n",
    "        self.text_list = text_list\n",
    "        self.bert_tokeinzer = bert_tokenizer     \n",
    "        \n",
    "                                        \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        chunk = self.text_list[idx:idx + B*T_g*comp_ratio]\n",
    "        enc_bert,labels = tokenize_text(chunk)\n",
    "                \n",
    "        return enc_bert,labels\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        \n",
    "        num_chunks = comp_ratio*B*T_g\n",
    "        return len(self.text_list)//num_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "be2e9161",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class BertGPT2EncoderDecoder(PreTrainedModel):\n",
    "    def __init__(self, bert_model, gpt_model):\n",
    "        config = bert_model.config\n",
    "        super().__init__(config)\n",
    "        self.bert_model = bert_model\n",
    "        self.gpt_model = gpt_model\n",
    "        \n",
    "        # Linear layer to match BERT output dim to GPT-2 input dim\n",
    "        self.linear = torch.nn.Linear(bert_model.config.hidden_size, gpt_model.config.n_embd)\n",
    "\n",
    "    def forward(self, bert_ids,bert_att_mask,labels):\n",
    "        bert_tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')     \n",
    "        bert_input_id = bert_ids.view(B, T_g)\n",
    "        bert_attention = bert_att_mask.view(B, T_g)\n",
    "        assert not torch.isnan(bert_input_id).any(), \"Input IDs contain NaNs\"\n",
    "        assert not torch.isinf(bert_input_id).any(), \"Input IDs contain Infs\"\n",
    "        #we split the 1024 dim tensor into two, each of 512 len \n",
    "        split_tensors_inp = torch.split(bert_input_id, T_b, dim=1)\n",
    "        bert_input_id_chunk1,bert_input_id_chunk2 = split_tensors_inp\n",
    "                \n",
    "        split_tensors_att = torch.split(bert_attention, T_b, dim=1)\n",
    "        bert_att_chunk1,bert_att_chunk2 = split_tensors_att\n",
    "               \n",
    "                 \n",
    "        encoder_outputs_chunk_1 = self.bert_model(input_ids=bert_input_id_chunk1, attention_mask=bert_att_chunk1)\n",
    "        encoder_outputs_chunk_2 = self.bert_model(input_ids=bert_input_id_chunk2, attention_mask=bert_att_chunk2)\n",
    "#         print(f\"encoder_outputs_chunk_1 = {encoder_outputs_chunk_1}\")\n",
    "#         print(f\"encoder_outputs_chunk_2 = {encoder_outputs_chunk_2}\")\n",
    "        \n",
    "        encoder_hidden_states_1 = encoder_outputs_chunk_1.last_hidden_state\n",
    "        encoder_hidden_states_2 = encoder_outputs_chunk_2.last_hidden_state# Shape: [batch_size, seq_len, bert_hidden_size]\n",
    "        \n",
    "#         encoder_hidden_states_1[bert_input_id_chunk1 == bert_tokenizer.pad_token_id] = 0\n",
    "#         encoder_hidden_states_2[bert_input_id_chunk2 == bert_tokenizer.pad_token_id] = 0\n",
    "        \n",
    "        extended_hidden_states = torch.cat((encoder_hidden_states_1, encoder_hidden_states_2),dim=1)  # Shape: [batch_size, 1024, gpt2_hidden_size]\n",
    "        extended_attention_mask = torch.cat((bert_att_chunk1,bert_att_chunk2), dim=1)  # Shape: [batch_size, 1024]\n",
    "        labels = labels.view(B,T_g)\n",
    "        \n",
    "        decoder_outputs = self.gpt_model(inputs_embeds=extended_hidden_states, attention_mask=extended_attention_mask, labels=labels)\n",
    "        #print(f\"decoder out = {decoder_outputs}\")\n",
    "        \n",
    "        return decoder_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7337d8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# low = 1\n",
    "# high = 1000  # high is exclusive in torch.randint, so we use 52548 to include 52547\n",
    "# shape1 = (32, 1024)\n",
    "# dtype = torch.long\n",
    "# shape2 = (32, 1024)\n",
    "# # Create the random integer tensor\n",
    "\n",
    "# m1 =  BertGPT2EncoderDecoder(bert_model = bert_model, gpt_model = gpt_model)\n",
    "# m1.to(device)\n",
    "# bert_id =  torch.randint(low, high, shape1, dtype=dtype,device = device)\n",
    "# att = torch.ones(32,1024,device = device)\n",
    "# labels =  torch.randint(low, high, shape2, dtype=dtype,device = device)\n",
    "# o = m1(bert_id,att,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95b6180",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "36e00c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertGPT2EncoderDecoder(bert_model = bert_model, gpt_model = gpt_model)\n",
    "model.to(device)\n",
    "model = torch.compile(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "008beeb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_dataset = dataset_pyt(filtered_df,tokenizer = tokenizer)\n",
    "train_dataset = dataset_pyt_train(train_list)\n",
    "val_dataset = dataset_pyt_val(val_list)\n",
    "\n",
    "train_loader = DataLoader(train_dataset,batch_size = 1, shuffle = True , num_workers = 4, pin_memory = True)\n",
    "val_loader = DataLoader(val_dataset,batch_size = 1, shuffle = True , num_workers = 4, pin_memory = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "15ce4273",
   "metadata": {},
   "outputs": [],
   "source": [
    "# b1 = torch.rand(64,512,768)\n",
    "# b1.shape\n",
    "# g1 = torch.rand(64,512)\n",
    "# g1.shape\n",
    "# cat = torch.cat((b1,g1),dim = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "20cf4c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# enc_bert,labels = train_dataset[1]\n",
    "# bert_ids = enc_bert['input_ids']\n",
    "# bert_att_mask = enc_bert['attention_mask']\n",
    "# bert_ids = bert_ids.to(device=device, non_blocking=True)\n",
    "# bert_ids = torch.squeeze(bert_ids, dim = 0)\n",
    "# bert_att_mask = bert_att_mask.to(device=device, non_blocking=True)\n",
    "# bert_att_mask =  torch.squeeze(bert_att_mask, dim = 0)\n",
    "# labels = labels.to(device=device, non_blocking=True)\n",
    "# labels = torch.squeeze(labels, dim = 0).view(B,T_g)\n",
    "# print(f\"bert_ids shape = {bert_ids.shape}|att_mask shape = {bert_att_mask.shape}| labels shape = {labels.shape}\")\n",
    "# model.to(device)\n",
    "# #bert_ids,bert_att_mask,ids_gpt,att_gpt, labels\n",
    "\n",
    "# o = model(bert_ids , bert_att_mask,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72216a28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2bcbd51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.to(device)\n",
    "# for i in range(0,len(train_dataset)):\n",
    "#     enc_bert,labels = train_dataset[i]\n",
    "#     bert_ids = enc_bert['input_ids']\n",
    "#     bert_att_mask = enc_bert['attention_mask']\n",
    "#     bert_ids = bert_ids.to(device=device, non_blocking=True)\n",
    "#     bert_ids = torch.squeeze(bert_ids, dim = 0)\n",
    "#     bert_att_mask = bert_att_mask.to(device=device, non_blocking=True)\n",
    "#     bert_att_mask =  torch.squeeze(bert_att_mask, dim = 0)\n",
    "#     #gpt_extraction\n",
    "#     labels = labels.to(device=device, non_blocking=True)\n",
    "#     labels = torch.squeeze(labels, dim = 0).view(B,T_g)\n",
    "# #bert_ids,bert_att_mask,ids_gpt,att_gpt, labels\n",
    "#     with autocast(dtype = torch.bfloat16):\n",
    "#         model_output = model(bert_ids = bert_ids ,bert_att_mask = bert_att_mask,labels = labels)\n",
    "#         total_loss = model_output.loss\n",
    "#         print(f\"index = {i}|loss = {total_loss}\")\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "56ee98fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = torch.rand(1,65536)\n",
    "# test = torch.squeeze(test,dim = 0)\n",
    "# test = test.view(B,T_g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68971d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a7504e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_file(log_message, model_name = \"enc_dec\" ,random_init_wts = random_init_wts ):\n",
    "    current_datetime = datetime.datetime.now()\n",
    "    # Extract date and time components\n",
    "    current_date = str(current_datetime.date())\n",
    "    log_file = model_name +'_nll_two_bert_encoder_'+'random_init_wts'+ '_'+str(random_init_wts)+'_' +current_date+'.log'\n",
    "    print(f\"*****LOGGING INFO IN {log_file}*********\")\n",
    "    filepath = os.path.join(\"model\",log_file)\n",
    "    logging.basicConfig(filename=filepath, \n",
    "                    filemode='a',  # Overwrite the log file each time\n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s', \n",
    "                    level=logging.DEBUG)\n",
    "    logger = logging.getLogger()\n",
    "    logger.info(log_message)\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f375acbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the train loader is 551\n",
      "Length of the val loader is 575\n",
      "num_tokens= 18055168\n"
     ]
    }
   ],
   "source": [
    "print(f\"Length of the train loader is {len(train_loader)}\")\n",
    "print(f\"Length of the val loader is {len(val_loader)}\")\n",
    "print(f\"num_tokens= {B*T_g*len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "31fc164c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#emb,att,inp = train_dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fc7455d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class check_train_metrics:\n",
    "    def __init__(self, patience=20, min_delta=0 , B = B, T = T_g,best_loss = torch.inf,early_stop = False):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.train_counter = 0\n",
    "        self.best_loss = best_loss\n",
    "        self.early_stop = early_stop\n",
    "        self.improvement = None\n",
    "\n",
    "    def __call__(self, loss, epoch , epoch_durn, norm , current_lr, num_token):\n",
    "        if self.best_loss - loss > self.min_delta:\n",
    "            \n",
    "            print(f\"training loss has decreased---> reducing the best loss from {self.best_loss:.2f} to {loss:.2f} | throughput = {int(num_token/epoch_durn)} tokens/second | norm = {norm:.4f} | learning rate = {current_lr:.5e}\")\n",
    "            self.best_loss = loss\n",
    "            self.train_counter = 0\n",
    "            self.improvement = True\n",
    "        else:\n",
    "            self.train_counter += 1\n",
    "            self.improvement = False\n",
    "            print(f\"No improvement in training  loss-->epoch= {epoch} and best loss is {self.best_loss:.2f}|current_loss = {loss}|counter = {self.train_counter}\")\n",
    "            if self.train_counter >= self.patience:\n",
    "                self.early_stop = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e787a32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class check_val_metrics:\n",
    "    def __init__(self, patience=10, min_delta=0, best_loss = torch.inf,early_stop = False):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.val_counter = 0\n",
    "        self.best_loss = best_loss\n",
    "        self.early_stop = early_stop\n",
    "        \n",
    "\n",
    "    def __call__(self, loss, epoch , model, bert_tokenizer):\n",
    "        if self.best_loss - loss > self.min_delta:\n",
    "            print(f\"Val loss has decreased -->reducing the global validation loss from {self.best_loss:.2f} to {loss:.2f}\")\n",
    "            s1 = (f\"Val loss has decreased -->reducing the global validation loss from {self.best_loss:.2f} to {loss:.2f}\")\n",
    "            print(f\" validation loss for epoch = {epoch} is {loss:.4f}\")\n",
    "            self.best_loss = loss\n",
    "            s2 = f\" validation loss for epoch = {epoch} is {loss:.4f}\"\n",
    "            print(f\" epoch= {epoch} :  val loss is {loss:.4f} \")\n",
    "            s3 = f\" epoch= {epoch} :  val loss is {loss:.4f} \"\n",
    "            #save the model\n",
    "            # Get the current date and time\n",
    "            current_datetime = datetime.datetime.now()\n",
    "            # Extract date and time components\n",
    "            current_date = str(current_datetime.date())\n",
    "            current_time = str(current_datetime.time()).split('.')[0]\n",
    "            file_name = 'model'+ current_date+current_time+'.pth'\n",
    "            path = os.path.join(\"model\",file_name)\n",
    "            print(f\"saving the model {file_name}\")\n",
    "            s4 = f\"saving the model {file_name}\"\n",
    "            #torch.save(model.state_dict(), path)\n",
    "            model.save_pretrained(path)\n",
    "            bert_tokenizer.save_pretrained(path)\n",
    "            log_message = s1+s2+s3+s4\n",
    "            write_file(log_message)\n",
    "            self.val_counter = 0\n",
    "        else:\n",
    "            self.val_counter += 1\n",
    "            print(f\"No improvement in validation loss-->epoch= {epoch} and best val loss is {self.best_loss:.2f}|current_Val loss = {loss}|val_counter = {self.val_counter}\")\n",
    "            if self.val_counter >= self.patience:\n",
    "                self.early_stop = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "399828ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_output = model(input_ids = inp ,attention_mask = att, labels = inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2ce05662",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad\n",
    "\n",
    "def eval_model(val_loader, model, epoch , device = device,tokenizer = bert_tokenizer):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    e = epoch+1\n",
    "    val_loss_accum = 0.0\n",
    "    \n",
    "    print(f\"inside validation data for epoch {e}\")\n",
    "    for ind,(enc_bert,labels) in enumerate(val_loader):\n",
    "        bert_ids = enc_bert['input_ids']\n",
    "        bert_att_mask = enc_bert['attention_mask']\n",
    "        bert_ids = bert_ids.to(device=device, non_blocking=True)\n",
    "        bert_ids = torch.squeeze(bert_ids, dim = 0)\n",
    "        bert_att_mask = bert_att_mask.to(device=device, non_blocking=True)\n",
    "        bert_att_mask =  torch.squeeze(bert_att_mask, dim = 0)\n",
    "        labels = labels.to(device=device, non_blocking=True)\n",
    "        labels = torch.squeeze(labels, dim = 0).view(B,T_g)\n",
    "        #bert_ids,bert_att_mask,labels\n",
    "        with autocast(dtype = torch.bfloat16):\n",
    "            model_output = model(bert_ids = bert_ids ,bert_att_mask = bert_att_mask,labels = labels)\n",
    "            total_loss = model_output.loss\n",
    "            \n",
    "    \n",
    "    \n",
    "        val_loss_accum+= total_loss.detach().item()\n",
    "        del bert_att_mask,bert_ids,labels,model_output\n",
    "    return val_loss_accum        \n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "83af6557",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_model(train_loader,val_loader,model,num_epoch = epoch,device = device,tokenizer = bert_tokenizer):\n",
    "    #model.train()\n",
    "    device = device\n",
    "    lr_custom = 1e-6\n",
    "    print(f\"inside train model. Device = {device}\")\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.AdamW(params =  model.parameters(), lr= lr_custom,fused = True ,weight_decay = .1)\n",
    "      \n",
    "    extra_train = .1*num_epoch\n",
    "    max_train_steps = int(num_epoch +extra_train )\n",
    "    import time\n",
    "    from transformers import get_linear_schedule_with_warmup\n",
    "    total_steps = len(train_loader) * num_epoch\n",
    "    scheduler_cos = transformers.get_cosine_schedule_with_warmup( optimizer= optimizer, num_warmup_steps =int(total_steps * 0.1) ,num_training_steps= total_steps )\n",
    "        \n",
    "    epoch_train_log = []\n",
    "    epoch_val_log = []\n",
    "    validate_val_metric = check_val_metrics()\n",
    "    validate_train_metric = check_train_metrics()\n",
    "    for i in range (max_train_steps):\n",
    "        \n",
    "        epoch_start_time = time.time()\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        # we use 2 schedulers - the first LR scheduler uses a cosine decay for 100 epochs the second scheduler takes the last LR from cosine scheduler and then maintains that LR for the next 10 epochs\n",
    "        if i >= num_epoch:\n",
    "            optimizer_reduced_lr = torch.optim.AdamW(params =  model.parameters(), lr= current_lr ,fused = True , weight_decay=.1)\n",
    "            scheduler_constant = transformers.get_constant_schedule_with_warmup( optimizer = optimizer_reduced_lr ,num_warmup_steps = 0, last_epoch = -1 )\n",
    "        \n",
    "        epoch_train_loss = 0       \n",
    "        for ind,(enc_bert,labels) in enumerate(train_loader):\n",
    "            if ind == int(len(train_loader)/2):\n",
    "                batch_time = time.time()\n",
    "                duration = batch_time - epoch_start_time\n",
    "                print(f\"executing epoch:{i+1}, it took {duration/60} mins from beginning of epoch till batch#{ind}\")\n",
    "            \n",
    "            #bert extraction\n",
    "            bert_ids = enc_bert['input_ids']\n",
    "            bert_att_mask = enc_bert['attention_mask']\n",
    "            bert_ids = bert_ids.to(device=device, non_blocking=True)\n",
    "            bert_ids = torch.squeeze(bert_ids, dim = 0)\n",
    "            bert_att_mask = bert_att_mask.to(device=device, non_blocking=True)\n",
    "            bert_att_mask =  torch.squeeze(bert_att_mask, dim = 0)\n",
    "            labels = labels.to(device=device, non_blocking=True)\n",
    "            labels = torch.squeeze(labels, dim = 0).view(B,T_g)\n",
    "            #bert_ids,bert_att_mask,labels\n",
    "            with autocast(dtype = torch.bfloat16):\n",
    "                model_output = model(bert_ids = bert_ids ,bert_att_mask = bert_att_mask,labels = labels)\n",
    "                total_loss = model_output.loss\n",
    "            \n",
    "                    \n",
    "            total_loss.backward()\n",
    "            epoch_train_loss += total_loss.detach().item()\n",
    "            norm = torch.nn.utils.clip_grad_norm(model.parameters() , 1.0)\n",
    "            if i <= num_epoch:\n",
    "                optimizer.step()\n",
    "                scheduler_cos.step()\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "            else:\n",
    "                optimizer_reduced_lr.step()\n",
    "                optimizer_reduced_lr.zero_grad(set_to_none=True)\n",
    "                scheduler_constant.step()\n",
    "                \n",
    "                         \n",
    "            del bert_att_mask,bert_ids,labels,model_output\n",
    "            \n",
    "        #batch processing complete \n",
    "        #print(f\"batch processing complete , lambda = {lambda_val} |total_loss for batch= {total_loss}\")\n",
    "        \n",
    "        if i <= num_epoch:\n",
    "            current_lr = scheduler_cos.get_last_lr()[0]\n",
    "        epoch_end_time = time.time()\n",
    "        epoch_durn = (epoch_end_time - epoch_start_time)\n",
    "        num_token = B*T_g*len(train_loader)\n",
    "        epoch_train_log.append(epoch_train_loss)\n",
    "        validate_train_metric(epoch_train_loss, i , epoch_durn, norm , current_lr, num_token)\n",
    "        \n",
    "        if validate_train_metric.improvement:\n",
    "            val_loss= eval_model(val_loader, model, epoch = i, device = device,tokenizer = bert_tokenizer)\n",
    "            epoch_val_log.append(val_loss)\n",
    "            validate_val_metric(val_loss, i , model, bert_tokenizer)\n",
    "            if validate_train_metric.early_stop or validate_val_metric.early_stop :\n",
    "                print(f\"early stopping trigerred either from training data or val data | train_counter = {validate_train_metric.train_counter}|val_counter = {validate_val_metric.val_counter}\")\n",
    "                break\n",
    "        else:\n",
    "            if validate_val_metric.early_stop:\n",
    "                print(f\"early stopping trigerred from validation data\")\n",
    "                break\n",
    "              \n",
    "    \n",
    "    return model,epoch_train_log,epoch_val_log\n",
    "        \n",
    "            \n",
    "            \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "72c34a51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inside train model. Device = cuda:0\n",
      "executing epoch:1, it took 2.5606335043907165 mins from beginning of epoch till batch#275\n",
      "training loss has decreased---> reducing the best loss from inf to 5970.55 | throughput = 68387 tokens/second | norm = 3.7244 | learning rate = 1.00000e-07\n",
      "inside validation data for epoch 1\n",
      "Val loss has decreased -->reducing the global validation loss from inf to 6048.80\n",
      " validation loss for epoch = 0 is 6048.8028\n",
      " epoch= 0 :  val loss is 6048.8028 \n",
      "saving the model model2024-07-3121:50:54.pth\n",
      "*****LOGGING INFO IN enc_dec_nll_two_bert_encoder_random_init_wts_True_2024-07-31.log*********\n",
      "executing epoch:2, it took 2.4919617255528768 mins from beginning of epoch till batch#275\n",
      "training loss has decreased---> reducing the best loss from 5970.55 to 5406.48 | throughput = 70691 tokens/second | norm = 4.7988 | learning rate = 2.00000e-07\n",
      "inside validation data for epoch 2\n",
      "Val loss has decreased -->reducing the global validation loss from 6048.80 to 5196.53\n",
      " validation loss for epoch = 1 is 5196.5320\n",
      " epoch= 1 :  val loss is 5196.5320 \n",
      "saving the model model2024-07-3121:57:17.pth\n",
      "*****LOGGING INFO IN enc_dec_nll_two_bert_encoder_random_init_wts_True_2024-07-31.log*********\n",
      "executing epoch:3, it took 1.822866110006968 mins from beginning of epoch till batch#275\n",
      "training loss has decreased---> reducing the best loss from 5406.48 to 4679.80 | throughput = 82940 tokens/second | norm = 4.0924 | learning rate = 3.00000e-07\n",
      "inside validation data for epoch 3\n",
      "Val loss has decreased -->reducing the global validation loss from 5196.53 to 4711.60\n",
      " validation loss for epoch = 2 is 4711.5998\n",
      " epoch= 2 :  val loss is 4711.5998 \n",
      "saving the model model2024-07-3122:03:02.pth\n",
      "*****LOGGING INFO IN enc_dec_nll_two_bert_encoder_random_init_wts_True_2024-07-31.log*********\n",
      "executing epoch:4, it took 1.7946237087249757 mins from beginning of epoch till batch#275\n",
      "training loss has decreased---> reducing the best loss from 4679.80 to 4395.44 | throughput = 82893 tokens/second | norm = 3.9486 | learning rate = 4.00000e-07\n",
      "inside validation data for epoch 4\n",
      "Val loss has decreased -->reducing the global validation loss from 4711.60 to 4520.84\n",
      " validation loss for epoch = 3 is 4520.8357\n",
      " epoch= 3 :  val loss is 4520.8357 \n",
      "saving the model model2024-07-3122:08:48.pth\n",
      "*****LOGGING INFO IN enc_dec_nll_two_bert_encoder_random_init_wts_True_2024-07-31.log*********\n",
      "executing epoch:5, it took 1.8207552552223205 mins from beginning of epoch till batch#275\n",
      "training loss has decreased---> reducing the best loss from 4395.44 to 4216.14 | throughput = 82616 tokens/second | norm = 3.7790 | learning rate = 5.00000e-07\n",
      "inside validation data for epoch 5\n",
      "Val loss has decreased -->reducing the global validation loss from 4520.84 to 4342.70\n",
      " validation loss for epoch = 4 is 4342.6976\n",
      " epoch= 4 :  val loss is 4342.6976 \n",
      "saving the model model2024-07-3122:14:34.pth\n",
      "*****LOGGING INFO IN enc_dec_nll_two_bert_encoder_random_init_wts_True_2024-07-31.log*********\n",
      "executing epoch:6, it took 1.8441206693649292 mins from beginning of epoch till batch#275\n",
      "training loss has decreased---> reducing the best loss from 4216.14 to 4024.92 | throughput = 82684 tokens/second | norm = 3.5436 | learning rate = 6.00000e-07\n",
      "inside validation data for epoch 6\n",
      "Val loss has decreased -->reducing the global validation loss from 4342.70 to 4142.83\n",
      " validation loss for epoch = 5 is 4142.8285\n",
      " epoch= 5 :  val loss is 4142.8285 \n",
      "saving the model model2024-07-3122:20:18.pth\n",
      "*****LOGGING INFO IN enc_dec_nll_two_bert_encoder_random_init_wts_True_2024-07-31.log*********\n",
      "executing epoch:7, it took 1.8198819001515707 mins from beginning of epoch till batch#275\n",
      "training loss has decreased---> reducing the best loss from 4024.92 to 3810.23 | throughput = 83230 tokens/second | norm = 3.2100 | learning rate = 7.00000e-07\n",
      "inside validation data for epoch 7\n",
      "Val loss has decreased -->reducing the global validation loss from 4142.83 to 3920.59\n",
      " validation loss for epoch = 6 is 3920.5930\n",
      " epoch= 6 :  val loss is 3920.5930 \n",
      "saving the model model2024-07-3122:26:02.pth\n",
      "*****LOGGING INFO IN enc_dec_nll_two_bert_encoder_random_init_wts_True_2024-07-31.log*********\n",
      "executing epoch:8, it took 1.8090359250704446 mins from beginning of epoch till batch#275\n",
      "training loss has decreased---> reducing the best loss from 3810.23 to 3578.02 | throughput = 82986 tokens/second | norm = 2.7587 | learning rate = 8.00000e-07\n",
      "inside validation data for epoch 8\n",
      "Val loss has decreased -->reducing the global validation loss from 3920.59 to 3685.55\n",
      " validation loss for epoch = 7 is 3685.5526\n",
      " epoch= 7 :  val loss is 3685.5526 \n",
      "saving the model model2024-07-3122:31:46.pth\n",
      "*****LOGGING INFO IN enc_dec_nll_two_bert_encoder_random_init_wts_True_2024-07-31.log*********\n",
      "executing epoch:9, it took 1.8013264099756876 mins from beginning of epoch till batch#275\n",
      "training loss has decreased---> reducing the best loss from 3578.02 to 3343.24 | throughput = 83319 tokens/second | norm = 2.1856 | learning rate = 9.00000e-07\n",
      "inside validation data for epoch 9\n",
      "Val loss has decreased -->reducing the global validation loss from 3685.55 to 3457.25\n",
      " validation loss for epoch = 8 is 3457.2513\n",
      " epoch= 8 :  val loss is 3457.2513 \n",
      "saving the model model2024-07-3122:37:31.pth\n",
      "*****LOGGING INFO IN enc_dec_nll_two_bert_encoder_random_init_wts_True_2024-07-31.log*********\n",
      "executing epoch:10, it took 1.8032696843147278 mins from beginning of epoch till batch#275\n",
      "training loss has decreased---> reducing the best loss from 3343.24 to 3127.47 | throughput = 83345 tokens/second | norm = 1.5438 | learning rate = 1.00000e-06\n",
      "inside validation data for epoch 10\n",
      "Val loss has decreased -->reducing the global validation loss from 3457.25 to 3262.24\n",
      " validation loss for epoch = 9 is 3262.2393\n",
      " epoch= 9 :  val loss is 3262.2393 \n",
      "saving the model model2024-07-3122:43:15.pth\n",
      "*****LOGGING INFO IN enc_dec_nll_two_bert_encoder_random_init_wts_True_2024-07-31.log*********\n",
      "executing epoch:11, it took 1.8085614244143169 mins from beginning of epoch till batch#275\n",
      "training loss has decreased---> reducing the best loss from 3127.47 to 2955.33 | throughput = 83669 tokens/second | norm = 0.9583 | learning rate = 9.99695e-07\n",
      "inside validation data for epoch 11\n",
      "Val loss has decreased -->reducing the global validation loss from 3262.24 to 3130.58\n",
      " validation loss for epoch = 10 is 3130.5828\n",
      " epoch= 10 :  val loss is 3130.5828 \n",
      "saving the model model2024-07-3122:48:59.pth\n",
      "*****LOGGING INFO IN enc_dec_nll_two_bert_encoder_random_init_wts_True_2024-07-31.log*********\n",
      "executing epoch:12, it took 1.8072959780693054 mins from beginning of epoch till batch#275\n",
      "training loss has decreased---> reducing the best loss from 2955.33 to 2851.46 | throughput = 83146 tokens/second | norm = 0.6064 | learning rate = 9.98782e-07\n",
      "inside validation data for epoch 12\n",
      "Val loss has decreased -->reducing the global validation loss from 3130.58 to 3073.16\n",
      " validation loss for epoch = 11 is 3073.1568\n",
      " epoch= 11 :  val loss is 3073.1568 \n",
      "saving the model model2024-07-3122:54:42.pth\n",
      "*****LOGGING INFO IN enc_dec_nll_two_bert_encoder_random_init_wts_True_2024-07-31.log*********\n",
      "executing epoch:13, it took 1.7920223077138264 mins from beginning of epoch till batch#275\n",
      "training loss has decreased---> reducing the best loss from 2851.46 to 2800.55 | throughput = 84479 tokens/second | norm = 0.4259 | learning rate = 9.97261e-07\n",
      "inside validation data for epoch 13\n",
      "Val loss has decreased -->reducing the global validation loss from 3073.16 to 3051.45\n",
      " validation loss for epoch = 12 is 3051.4508\n",
      " epoch= 12 :  val loss is 3051.4508 \n",
      "saving the model model2024-07-3123:00:21.pth\n",
      "*****LOGGING INFO IN enc_dec_nll_two_bert_encoder_random_init_wts_True_2024-07-31.log*********\n",
      "executing epoch:14, it took 1.8116762598355611 mins from beginning of epoch till batch#275\n",
      "training loss has decreased---> reducing the best loss from 2800.55 to 2766.29 | throughput = 83234 tokens/second | norm = 0.3195 | learning rate = 9.95134e-07\n",
      "inside validation data for epoch 14\n",
      "Val loss has decreased -->reducing the global validation loss from 3051.45 to 3044.08\n",
      " validation loss for epoch = 13 is 3044.0767\n",
      " epoch= 13 :  val loss is 3044.0767 \n",
      "saving the model model2024-07-3123:06:04.pth\n",
      "*****LOGGING INFO IN enc_dec_nll_two_bert_encoder_random_init_wts_True_2024-07-31.log*********\n",
      "executing epoch:15, it took 1.799920658270518 mins from beginning of epoch till batch#275\n",
      "training loss has decreased---> reducing the best loss from 2766.29 to 2739.48 | throughput = 83526 tokens/second | norm = 0.3808 | learning rate = 9.92404e-07\n",
      "inside validation data for epoch 15\n",
      "No improvement in validation loss-->epoch= 14 and best val loss is 3044.08|current_Val loss = 3045.630513191223|val_counter = 1\n",
      "executing epoch:16, it took 1.8006656209627787 mins from beginning of epoch till batch#275\n",
      "training loss has decreased---> reducing the best loss from 2739.48 to 2708.67 | throughput = 83816 tokens/second | norm = 0.6007 | learning rate = 9.89074e-07\n",
      "inside validation data for epoch 16\n",
      "No improvement in validation loss-->epoch= 15 and best val loss is 3044.08|current_Val loss = 3051.864559173584|val_counter = 2\n",
      "executing epoch:17, it took 1.796460751692454 mins from beginning of epoch till batch#275\n",
      "training loss has decreased---> reducing the best loss from 2708.67 to 2673.29 | throughput = 83284 tokens/second | norm = 1.2290 | learning rate = 9.85148e-07\n",
      "inside validation data for epoch 17\n",
      "No improvement in validation loss-->epoch= 16 and best val loss is 3044.08|current_Val loss = 3060.584671020508|val_counter = 3\n",
      "executing epoch:18, it took 1.801487147808075 mins from beginning of epoch till batch#275\n",
      "training loss has decreased---> reducing the best loss from 2673.29 to 2647.26 | throughput = 83582 tokens/second | norm = 2.9618 | learning rate = 9.80631e-07\n",
      "inside validation data for epoch 18\n",
      "No improvement in validation loss-->epoch= 17 and best val loss is 3044.08|current_Val loss = 3066.8265743255615|val_counter = 4\n",
      "executing epoch:19, it took 2.197329819202423 mins from beginning of epoch till batch#275\n",
      "training loss has decreased---> reducing the best loss from 2647.26 to 2626.92 | throughput = 68792 tokens/second | norm = 4.0760 | learning rate = 9.75528e-07\n",
      "inside validation data for epoch 19\n",
      "No improvement in validation loss-->epoch= 18 and best val loss is 3044.08|current_Val loss = 3073.9782538414|val_counter = 5\n",
      "executing epoch:20, it took 2.212376383940379 mins from beginning of epoch till batch#275\n",
      "training loss has decreased---> reducing the best loss from 2626.92 to 2609.91 | throughput = 68233 tokens/second | norm = 4.6805 | learning rate = 9.69846e-07\n",
      "inside validation data for epoch 20\n",
      "No improvement in validation loss-->epoch= 19 and best val loss is 3044.08|current_Val loss = 3076.548236846924|val_counter = 6\n",
      "executing epoch:21, it took 2.2381405154863994 mins from beginning of epoch till batch#275\n",
      "training loss has decreased---> reducing the best loss from 2609.91 to 2594.18 | throughput = 67442 tokens/second | norm = 4.4164 | learning rate = 9.63592e-07\n",
      "inside validation data for epoch 21\n",
      "No improvement in validation loss-->epoch= 20 and best val loss is 3044.08|current_Val loss = 3079.3595662117004|val_counter = 7\n",
      "executing epoch:22, it took 1.8233633438746135 mins from beginning of epoch till batch#275\n",
      "training loss has decreased---> reducing the best loss from 2594.18 to 2577.87 | throughput = 82982 tokens/second | norm = 4.3703 | learning rate = 9.56773e-07\n",
      "inside validation data for epoch 22\n",
      "No improvement in validation loss-->epoch= 21 and best val loss is 3044.08|current_Val loss = 3086.1982641220093|val_counter = 8\n",
      "executing epoch:23, it took 1.7998080174128215 mins from beginning of epoch till batch#275\n",
      "training loss has decreased---> reducing the best loss from 2577.87 to 2561.60 | throughput = 83893 tokens/second | norm = 9.1222 | learning rate = 9.49397e-07\n",
      "inside validation data for epoch 23\n",
      "No improvement in validation loss-->epoch= 22 and best val loss is 3044.08|current_Val loss = 3086.8672313690186|val_counter = 9\n",
      "executing epoch:24, it took 1.8028644919395447 mins from beginning of epoch till batch#275\n",
      "training loss has decreased---> reducing the best loss from 2561.60 to 2545.30 | throughput = 83553 tokens/second | norm = 7.9988 | learning rate = 9.41474e-07\n",
      "inside validation data for epoch 24\n",
      "No improvement in validation loss-->epoch= 23 and best val loss is 3044.08|current_Val loss = 3093.1449308395386|val_counter = 10\n",
      "early stopping trigerred either from training data or val data | train_counter = 0|val_counter = 10\n"
     ]
    }
   ],
   "source": [
    "tr_model,epoch_train_log,epoch_val_log = train_model(train_loader, val_loader,model=model,tokenizer = bert_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6a97cb8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32768"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "64*512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f7a09e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json , os\n",
    "# path_var_train_log = os.path.join(\".\",\"Tokenizers_and_loss_vals\",\"epoch_train_plain_loss.json\")\n",
    "# path_var_val_log = os.path.join(\".\",\"Tokenizers_and_loss_vals\",\"epoch_val_val_loss_.json\")\n",
    "\n",
    "# #print(path_var)\n",
    "# #Write the list to a JSON file\n",
    "# with open(path_var_train_log, \"w\") as file:\n",
    "#     json.dump(epoch_train_log, file)\n",
    "\n",
    "# with open(path_var_val_log, \"w\") as file:\n",
    "#     json.dump(epoch_val_log, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c23d5833",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(path_var_train_log, \"r\") as file:\n",
    "#     train_loss = json.load(file)\n",
    "# with open(path_var_val_log, \"r\") as file:\n",
    "#     val_loss = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7fa99154",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f1745a5dfa0>]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_values = range(len(epoch_train_log))\n",
    "plt.plot(x_values, epoch_train_log, label='Train_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4413b18c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f1745c7f6d0>]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#epoch_val_log= [t.detach().cpu().numpy() for t in epoch_val_log]\n",
    "x_values_val = range(len(epoch_val_log))\n",
    "plt.plot(x_values_val, epoch_val_log, label='val_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3b1154ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f150e2285e0>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEWCAYAAACXGLsWAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8/fFQqAAAACXBIWXMAAAsTAAALEwEAmpwYAAA8dklEQVR4nO3dd3hUZfbA8e9Jr6SQEEroYkHRgKGpKKAiLLjYVsAGArJ2XV3ruj/Luk1d27qioKK4IrJWRBEVYQFFIHREXDG0QIAQSAIJ6ef3x72BAVJJJkOS83me+8y97y1zZoxzeMu9r6gqxhhjTGX8fB2AMcaYE58lC2OMMVWyZGGMMaZKliyMMcZUyZKFMcaYKlmyMMYYUyVLFsYYY6pkycI0CCIyW0RG+zoOXxGR/iKS5oXrdhARFZGAur62aVwsWRivEZEDHkupiBz02L62JtdS1SGq+tZxxrFZRC46nnMbAhHZICJjyym/S0RS6ug95ovI+Lq4lmmYLFkYr1HViLIF2Apc6lH2Ttlx9q/aWnsLuKGc8uvdfcbUmiULU+/KmlRE5AER2QlMEZEYEZklIhkiss9dT/Q459C/bEVkjIgsEpFn3GM3iciQ44gjWESeF5Ed7vK8iAS7++LcGLJEZK+ILBQRP3ffAyKyXUT2i8hPInJhBdcfKiIrRSRHRLaJyGMe+8qaf0aLyFYR2SMif/DYHyoib7qfbz3Qs5KP8jZwnoi09zi/K3Am8G5lcdSWiPiJyCMiskVEdovIVBGJcveFiMi/RSTT/R6XiUiCu2+MiKS63+GmmtY0Tf2zZGF8pSUQC7QHJuD8LU5xt9sBB4GXKjm/N/ATEAc8BbwuIlLDGP4A9AGSgLOAXsAj7r57gTQgHkgAHgZURE4Bbgd6qmokcAmwuYLr5+L8iz8aGArcIiKXHXXMecApwIXA/4nIaW75o0Bnd7kEqLC/RlXTgHk4NYky1wOfq+qeasZxvMa4ywCgExDB4f9uo4EooC3QHLgZOCgi4cCLwBD3OzwHWFVH8RgvsWRhfKUUeFRVC1T1oKpmquoHqpqnqvuBPwMXVHL+FlWdrKolOE0trXB+1GviWuAJVd2tqhnA4xz+wS1yr9leVYtUdaE6T90sAYKBriISqKqbVfWX8i6uqvNVda2qlqrqGuDdcj7T4+7nXw2sxklaAFcDf1bVvaq6DefHtTJvlcXu1oCudcuqG8fxuhZ4VlVTVfUA8BAw0m1aLMJJEiepaomqLlfVHPe8UuAMEQlV1XRV/aGO4jFeYsnC+EqGquaXbYhImIi86jZn5AALgGgR8a/g/J1lK6qa565G1DCG1sAWj+0tbhnA08BG4Eu3ueRB9702AncDjwG7RWS6iLSmHCLSW0TmuU1r2Tj/so6r6HMAeR6foTWw7ajYKvMh0EpE+gD9gTDgsxrEcbzK+w4DcBL328AcYLrbzPeUm2BzgRFuHOki8pmInFpH8RgvsWRhfOXoZ+Pfi9Mc01tVmwHnu+U1bVqqiR04zV5l2rllqOp+Vb1XVTsBvwbuKeubUNVpqnqee64Cf6/g+tOAmUBbVY0CXqH6nycdp/nGM7YKuQnzfZzmpuuB6apaWAdxVKW877AY2OXWyB5X1a44TU3D3PhQ1TmqejFO7W0DMLmO4jFeYsnCnCgicfopskQkFqfNvi4Fuh2uZUsATnPMIyISLyJxwP8B/wYQkWEicpLbD5KN0/xUKiKniMhAtyM83425tJLPtFdV80WkF3BNDeKdATzkdvwnAndU45y3cP7FfiVHjoKqTRyeAo76DgNxvsPfiUhHEYkA/gK8p6rFIjJARLq5tcMcnGapUhFJEJHhbt9FAXCAir9Dc4KwZGFOFM8DocAe4Hvgizq+/uc4P+xly2PAk0AKsAZYC6xwywC6AF/j/JAtBl5W1Xk4/RV/c+PcCbTAaacvz63AEyKyHycRzahBvI/jNOlsAr7EadKpygKcxJamqsvqKA5PEznyO5wCvOHGtsCNNZ/Dia0lTm0nB/gR+K97rB9wD06tZC9O/8ktxxmTqSdiM+UZY4ypitUsjDHGVMnunDXGAM7jWSrYNURVF9ZrMOaEY81QxhhjqtQoaxZxcXHaoUMHX4dhjDENyvLly/eoanx5+xplsujQoQMpKXXysE1jjGkyRKTCmz+tg9sYY0yVLFkYY4ypkiULY4wxVWqUfRbGmManqKiItLQ08vPzqz7YVCokJITExEQCAwOrfY4lC2NMg5CWlkZkZCQdOnSg5lOXmDKqSmZmJmlpaXTs2LHa51kzlDGmQcjPz6d58+aWKGpJRGjevHmNa2iWLIwxDYYlirpxPN+jJQtPeXth/t8hfbWvIzHGmBOKV5OFiESLyPsiskFEfhSRviISKyJficjP7muMe6yIyIsislFE1ohID4/rjHaP/1lEKpyLuNb8/OG/f4f1M732FsYY0xB5u2bxAvCFqp6KM7fwj8CDwFxV7QLMdbcBhuDMIdAFmIDz7Hw8JsLpDfQCHi1LMHUuJAoSe8Iv33jl8saYhiszM5OkpCSSkpJo2bIlbdq0ObRdWFhY6bkpKSnceeedx/W+ERE1nS3YO7w2GkpEonCmxhwD4E7xWCgiw3HmCAZnNq/5wAPAcGCqOk82/N6tlbRyj/1KVfe61/0KGIwzQ1fd6zwQ5v8VcjMhvLlX3sIY0/A0b96cVatWAfDYY48RERHB73//+0P7i4uLCQgo/yc1OTmZ5OTk+gjTa7w5dLYjkAFMEZGzgOXAXUCCqqa7x+zEmdgdoA1HTlCf5pZVVH4EEZmAUyOhXbtKpyuuVGbL82iOwqb5cMaVx30dY4z3PP7pD6zfkVOn1+zauhmPXnp6jc4ZM2YMISEhrFy5knPPPZeRI0dy1113kZ+fT2hoKFOmTOGUU05h/vz5PPPMM8yaNYvHHnuMrVu3kpqaytatW7n77rurVetQVe6//35mz56NiPDII48wYsQI0tPTGTFiBDk5ORQXFzNx4kTOOeccxo0bR0pKCiLC2LFj+d3vfne8Xw3g3WQRAPQA7lDVJSLyAoebnABQVRWROnlGuqpOAiYBJCcnH9c1N+/J5eKp+1gX1ozgjd9YsjDGVCktLY3vvvsOf39/cnJyWLhwIQEBAXz99dc8/PDDfPDBB8ecs2HDBubNm8f+/fs55ZRTuOWWW6q8Qe7DDz9k1apVrF69mj179tCzZ0/OP/98pk2bxiWXXMIf/vAHSkpKyMvLY9WqVWzfvp1169YBkJWVVevP6c1kkYYzF/ASd/t9nGSxS0RaqWq628y0292/HWjrcX6iW7adw81WZeXzvRFw++ZhnNoqmu9zunH+L98gqmBD9Yw54dS0BuBNv/nNb/D39wcgOzub0aNH8/PPPyMiFBUVlXvO0KFDCQ4OJjg4mBYtWrBr1y4SExMrfZ9FixYxatQo/P39SUhI4IILLmDZsmX07NmTsWPHUlRUxGWXXUZSUhKdOnUiNTWVO+64g6FDhzJo0KBaf06vdXCr6k5gm4ic4hZdCKwHZgJlI5pGA5+46zOBG9xRUX2AbLe5ag4wSERi3I7tQW5ZnRMRru/Tns/zuiL7d0DGBm+8jTGmEQkPDz+0/sc//pEBAwawbt06Pv300wpvfAsODj607u/vT3Fx8XG///nnn8+CBQto06YNY8aMYerUqcTExLB69Wr69+/PK6+8wvjx44/7+mW8PRrqDuAdEVkDJAF/Af4GXCwiPwMXudsAnwOpwEZgMnArgNux/Sdgmbs8UdbZ7Q2XntWaVYHdnQ0bFWWMqYHs7GzatHG6VN988806vXa/fv147733KCkpISMjgwULFtCrVy+2bNlCQkICN910E+PHj2fFihXs2bOH0tJSrrzySp588klWrFhR6/f36rOhVHUVUN4QgAvLOVaB2yq4zhvAG3UaXAVCg/zpl9ydjcta0/anrwnuW25IxhhzjPvvv5/Ro0fz5JNPMnTo0Dq99uWXX87ixYs566yzEBGeeuopWrZsyVtvvcXTTz9NYGAgERERTJ06le3bt3PjjTdSWloKwF//+tdav3+jnIM7OTlZazNT3uY9ucx7/kauC5pP4ENbITCkDqMzxhyPH3/8kdNOO83XYTQa5X2fIrJcVcsd42uP+yhHh7hwMlv1I7C0gOLN3/k6HGOM8TlLFhXo0W8YBRrA1qWf+joUY0wj53l3uOeSmZnp69AOsfksKnDBGR1Y+fFpxG+a5+tQjDGNnOfd4Scqq1lUwN9PKGzfn/bFm9i0aaOvwzHGGJ+yZFGJ0/pdBsCq+R/7NA5jjPE1SxaViOnQg/3+MQRtnkduwfHfNGOMMQ2dJYvK+PlR2KE/vVnDxyu3VX28McY0UpYsqhDb7RLiJIfvFs2nMd6TYoypngEDBjBnzpFPGnr++ee55ZZbyj2+f//+VHa/V4cOHdizZ0+dxuhNliyqIJ0HAtBu3/ekbNnn42iMMb4yatQopk+ffkTZ9OnTGTVqlI8iql82dLYqkQmUtjiD/rvX8vbiLfTsEOvriIwxsx+EnWvr9potu8GQv1W4+6qrruKRRx6hsLCQoKAgNm/ezI4dO3j33Xe55557OHjwIFdddRWPP/54jd/62Wef5Y03nCcajR8/nrvvvpvc3Fyuvvpq0tLSKCkp4Y9//CMjRozgwQcfZObMmQQEBDBo0CCeeeaZ4/7INWHJohr8ThpIcsbLTFi3iYz9XYmPDK76JGNMoxIbG0uvXr2YPXs2w4cPZ/r06Vx99dU8/PDDxMbGUlJSwoUXXsiaNWs488wzq33d5cuXM2XKFJYsWYKq0rt3by644AJSU1Np3bo1n332GeA8pDAzM5OPPvqIDRs2ICJ1Mk9FdVmyqI6TLsT/uxfpoet5b1k3bh/YxdcRGdO0VVID8KaypqiyZPH6668zY8YMJk2aRHFxMenp6axfv75GyWLRokVcfvnlhx51fsUVV7Bw4UIGDx7MvffeywMPPMCwYcPo168fxcXFhISEMG7cOIYNG8awYcO89VGPYX0W1dG2DwSEMjL2f7yzZCvFJaW+jsgY4wPDhw9n7ty5rFixgry8PGJjY3nmmWeYO3cua9asYejQoRXOYVFTJ598MitWrKBbt2488sgjPPHEEwQEBLB06VKuuuoqZs2axeDBg+vkvarDkkV1BIZAh/M4V9aQnp3P3A27qz7HGNPoREREMGDAAMaOHcuoUaPIyckhPDycqKgodu3axezZs2t8zX79+vHxxx+Tl5dHbm4uH330Ef369WPHjh2EhYVx3XXXcd9997FixQoOHDhAdnY2v/rVr3juuedYvXq1Fz5l+awZqro6DyRi41f0aJbD24u3cMnpLX0dkTHGB0aNGsXll1/O9OnTOfXUU+nevTunnnoqbdu25dxzz63x9Xr06MGYMWPo1asX4HRwd+/enTlz5nDffffh5+dHYGAgEydOZP/+/QwfPpz8/HxUlWeffbauP16FbD6L6sr4Cf7Vi2+6/IGxa09n7r0X0Dk+om7fwxhTIZvPom7ZfBbeEncyNGvDubKGQH/h399v8XVExhhTbyxZVJcIdB5I8NYFDD09nveXp5FXaM+LMsZUrnfv3sfMU7F2bR3fI1IPvNpnISKbgf1ACVCsqski8hhwE5DhHvawqn7uHv8QMM49/k5VneOWDwZeAPyB11TVN+PmOg+ElW8z4aQcPl5TzMxVOxjZq51PQjGmKVJVRMTXYdTIkiVLfB3CMY6n+6E+ahYDVDXpqHaw59yyJI9E0RUYCZwODAZeFhF/EfEH/gUMAboCo9xj61+n/oBwWu5STm0ZydTFW+x5UcbUk5CQEDIzM+3/uVpSVTIzMwkJCanReSfSaKjhwHRVLQA2ichGoJe7b6OqpgKIyHT32PX1HmFYLLTpgaTO4/q+1/CHj9axYmsWZ7ePqfdQjGlqEhMTSUtLIyMjo+qDTaVCQkJITEys0TneThYKfCkiCryqqpPc8ttF5AYgBbhXVfcBbYDvPc5Nc8sAth1V3vvoNxKRCcAEgHbtvNg01PlCWPgPLrsqgr8FB/D24s2WLIypB4GBgXTs2NHXYTRZ3m6GOk9Ve+A0Id0mIucDE4HOQBKQDvyjLt5IVSeparKqJsfHx9fFJcvXeSBoCeHbv+XKsxP5fO1O9hwo8N77GWPMCcCryUJVt7uvu4GPgF6quktVS1S1FJjM4aam7UBbj9MT3bKKyn0jMRmCIuGXuVzXpx2FJaW8t8wmRjLGNG5eSxYiEi4ikWXrwCBgnYi08jjscmCduz4TGCkiwSLSEegCLAWWAV1EpKOIBOF0gs/0VtxV8g+EThfAxm84KT6Cczo3Z9qSrZSUWqebMabx8mbNIgFYJCKrcX70P1PVL4CnRGStiKwBBgC/A1DVH4AZOB3XXwC3uTWQYuB2YA7wIzDDPdZ3Og+A7K2wN5Xr+7Rne9ZB5tnzoowxjZjXOrjd0UtnlVN+fSXn/Bn4cznlnwOf12mAtdH5Qud141wuTh5PQrNgpi/bxkVdE3wblzHGeIndwX08YjtCTEf45RsC/P34VbdWLPg5gwMFdke3MaZxsmRxvE66EDYvhOJCBp/eksLiUmuKMsY0WpYsjlfngVB4ANKWktwhlriIIL74YaevozLGGK+wZHG8OvQDvwDYOBd/P2HQ6S2Zt2E3+UUlvo7MGGPqnCWL4xXSDBJ7wS/fADD49JbkFZaw4H/2KAJjTONjyaI2ThoI6ashdw99OzcnKjTQmqKMMY2SJYva6DwQUEidT6C/HxedlsDX63dRWFzq68iMMaZOWbKojVZJEBp7uCnqjJbk5BfzfWqmb+Myxpg6ZsmiNvz8nTkufvkGVOnXJY7wIH9mr7OmKGNM42LJorY6D4T96bD7R0IC/Rlwagu+Wr/TnhVljGlULFnUVueBzusvcwGnKWrPgUJSNu/1YVDGGFO3LFnUVlQbSOgGq98DVQac0oKgAD9rijLGNCqWLOpC39tg11r43xzCgwM4v0s8c37YSak1RRljGglLFnWh21UQ3R4WPA2qDDmjJenZ+azZnu3ryIwxpk5YsqgL/oFw3u9gewqkzuei0xII8BNmr0v3dWTGGFMnLFnUlaRrILI1LHiGqLBA+nZuzhfrdqJqTVHGmIbPkkVdCQiGc++ELYtgy2KGnNGKLZl5bNi539eRGWNMrVmyqEs9RkNYHCx8hkGnJyCCjYoyxjQKlizqUlAYnHM7bPyauOwf6NkhljmWLIwxjYBXk4WIbBaRtSKySkRS3LJYEflKRH52X2PcchGRF0Vko4isEZEeHtcZ7R7/s4iM9mbMtZY8DkKiYeE/GHJGS37atZ/UjAO+jsoYY2qlPmoWA1Q1SVWT3e0Hgbmq2gWY624DDAG6uMsEYCI4yQV4FOgN9AIeLUswJ6SQZtDnFtgwi6EJ+wBrijLGNHy+aIYaDrzlrr8FXOZRPlUd3wPRItIKuAT4SlX3quo+4CtgcD3HXDO9JkBQBC1WvcRZbaOZY3NcGGMaOG8nCwW+FJHlIjLBLUtQ1bIbEHYCCe56G2Cbx7lpbllF5UcQkQkikiIiKRkZPp6tLiwWeo6HHz5iZKcC1qRlk7Yvz7cxGWNMLXg7WZynqj1wmphuE5HzPXeqcxNCndyIoKqTVDVZVZPj4+Pr4pK10/d28A9mWM50AL6wpihjTAPm1WShqtvd193ARzh9Drvc5iXc193u4duBth6nJ7plFZWf2CLi4ewxRP70ARfE51lTlDGmQfNashCRcBGJLFsHBgHrgJlA2Yim0cAn7vpM4AZ3VFQfINttrpoDDBKRGLdje5BbduI75w4QP+4N/4KULfvYvT/f1xEZY8xx8WbNIgFYJCKrgaXAZ6r6BfA34GIR+Rm4yN0G+BxIBTYCk4FbAVR1L/AnYJm7POGWnfii2kDStZyxeybxuo85P+zydUTGGHNcpDE+uyg5OVlTUlJ8HYZj32b0xR68HzCUTxJu59/je/s6ImOMKZeILPe4zeEIdge3t8V0QM68mstK5vBT6ib25Rb6OiJjjKkxSxb14bx7CCgtZLTf53z9ozVFGWMaHksW9SH+ZDj9Mm4M+JIFq3/2dTTGGFNjlizqifT7PeEcpPPmaezPL/J1OMYYUyOWLOpLyzPY1/YiRvvNZsG6zb6OxhhjasSSRT2KGvQQMXKAgu8n+ToUY4ypEUsW9civbTIbI3vSL+M9DubaY8uNMQ2HJYt6ltf7HuIlm81fv+LrUIwxptosWdSz0/oOZjmn0XLtK1Bs91wYYxoGSxb1LNDfjxXtxxFTnEHxymm+DscYY6rFkoUPdOp9KatKO1E690+wfYWvwzHGmCpZsvCBc7vE85jcSm6JP0wZAmtm+DokY4yplCULHwgJ9KfDackMK3iSvPiz4MOb4Ms/QmmJr0MzxphyWbLwkQeHnEZRcCxD9v2e/KQb4bsXYdrVcHCfr0MzxphjWLLwkZZRIUy6IZn0AyWM3jWC4l89B6n/hckXQsZPvg7PGGOOYMnCh5LaRvPUlWeyZNNe/m97T3T0TMjPdhLGT1/4OjxjjDnEkoWPXda9DTdf0JlpS7by9o7WMGE+NO8E746EBc9AI5ycyhjT8FiyOAHcd8kpXHhqCx7/dD3f7gmFG7+AM66Eb/4E74+Fwlxfh2iMaeIsWZwA/P2E50cm0Tk+nFvfWcGmHIUrX4OLHocfPoI3LoGsrb4O0xjThHk9WYiIv4isFJFZ7vabIrJJRFa5S5JbLiLyoohsFJE1ItLD4xqjReRndxnt7Zh9ITIkkNdu6ImfwPi3lpFTUAzn3Q3XzIB9W2BSf9j8ra/DNMY0UfVRs7gL+PGosvtUNcldVrllQ4Au7jIBmAggIrHAo0BvoBfwqIjE1EPc9a5d8zBevvZstmTmcee7KykpVTh5ENz0DYTGwNRfw7LXrB/DGFPvvJosRCQRGAq8Vo3DhwNT1fE9EC0irYBLgK9Uda+q7gO+AgZ7LWgf69u5OY/9+nTm/5TB37/Y4BTGdYHxc6HzQPjsXvjoZuvHMMbUK2/XLJ4H7gdKjyr/s9vU9JyIBLtlbYBtHsekuWUVlTda1/Vpz/V92jNpQSrvL09zCkOjYdR06P8QrHnP7scwxtSraicLEQmryYVFZBiwW1WXH7XrIeBUoCcQCzxQk+tW8n4TRCRFRFIyMjLq4pI+9X+XdqVvp+Y8/OFalm9x7+r284f+D8L1H0JuBkwaAGv+49tAjTFNQpXJQkTOEZH1wAZ3+ywRebka1z4X+LWIbAamAwNF5N+qmu42NRUAU3D6IQC2A209zk90yyoqP4KqTlLVZFVNjo+Pr0Z4J7ZAfz9evrYHraJD+O3by9mRdfDwzs4D4eaF0OpM+HA8zPodFOX7LlhjTKNXnZrFczj9BpkAqroaOL+qk1T1IVVNVNUOwEjgG1W9zu2HQEQEuAxY554yE7jBHRXVB8hW1XRgDjBIRGLcju1BblmjFxMexGs3JJNfVMJNU1PIKyw+vLNZaxj9KZx7F6S8AW8Mgr2bfBesMaZRq1YzlKpuO6qoNo9HfUdE1gJrgTjgSbf8cyAV2AhMBm5133sv8Cdgmbs84ZY1CV0SInlxVBLr03O47z9rUM+RUP6BcPETMPJd2LcZXr0AfvzUZ7EaYxov0SqGYYrI+8CzwEs4w1fvApJVdaT3wzs+ycnJmpKS4usw6tSr//2Fv87ewO8uOpm7Lupy7AH7tsB/RsOOldD3drjoMSeZGGNMNYnIclVNLm9fdWoWNwO34YxA2g4kudumHk04vxNXdG/Dc1//jydnraeo5KgBZjHtYewc6DUBFr8EU34F2Wm+CdYY0+hUWbNoiBpjzQKgsLiUP3+2nrcWb+Hs9jG8dE13WkWFHnvgug9g5p3gHwRXTIYuF9V/sMaYBqeymkV1mqGmAMccpKpj6ya8utdYk0WZT1fv4MEP1hAc6M8LI5Po16Wc0V97NsKMG2D3ejj/9879GX7+9R+sMabBqG0z1CzgM3eZCzQDDtRdeKamLj2rNZ/cfh5xEUHc8MZSnv/6f86jQTzFnQTjv4bu18KCp+HdUVCw3zcBG2MavBo3Q4mIH7BIVc/xTki119hrFmXyCot55KN1fLhyO/26xPHCyO7Ehgcde+Cy1+Dz+6FFV7hmOkQl1n+wxpgTXm1rFkfrArSoXUimLoQFBfCPq8/ir1d0Y8mmvQx9ceHhu7099RwP185whtdOvtAZMWWMMTVQnTu494tITtkr8Cl19IgOU3siwqhe7fjwlnMI8BdGvLqYNxZt4pga40kXwbgvnU7vN4bAj7N8E7AxpkGqMlmoaqSqNvN4PVlVP6iP4Ez1ndEmill39GPAqS14YtZ6bn1nBfvzi448KKEr3DQXEk6H966Db1+0x50bY6qlwj4Lz8mHyqOqK7wSUR1oKn0W5VFVJi9M5e9f/ES72DBevrYHp7VqduRBRQedx5yv/xh6jIah/7Ab+Iwxxzd0VkTmVXJNVdWBdRGcNzTlZFFm6aa93D5tBdkHi/jTZWdwdXLbIw8oLYV5T8LCf0DHC+Dqqc5j0I0xTVat7rNoiCxZODL2F3DX9JV890sm1/Rux2OXnk5QwFEtjyvfgU/vgtiOzhSusR19E6wxxudqPRpKRM4QkatF5IaypW5DNN4QHxnM2+N6c/MFnZm2ZCvXvbaEPQcKjjyo+7Vw/UdwYDe8diFsXeKbYI0xJ7TqjIZ6FPinuwwAngJ+7eW4TB3x9xMeHHIqL4xMYnVaFsNf+pYfdmQfeVDHfs60rSFR8NalsPZ93wRrjDlhVadmcRVwIbBTVW8EzgKivBqVqXPDk9rw/s3nUKrKlRO/Y9aaHUceEHeSkzDanA0fjIP5f7eRUsaYQ6qTLPJVtRQoFpFmwG6OnLnONBDdEqP45PZzOb11FLdPW8kzc36i1PMxIWGxcMPHcOZImP8X+OR2KK3N1CXGmMaiwmQhIv8SkfOApSISjTMh0XJgBbC4fsIzda1FZAjTburNiOS2vDRvIxPeTjnyfoyAYLj8FTj/flj1b3h/LBQX+i5gY8wJIaCSff8DngZaA7nAu8DFQDNVXVMPsRkvCQ7w529XdqNr62Y8MWs9V7z8HZNvSKZDXLhzgAgM/AOENIMvH4GiPGdobWA5j0M3xjQJFdYsVPUFVe2LM992JvAG8AVwuYiUM1WbaUhEhNHndODtsb3IOFDA8H99y8KfM4486Jw7YNjz8PNX8M5v7Km1xjRh1XncxxZV/buqdgdGAZcBG7wdmKkf55wUx8zbzqNlsxBGv7GU149+rlTyjXDFJNjyHUwdDnlNZvpzY4yH6gydDRCRS0XkHWA28BNwhdcjM/WmXfMwPrj1HC46LYE/zVrPfe+voaDYo2P7zKudZqida52htQd2+y5YY4xPVNbBfbGIvAGkATfhTH7UWVVHquon1X0DEfEXkZUiMsvd7igiS0Rko4i8JyJBbnmwu73R3d/B4xoPueU/icglx/lZTSUiggN45bqzufPCLry/PI2Rk75nd07+4QNOGwbXvAd7U2HKEMja5rtgjTH1rrKaxUPAd8BpqvprVZ2mqrnH8R53AT96bP8deE5VTwL2AePc8nHAPrf8Ofc4RKQrMBI4HRgMvCwiNj+oF/j5CfdcfDITr+3BhvT9XP7yd2za4/GfvPPAw3d7TxkCmb/4LlhjTL2qrIN7oKq+pqrlzKZTPSKSCAwFXnO3BRgIlN0i/BZOHwjAcHcbd/+F7vHDgemqWqCqm4CNQK/jjclUbUi3Vrz32z4cLCrhqonfsW67xx3f7frA6E+hMNdJGLvW+y5QY0y9OZ6Z8mrieeB+oNTdbg5kqWqxu50GtHHX2wDbANz92e7xh8rLOecQEZkgIikikpKRkXH0blNDZyZG8/7NfQkJ9GfkpO/5buOewztbJ8GNswGBN38F20/Yp9UbY+qI15KFiAwDdqvqcm+9hydVnaSqyaqaHB8fXx9v2eh1io/gg1vOoXV0CGOmLGP22vTDO1ucCmO/gOBIeOvXzmgpY0yj5c2axbnAr0VkMzAdp/npBSBaRMpuBkwEtrvr23EfI+Luj8K5v+NQeTnnGC9rGRXCjN/2pVtiFLdOW8G0JVsP74ztCDd+AZEt4e0rYOPXvgvUGONVXksWqvqQqiaqagecDupvVPVaYB7OwwkBRgNlI6tmutu4+79RZ8D/TGCkO1qqI9AFWOqtuM2xosOC+Pe43vQ/OZ6HP1rLS9/8fPhejKg2TpNU85Ng2khYP9O3wRpjvMLbfRbleQC4R0Q24vRJvO6Wvw40d8vvAR4EUNUfgBnAepw7yG9TVXu6XT0LDfJn0g3JXN69Dc98+T8e/3T94YcQRsTDmE+dvoz/jIH11R5ZbYxpIGymPFMjpaXKnz//kdcXbWJ4Umuevuqsw7Pv5efAO1dBWgr8Zgp0He7bYI0xNVLrmfKMKePnJzwy9DTuH3wKn6zawU1TU8grdAe3hTSDa9935sR4f6zVMIxpRCxZmBoTEW7tfxJ/u6IbC3/O4JrJS9iX6z7GPKQZXPcBtO7hJgzrwzCmMbBkYY7byF7tmHjd2axPz+E3ry5mR9ZBZ8cRCeNGSxjGNAKWLEytXHJ6S6aO7cWu7HyumvgdG3cfcHYcShjdnYTx46e+DdQYUyuWLEyt9enUnHcn9KGwpJQRry5mw84cZ0dIM7juQydh/GeMJQxjGjBLFqZOnNEmihm/7UuAv3DN5CWs3+GZMD6whGFMA2fJwtSZTvERvDehL8EBflz72vf8sMN9AGFIlJMwWiW5CWOWL8M0xhwHSxamTnWIC2f6hD6EBvpz7WtLDj+xNiQKrv/QTRijLWEY08BYsjB1rn3zcKZP6Et4UADXvraEtWkVJIwNn/k0TmNM9VmyMF7RrnkY0yf0ISI4gGtf+57V27KcHYcSxlkw4wZLGMY0EJYsjNe0jQ3jvd/2oVloINe9voRVRySMj9yEYTUMYxoCSxbGqxJjwnjvt32JCQvi+teWsGKrO/FiSJQzrLbVmU4Nw27cM+aEZsnCeF2b6FCmT+hDbEQQN7y+lOVb3IQRGu3UMMqG1a77wJdhGmMqYcnC1IvWbsKIjwzmhteXkLJ5r7OjrEmqbW/4YDysnu7bQI0x5bJkYepNq6hQ3r2pDwnNQrjhjaUs3eQmjOBIuO596HAefHQzrHjbt4EaY45hycLUq5ZRIUyf0IeWUSGMmbKUJamZzo6gcLhmBnQeCDNvh2Wv+TZQY8wRLFmYeteimZMwWkeHMmbKMhb/4iaMwFAYOQ1OHgyf3QvfT/RtoMaYQyxZGJ9oERnCuzf1ITEmlBvfXMp3v+xxdgSGwNVvw2mXwhcPwqLnfRqnMcZhycL4THxkMO9O6EO72DDGvulRwwgIgqumwBlXwtePwn+f8m2gxhjvJQsRCRGRpSKyWkR+EJHH3fI3RWSTiKxylyS3XETkRRHZKCJrRKSHx7VGi8jP7jLaWzGb+hcXEcy0m/rQNiaMG99cejhh+AfCFZPhzJEw78/wzZPQCOeLN6ah8GbNogAYqKpnAUnAYBHp4+67T1WT3GWVWzYE6OIuE4CJACISCzwK9AZ6AY+KSIwX4zb1LC7CqWEckzD8/OGyl6H79bDgaaeWYQnDGJ/wWrJQhzttGoHuUtn/6cOBqe553wPRItIKuAT4SlX3quo+4CtgsLfiNr5RacK49EVIHgffvgBfPGQJwxgf8GqfhYj4i8gqYDfOD/4Sd9ef3aam50Qk2C1rA2zzOD3NLauo/Oj3miAiKSKSkpGRUdcfxdQDz4RxRB+Gnx8M/Qf0uRWWTHRGSpWW+jZYY5oYryYLVS1R1SQgEeglImcADwGnAj2BWOCBOnqvSaqarKrJ8fHxdXFJ4wNlfRiJMaFHJgwRuOQvcO7dkPI6fHonlJb4NFZjmpJ6GQ2lqlnAPGCwqqa7TU0FwBScfgiA7UBbj9MS3bKKyk0jFR9ZScK46DG44AFY+TZMvwYK9vs0VmOaCm+OhooXkWh3PRS4GNjg9kMgIgJcBqxzT5kJ3OCOiuoDZKtqOjAHGCQiMW7H9iC3zDRiRyeM71M9EsaAh51mqZ+/gtcHwb7NPo3VmKbAmzWLVsA8EVkDLMPps5gFvCMia4G1QBzwpHv850AqsBGYDNwKoKp7gT+511gGPOGWmUbOM2HcOMUjYQD0HO9MopSzAyYPhC3f+S5QY5oA0UY4siQ5OVlTUlJ8HYapIxn7C7hm8vek7TvIlBt70qdT88M7M3+BaSOc2sWw56DH9T6L05iGTkSWq2pyefvsDm5zwiurYbQpr4bRvDOM/xo69nMeQDjnD9bxbYwXWLIwDUJ8ZDDvVpQwQqPhmv9A75th8UtOTSM/22exGtMYWbIwDcbRCWPRz3sO7/QPgCF/h2HPQ+o8eO1i2Jvqs1iNaWwsWZgGpSxhtG/u3Lj3+dr0Iw9IvtGZeS93t9PxvWmhbwI1ppGxZGEanPjIYN6b0JduiVHcNm0F05ZsPfKAjufD+LkQ3gLevgxSpvgkTmMaE0sWpkGKCgvk3+N6c8HJ8Tz80Vr+NW8jR4zsa94Zxn8FnfrDrLvh8/uhpNhX4RrT4FmyMA1WaJA/k29I5rKk1jw95yee/OxHSks9EkZIFIx6z3mm1NJXYdpvIM9u0THmeFiyMA1aoL8fz16dxJhzOvD6ok38/j+rKSrxeMigfwAM/qvz5NpNC+CfPWDJq1BS5LugjWmALFmYBs/PT3j00q7ce/HJfLhyO799ezkHC4+61+Ls0XDTPEg4A2bfDy/3gZ9m2+POjakmSxamURAR7riwC3+67Azm/bSbG95YQvbBo2oPrc6E0Z/CqOnO9rsjYeqvIX1N/QdsTANjycI0Ktf3ac8/R3Vn1bYsRry6mN05+UceIAKnDIFbv4chT8POdfDq+fDxbZCTXv5FjTGWLEzjM+zM1rwxpidb9+Zx1SuL2ZKZe+xB/oHQewLcuRLOuR3WvOf0Z8z/OxSWc7wxTZwlC9Mo9esSzzvje5OTX8SVExezfkdO+QeGRsOgJ+H2pdDlYpj/F/hnMqx612bjM8aDJQvTaHVvF8N/ftuXAD9hxKTFLN1UybDZ2E5w9VS48Qto1go+vhkm97c7wI1xWbIwjVqXhEjev6Uv8RHBXP/6Ej5bU0W/RPu+MO5ruPJ1556Mt4Y5Eyx98yT88o01UZkmy+azME1C5oECxk9NYeXWLK7p3Y7/G9aVkED/yk8qOujck7H+E0hfDVoCfgHQuju0Pwfanwftejs3/xnTCFQ2n4UlC9NkFBaX8o8vf+LVBamckhDJS9d0p0tCZPVOLtgP25bA5m9hy7ewfQWUFoH4QctuTuJof46zhMV694MY31OF4gIozoeSQue1uBBKCg6vH71Py/rA1L2/Rw9f61DZUftLipxrlBR6XL+g8rLmJznTDh8HSxbGeJj/027unbGa3MJiHrv0dEb0bIszJXwNFOZB2jIncWz5DrYtdf6nBWjRFdqcDZGtIKIFRCS4S7zzGhRe9x+qIVCF0uKjfuwKPF4LnB/H4oLD22U/yMX5HusFHotHeWmRMyhBS51aYGmJ86qlbvnRZZ6vHvuOPtazvLTYXXz0BADxh4Bg8A9yX4MhIMjZLitLON2ZNfJ4Lm/Jwpgj7c7J53czVvHtxkyGndmKv1zRjWYhgcd/weIC2L7cSR6bv4Vd6yB3D4f+9egpKMJJIuEtPJJJCwiPg8Aw53/4gBCPJfioV491/4Ajr63q/liWvZY6MZStl5VX+qNdwQ95cSEU5Tk/zEV5UJTvNNUVH3Rey5ZD2/nOuud1yvs+jof/0d9JEPgFgp+/cy+N+LvrZa9+znJMmT/4+ZVzfEXlfk5TZNl7BoS4P9JlsZT9gAcfuy0eXcQigLiveKzLkfv9Az0SQ5ATgxf5JFmISAiwAAgGAoD3VfVREekITAeaA8uB61W1UESCganA2UAmMEJVN7vXeggYB5QAd6rqnMre25KFqY6SUuWV//7Cs1/9j9bRIfxzVA+S2kbX4RsUQ14mHNgFB3Y7c2yUrR96ddfzs47vPcQPkMNJob4EhEJgiJvc3NfAEAgMdfe5S9mPaGX/Ei6vrLwk6e/x4+xnY3O8wVfJQoBwVT0gIoHAIuAu4B7gQ1WdLiKvAKtVdaKI3Aqcqao3i8hI4HJVHSEiXYF3gV5Aa+Br4GRVrXCiZUsWpiaWb9nLne+uYldOPvcPPoXx53XCz6+GzVK1VVzgJJaig8c2r1T1Cu6/pv0OJw/xc8sqKPcLrOJHu2zdoywg5HACqGmznWkQKksWAeUV1gV1stABdzPQXRQYCFzjlr8FPAZMBIa76wDvAy+5CWc4MF1VC4BNIrIRJ3Es9lbspmk5u30sn9/Zj/s/WM1fPt/Atxsz+cfVZxEXEVx/QQQEQ7PW9fd+xtSQV+tyIuIvIquA3cBXwC9AlqqWzUKTBrRx19sA2wDc/dk4TVWHyss5x/O9JohIioikZGRkeOHTmMYsKiyQV647mz9ddgaLUzMZ8sJCvt24p+oTjWkivJosVLVEVZOARJzawKlefK9Jqpqsqsnx8fHeehvTiIkI1/dpzye3nUuzkACue30JT8/ZQHGJPfbDmHrpJVLVLGAe0BeIFpGy5q9EYLu7vh1oC+Duj8Lp6D5UXs45xtS501o149M7zuM3Zyfyr3m/MPTFRcxI2UZBcYXdZMY0el5LFiISLyLR7noocDHwI07SuMo9bDTwibs+093G3f+N2+8xExgpIsHuSKouwFJvxW0MQFhQAE9ddRYvX9sDgPvfX8O5f5vHC1//TOaBAh9HZ0z98+ZoqDNxOrD9cZLSDFV9QkQ64QydjQVWAtepaoE71PZtoDuwFxipqqnutf4AjAWKgbtVdXZl722joUxdUlW+3ZjJa4tSmf9TBkEBflzRvQ1jz+vIydW9A9yYBsBuyjOmjmzcvZ/XF23mwxVpFBSXcv7J8Yw7ryPnd4mr+V3gxpxgLFkYU8f25hYybckW3lq8hYz9BZycEMHYcztyWfc2VT+g0JgTlCULY7ykoLiEWavTeX3RJtan5xAbHsR1vdtxXZ/2tGgW4uvwjKkRSxbGeJmq8n3qXl5flMrcDbtRhY5x4SS1jaZ7u2iS2kZzWqtmBPrbYyrMicsnd3Ab05SICH07N6dv5+akZhzgix92smprFos27uGjlc5I7+AAP7q1iXITSAxJ7aJpHRVifR2mQbCahTFepKrsyM5n5dZ9rNqaxcptWazdnk1hsXOjX4vIYLfmEcNZbaM4qUUE8RHBlkCMT1jNwhgfERHaRIfSJjqUYWc6z34qLC5lw84cVm7NYtW2LFZu3cecH3YdOiciOIAOcWF0aB5Oxzhn6RAXTsfm4cSEB/nqo5gmzpKFMfUsKMCPMxOjOTMx+tBdqHtzC1m3PZtNe3IPLWvSsvl8bTqlHpX/6LDAY5JIYkworaNCiY8Mxr++n5ZrmgxrhjLmBFZYXMq2fXlsyshlc2YuqXty2ewuO7Lzjzg2wE9IaBZC6+gQWkWF0io6hDbRoc56VAito0OJCQu0Ji5TIWuGMqaBCgrwo3N8BJ3jI47Zd7CwhC17c9mRdZAdWfnsyDpIerbzumpbFrPXHaSo5Mh/DIYE+tE6KpQ2MaG0jQ2jXWwYbWOc13axYUSF1WK2QNOoWbIwpoEKDfLn1JbNOLVls3L3l5Yqe3ILSM/KJz37INuz8kl3E0ravjxmr01nX96Rc0lHhgQcShxt3cVJKKEkxoQRFGBDf5sqSxbGNFJ+fkKLyBBaRIZwVgXTxe7PL2Lb3oNs3ZtH2r48tu51lv/t2s/cDbsPjdoC8BNoGxtGx7hwOsVF0Ck+nE5x4XSKjyChmY3gauwsWRjThEWGBNK1dSBdWx9bOyktVTIOFDgJJDOPLW6fSWpGLktS93Kw6PAj28OC/J0kEh9Bx7hwOsc7CaVDXBiRIda01RhYsjDGlMvP7TBPaBZCzw6xR+wrLVV27c8nNaMsgRwgNSOX1duy+GzNjiNGcEWGBNAqKuRQR3vLqBBaR4U6r9EhtIwKJSLYfopOdPZfyBhTY35+4v74h3LuSXFH7CsoLmFrZh6/ZOSyJTOX9GynzyQ9O5/16Tlk7D92PpDI4ABauYmjVbMQ4iODaR4RRPOIYOIigoiLCKZ5eBDRYUE2PNhHLFkYY+pUcIA/XRIi6VLBXB+FxaXsyslnZ44zcmtndv4RCeXH9Bz25hZSUnrssH4/gdjwIJqHBxMX6bw2d5NJTFgQ0WGBRIcFHlqPCQuypwDXEUsWxph6FRTgd2ikVUVKS5Wsg0VkHihgz4FCMnML2LO/gMzcQmf7gLO+Oi2LzAOFHCgorvBaIYF+RIcem0iiw4KIDg0kqmwJ81gPDSQiOMA67T1YsjDGnHD8/ITY8CBiw4PoklD18flFJezLKyQrr4h9eYVk5xWxr2z9YBH7cgvJOlhEVl4hG3cfYF+es15cTu2ljL+f0CwkwE0kQYeSSFmCiQ4re3WTj0fSCQ5ofLUZSxbGmAYvJND/UB9KdakquYUlZB8sIjuvyHk9WETOwSKyDhYe2s4+WHxofWtm7qH1SvIMoYH+HsmkLMk4SSXq6G2P5HMi12YsWRhjmiQRISI4gIjgANpEVz/JgNNMdqCwmOy8IrLynOSS5ZFwstxaTpabiDbtySX7YBb78oqOuHflaP5+ckQNJdqtuXgmlMMJ5nCNpllooNfnSvFashCRtsBUIAFQYJKqviAijwE3ARnuoQ+r6ufuOQ8B44AS4E5VneOWDwZeAPyB11T1b96K2xhjquLnJzQLCaRZSCBtY6s+3lN+UYmbUNykUlaDOSrpZOUVsedAIRszDpCdV0ROfsX9MuA8rTgqNJAe7WP456jutfh05fNmzaIYuFdVV4hIJLBcRL5y9z2nqs94HiwiXYGRwOlAa+BrETnZ3f0v4GIgDVgmIjNVdb0XYzfGGK8ICfQnJNCfhBpOu1tSqm4T2eHaS1lSOZR8DhbSKso70/l6LVmoajqQ7q7vF5EfgTaVnDIcmK6qBcAmEdkI9HL3bVTVVAARme4ea8nCGNNk+PsJMeFBPpvTpF6eCiYiHYDuwBK36HYRWSMib4hIjFvWBtjmcVqaW1ZR+dHvMUFEUkQkJSMj4+jdxhhjasHryUJEIoAPgLtVNQeYCHQGknBqHv+oi/dR1UmqmqyqyfHx8XVxSWOMMS6vjoYSkUCcRPGOqn4IoKq7PPZPBma5m9uBth6nJ7plVFJujDGmHnitZiHOYOHXgR9V9VmP8lYeh10OrHPXZwIjRSRYRDoCXYClwDKgi4h0FJEgnE7wmd6K2xhjzLG8WbM4F7geWCsiq9yyh4FRIpKEM5x2M/BbAFX9QURm4HRcFwO3qWoJgIjcDszBGTr7hqr+4MW4jTHGHMXm4DbGGANUPge3zZFojDGmSpYsjDHGVKlRNkOJSAawpRaXiAP21FE4DZl9Dw77Hhz2PTga8/fQXlXLvfegUSaL2hKRlIra7ZoS+x4c9j047HtwNNXvwZqhjDHGVMmShTHGmCpZsijfJF8HcIKw78Fh34PDvgdHk/werM/CGGNMlaxmYYwxpkqWLIwxxlTJkoUHERksIj+JyEYRedDX8fiKiGwWkbUiskpEmtRzU9w5VnaLyDqPslgR+UpEfnZfYyq7RmNQwffwmIhsd/8uVonIr3wZY30QkbYiMk9E1ovIDyJyl1ve5P4mLFm4RMQfZ/rWIUBXnAcedvVtVD41QFWTmuB48jeBwUeVPQjMVdUuwFx3u7F7k2O/B3CmRE5yl8/rOSZfKJseuivQB7jN/V1ocn8TliwO64U7fauqFgJl07eaJkRVFwB7jyoeDrzlrr8FXFafMflCBd9Dk6Oq6aq6wl3fD5RND93k/iYsWRxWrelbmwgFvhSR5SIywdfBnAAS3DnlAXYCCb4MxsfKmxK5SThqeugm9zdhycKU5zxV7YHTJHebiJzv64BOFOqMNW+q4829MiVyQ1DO9NCHNJW/CUsWh1U2rWuToqrb3dfdwEc4TXRN2a6yGR7d190+jscnVHWXqpaoaikwmSbyd1He9NA0wb8JSxaH2fStgIiEi0hk2TowiMNT3zZVM4HR7vpo4BMfxuIzlUyJ3GhVND00TfBvwu7g9uAOBXyew9O3/tm3EdU/EemEU5sAZ9rdaU3pexCRd4H+OI+h3gU8CnwMzADa4Tz6/mpVbdSdvxV8D/1xmqAOTYns0W7fKInIecBCYC1Q6hY/jNNv0bT+JixZGGOMqYo1QxljjKmSJQtjjDFVsmRhjDGmSpYsjDHGVMmShTHGmCpZsjDmOIlIiccTWFfV5ZOKRaSD5xNfjfG1AF8HYEwDdlBVk3wdhDH1wWoWxtQxdz6Qp9w5QZaKyElueQcR+cZ9EN9cEWnnlieIyEcistpdznEv5S8ik915FL4UkVCffSjT5FmyMOb4hR7VDDXCY1+2qnYDXsJ5KgDAP4G3VPVM4B3gRbf8ReC/qnoW0AP4wS3vAvxLVU8HsoArvfppjKmE3cFtzHESkQOqGlFO+WZgoKqmug+h26mqzUVkD9BKVYvc8nRVjRORDCBRVQs8rtEB+MqdXAcReQAIVNUn6+GjGXMMq1kY4x1awXpNFHisl2B9jMaHLFkY4x0jPF4Xu+vf4TzNGOBanAfUgTMt5y3gTO8rIlH1FaQx1WX/UjHm+IWKyCqP7S9UtWz4bIyIrMGpHYxyy+4ApojIfUAGcKNbfhcwSUTG4dQgbsGZXMiYE4b1WRhTx9w+i2RV3ePrWIypK9YMZYwxpkpWszDGGFMlq1kYY4ypkiULY4wxVbJkYYwxpkqWLIwxxlTJkoUxxpgq/T/xgK5r2Ha7oQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "min_length = min(len(epoch_train_log), len(epoch_val_log))\n",
    "list1 = epoch_train_log[:min_length]\n",
    "list2 = epoch_val_log[:min_length]\n",
    "\n",
    "# Create x-axis values\n",
    "x_values = range(min_length)\n",
    "\n",
    "# Plot the lists\n",
    "plt.plot(x_values, list1, label='Train_loss')\n",
    "plt.plot(x_values, list2, label='Val_loss')\n",
    "\n",
    "# Adding labels and title\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Train Loss and Val_Loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d4ab91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019e9c93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e49228",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30077b2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23402b51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c895474",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
