{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5981a52",
   "metadata": {},
   "source": [
    "## In this notebook, we scale the Cosine with (1-lambda)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6087d08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "414af83f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: jupyter in /opt/conda/lib/python3.8/site-packages (1.0.0)\n",
      "Requirement already satisfied: ipywidgets in /opt/conda/lib/python3.8/site-packages (8.1.3)\n",
      "Requirement already satisfied: qtconsole in /opt/conda/lib/python3.8/site-packages (from jupyter) (5.5.2)\n",
      "Requirement already satisfied: notebook in /opt/conda/lib/python3.8/site-packages (from jupyter) (6.4.1)\n",
      "Requirement already satisfied: jupyter-console in /opt/conda/lib/python3.8/site-packages (from jupyter) (6.6.3)\n",
      "Requirement already satisfied: ipykernel in /opt/conda/lib/python3.8/site-packages (from jupyter) (6.29.5)\n",
      "Requirement already satisfied: nbconvert in /opt/conda/lib/python3.8/site-packages (from jupyter) (6.3.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (5.14.3)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.11 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (3.0.11)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.11 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (4.0.11)\n",
      "Requirement already satisfied: comm>=0.1.3 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (7.30.0)\n",
      "Requirement already satisfied: decorator in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (5.1.0)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.47)\n",
      "Requirement already satisfied: matplotlib-inline in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.3)\n",
      "Requirement already satisfied: pickleshare in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.7.5)\n",
      "Requirement already satisfied: setuptools>=18.5 in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (59.4.0)\n",
      "Requirement already satisfied: jedi>=0.16 in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.18.1)\n",
      "Requirement already satisfied: pygments in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (2.10.0)\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (4.8.0)\n",
      "Requirement already satisfied: backcall in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /opt/conda/lib/python3.8/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.8/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.8/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=6.1.0->ipywidgets) (0.2.5)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in /opt/conda/lib/python3.8/site-packages (from ipykernel->jupyter) (7.1.0)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in /opt/conda/lib/python3.8/site-packages (from ipykernel->jupyter) (1.8.2)\n",
      "Requirement already satisfied: nest-asyncio in /opt/conda/lib/python3.8/site-packages (from ipykernel->jupyter) (1.5.4)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.8/site-packages (from ipykernel->jupyter) (21.3)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.8/site-packages (from ipykernel->jupyter) (5.8.0)\n",
      "Requirement already satisfied: tornado>=6.1 in /opt/conda/lib/python3.8/site-packages (from ipykernel->jupyter) (6.1)\n",
      "Requirement already satisfied: pyzmq>=24 in /opt/conda/lib/python3.8/site-packages (from ipykernel->jupyter) (26.0.3)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /opt/conda/lib/python3.8/site-packages (from ipykernel->jupyter) (5.7.2)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /opt/conda/lib/python3.8/site-packages (from jupyter-client>=6.1.12->ipykernel->jupyter) (2.8.2)\n",
      "Requirement already satisfied: entrypoints in /opt/conda/lib/python3.8/site-packages (from jupyter-client>=6.1.12->ipykernel->jupyter) (0.3)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /opt/conda/lib/python3.8/site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel->jupyter) (4.2.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.8/site-packages (from python-dateutil>=2.1->jupyter-client>=6.1.12->ipykernel->jupyter) (1.16.0)\n",
      "Requirement already satisfied: bleach in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (4.1.0)\n",
      "Requirement already satisfied: defusedxml in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (0.7.1)\n",
      "Requirement already satisfied: jupyterlab-pygments in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (0.1.2)\n",
      "Requirement already satisfied: nbclient<0.6.0,>=0.5.0 in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (0.5.9)\n",
      "Requirement already satisfied: mistune<2,>=0.8.1 in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (0.8.4)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (1.5.0)\n",
      "Requirement already satisfied: testpath in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (0.5.0)\n",
      "Requirement already satisfied: nbformat>=4.4 in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (5.1.3)\n",
      "Requirement already satisfied: jinja2>=2.4 in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (3.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.8/site-packages (from jinja2>=2.4->nbconvert->jupyter) (2.0.1)\n",
      "Requirement already satisfied: ipython-genutils in /opt/conda/lib/python3.8/site-packages (from nbformat>=4.4->nbconvert->jupyter) (0.2.0)\n",
      "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /opt/conda/lib/python3.8/site-packages (from nbformat>=4.4->nbconvert->jupyter) (4.2.1)\n",
      "Requirement already satisfied: importlib-resources>=1.4.0 in /opt/conda/lib/python3.8/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.4->nbconvert->jupyter) (5.4.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /opt/conda/lib/python3.8/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.4->nbconvert->jupyter) (21.2.0)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /opt/conda/lib/python3.8/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.4->nbconvert->jupyter) (0.18.0)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /opt/conda/lib/python3.8/site-packages (from importlib-resources>=1.4.0->jsonschema!=2.5.0,>=2.4->nbformat>=4.4->nbconvert->jupyter) (3.6.0)\n",
      "Requirement already satisfied: webencodings in /opt/conda/lib/python3.8/site-packages (from bleach->nbconvert->jupyter) (0.5.1)\n",
      "Requirement already satisfied: prometheus-client in /opt/conda/lib/python3.8/site-packages (from notebook->jupyter) (0.12.0)\n",
      "Requirement already satisfied: Send2Trash>=1.5.0 in /opt/conda/lib/python3.8/site-packages (from notebook->jupyter) (1.8.0)\n",
      "Requirement already satisfied: argon2-cffi in /opt/conda/lib/python3.8/site-packages (from notebook->jupyter) (21.1.0)\n",
      "Requirement already satisfied: terminado>=0.8.3 in /opt/conda/lib/python3.8/site-packages (from notebook->jupyter) (0.12.1)\n",
      "Requirement already satisfied: cffi>=1.0.0 in /opt/conda/lib/python3.8/site-packages (from argon2-cffi->notebook->jupyter) (1.15.0)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.8/site-packages (from cffi>=1.0.0->argon2-cffi->notebook->jupyter) (2.21)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging->ipykernel->jupyter) (3.0.6)\n",
      "Requirement already satisfied: qtpy>=2.4.0 in /opt/conda/lib/python3.8/site-packages (from qtconsole->jupyter) (2.4.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#installing some libraries\n",
    "!pip install --upgrade jupyter ipywidgets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1f8bc09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch, transformers\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import PreTrainedModel, PretrainedConfig\n",
    "from transformers import AutoModel, AutoConfig,AutoModelForCausalLM, AutoConfig\n",
    "\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "#from datasets import load_dataset\n",
    "import pandas as pd, numpy as np\n",
    "from torch import cuda\n",
    "import datetime\n",
    "import warnings,itertools\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import json,math\n",
    "pre_train = False\n",
    "\n",
    "# Ignore all warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "#pip install transformers bitsandbytes>=0.39.0 -q\n",
    "import zipfile,logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "788c7f87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "#global params for training\n",
    "\n",
    "B,T = 64,1024\n",
    "epoch = 200\n",
    "# this controls whether we are using pre-trained wts or not\n",
    "random_init_wts = True\n",
    "nll_learning_steps = int(epoch/2)\n",
    "if cuda.is_available():\n",
    "    device = torch.device('cuda:0')\n",
    "    print(device)\n",
    "else:\n",
    "    device = 'cpu'\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "# these 2 global vars help track the training and val loss\n",
    "#directory to save wts\n",
    "model_path = os.path.join(\"model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db7754be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./data/unzip_text_10M'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "directory = os.path.join('.','data','unzip_text_10M')  # Replace with your directory path\n",
    "directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ab85fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_text(directory):\n",
    "    \"\"\"This function loads the dataset,provided in the zipped format and stores all the text files in list\n",
    "    each file has its contents stored as an item in list and at the end we concatenate all the sub-lists to create a flattened list and return it\"\"\"\n",
    "    directory = os.path.join('.','data','unzip_text_10M',str(directory))  # Replace with your directory path\n",
    "    print(f\"directory :{directory}\")\n",
    "    # List all files in the directory\n",
    "    files = [f for f in os.listdir(directory) if os.path.isfile(os.path.join(directory, f))]\n",
    "    print(f\"files:{files}\")\n",
    "    text_content = []\n",
    "    # Read each file\n",
    "    total_lines = 0\n",
    "    for filenum,filename in enumerate(files):\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            text = file.read()\n",
    "            text_content.append(text)\n",
    "            print(f\"the file:{filename} has been appeneded to the uber list and its length is {len(text_content)} \")\n",
    "            \n",
    "    \n",
    "    flattened_list = ''.join(text_content)\n",
    "    assert (len(flattened_list) == total_lines , f\"Expected {len(flattened_list)} to be equal to {total_lines}\" )\n",
    "    \n",
    "    return flattened_list\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3aab440",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "directory :./data/unzip_text_10M/train_10M\n",
      "files:['switchboard.train', 'simple_wiki.train', 'open_subtitles.train', 'gutenberg.train', 'childes.train', 'bnc_spoken.train']\n",
      "the file:switchboard.train has been appeneded to the uber list and its length is 1 \n",
      "the file:simple_wiki.train has been appeneded to the uber list and its length is 2 \n",
      "the file:open_subtitles.train has been appeneded to the uber list and its length is 3 \n",
      "the file:gutenberg.train has been appeneded to the uber list and its length is 4 \n",
      "the file:childes.train has been appeneded to the uber list and its length is 5 \n",
      "the file:bnc_spoken.train has been appeneded to the uber list and its length is 6 \n"
     ]
    }
   ],
   "source": [
    "#read the dataset and get a list\n",
    "train_list = read_text(\"train_10M\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f7ae51f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54215049"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "14b6b034",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "827\n"
     ]
    }
   ],
   "source": [
    "chunks = len(train_list)//(B*T)\n",
    "print(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4b82842",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'AutoTokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3488/846851718.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#tokeinze the training data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"distilgpt2\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreturn_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"pt\"\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mtruncate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_overflowing_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0menc_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'AutoTokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "#tokeinze the training data\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilgpt2\",return_tensors = \"pt\" , truncate = True, return_overflowing_tokens=True , padding = False,)\n",
    "enc_train = tokenizer(train_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0f58989d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.1535097982657136"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comp_ratio = len(train_list)/len(enc_train['input_ids'])\n",
    "comp_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "021a6b67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "directory :./data/unzip_text_10M/dev\n",
      "files:['switchboard.dev', 'simple_wiki.dev', 'open_subtitles.dev', 'gutenberg.dev', 'childes.dev', 'bnc_spoken.dev']\n",
      "the file:switchboard.dev has been appeneded to the uber list and its length is 1 \n",
      "the file:simple_wiki.dev has been appeneded to the uber list and its length is 2 \n",
      "the file:open_subtitles.dev has been appeneded to the uber list and its length is 3 \n",
      "the file:gutenberg.dev has been appeneded to the uber list and its length is 4 \n",
      "the file:childes.dev has been appeneded to the uber list and its length is 5 \n",
      "the file:bnc_spoken.dev has been appeneded to the uber list and its length is 6 \n"
     ]
    }
   ],
   "source": [
    "#read validation data and tokenize\n",
    "val_list = read_text(\"dev\")\n",
    "enc_val = tokenizer(val_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9253846b",
   "metadata": {},
   "source": [
    "### we read the tokenize data and store the input_ids and attention mask as a single long list at run time we reshape the single [1,B*T] tensor into a batched tensor of dimension [B,T].\n",
    "This approach turns out to be more efficient as it removes the need for any extra tokens in the form of padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b1059e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df_from_list(enc , B= B, T = T, val= False):\n",
    "    \"\"\"This function takes the tokenized list and reated a dataframe where each row contains the input_ids and attention_mask from the tokenizer.\n",
    "    each row of the dataframe is a list with items B*T\"\"\"\n",
    "    chunk_size = B*T\n",
    "    if val:\n",
    "        chunk_size = 2*B*T\n",
    "        \n",
    "    long_list_inp = enc['input_ids']\n",
    "    long_list_attention = enc['attention_mask']\n",
    "\n",
    "    # Step 3: Split the list into chunks and pad the last chunk if necessary\n",
    "    chunks_inp = [long_list_inp[i:i + chunk_size] for i in range(0, len(long_list_inp), chunk_size)]\n",
    "    chunks_att = [long_list_attention[i:i + chunk_size] for i in range(0, len(long_list_attention), chunk_size)]\n",
    "    df = pd.DataFrame({'input_ids': chunks_inp,'attention_mask':chunks_att})\n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3edb0a26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the dataframe is = 263\n"
     ]
    }
   ],
   "source": [
    "df_train_temp = get_df_from_list(enc_train)\n",
    "# Display the DataFrame\n",
    "df_train_temp.head()\n",
    "print(f\"Length of the dataframe is = {len(df_train_temp)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e8f3b802",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the VALDATION dataframe is = 266\n"
     ]
    }
   ],
   "source": [
    "df_val_temp = get_df_from_list(enc_val)\n",
    "# Display the DataFrame\n",
    "df_val_temp.head()\n",
    "print(f\"Length of the VALDATION dataframe is = {len(df_val_temp)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a79868cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_df(df,eos_char = tokenizer.eos_token_id,B = B, T = T,val = False):\n",
    "    \"\"\"The 2 functions below makes sure that each row of the dataframe is of equal length and if not it adds the eos token to make it B*T\"\"\"\n",
    "    if val:\n",
    "        B = 2*B\n",
    "    for ind,row in df.iterrows():\n",
    "        if len(row['input_ids']) != B*T :\n",
    "            print(f\"row = {ind} and input_id length = {len(row['input_ids'])}\")\n",
    "            print(f\"row = {ind} and attention length = {len(row['attention_mask'])}\")\n",
    "            pad_len = B*T - len(row['input_ids'])\n",
    "            print(f\"padding the row index {ind} with {pad_len} character\")\n",
    "            row['input_ids'] = row['input_ids']+ [eos_char] * pad_len\n",
    "            #attention mask should be padded to 0\n",
    "            row['attention_mask'] = row['attention_mask']+ [0] * pad_len\n",
    "            print(\"#### POST CONCAT####\")\n",
    "            print(f\"row = {ind} and input_id length = {len(row['input_ids'])}\")\n",
    "            print(f\"row = {ind} and attention length ={len(row['attention_mask'])}\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def verify_len(df,val = False, B= B):\n",
    "    row_ind = []\n",
    "    if val:\n",
    "        B = 2*B\n",
    "    for ind,row in df.iterrows():\n",
    "        if len(row['input_ids']) != B*T :\n",
    "            row_ind.append(ind)\n",
    "        else:\n",
    "            continue\n",
    "    if len(row_ind) !=0:\n",
    "        print(\"CONCATENATION Did not work\")\n",
    "    else:\n",
    "        print(\"CONCATENATION worked\")\n",
    "        \n",
    "            \n",
    "        \n",
    "    \n",
    "        \n",
    "        \n",
    "   \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "04936fdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row = 262 and input_id length = 21539\n",
      "row = 262 and attention length = 21539\n",
      "padding the row index 262 with 43997 character\n",
      "#### POST CONCAT####\n",
      "row = 262 and input_id length = 65536\n",
      "row = 262 and attention length =65536\n",
      "CONCATENATION worked\n"
     ]
    }
   ],
   "source": [
    "df_train  = pad_df(df_train_temp)\n",
    "verify_len(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ac3c0b25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row = 265 and input_id length = 46356\n",
      "row = 265 and attention length = 46356\n",
      "padding the row index 265 with 19180 character\n",
      "#### POST CONCAT####\n",
      "row = 265 and input_id length = 65536\n",
      "row = 265 and attention length =65536\n",
      "CONCATENATION worked\n"
     ]
    }
   ],
   "source": [
    "df_val  = pad_df(df_val_temp)\n",
    "verify_len(df_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9987dc33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_ids</th>\n",
       "      <th>attention_mask</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[32, 25, 197, 40, 1101, 1654, 484, 389, 13, 19...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[314, 1612, 11, 340, 338, 1611, 286, 43244, 11...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[198, 32, 25, 197, 1870, 11, 21480, 11, 314, 4...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[25, 197, 2396, 314, 466, 21099, 326, 11, 475,...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[329, 6835, 10807, 13, 198, 32610, 1810, 13, 1...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           input_ids  \\\n",
       "0  [32, 25, 197, 40, 1101, 1654, 484, 389, 13, 19...   \n",
       "1  [314, 1612, 11, 340, 338, 1611, 286, 43244, 11...   \n",
       "2  [198, 32, 25, 197, 1870, 11, 21480, 11, 314, 4...   \n",
       "3  [25, 197, 2396, 314, 466, 21099, 326, 11, 475,...   \n",
       "4  [329, 6835, 10807, 13, 198, 32610, 1810, 13, 1...   \n",
       "\n",
       "                                      attention_mask  \n",
       "0  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "1  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "2  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "3  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "4  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c07131d",
   "metadata": {},
   "source": [
    "## Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "686f7481",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_nan(tensor, name):\n",
    "    if torch.isnan(tensor).any():\n",
    "        print(f\"NaN detected in {name}\")\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dd400511",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import GPT2Config, GPT2LMHeadModel\n",
    "\n",
    "class CustomGPT2Model(AutoModelForCausalLM):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        \n",
    "    def forward(self, input_ids=None, past_key_values=None, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None, inputs_embeds=None, labels=None, use_cache=None, output_attentions=None, output_hidden_states=None, return_dict=None):\n",
    "        if inputs_embeds is None and input_ids is None:\n",
    "            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n",
    "        \n",
    "        return super().forward(\n",
    "            input_ids=input_ids,\n",
    "            past_key_values=past_key_values,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            labels=labels,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "\n",
    "config = GPT2Config.from_pretrained('distilgpt2')\n",
    "model = CustomGPT2Model.from_config(config)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7acba0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(module):\n",
    "    if isinstance(module, (nn.Linear, nn.Embedding)):\n",
    "        module.weight.data.normal_(mean=0.0, std=0.02)\n",
    "        if isinstance(module, nn.Linear) and module.bias is not None:\n",
    "            module.bias.data.zero_()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "eb5f907a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.apply(init_weights)\n",
    "model = torch.compile(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1d899c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(logits, labels, epsilon=1e-8):\n",
    "    return F.cross_entropy(logits.view(-1, logits.size(-1)), labels.view(-1), reduction='mean', ignore_index=-100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "12222770",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "# def compute_loss(logits, labels, ignore_index=-100):\n",
    "#     # Check for NaN values in logits\n",
    "#     if torch.isnan(logits).any():\n",
    "#         print(\"NaN detected in logits\")\n",
    "#         return torch.tensor(0.0, requires_grad=True)  # Return a zero loss that's still differentiable\n",
    "\n",
    "#     # Clip logits to prevent extreme values\n",
    "#     logits_clipped = torch.clamp(logits, min=-100, max=100)\n",
    "\n",
    "#     # Compute the cross-entropy loss\n",
    "#     loss = F.cross_entropy(logits_clipped.view(-1, logits_clipped.size(-1)), labels.view(-1), reduction='mean', ignore_index=ignore_index)\n",
    "\n",
    "#     # Final NaN check\n",
    "#     if torch.isnan(loss):\n",
    "#         print(\"NaN detected in final loss computation\")\n",
    "#         return torch.tensor(0.0, requires_grad=True)  # Return a zero loss that's still differentiable\n",
    "\n",
    "#     return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2119536d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "263"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2167463",
   "metadata": {},
   "source": [
    "### Data loaders and Dataset for batched training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4f276e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_config = AutoConfig.from_pretrained('distilgpt2')\n",
    "    # Initialize the model with random weights\n",
    "embed_model = AutoModelForCausalLM.from_config(embed_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "78fcd9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataset_pyt_train(Dataset):\n",
    "    def __init__(self, df, B = B, T = T, model = embed_model ):\n",
    "        self.df = df\n",
    "                                        \n",
    "    def __getitem__(self, idx):\n",
    "        #print(f\"************* idx for dataloader = {idx}\")\n",
    "        embed_model.to(device)\n",
    "        \n",
    "        input_id_temp = torch.tensor(self.df.iloc[idx]['input_ids'],dtype = torch.long).to(device)\n",
    "        att_mask = torch.tensor(self.df.iloc[idx]['attention_mask']).to(device)\n",
    "               \n",
    "        inp_emb = embed_model.transformer.wte(input_id_temp)\n",
    "        inp_emb =   inp_emb.view(-1,T,768)    \n",
    "        attention_mask = att_mask.view(B,T)   \n",
    "        labels = input_id_temp.view(B,T)\n",
    "        #print(f\"inside train dataloader|Model device = {model.device}|inp_emb device = {inp_emb.device}\")\n",
    "        return inp_emb, attention_mask,labels\n",
    "        \n",
    "        \n",
    "               \n",
    "    def __len__(self):\n",
    "        #return the length of the dataframe\n",
    "        return len(self.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2c671a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataset_pyt_val(Dataset):\n",
    "    \"\"\"Dataset for validation data\"\"\"\n",
    "    def __init__(self, df, B = B, T = T, model = embed_model ):\n",
    "        self.df = df\n",
    "        print(f\"Value of B {B}\")\n",
    "                                        \n",
    "    def __getitem__(self, idx):\n",
    "        embed_model.to(device)\n",
    "        input_id_temp = torch.tensor(self.df.iloc[idx]['input_ids'],dtype = torch.long).to(device)\n",
    "        att_mask = torch.tensor(self.df.iloc[idx]['attention_mask'],dtype = torch.long).to(device)\n",
    "               \n",
    "        inp_emb = embed_model.transformer.wte(input_id_temp)\n",
    "        inp_emb =   inp_emb.view(-1,T,768)    \n",
    "        attention_mask = att_mask.view(B,T)   \n",
    "        labels = input_id_temp.view(B,T)\n",
    "        \n",
    "        return inp_emb, attention_mask,labels\n",
    "        \n",
    "    def __len__(self):\n",
    "        #return the length of the dataframe\n",
    "        return len(self.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "99ddbe83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_embeddings(embeddings):\n",
    "        # Compute mean and std across the embedding dimension\n",
    "        mean = embeddings.mean(dim=-1, keepdim=True)\n",
    "        std = embeddings.std(dim=-1, keepdim=True)\n",
    "        \n",
    "        # Normalize\n",
    "        normalized_embeddings = (embeddings - mean) / (std + 1e-8)  # Adding epsilon for numerical stability\n",
    "        \n",
    "        return normalized_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c613e22",
   "metadata": {},
   "source": [
    "### Note - batch_size is 1 here because we reshape our input data in the shape [B,T] for a batched training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e3fe9000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value of B 64\n"
     ]
    }
   ],
   "source": [
    "#train_dataset = dataset_pyt(filtered_df,tokenizer = tokenizer)\n",
    "df_train_without_last = df_train.iloc[:-1]\n",
    "df_val_without_last = df_val.iloc[:-1]\n",
    "\n",
    "train_dataset = dataset_pyt_train(df_train_without_last)\n",
    "val_dataset = dataset_pyt_val(df_val_without_last)\n",
    "#test_dataset = dataset_pyt(df_test,tokenizer = tokenizer)\n",
    "\n",
    "train_loader = DataLoader(train_dataset,batch_size = 1, shuffle = True , num_workers = 0, pin_memory = False)\n",
    "val_loader = DataLoader(val_dataset,batch_size = 1, shuffle = True , num_workers = 0, pin_memory = False)\n",
    "#test_loader = DataLoader(test_dataset,batch_size = batch_size, shuffle = False, collate_fn = custom_collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "efc0e989",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inp,att,lab = train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4da902fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the train loader is 262\n",
      "Length of the val loader is 265\n",
      "num_tokens= 17170432\n"
     ]
    }
   ],
   "source": [
    "print(f\"Length of the train loader is {len(train_loader)}\")\n",
    "print(f\"Length of the val loader is {len(val_loader)}\")\n",
    "print(f\"num_tokens= {B*T*len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "76b07c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embed_model.to(device)\n",
    "# model.to(device)\n",
    "# df_without_last = df_train.iloc[:-1]\n",
    "\n",
    "\n",
    "# for i in range(0,len(df_without_last)):\n",
    "#     input_id_temp = torch.tensor(df_train.iloc[i]['input_ids'],dtype = torch.long).to(device)\n",
    "#     att_mask = torch.tensor(df_train.iloc[i]['attention_mask']).to(device)\n",
    "#     inp_emb = embed_model.transformer.wte(input_id_temp)\n",
    "#     inp_emb =   inp_emb.view(-1,T,768)    \n",
    "#     attention_mask = att_mask.view(B,T)   \n",
    "#     labels = input_id_temp.view(B,T)\n",
    "#     with autocast(dtype = torch.bfloat16):\n",
    "#         inp_embedding = inp_emb.to(device=device, non_blocking=True)\n",
    "#         inp_embedding = torch.squeeze(inp_embedding, dim = 0).to(device)\n",
    "#         norm_embedding = normalize_embeddings(inp_embedding).to(device)\n",
    "#         att_mask = attention_mask.to(device=device, non_blocking=True)\n",
    "#         att_mask = torch.squeeze(att_mask, dim = 0).to(device)\n",
    "#         labels = labels.to(device)\n",
    "#         labels = torch.squeeze(labels, dim = 0)\n",
    "#         #print(f\"shapes=  |inp_embedding {inp_embedding.shape}|att_mask = {att_mask.shape}\")\n",
    "#         #with autocast(dtype = torch.bfloat16):\n",
    "#         #print(f\"pre_mean_embeddings ={pre_mean_embeddings}| post_mean_embedding = {post_mean_embedding}\")\n",
    "#         output = model(inputs_embeds = norm_embedding ,attention_mask = att_mask, labels = None)\n",
    "#         #print(f\" model_output = {model_output.loss}\")\n",
    "#         loss = compute_loss(output.logits, labels)\n",
    "#         if torch.isnan(loss):\n",
    "#             print(\"&&&&*****NAN*******&&&&&\")\n",
    "#         print(f\"i = {i}| loss = {loss}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f95dbd21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #read a row of dataframe\n",
    "# model.to(device)\n",
    "# for ind,(embed,attention_mask, lab) in enumerate(train_loader):\n",
    "#     with autocast(dtype = torch.bfloat16):\n",
    "#         inp_embedding = embed.to(device=device, non_blocking=True)\n",
    "#         inp_embedding = torch.squeeze(inp_embedding, dim = 0)\n",
    "#         norm_embedding = normalize_embeddings(inp_embedding)\n",
    "#         att_mask = attention_mask.to(device=device, non_blocking=True)\n",
    "#         att_mask = torch.squeeze(att_mask, dim = 0)\n",
    "#         labels = lab.to(device)\n",
    "#         labels = torch.squeeze(labels, dim = 0)\n",
    "#         #print(f\"shapes=  |inp_embedding {inp_embedding.shape}|att_mask = {att_mask.shape}\")\n",
    "#         #with autocast(dtype = torch.bfloat16):\n",
    "#         #print(f\"pre_mean_embeddings ={pre_mean_embeddings}| post_mean_embedding = {post_mean_embedding}\")\n",
    "#         output = model(inputs_embeds = norm_embedding ,attention_mask = att_mask, labels = None)\n",
    "#         #print(f\" model_output = {model_output.loss}\")\n",
    "#         loss = compute_loss(output.logits, labels)\n",
    "#         if torch.isnan(loss):\n",
    "#             print(\"&&&&*****NAN*******&&&&&\")\n",
    "#         print(f\"idx = {ind}| loss = {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1f9c8d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_file(log_message, model_name = \"distl_gpt2\" ,random_init_wts = random_init_wts ):\n",
    "    \"\"\"This function logs the training performance for future reference\"\"\"\n",
    "    current_datetime = datetime.datetime.now()\n",
    "    # Extract date and time components\n",
    "    current_date = str(current_datetime.date())\n",
    "    log_file = model_name +'_Random_init_nll_and_cos_loss'+'random_init_wts'+ '_'+str(random_init_wts)+'_' +current_date+'.log'\n",
    "    print(f\"*****LOGGING INFO IN {log_file}*********\")\n",
    "    filepath = os.path.join(\"model\",log_file)\n",
    "    logging.basicConfig(filename=filepath, \n",
    "                    filemode='a',  # Overwrite the log file each time\n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s', \n",
    "                    level=logging.DEBUG)\n",
    "    logger = logging.getLogger()\n",
    "    logger.info(log_message)\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "574b6502",
   "metadata": {},
   "outputs": [],
   "source": [
    "class check_train_metrics:\n",
    "    def __init__(self, patience=10, min_delta=0 , B = T, T = T,best_loss = torch.inf,early_stop = False):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = best_loss\n",
    "        self.early_stop = early_stop\n",
    "        self.B = B\n",
    "        self.T = T\n",
    "        self.improvement = None\n",
    "\n",
    "    def __call__(self, loss, epoch , epoch_durn, norm , current_lr, num_token):\n",
    "        if self.best_loss - loss > self.min_delta:\n",
    "            \n",
    "            print(f\"training loss has decreased---> reducing the best loss from {self.best_loss:.2f} to {loss:.2f} | throughput = {int(num_token/epoch_durn)} tokens/second | norm = {norm:.4f} | learning rate = {current_lr:.5e}\")\n",
    "            self.best_loss = loss\n",
    "            self.counter = 0\n",
    "            self.improvement = True\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            self.improvement = False\n",
    "            print(f\"No improvement in training  loss-->epoch= {epoch} and best loss is {self.best_loss:.2f}|current_loss = {loss}|counter = {self.counter}\")\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "56558797",
   "metadata": {},
   "outputs": [],
   "source": [
    "class check_val_metrics:\n",
    "    def __init__(self, patience=10, min_delta=0, best_loss = torch.inf,early_stop = False):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = best_loss\n",
    "        self.early_stop = early_stop\n",
    "        \n",
    "\n",
    "    def __call__(self, loss, epoch , model, tokenizer):\n",
    "        if self.best_loss - loss > self.min_delta:\n",
    "            print(f\"Val loss has decreased -->reducing the global validation loss from {self.best_loss:.2f} to {loss:.2f}\")\n",
    "            s1 = (f\"Val loss has decreased -->reducing the global validation loss from {self.best_loss:.2f} to {loss:.2f}\")\n",
    "            print(f\" validation loss for epoch = {epoch} is {loss:.4f}\")\n",
    "            self.best_loss = loss\n",
    "            s2 = f\" validation loss for epoch = {epoch} is {loss:.4f}\"\n",
    "            print(f\" epoch= {epoch} :  val loss is {loss:.4f} \")\n",
    "            s3 = f\" epoch= {epoch} :  val loss is {loss:.4f} \"\n",
    "            #save the model\n",
    "            # Get the current date and time\n",
    "            current_datetime = datetime.datetime.now()\n",
    "            # Extract date and time components\n",
    "            current_date = str(current_datetime.date())\n",
    "            current_time = str(current_datetime.time()).split('.')[0]\n",
    "            file_name = 'model'+ current_date+current_time+'.pth'\n",
    "            path = os.path.join(\"model\",file_name)\n",
    "            print(f\"saving the model {file_name}\")\n",
    "            s4 = f\"saving the model {file_name}\"\n",
    "            #torch.save(model.state_dict(), path)\n",
    "            model.save_pretrained(path)\n",
    "            tokenizer.save_pretrained(path)\n",
    "            log_message = s1+s2+s3+s4\n",
    "            write_file(log_message)\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            print(f\"No improvement in validation loss-->epoch= {epoch} and best val loss is {self.best_loss:.2f}|current_Val loss = {loss}|counter = {self.counter}\")\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "27b9d277",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_nan(tensor, name):\n",
    "    if torch.isnan(tensor).any():\n",
    "        print(f\"NaN detected in {name}\")\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1720ee48",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad\n",
    "\n",
    "def eval_model(val_loader, model, epoch , device = device,tokenizer = tokenizer,nll_learning_steps = nll_learning_steps):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    e = epoch\n",
    "    embedding_layer = model.transformer.wte\n",
    "    val_loss_accum = 0.0\n",
    "    print(f\"inside validation data for epoch {e}\")\n",
    "    for ind,(embed,attention_mask, lab) in enumerate(val_loader):\n",
    "        inp_embedding = embed.to(device=device, non_blocking=True)\n",
    "        inp_embedding = torch.squeeze(inp_embedding, dim = 0)\n",
    "        att_mask = attention_mask.to(device=device, non_blocking=True)\n",
    "        att_mask = torch.squeeze(att_mask, dim = 0)\n",
    "        labels = lab.to(device)\n",
    "        labels = torch.squeeze(labels, dim = 0)\n",
    "        post_mean_embedding = normalize_embeddings(inp_embedding)\n",
    "        if e < nll_learning_steps:\n",
    "            with autocast(dtype = torch.bfloat16):\n",
    "                model_output = model(inputs_embeds = post_mean_embedding.to(device) ,attention_mask = att_mask, labels = None)\n",
    "                total_loss = compute_loss(model_output.logits, labels)\n",
    "                                \n",
    "           \n",
    "        else:\n",
    "            with autocast(dtype = torch.bfloat16):\n",
    "                model_output = model(inputs_embeds = post_mean_embedding.to(device) ,attention_mask = att_mask, labels = None)\n",
    "                logits = model_output.logits\n",
    "                predictions = torch.argmax(logits, dim=-1)\n",
    "                #print(f\"predictions = {predictions}\")\n",
    "                prediction_embeddings = embedding_layer(predictions)\n",
    "                cos_sim = F.cosine_similarity(torch.squeeze(prediction_embeddings,dim = 0), torch.squeeze(post_mean_embedding,dim = 0), dim=1)\n",
    "                cos_loss = 1- cos_sim.mean()\n",
    "                total_loss = cos_loss\n",
    "                del logits,predictions,cos_sim,cos_loss\n",
    "               \n",
    "        #print(f\"Loss value from val_data = {total_loss}\")\n",
    "        val_loss_accum+=total_loss.detach().item()\n",
    "        del inp_embedding,att_mask,labels,model_output,post_mean_embedding\n",
    "    return val_loss_accum\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "75f2fbcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_loader,val_loader,model,num_epoch = epoch,device = device,tokenizer = tokenizer,nll_learning_steps = nll_learning_steps):\n",
    "    model.train()\n",
    "    device = device\n",
    "    lr_custom = 1e-5\n",
    "    print(f\"inside train model. Device = {device}\")\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.AdamW(params =  model.parameters(), lr= lr_custom,fused = True ,weight_decay = .1)\n",
    "    total_batch_size = 2**19\n",
    "    grad_accum_step = total_batch_size//(B*T)\n",
    "    \n",
    "    extra_train = .1*num_epoch\n",
    "    max_train_steps = int(num_epoch +extra_train )\n",
    "    import time\n",
    "    from transformers import get_linear_schedule_with_warmup\n",
    "    total_steps = len(train_loader) * num_epoch\n",
    "    scheduler_cos = transformers.get_cosine_schedule_with_warmup( optimizer= optimizer, num_warmup_steps =int(total_steps * 0.1) ,num_training_steps= total_steps )\n",
    "    embedding_layer = model.transformer.wte    \n",
    "    epoch_train_log = []\n",
    "    epoch_val_log = []\n",
    "    validate_val_metric = check_val_metrics()\n",
    "    validate_train_metric = check_train_metrics(patience=5, min_delta=0 , B = T, T = T)\n",
    "        \n",
    "    for i in range (max_train_steps):\n",
    "        \n",
    "        epoch_start_time = time.time()\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        # we use 2 schedulers - the first LR scheduler uses a cosine decay for 100 epochs the second scheduler takes the last LR from cosine scheduler and then maintains that LR for the next 10 epochs\n",
    "        if i >= num_epoch:\n",
    "            optimizer_reduced_lr = torch.optim.AdamW(params =  model.parameters(), lr= current_lr ,fused = True , weight_decay=.1)\n",
    "            scheduler_constant = transformers.get_constant_schedule_with_warmup( optimizer = optimizer_reduced_lr ,num_warmup_steps = 0, last_epoch = -1 )\n",
    "        \n",
    "        epoch_train_loss = 0       \n",
    "        for ind,(embed,attention_mask, lab) in enumerate(train_loader):\n",
    "            if ind == int(len(train_loader)/2):\n",
    "                batch_time = time.time()\n",
    "                duration = batch_time - epoch_start_time\n",
    "                print(f\"executing epoch:{i+1}, it took {duration/60} mins from beginning of epoch till batch#{ind}\")\n",
    "            \n",
    "            inp_embedding = embed.to(device=device, non_blocking=True)\n",
    "            inp_embedding = torch.squeeze(inp_embedding, dim = 0)\n",
    "            att_mask = attention_mask.to(device=device, non_blocking=True)\n",
    "            att_mask = torch.squeeze(att_mask, dim = 0)\n",
    "            labels = lab.to(device)\n",
    "            labels = torch.squeeze(labels, dim = 0)\n",
    "            post_mean_embedding = normalize_embeddings(inp_embedding)\n",
    "            if check_nan(post_mean_embedding, \"embeddings\"):\n",
    "                continue\n",
    "            \n",
    "            if i < nll_learning_steps:\n",
    "                with autocast(dtype = torch.bfloat16):\n",
    "                    model_output = model(inputs_embeds = post_mean_embedding.to(device) ,attention_mask = att_mask, labels = None)\n",
    "                    loss = compute_loss(model_output.logits, labels)\n",
    "                    #print(f\"ind = {ind}|post_mean_embedding mean = {post_mean_embedding.mean()}|logits mean = {model_output.logits.mean()}|loss = {loss}\")\n",
    "                    if torch.isnan(loss):\n",
    "                        print(\"f nan values encountered..\")\n",
    "                        decoded_text = []\n",
    "                        continue\n",
    "                    total_loss =loss \n",
    "                                \n",
    "            \n",
    "            else:\n",
    "                with autocast(dtype = torch.bfloat16):\n",
    "                    model_output = model(inputs_embeds = post_mean_embedding.to(device) ,attention_mask = att_mask, labels = None)\n",
    "                    logits = model_output.logits\n",
    "                    predictions = torch.argmax(logits, dim=-1)\n",
    "                    #print(f\"predictions = {predictions}\")\n",
    "                    prediction_embeddings = embedding_layer(predictions)\n",
    "                    cos_sim = F.cosine_similarity(torch.squeeze(prediction_embeddings,dim = 0), torch.squeeze(post_mean_embedding,dim = 0), dim=1)\n",
    "                    cos_loss = 1- cos_sim.mean()\n",
    "                    total_loss = cos_loss\n",
    "                del logits,predictions,cos_sim,cos_loss\n",
    "                     \n",
    "            total_loss.backward()\n",
    "            epoch_train_loss += total_loss.detach().item()\n",
    "            norm = torch.nn.utils.clip_grad_norm(model.parameters() , 1.0)\n",
    "            if i <= num_epoch:\n",
    "                optimizer.step()\n",
    "                scheduler_cos.step()\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "            else:\n",
    "                optimizer_reduced_lr.step()\n",
    "                optimizer_reduced_lr.zero_grad(set_to_none=True)\n",
    "                scheduler_constant.step()\n",
    "                \n",
    "                         \n",
    "            del att_mask,labels,model_output,post_mean_embedding\n",
    "            \n",
    "        #batch processing complete \n",
    "        #print(f\"batch processing complete , lambda = {lambda_val} |total_loss for batch= {total_loss}\")\n",
    "        \n",
    "        if i <= num_epoch:\n",
    "            current_lr = scheduler_cos.get_last_lr()[0]\n",
    "        epoch_end_time = time.time()\n",
    "        epoch_durn = (epoch_end_time - epoch_start_time)\n",
    "        num_token = B*T*len(train_loader)\n",
    "        epoch_train_log.append(epoch_train_loss)\n",
    "        validate_train_metric(epoch_train_loss, i , epoch_durn, norm , current_lr, num_token)\n",
    "        \n",
    "        if validate_train_metric.improvement:\n",
    "            val_loss= eval_model(val_loader, model, epoch = i, device = device,tokenizer = tokenizer)\n",
    "            epoch_val_log.append(val_loss)\n",
    "            validate_val_metric(val_loss, i , model, tokenizer)\n",
    "            if validate_train_metric.early_stop or validate_val_metric.early_stop :\n",
    "                print(f\"early stopping trigerred either from training data or val data | train_counter = {validate_train_metric.counter}|val_counter = {validate_val_metric.counter}\")\n",
    "                break\n",
    "        else:\n",
    "            if validate_val_metric.early_stop:\n",
    "                print(f\"early stopping trigerred from validation data\")\n",
    "                break\n",
    "              \n",
    "    \n",
    "    return model,epoch_train_log,epoch_val_log\n",
    "        \n",
    "            \n",
    "            \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "365bb169",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inside train model. Device = cuda:0\n",
      "executing epoch:1, it took 1.4541966875394186 mins from beginning of epoch till batch#131\n",
      "training loss has decreased---> reducing the best loss from inf to 2835.40 | throughput = 110375 tokens/second | norm = 7.2941 | learning rate = 5.00000e-07\n",
      "inside validation data for epoch 0\n",
      "Val loss has decreased -->reducing the global validation loss from inf to 2762.13\n",
      " validation loss for epoch = 0 is 2762.1281\n",
      " epoch= 0 :  val loss is 2762.1281 \n",
      "saving the model model2024-07-2200:03:24.pth\n",
      "*****LOGGING INFO IN distl_gpt2_Random_init_nll_and_cos_lossrandom_init_wts_True_2024-07-22.log*********\n",
      "executing epoch:2, it took 1.2265158454577128 mins from beginning of epoch till batch#131\n",
      "training loss has decreased---> reducing the best loss from 2835.40 to 2515.86 | throughput = 125335 tokens/second | norm = 4.9816 | learning rate = 1.00000e-06\n",
      "inside validation data for epoch 1\n",
      "Val loss has decreased -->reducing the global validation loss from 2762.13 to 2297.36\n",
      " validation loss for epoch = 1 is 2297.3550\n",
      " epoch= 1 :  val loss is 2297.3550 \n",
      "saving the model model2024-07-2200:06:32.pth\n",
      "*****LOGGING INFO IN distl_gpt2_Random_init_nll_and_cos_lossrandom_init_wts_True_2024-07-22.log*********\n",
      "executing epoch:3, it took 1.0577618598937988 mins from beginning of epoch till batch#131\n",
      "training loss has decreased---> reducing the best loss from 2515.86 to 1971.15 | throughput = 135445 tokens/second | norm = 2.2254 | learning rate = 1.50000e-06\n",
      "inside validation data for epoch 2\n",
      "Val loss has decreased -->reducing the global validation loss from 2297.36 to 1761.39\n",
      " validation loss for epoch = 2 is 1761.3891\n",
      " epoch= 2 :  val loss is 1761.3891 \n",
      "saving the model model2024-07-2200:09:30.pth\n",
      "*****LOGGING INFO IN distl_gpt2_Random_init_nll_and_cos_lossrandom_init_wts_True_2024-07-22.log*********\n",
      "executing epoch:4, it took 1.0524160305658976 mins from beginning of epoch till batch#131\n",
      "training loss has decreased---> reducing the best loss from 1971.15 to 1501.20 | throughput = 135834 tokens/second | norm = 1.1481 | learning rate = 2.00000e-06\n",
      "inside validation data for epoch 3\n",
      "Val loss has decreased -->reducing the global validation loss from 1761.39 to 1387.09\n",
      " validation loss for epoch = 3 is 1387.0870\n",
      " epoch= 3 :  val loss is 1387.0870 \n",
      "saving the model model2024-07-2200:12:27.pth\n",
      "*****LOGGING INFO IN distl_gpt2_Random_init_nll_and_cos_lossrandom_init_wts_True_2024-07-22.log*********\n",
      "executing epoch:5, it took 1.0474263032277424 mins from beginning of epoch till batch#131\n",
      "training loss has decreased---> reducing the best loss from 1501.20 to 1199.09 | throughput = 136491 tokens/second | norm = 1.2781 | learning rate = 2.50000e-06\n",
      "inside validation data for epoch 4\n",
      "Val loss has decreased -->reducing the global validation loss from 1387.09 to 1126.05\n",
      " validation loss for epoch = 4 is 1126.0518\n",
      " epoch= 4 :  val loss is 1126.0518 \n",
      "saving the model model2024-07-2200:15:23.pth\n",
      "*****LOGGING INFO IN distl_gpt2_Random_init_nll_and_cos_lossrandom_init_wts_True_2024-07-22.log*********\n",
      "executing epoch:6, it took 1.0476132154464721 mins from beginning of epoch till batch#131\n",
      "training loss has decreased---> reducing the best loss from 1199.09 to 977.09 | throughput = 136362 tokens/second | norm = 0.9646 | learning rate = 3.00000e-06\n",
      "inside validation data for epoch 5\n",
      "Val loss has decreased -->reducing the global validation loss from 1126.05 to 925.04\n",
      " validation loss for epoch = 5 is 925.0407\n",
      " epoch= 5 :  val loss is 925.0407 \n",
      "saving the model model2024-07-2200:18:20.pth\n",
      "*****LOGGING INFO IN distl_gpt2_Random_init_nll_and_cos_lossrandom_init_wts_True_2024-07-22.log*********\n",
      "executing epoch:7, it took 1.0558361570040384 mins from beginning of epoch till batch#131\n",
      "training loss has decreased---> reducing the best loss from 977.09 to 808.63 | throughput = 135494 tokens/second | norm = 0.4248 | learning rate = 3.50000e-06\n",
      "inside validation data for epoch 6\n",
      "Val loss has decreased -->reducing the global validation loss from 925.04 to 772.42\n",
      " validation loss for epoch = 6 is 772.4170\n",
      " epoch= 6 :  val loss is 772.4170 \n",
      "saving the model model2024-07-2200:21:18.pth\n",
      "*****LOGGING INFO IN distl_gpt2_Random_init_nll_and_cos_lossrandom_init_wts_True_2024-07-22.log*********\n",
      "executing epoch:8, it took 1.0533757050832113 mins from beginning of epoch till batch#131\n",
      "training loss has decreased---> reducing the best loss from 808.63 to 679.76 | throughput = 135722 tokens/second | norm = 0.5583 | learning rate = 4.00000e-06\n",
      "inside validation data for epoch 7\n",
      "Val loss has decreased -->reducing the global validation loss from 772.42 to 657.06\n",
      " validation loss for epoch = 7 is 657.0566\n",
      " epoch= 7 :  val loss is 657.0566 \n",
      "saving the model model2024-07-2200:24:15.pth\n",
      "*****LOGGING INFO IN distl_gpt2_Random_init_nll_and_cos_lossrandom_init_wts_True_2024-07-22.log*********\n",
      "executing epoch:9, it took 1.0541266600290935 mins from beginning of epoch till batch#131\n",
      "training loss has decreased---> reducing the best loss from 679.76 to 580.95 | throughput = 135696 tokens/second | norm = 0.5238 | learning rate = 4.50000e-06\n",
      "inside validation data for epoch 8\n",
      "Val loss has decreased -->reducing the global validation loss from 657.06 to 567.43\n",
      " validation loss for epoch = 8 is 567.4307\n",
      " epoch= 8 :  val loss is 567.4307 \n",
      "saving the model model2024-07-2200:27:12.pth\n",
      "*****LOGGING INFO IN distl_gpt2_Random_init_nll_and_cos_lossrandom_init_wts_True_2024-07-22.log*********\n",
      "executing epoch:10, it took 1.0542719841003418 mins from beginning of epoch till batch#131\n",
      "training loss has decreased---> reducing the best loss from 580.95 to 502.42 | throughput = 135649 tokens/second | norm = 0.8011 | learning rate = 5.00000e-06\n",
      "inside validation data for epoch 9\n",
      "Val loss has decreased -->reducing the global validation loss from 567.43 to 495.71\n",
      " validation loss for epoch = 9 is 495.7124\n",
      " epoch= 9 :  val loss is 495.7124 \n",
      "saving the model model2024-07-2200:30:10.pth\n",
      "*****LOGGING INFO IN distl_gpt2_Random_init_nll_and_cos_lossrandom_init_wts_True_2024-07-22.log*********\n",
      "executing epoch:11, it took 1.054562759399414 mins from beginning of epoch till batch#131\n",
      "training loss has decreased---> reducing the best loss from 502.42 to 439.18 | throughput = 135658 tokens/second | norm = 0.4390 | learning rate = 5.50000e-06\n",
      "inside validation data for epoch 10\n",
      "Val loss has decreased -->reducing the global validation loss from 495.71 to 436.98\n",
      " validation loss for epoch = 10 is 436.9774\n",
      " epoch= 10 :  val loss is 436.9774 \n",
      "saving the model model2024-07-2200:33:07.pth\n",
      "*****LOGGING INFO IN distl_gpt2_Random_init_nll_and_cos_lossrandom_init_wts_True_2024-07-22.log*********\n",
      "executing epoch:12, it took 1.0526863058408102 mins from beginning of epoch till batch#131\n",
      "training loss has decreased---> reducing the best loss from 439.18 to 386.75 | throughput = 135843 tokens/second | norm = 0.3108 | learning rate = 6.00000e-06\n",
      "inside validation data for epoch 11\n",
      "Val loss has decreased -->reducing the global validation loss from 436.98 to 387.72\n",
      " validation loss for epoch = 11 is 387.7168\n",
      " epoch= 11 :  val loss is 387.7168 \n",
      "saving the model model2024-07-2200:36:04.pth\n",
      "*****LOGGING INFO IN distl_gpt2_Random_init_nll_and_cos_lossrandom_init_wts_True_2024-07-22.log*********\n",
      "executing epoch:13, it took 1.0548568725585938 mins from beginning of epoch till batch#131\n",
      "training loss has decreased---> reducing the best loss from 386.75 to 342.16 | throughput = 135693 tokens/second | norm = 0.6758 | learning rate = 6.50000e-06\n",
      "inside validation data for epoch 12\n",
      "Val loss has decreased -->reducing the global validation loss from 387.72 to 345.03\n",
      " validation loss for epoch = 12 is 345.0284\n",
      " epoch= 12 :  val loss is 345.0284 \n",
      "saving the model model2024-07-2200:39:02.pth\n",
      "*****LOGGING INFO IN distl_gpt2_Random_init_nll_and_cos_lossrandom_init_wts_True_2024-07-22.log*********\n",
      "executing epoch:14, it took 1.0498058040936789 mins from beginning of epoch till batch#131\n",
      "training loss has decreased---> reducing the best loss from 342.16 to 303.31 | throughput = 136075 tokens/second | norm = 0.4390 | learning rate = 7.00000e-06\n",
      "inside validation data for epoch 13\n",
      "Val loss has decreased -->reducing the global validation loss from 345.03 to 307.53\n",
      " validation loss for epoch = 13 is 307.5348\n",
      " epoch= 13 :  val loss is 307.5348 \n",
      "saving the model model2024-07-2200:41:59.pth\n",
      "*****LOGGING INFO IN distl_gpt2_Random_init_nll_and_cos_lossrandom_init_wts_True_2024-07-22.log*********\n",
      "executing epoch:15, it took 1.0538137197494506 mins from beginning of epoch till batch#131\n",
      "training loss has decreased---> reducing the best loss from 303.31 to 269.08 | throughput = 135779 tokens/second | norm = 0.1737 | learning rate = 7.50000e-06\n",
      "inside validation data for epoch 14\n",
      "Val loss has decreased -->reducing the global validation loss from 307.53 to 274.31\n",
      " validation loss for epoch = 14 is 274.3134\n",
      " epoch= 14 :  val loss is 274.3134 \n",
      "saving the model model2024-07-2200:44:56.pth\n",
      "*****LOGGING INFO IN distl_gpt2_Random_init_nll_and_cos_lossrandom_init_wts_True_2024-07-22.log*********\n",
      "executing epoch:16, it took 1.0504333217938742 mins from beginning of epoch till batch#131\n",
      "training loss has decreased---> reducing the best loss from 269.08 to 238.70 | throughput = 136172 tokens/second | norm = 0.4474 | learning rate = 8.00000e-06\n",
      "inside validation data for epoch 15\n",
      "Val loss has decreased -->reducing the global validation loss from 274.31 to 244.41\n",
      " validation loss for epoch = 15 is 244.4080\n",
      " epoch= 15 :  val loss is 244.4080 \n",
      "saving the model model2024-07-2200:47:53.pth\n",
      "*****LOGGING INFO IN distl_gpt2_Random_init_nll_and_cos_lossrandom_init_wts_True_2024-07-22.log*********\n",
      "executing epoch:17, it took 1.052696136633555 mins from beginning of epoch till batch#131\n",
      "training loss has decreased---> reducing the best loss from 238.70 to 211.58 | throughput = 135836 tokens/second | norm = 0.3556 | learning rate = 8.50000e-06\n",
      "inside validation data for epoch 16\n",
      "Val loss has decreased -->reducing the global validation loss from 244.41 to 217.67\n",
      " validation loss for epoch = 16 is 217.6679\n",
      " epoch= 16 :  val loss is 217.6679 \n",
      "saving the model model2024-07-2200:50:50.pth\n",
      "*****LOGGING INFO IN distl_gpt2_Random_init_nll_and_cos_lossrandom_init_wts_True_2024-07-22.log*********\n",
      "executing epoch:18, it took 1.0545294761657715 mins from beginning of epoch till batch#131\n",
      "training loss has decreased---> reducing the best loss from 211.58 to 187.23 | throughput = 135769 tokens/second | norm = 0.1135 | learning rate = 9.00000e-06\n",
      "inside validation data for epoch 17\n",
      "Val loss has decreased -->reducing the global validation loss from 217.67 to 193.42\n",
      " validation loss for epoch = 17 is 193.4245\n",
      " epoch= 17 :  val loss is 193.4245 \n",
      "saving the model model2024-07-2200:53:48.pth\n",
      "*****LOGGING INFO IN distl_gpt2_Random_init_nll_and_cos_lossrandom_init_wts_True_2024-07-22.log*********\n",
      "executing epoch:19, it took 1.0512734731038411 mins from beginning of epoch till batch#131\n",
      "training loss has decreased---> reducing the best loss from 187.23 to 165.26 | throughput = 135897 tokens/second | norm = 0.1302 | learning rate = 9.50000e-06\n",
      "inside validation data for epoch 18\n",
      "Val loss has decreased -->reducing the global validation loss from 193.42 to 171.67\n",
      " validation loss for epoch = 18 is 171.6692\n",
      " epoch= 18 :  val loss is 171.6692 \n",
      "saving the model model2024-07-2200:56:45.pth\n",
      "*****LOGGING INFO IN distl_gpt2_Random_init_nll_and_cos_lossrandom_init_wts_True_2024-07-22.log*********\n",
      "executing epoch:20, it took 1.0538430253664652 mins from beginning of epoch till batch#131\n",
      "training loss has decreased---> reducing the best loss from 165.26 to 145.44 | throughput = 135979 tokens/second | norm = 0.3275 | learning rate = 1.00000e-05\n",
      "inside validation data for epoch 19\n",
      "Val loss has decreased -->reducing the global validation loss from 171.67 to 152.39\n",
      " validation loss for epoch = 19 is 152.3881\n",
      " epoch= 19 :  val loss is 152.3881 \n",
      "saving the model model2024-07-2200:59:42.pth\n",
      "*****LOGGING INFO IN distl_gpt2_Random_init_nll_and_cos_lossrandom_init_wts_True_2024-07-22.log*********\n",
      "executing epoch:21, it took 1.0527996937433879 mins from beginning of epoch till batch#131\n",
      "training loss has decreased---> reducing the best loss from 145.44 to 127.73 | throughput = 135846 tokens/second | norm = 0.3787 | learning rate = 9.99924e-06\n",
      "inside validation data for epoch 20\n",
      "Val loss has decreased -->reducing the global validation loss from 152.39 to 134.82\n",
      " validation loss for epoch = 20 is 134.8227\n",
      " epoch= 20 :  val loss is 134.8227 \n",
      "saving the model model2024-07-2201:02:39.pth\n",
      "*****LOGGING INFO IN distl_gpt2_Random_init_nll_and_cos_lossrandom_init_wts_True_2024-07-22.log*********\n",
      "executing epoch:22, it took 1.0542701840400697 mins from beginning of epoch till batch#131\n",
      "training loss has decreased---> reducing the best loss from 127.73 to 112.35 | throughput = 135750 tokens/second | norm = 0.3811 | learning rate = 9.99695e-06\n",
      "inside validation data for epoch 21\n",
      "Val loss has decreased -->reducing the global validation loss from 134.82 to 120.00\n",
      " validation loss for epoch = 21 is 119.9964\n",
      " epoch= 21 :  val loss is 119.9964 \n",
      "saving the model model2024-07-2201:05:36.pth\n",
      "*****LOGGING INFO IN distl_gpt2_Random_init_nll_and_cos_lossrandom_init_wts_True_2024-07-22.log*********\n",
      "executing epoch:23, it took 1.0536199847857157 mins from beginning of epoch till batch#131\n",
      "training loss has decreased---> reducing the best loss from 112.35 to 99.08 | throughput = 136057 tokens/second | norm = 0.1576 | learning rate = 9.99315e-06\n",
      "inside validation data for epoch 22\n",
      "Val loss has decreased -->reducing the global validation loss from 120.00 to 107.18\n",
      " validation loss for epoch = 22 is 107.1834\n",
      " epoch= 22 :  val loss is 107.1834 \n",
      "saving the model model2024-07-2201:08:33.pth\n",
      "*****LOGGING INFO IN distl_gpt2_Random_init_nll_and_cos_lossrandom_init_wts_True_2024-07-22.log*********\n",
      "executing epoch:24, it took 1.053180201848348 mins from beginning of epoch till batch#131\n",
      "training loss has decreased---> reducing the best loss from 99.08 to 87.61 | throughput = 135873 tokens/second | norm = 0.0966 | learning rate = 9.98782e-06\n",
      "inside validation data for epoch 23\n",
      "Val loss has decreased -->reducing the global validation loss from 107.18 to 95.76\n",
      " validation loss for epoch = 23 is 95.7556\n",
      " epoch= 23 :  val loss is 95.7556 \n",
      "saving the model model2024-07-2201:11:30.pth\n",
      "*****LOGGING INFO IN distl_gpt2_Random_init_nll_and_cos_lossrandom_init_wts_True_2024-07-22.log*********\n",
      "executing epoch:25, it took 1.0543183048566183 mins from beginning of epoch till batch#131\n",
      "training loss has decreased---> reducing the best loss from 87.61 to 77.58 | throughput = 135714 tokens/second | norm = 0.0556 | learning rate = 9.98097e-06\n",
      "inside validation data for epoch 24\n",
      "Val loss has decreased -->reducing the global validation loss from 95.76 to 85.84\n",
      " validation loss for epoch = 24 is 85.8377\n",
      " epoch= 24 :  val loss is 85.8377 \n",
      "saving the model model2024-07-2201:14:28.pth\n",
      "*****LOGGING INFO IN distl_gpt2_Random_init_nll_and_cos_lossrandom_init_wts_True_2024-07-22.log*********\n",
      "executing epoch:26, it took 1.0536793748537698 mins from beginning of epoch till batch#131\n",
      "training loss has decreased---> reducing the best loss from 77.58 to 68.83 | throughput = 136046 tokens/second | norm = 0.1567 | learning rate = 9.97261e-06\n",
      "inside validation data for epoch 25\n",
      "Val loss has decreased -->reducing the global validation loss from 85.84 to 77.24\n",
      " validation loss for epoch = 25 is 77.2356\n",
      " epoch= 25 :  val loss is 77.2356 \n",
      "saving the model model2024-07-2201:17:25.pth\n",
      "*****LOGGING INFO IN distl_gpt2_Random_init_nll_and_cos_lossrandom_init_wts_True_2024-07-22.log*********\n",
      "executing epoch:27, it took 1.0533191959063213 mins from beginning of epoch till batch#131\n",
      "training loss has decreased---> reducing the best loss from 68.83 to 61.11 | throughput = 135692 tokens/second | norm = 0.3497 | learning rate = 9.96273e-06\n",
      "inside validation data for epoch 26\n",
      "Val loss has decreased -->reducing the global validation loss from 77.24 to 69.38\n",
      " validation loss for epoch = 26 is 69.3793\n",
      " epoch= 26 :  val loss is 69.3793 \n",
      "saving the model model2024-07-2201:20:22.pth\n",
      "*****LOGGING INFO IN distl_gpt2_Random_init_nll_and_cos_lossrandom_init_wts_True_2024-07-22.log*********\n",
      "executing epoch:28, it took 1.0543058594067891 mins from beginning of epoch till batch#131\n",
      "training loss has decreased---> reducing the best loss from 61.11 to 54.31 | throughput = 135881 tokens/second | norm = 0.1513 | learning rate = 9.95134e-06\n",
      "inside validation data for epoch 27\n",
      "Val loss has decreased -->reducing the global validation loss from 69.38 to 62.51\n",
      " validation loss for epoch = 27 is 62.5142\n",
      " epoch= 27 :  val loss is 62.5142 \n",
      "saving the model model2024-07-2201:23:19.pth\n",
      "*****LOGGING INFO IN distl_gpt2_Random_init_nll_and_cos_lossrandom_init_wts_True_2024-07-22.log*********\n",
      "executing epoch:29, it took 1.0533151427904766 mins from beginning of epoch till batch#131\n",
      "training loss has decreased---> reducing the best loss from 54.31 to 48.30 | throughput = 135732 tokens/second | norm = 0.0974 | learning rate = 9.93844e-06\n",
      "inside validation data for epoch 28\n",
      "Val loss has decreased -->reducing the global validation loss from 62.51 to 56.37\n",
      " validation loss for epoch = 28 is 56.3680\n",
      " epoch= 28 :  val loss is 56.3680 \n",
      "saving the model model2024-07-2201:26:17.pth\n",
      "*****LOGGING INFO IN distl_gpt2_Random_init_nll_and_cos_lossrandom_init_wts_True_2024-07-22.log*********\n",
      "executing epoch:30, it took 1.0535526434580484 mins from beginning of epoch till batch#131\n",
      "training loss has decreased---> reducing the best loss from 48.30 to 43.02 | throughput = 135816 tokens/second | norm = 0.1793 | learning rate = 9.92404e-06\n",
      "inside validation data for epoch 29\n",
      "Val loss has decreased -->reducing the global validation loss from 56.37 to 50.91\n",
      " validation loss for epoch = 29 is 50.9134\n",
      " epoch= 29 :  val loss is 50.9134 \n",
      "saving the model model2024-07-2201:29:14.pth\n",
      "*****LOGGING INFO IN distl_gpt2_Random_init_nll_and_cos_lossrandom_init_wts_True_2024-07-22.log*********\n",
      "executing epoch:31, it took 1.053254814942678 mins from beginning of epoch till batch#131\n",
      "training loss has decreased---> reducing the best loss from 43.02 to 38.32 | throughput = 135833 tokens/second | norm = 0.1003 | learning rate = 9.90814e-06\n",
      "inside validation data for epoch 30\n",
      "Val loss has decreased -->reducing the global validation loss from 50.91 to 46.11\n",
      " validation loss for epoch = 30 is 46.1132\n",
      " epoch= 30 :  val loss is 46.1132 \n",
      "saving the model model2024-07-2201:32:11.pth\n",
      "*****LOGGING INFO IN distl_gpt2_Random_init_nll_and_cos_lossrandom_init_wts_True_2024-07-22.log*********\n",
      "executing epoch:32, it took 1.0552358667055766 mins from beginning of epoch till batch#131\n",
      "training loss has decreased---> reducing the best loss from 38.32 to 34.22 | throughput = 135976 tokens/second | norm = 0.1095 | learning rate = 9.89074e-06\n",
      "inside validation data for epoch 31\n",
      "Val loss has decreased -->reducing the global validation loss from 46.11 to 41.78\n",
      " validation loss for epoch = 31 is 41.7826\n",
      " epoch= 31 :  val loss is 41.7826 \n",
      "saving the model model2024-07-2201:35:08.pth\n",
      "*****LOGGING INFO IN distl_gpt2_Random_init_nll_and_cos_lossrandom_init_wts_True_2024-07-22.log*********\n",
      "executing epoch:33, it took 1.054530394077301 mins from beginning of epoch till batch#131\n",
      "training loss has decreased---> reducing the best loss from 34.22 to 30.51 | throughput = 135700 tokens/second | norm = 0.1000 | learning rate = 9.87185e-06\n",
      "inside validation data for epoch 32\n",
      "Val loss has decreased -->reducing the global validation loss from 41.78 to 38.14\n",
      " validation loss for epoch = 32 is 38.1360\n",
      " epoch= 32 :  val loss is 38.1360 \n",
      "saving the model model2024-07-2201:38:06.pth\n",
      "*****LOGGING INFO IN distl_gpt2_Random_init_nll_and_cos_lossrandom_init_wts_True_2024-07-22.log*********\n",
      "executing epoch:34, it took 1.0532562414805093 mins from beginning of epoch till batch#131\n",
      "training loss has decreased---> reducing the best loss from 30.51 to 27.25 | throughput = 135775 tokens/second | norm = 0.1445 | learning rate = 9.85148e-06\n",
      "inside validation data for epoch 33\n",
      "Val loss has decreased -->reducing the global validation loss from 38.14 to 34.39\n",
      " validation loss for epoch = 33 is 34.3904\n",
      " epoch= 33 :  val loss is 34.3904 \n",
      "saving the model model2024-07-2201:41:03.pth\n",
      "*****LOGGING INFO IN distl_gpt2_Random_init_nll_and_cos_lossrandom_init_wts_True_2024-07-22.log*********\n",
      "executing epoch:35, it took 1.054492211341858 mins from beginning of epoch till batch#131\n",
      "training loss has decreased---> reducing the best loss from 27.25 to 24.31 | throughput = 135837 tokens/second | norm = 0.0474 | learning rate = 9.82963e-06\n",
      "inside validation data for epoch 34\n",
      "Val loss has decreased -->reducing the global validation loss from 34.39 to 31.37\n",
      " validation loss for epoch = 34 is 31.3693\n",
      " epoch= 34 :  val loss is 31.3693 \n",
      "saving the model model2024-07-2201:44:00.pth\n",
      "*****LOGGING INFO IN distl_gpt2_Random_init_nll_and_cos_lossrandom_init_wts_True_2024-07-22.log*********\n",
      "executing epoch:36, it took 1.0514441092809041 mins from beginning of epoch till batch#131\n",
      "training loss has decreased---> reducing the best loss from 24.31 to 21.72 | throughput = 135924 tokens/second | norm = 0.1052 | learning rate = 9.80631e-06\n",
      "inside validation data for epoch 35\n",
      "Val loss has decreased -->reducing the global validation loss from 31.37 to 28.58\n",
      " validation loss for epoch = 35 is 28.5758\n",
      " epoch= 35 :  val loss is 28.5758 \n",
      "saving the model model2024-07-2201:46:57.pth\n",
      "*****LOGGING INFO IN distl_gpt2_Random_init_nll_and_cos_lossrandom_init_wts_True_2024-07-22.log*********\n",
      "executing epoch:37, it took 1.0533745209376018 mins from beginning of epoch till batch#131\n",
      "training loss has decreased---> reducing the best loss from 21.72 to 19.39 | throughput = 135810 tokens/second | norm = 0.0692 | learning rate = 9.78152e-06\n",
      "inside validation data for epoch 36\n",
      "Val loss has decreased -->reducing the global validation loss from 28.58 to 26.10\n",
      " validation loss for epoch = 36 is 26.0985\n",
      " epoch= 36 :  val loss is 26.0985 \n",
      "saving the model model2024-07-2201:49:54.pth\n",
      "*****LOGGING INFO IN distl_gpt2_Random_init_nll_and_cos_lossrandom_init_wts_True_2024-07-22.log*********\n",
      "executing epoch:38, it took 1.0524916569391887 mins from beginning of epoch till batch#131\n",
      "training loss has decreased---> reducing the best loss from 19.39 to 17.33 | throughput = 135844 tokens/second | norm = 0.0781 | learning rate = 9.75528e-06\n",
      "inside validation data for epoch 37\n",
      "Val loss has decreased -->reducing the global validation loss from 26.10 to 23.66\n",
      " validation loss for epoch = 37 is 23.6639\n",
      " epoch= 37 :  val loss is 23.6639 \n",
      "saving the model model2024-07-2201:52:51.pth\n",
      "*****LOGGING INFO IN distl_gpt2_Random_init_nll_and_cos_lossrandom_init_wts_True_2024-07-22.log*********\n",
      "executing epoch:39, it took 1.053975268205007 mins from beginning of epoch till batch#131\n",
      "training loss has decreased---> reducing the best loss from 17.33 to 15.47 | throughput = 135987 tokens/second | norm = 0.0836 | learning rate = 9.72759e-06\n",
      "inside validation data for epoch 38\n",
      "Val loss has decreased -->reducing the global validation loss from 23.66 to 21.66\n",
      " validation loss for epoch = 38 is 21.6569\n",
      " epoch= 38 :  val loss is 21.6569 \n",
      "saving the model model2024-07-2201:55:48.pth\n",
      "*****LOGGING INFO IN distl_gpt2_Random_init_nll_and_cos_lossrandom_init_wts_True_2024-07-22.log*********\n",
      "executing epoch:40, it took 1.0522202571233115 mins from beginning of epoch till batch#131\n",
      "training loss has decreased---> reducing the best loss from 15.47 to 13.81 | throughput = 135883 tokens/second | norm = 0.0964 | learning rate = 9.69846e-06\n",
      "inside validation data for epoch 39\n",
      "Val loss has decreased -->reducing the global validation loss from 21.66 to 19.77\n",
      " validation loss for epoch = 39 is 19.7713\n",
      " epoch= 39 :  val loss is 19.7713 \n",
      "saving the model model2024-07-2201:58:46.pth\n",
      "*****LOGGING INFO IN distl_gpt2_Random_init_nll_and_cos_lossrandom_init_wts_True_2024-07-22.log*********\n",
      "executing epoch:41, it took 1.0546280145645142 mins from beginning of epoch till batch#131\n",
      "training loss has decreased---> reducing the best loss from 13.81 to 12.36 | throughput = 135679 tokens/second | norm = 0.0171 | learning rate = 9.66790e-06\n",
      "inside validation data for epoch 40\n",
      "Val loss has decreased -->reducing the global validation loss from 19.77 to 18.12\n",
      " validation loss for epoch = 40 is 18.1194\n",
      " epoch= 40 :  val loss is 18.1194 \n",
      "saving the model model2024-07-2202:01:43.pth\n",
      "*****LOGGING INFO IN distl_gpt2_Random_init_nll_and_cos_lossrandom_init_wts_True_2024-07-22.log*********\n",
      "executing epoch:42, it took 1.0522632559140523 mins from beginning of epoch till batch#131\n",
      "training loss has decreased---> reducing the best loss from 12.36 to 11.08 | throughput = 135865 tokens/second | norm = 0.0232 | learning rate = 9.63592e-06\n",
      "inside validation data for epoch 41\n",
      "Val loss has decreased -->reducing the global validation loss from 18.12 to 16.89\n",
      " validation loss for epoch = 41 is 16.8879\n",
      " epoch= 41 :  val loss is 16.8879 \n",
      "saving the model model2024-07-2202:04:40.pth\n",
      "*****LOGGING INFO IN distl_gpt2_Random_init_nll_and_cos_lossrandom_init_wts_True_2024-07-22.log*********\n",
      "executing epoch:43, it took 1.0531970143318177 mins from beginning of epoch till batch#131\n",
      "training loss has decreased---> reducing the best loss from 11.08 to 9.90 | throughput = 136057 tokens/second | norm = 0.0422 | learning rate = 9.60252e-06\n",
      "inside validation data for epoch 42\n",
      "Val loss has decreased -->reducing the global validation loss from 16.89 to 15.33\n",
      " validation loss for epoch = 42 is 15.3292\n",
      " epoch= 42 :  val loss is 15.3292 \n",
      "saving the model model2024-07-2202:07:37.pth\n",
      "*****LOGGING INFO IN distl_gpt2_Random_init_nll_and_cos_lossrandom_init_wts_True_2024-07-22.log*********\n",
      "executing epoch:44, it took 1.0511961221694945 mins from beginning of epoch till batch#131\n",
      "training loss has decreased---> reducing the best loss from 9.90 to 8.85 | throughput = 135937 tokens/second | norm = 0.0829 | learning rate = 9.56773e-06\n",
      "inside validation data for epoch 43\n",
      "Val loss has decreased -->reducing the global validation loss from 15.33 to 14.00\n",
      " validation loss for epoch = 43 is 14.0017\n",
      " epoch= 43 :  val loss is 14.0017 \n",
      "saving the model model2024-07-2202:10:34.pth\n",
      "*****LOGGING INFO IN distl_gpt2_Random_init_nll_and_cos_lossrandom_init_wts_True_2024-07-22.log*********\n",
      "executing epoch:45, it took 1.0522798617680869 mins from beginning of epoch till batch#131\n",
      "training loss has decreased---> reducing the best loss from 8.85 to 7.90 | throughput = 136102 tokens/second | norm = 0.0630 | learning rate = 9.53154e-06\n",
      "inside validation data for epoch 44\n",
      "Val loss has decreased -->reducing the global validation loss from 14.00 to 12.90\n",
      " validation loss for epoch = 44 is 12.9021\n",
      " epoch= 44 :  val loss is 12.9021 \n",
      "saving the model model2024-07-2202:13:31.pth\n",
      "*****LOGGING INFO IN distl_gpt2_Random_init_nll_and_cos_lossrandom_init_wts_True_2024-07-22.log*********\n",
      "executing epoch:46, it took 1.0517142057418822 mins from beginning of epoch till batch#131\n",
      "training loss has decreased---> reducing the best loss from 7.90 to 7.07 | throughput = 135868 tokens/second | norm = 0.0074 | learning rate = 9.49397e-06\n",
      "inside validation data for epoch 45\n",
      "Val loss has decreased -->reducing the global validation loss from 12.90 to 11.96\n",
      " validation loss for epoch = 45 is 11.9618\n",
      " epoch= 45 :  val loss is 11.9618 \n",
      "saving the model model2024-07-2202:16:28.pth\n",
      "*****LOGGING INFO IN distl_gpt2_Random_init_nll_and_cos_lossrandom_init_wts_True_2024-07-22.log*********\n",
      "executing epoch:47, it took 1.0516671498616537 mins from beginning of epoch till batch#131\n",
      "training loss has decreased---> reducing the best loss from 7.07 to 6.32 | throughput = 136093 tokens/second | norm = 0.0170 | learning rate = 9.45503e-06\n",
      "inside validation data for epoch 46\n",
      "Val loss has decreased -->reducing the global validation loss from 11.96 to 10.93\n",
      " validation loss for epoch = 46 is 10.9296\n",
      " epoch= 46 :  val loss is 10.9296 \n",
      "saving the model model2024-07-2202:19:25.pth\n",
      "*****LOGGING INFO IN distl_gpt2_Random_init_nll_and_cos_lossrandom_init_wts_True_2024-07-22.log*********\n",
      "executing epoch:48, it took 1.0522895534833272 mins from beginning of epoch till batch#131\n",
      "training loss has decreased---> reducing the best loss from 6.32 to 5.65 | throughput = 135907 tokens/second | norm = 0.0667 | learning rate = 9.41474e-06\n",
      "inside validation data for epoch 47\n",
      "Val loss has decreased -->reducing the global validation loss from 10.93 to 10.09\n",
      " validation loss for epoch = 47 is 10.0894\n",
      " epoch= 47 :  val loss is 10.0894 \n",
      "saving the model model2024-07-2202:22:23.pth\n",
      "*****LOGGING INFO IN distl_gpt2_Random_init_nll_and_cos_lossrandom_init_wts_True_2024-07-22.log*********\n",
      "executing epoch:49, it took 1.053251067797343 mins from beginning of epoch till batch#131\n",
      "training loss has decreased---> reducing the best loss from 5.65 to 5.06 | throughput = 135841 tokens/second | norm = 0.0238 | learning rate = 9.37310e-06\n",
      "inside validation data for epoch 48\n",
      "Val loss has decreased -->reducing the global validation loss from 10.09 to 9.40\n",
      " validation loss for epoch = 48 is 9.4000\n",
      " epoch= 48 :  val loss is 9.4000 \n",
      "saving the model model2024-07-2202:25:20.pth\n",
      "*****LOGGING INFO IN distl_gpt2_Random_init_nll_and_cos_lossrandom_init_wts_True_2024-07-22.log*********\n",
      "executing epoch:50, it took 1.0536251664161682 mins from beginning of epoch till batch#131\n",
      "training loss has decreased---> reducing the best loss from 5.06 to 4.53 | throughput = 135810 tokens/second | norm = 0.0116 | learning rate = 9.33013e-06\n",
      "inside validation data for epoch 49\n",
      "Val loss has decreased -->reducing the global validation loss from 9.40 to 8.65\n",
      " validation loss for epoch = 49 is 8.6475\n",
      " epoch= 49 :  val loss is 8.6475 \n",
      "saving the model model2024-07-2202:28:17.pth\n",
      "*****LOGGING INFO IN distl_gpt2_Random_init_nll_and_cos_lossrandom_init_wts_True_2024-07-22.log*********\n",
      "executing epoch:51, it took 1.0508432229359945 mins from beginning of epoch till batch#131\n",
      "training loss has decreased---> reducing the best loss from 4.53 to 4.05 | throughput = 136239 tokens/second | norm = 0.0142 | learning rate = 9.28584e-06\n",
      "inside validation data for epoch 50\n",
      "Val loss has decreased -->reducing the global validation loss from 8.65 to 8.10\n",
      " validation loss for epoch = 50 is 8.0984\n",
      " epoch= 50 :  val loss is 8.0984 \n",
      "saving the model model2024-07-2202:31:14.pth\n",
      "*****LOGGING INFO IN distl_gpt2_Random_init_nll_and_cos_lossrandom_init_wts_True_2024-07-22.log*********\n",
      "executing epoch:52, it took 1.0519250551859538 mins from beginning of epoch till batch#131\n",
      "training loss has decreased---> reducing the best loss from 4.05 to 3.65 | throughput = 135921 tokens/second | norm = 0.0634 | learning rate = 9.24024e-06\n",
      "inside validation data for epoch 51\n",
      "Val loss has decreased -->reducing the global validation loss from 8.10 to 7.46\n",
      " validation loss for epoch = 51 is 7.4635\n",
      " epoch= 51 :  val loss is 7.4635 \n",
      "saving the model model2024-07-2202:34:11.pth\n",
      "*****LOGGING INFO IN distl_gpt2_Random_init_nll_and_cos_lossrandom_init_wts_True_2024-07-22.log*********\n",
      "executing epoch:53, it took 1.05290740331014 mins from beginning of epoch till batch#131\n",
      "training loss has decreased---> reducing the best loss from 3.65 to 3.25 | throughput = 136141 tokens/second | norm = 0.0070 | learning rate = 9.19335e-06\n",
      "inside validation data for epoch 52\n",
      "Val loss has decreased -->reducing the global validation loss from 7.46 to 6.94\n",
      " validation loss for epoch = 52 is 6.9406\n",
      " epoch= 52 :  val loss is 6.9406 \n",
      "saving the model model2024-07-2202:37:08.pth\n",
      "*****LOGGING INFO IN distl_gpt2_Random_init_nll_and_cos_lossrandom_init_wts_True_2024-07-22.log*********\n",
      "executing epoch:54, it took 1.052519158522288 mins from beginning of epoch till batch#131\n",
      "training loss has decreased---> reducing the best loss from 3.25 to 2.91 | throughput = 135915 tokens/second | norm = 0.0244 | learning rate = 9.14519e-06\n",
      "inside validation data for epoch 53\n",
      "Val loss has decreased -->reducing the global validation loss from 6.94 to 6.49\n",
      " validation loss for epoch = 53 is 6.4935\n",
      " epoch= 53 :  val loss is 6.4935 \n",
      "saving the model model2024-07-2202:40:05.pth\n",
      "*****LOGGING INFO IN distl_gpt2_Random_init_nll_and_cos_lossrandom_init_wts_True_2024-07-22.log*********\n",
      "executing epoch:55, it took 1.0529203136761984 mins from beginning of epoch till batch#131\n",
      "training loss has decreased---> reducing the best loss from 2.91 to 2.60 | throughput = 135926 tokens/second | norm = 0.0162 | learning rate = 9.09576e-06\n",
      "inside validation data for epoch 54\n",
      "Val loss has decreased -->reducing the global validation loss from 6.49 to 6.08\n",
      " validation loss for epoch = 54 is 6.0841\n",
      " epoch= 54 :  val loss is 6.0841 \n",
      "saving the model model2024-07-2202:43:02.pth\n",
      "*****LOGGING INFO IN distl_gpt2_Random_init_nll_and_cos_lossrandom_init_wts_True_2024-07-22.log*********\n",
      "executing epoch:56, it took 1.0521915157636006 mins from beginning of epoch till batch#131\n",
      "training loss has decreased---> reducing the best loss from 2.60 to 2.34 | throughput = 135940 tokens/second | norm = 0.0040 | learning rate = 9.04508e-06\n",
      "inside validation data for epoch 55\n",
      "Val loss has decreased -->reducing the global validation loss from 6.08 to 5.70\n",
      " validation loss for epoch = 55 is 5.7044\n",
      " epoch= 55 :  val loss is 5.7044 \n",
      "saving the model model2024-07-2202:45:59.pth\n",
      "*****LOGGING INFO IN distl_gpt2_Random_init_nll_and_cos_lossrandom_init_wts_True_2024-07-22.log*********\n",
      "executing epoch:57, it took 1.053289572397868 mins from beginning of epoch till batch#131\n",
      "training loss has decreased---> reducing the best loss from 2.34 to 2.09 | throughput = 135961 tokens/second | norm = 0.0084 | learning rate = 8.99318e-06\n",
      "inside validation data for epoch 56\n",
      "Val loss has decreased -->reducing the global validation loss from 5.70 to 5.36\n",
      " validation loss for epoch = 56 is 5.3557\n",
      " epoch= 56 :  val loss is 5.3557 \n",
      "saving the model model2024-07-2202:48:56.pth\n",
      "*****LOGGING INFO IN distl_gpt2_Random_init_nll_and_cos_lossrandom_init_wts_True_2024-07-22.log*********\n",
      "executing epoch:58, it took 1.0500109752019247 mins from beginning of epoch till batch#131\n",
      "training loss has decreased---> reducing the best loss from 2.09 to 1.87 | throughput = 136122 tokens/second | norm = 0.0233 | learning rate = 8.94005e-06\n",
      "inside validation data for epoch 57\n",
      "Val loss has decreased -->reducing the global validation loss from 5.36 to 5.04\n",
      " validation loss for epoch = 57 is 5.0382\n",
      " epoch= 57 :  val loss is 5.0382 \n",
      "saving the model model2024-07-2202:51:53.pth\n",
      "*****LOGGING INFO IN distl_gpt2_Random_init_nll_and_cos_lossrandom_init_wts_True_2024-07-22.log*********\n",
      "executing epoch:59, it took 1.0531861464182535 mins from beginning of epoch till batch#131\n",
      "training loss has decreased---> reducing the best loss from 1.87 to 1.68 | throughput = 135890 tokens/second | norm = 0.0146 | learning rate = 8.88573e-06\n",
      "inside validation data for epoch 58\n",
      "Val loss has decreased -->reducing the global validation loss from 5.04 to 4.75\n",
      " validation loss for epoch = 58 is 4.7535\n",
      " epoch= 58 :  val loss is 4.7535 \n",
      "saving the model model2024-07-2202:54:50.pth\n",
      "*****LOGGING INFO IN distl_gpt2_Random_init_nll_and_cos_lossrandom_init_wts_True_2024-07-22.log*********\n",
      "executing epoch:60, it took 1.0484658281008403 mins from beginning of epoch till batch#131\n",
      "training loss has decreased---> reducing the best loss from 1.68 to 1.51 | throughput = 136482 tokens/second | norm = 0.0440 | learning rate = 8.83022e-06\n",
      "inside validation data for epoch 59\n",
      "Val loss has decreased -->reducing the global validation loss from 4.75 to 4.49\n",
      " validation loss for epoch = 59 is 4.4854\n",
      " epoch= 59 :  val loss is 4.4854 \n",
      "saving the model model2024-07-2202:57:46.pth\n",
      "*****LOGGING INFO IN distl_gpt2_Random_init_nll_and_cos_lossrandom_init_wts_True_2024-07-22.log*********\n",
      "executing epoch:61, it took 1.0503722389539083 mins from beginning of epoch till batch#131\n",
      "training loss has decreased---> reducing the best loss from 1.51 to 1.35 | throughput = 136118 tokens/second | norm = 0.0392 | learning rate = 8.77355e-06\n",
      "inside validation data for epoch 60\n",
      "Val loss has decreased -->reducing the global validation loss from 4.49 to 4.25\n",
      " validation loss for epoch = 60 is 4.2468\n",
      " epoch= 60 :  val loss is 4.2468 \n",
      "saving the model model2024-07-2203:00:43.pth\n",
      "*****LOGGING INFO IN distl_gpt2_Random_init_nll_and_cos_lossrandom_init_wts_True_2024-07-22.log*********\n",
      "executing epoch:62, it took 1.0529792785644532 mins from beginning of epoch till batch#131\n",
      "training loss has decreased---> reducing the best loss from 1.35 to 1.21 | throughput = 135932 tokens/second | norm = 0.0385 | learning rate = 8.71572e-06\n",
      "inside validation data for epoch 61\n",
      "Val loss has decreased -->reducing the global validation loss from 4.25 to 4.01\n",
      " validation loss for epoch = 61 is 4.0089\n",
      " epoch= 61 :  val loss is 4.0089 \n",
      "saving the model model2024-07-2203:03:40.pth\n",
      "*****LOGGING INFO IN distl_gpt2_Random_init_nll_and_cos_lossrandom_init_wts_True_2024-07-22.log*********\n",
      "executing epoch:63, it took 1.0526436924934388 mins from beginning of epoch till batch#131\n",
      "training loss has decreased---> reducing the best loss from 1.21 to 1.09 | throughput = 135947 tokens/second | norm = 0.0089 | learning rate = 8.65677e-06\n",
      "inside validation data for epoch 62\n",
      "Val loss has decreased -->reducing the global validation loss from 4.01 to 3.82\n",
      " validation loss for epoch = 62 is 3.8191\n",
      " epoch= 62 :  val loss is 3.8191 \n",
      "saving the model model2024-07-2203:06:37.pth\n",
      "*****LOGGING INFO IN distl_gpt2_Random_init_nll_and_cos_lossrandom_init_wts_True_2024-07-22.log*********\n",
      "executing epoch:64, it took 1.0504960576693216 mins from beginning of epoch till batch#131\n",
      "training loss has decreased---> reducing the best loss from 1.09 to 0.98 | throughput = 136066 tokens/second | norm = 0.0147 | learning rate = 8.59670e-06\n",
      "inside validation data for epoch 63\n",
      "Val loss has decreased -->reducing the global validation loss from 3.82 to 3.63\n",
      " validation loss for epoch = 63 is 3.6268\n",
      " epoch= 63 :  val loss is 3.6268 \n",
      "saving the model model2024-07-2203:09:34.pth\n",
      "*****LOGGING INFO IN distl_gpt2_Random_init_nll_and_cos_lossrandom_init_wts_True_2024-07-22.log*********\n",
      "executing epoch:65, it took 1.0527343908945719 mins from beginning of epoch till batch#131\n",
      "training loss has decreased---> reducing the best loss from 0.98 to 0.90 | throughput = 135978 tokens/second | norm = 0.0102 | learning rate = 8.53553e-06\n",
      "inside validation data for epoch 64\n",
      "Val loss has decreased -->reducing the global validation loss from 3.63 to 3.48\n",
      " validation loss for epoch = 64 is 3.4821\n",
      " epoch= 64 :  val loss is 3.4821 \n",
      "saving the model model2024-07-2203:12:31.pth\n",
      "*****LOGGING INFO IN distl_gpt2_Random_init_nll_and_cos_lossrandom_init_wts_True_2024-07-22.log*********\n",
      "executing epoch:66, it took 1.0506710012753804 mins from beginning of epoch till batch#131\n",
      "training loss has decreased---> reducing the best loss from 0.90 to 0.79 | throughput = 136098 tokens/second | norm = 0.0030 | learning rate = 8.47329e-06\n",
      "inside validation data for epoch 65\n",
      "Val loss has decreased -->reducing the global validation loss from 3.48 to 3.29\n",
      " validation loss for epoch = 65 is 3.2898\n",
      " epoch= 65 :  val loss is 3.2898 \n",
      "saving the model model2024-07-2203:15:28.pth\n",
      "*****LOGGING INFO IN distl_gpt2_Random_init_nll_and_cos_lossrandom_init_wts_True_2024-07-22.log*********\n",
      "executing epoch:67, it took 1.0524964173634848 mins from beginning of epoch till batch#131\n",
      "training loss has decreased---> reducing the best loss from 0.79 to 0.71 | throughput = 135966 tokens/second | norm = 0.0031 | learning rate = 8.40999e-06\n",
      "inside validation data for epoch 66\n",
      "Val loss has decreased -->reducing the global validation loss from 3.29 to 3.14\n",
      " validation loss for epoch = 66 is 3.1424\n",
      " epoch= 66 :  val loss is 3.1424 \n",
      "saving the model model2024-07-2203:18:25.pth\n",
      "*****LOGGING INFO IN distl_gpt2_Random_init_nll_and_cos_lossrandom_init_wts_True_2024-07-22.log*********\n",
      "executing epoch:68, it took 1.0510565797487894 mins from beginning of epoch till batch#131\n",
      "training loss has decreased---> reducing the best loss from 0.71 to 0.64 | throughput = 136057 tokens/second | norm = 0.0018 | learning rate = 8.34565e-06\n",
      "inside validation data for epoch 67\n",
      "Val loss has decreased -->reducing the global validation loss from 3.14 to 3.00\n",
      " validation loss for epoch = 67 is 2.9982\n",
      " epoch= 67 :  val loss is 2.9982 \n",
      "saving the model model2024-07-2203:21:22.pth\n",
      "*****LOGGING INFO IN distl_gpt2_Random_init_nll_and_cos_lossrandom_init_wts_True_2024-07-22.log*********\n",
      "executing epoch:69, it took 1.052551233768463 mins from beginning of epoch till batch#131\n",
      "training loss has decreased---> reducing the best loss from 0.64 to 0.58 | throughput = 135919 tokens/second | norm = 0.0035 | learning rate = 8.28030e-06\n",
      "inside validation data for epoch 68\n",
      "Val loss has decreased -->reducing the global validation loss from 3.00 to 2.86\n",
      " validation loss for epoch = 68 is 2.8603\n",
      " epoch= 68 :  val loss is 2.8603 \n",
      "saving the model model2024-07-2203:24:20.pth\n",
      "*****LOGGING INFO IN distl_gpt2_Random_init_nll_and_cos_lossrandom_init_wts_True_2024-07-22.log*********\n",
      "executing epoch:70, it took 1.0502703269322713 mins from beginning of epoch till batch#131\n",
      "training loss has decreased---> reducing the best loss from 0.58 to 0.52 | throughput = 136088 tokens/second | norm = 0.0062 | learning rate = 8.21394e-06\n",
      "inside validation data for epoch 69\n",
      "Val loss has decreased -->reducing the global validation loss from 2.86 to 2.73\n",
      " validation loss for epoch = 69 is 2.7342\n",
      " epoch= 69 :  val loss is 2.7342 \n",
      "saving the model model2024-07-2203:27:17.pth\n",
      "*****LOGGING INFO IN distl_gpt2_Random_init_nll_and_cos_lossrandom_init_wts_True_2024-07-22.log*********\n",
      "executing epoch:71, it took 1.0524782458941142 mins from beginning of epoch till batch#131\n",
      "training loss has decreased---> reducing the best loss from 0.52 to 0.47 | throughput = 135932 tokens/second | norm = 0.0254 | learning rate = 8.14660e-06\n",
      "inside validation data for epoch 70\n",
      "Val loss has decreased -->reducing the global validation loss from 2.73 to 2.63\n",
      " validation loss for epoch = 70 is 2.6305\n",
      " epoch= 70 :  val loss is 2.6305 \n",
      "saving the model model2024-07-2203:30:14.pth\n",
      "*****LOGGING INFO IN distl_gpt2_Random_init_nll_and_cos_lossrandom_init_wts_True_2024-07-22.log*********\n",
      "executing epoch:72, it took 1.0526586890220642 mins from beginning of epoch till batch#131\n",
      "training loss has decreased---> reducing the best loss from 0.47 to 0.42 | throughput = 135908 tokens/second | norm = 0.0036 | learning rate = 8.07831e-06\n",
      "inside validation data for epoch 71\n",
      "Val loss has decreased -->reducing the global validation loss from 2.63 to 2.50\n",
      " validation loss for epoch = 71 is 2.4964\n",
      " epoch= 71 :  val loss is 2.4964 \n",
      "saving the model model2024-07-2203:33:11.pth\n",
      "*****LOGGING INFO IN distl_gpt2_Random_init_nll_and_cos_lossrandom_init_wts_True_2024-07-22.log*********\n",
      "executing epoch:73, it took 1.0524869283040366 mins from beginning of epoch till batch#131\n",
      "training loss has decreased---> reducing the best loss from 0.42 to 0.38 | throughput = 135950 tokens/second | norm = 0.0054 | learning rate = 8.00908e-06\n",
      "inside validation data for epoch 72\n",
      "Val loss has decreased -->reducing the global validation loss from 2.50 to 2.42\n",
      " validation loss for epoch = 72 is 2.4162\n",
      " epoch= 72 :  val loss is 2.4162 \n",
      "saving the model model2024-07-2203:36:08.pth\n",
      "*****LOGGING INFO IN distl_gpt2_Random_init_nll_and_cos_lossrandom_init_wts_True_2024-07-22.log*********\n",
      "executing epoch:74, it took 1.049341893196106 mins from beginning of epoch till batch#131\n",
      "training loss has decreased---> reducing the best loss from 0.38 to 0.34 | throughput = 136418 tokens/second | norm = 0.0108 | learning rate = 7.93893e-06\n",
      "inside validation data for epoch 73\n",
      "Val loss has decreased -->reducing the global validation loss from 2.42 to 2.31\n",
      " validation loss for epoch = 73 is 2.3082\n",
      " epoch= 73 :  val loss is 2.3082 \n",
      "saving the model model2024-07-2203:39:05.pth\n",
      "*****LOGGING INFO IN distl_gpt2_Random_init_nll_and_cos_lossrandom_init_wts_True_2024-07-22.log*********\n",
      "executing epoch:75, it took 1.051598060131073 mins from beginning of epoch till batch#131\n",
      "training loss has decreased---> reducing the best loss from 0.34 to 0.31 | throughput = 136061 tokens/second | norm = 0.0033 | learning rate = 7.86788e-06\n",
      "inside validation data for epoch 74\n",
      "Val loss has decreased -->reducing the global validation loss from 2.31 to 2.23\n",
      " validation loss for epoch = 74 is 2.2263\n",
      " epoch= 74 :  val loss is 2.2263 \n",
      "saving the model model2024-07-2203:42:02.pth\n",
      "*****LOGGING INFO IN distl_gpt2_Random_init_nll_and_cos_lossrandom_init_wts_True_2024-07-22.log*********\n",
      "executing epoch:76, it took 1.0521104693412782 mins from beginning of epoch till batch#131\n",
      "training loss has decreased---> reducing the best loss from 0.31 to 0.28 | throughput = 136035 tokens/second | norm = 0.0057 | learning rate = 7.79596e-06\n",
      "inside validation data for epoch 75\n",
      "Val loss has decreased -->reducing the global validation loss from 2.23 to 2.13\n",
      " validation loss for epoch = 75 is 2.1266\n",
      " epoch= 75 :  val loss is 2.1266 \n",
      "saving the model model2024-07-2203:44:59.pth\n",
      "*****LOGGING INFO IN distl_gpt2_Random_init_nll_and_cos_lossrandom_init_wts_True_2024-07-22.log*********\n",
      "executing epoch:77, it took 1.0520344932874044 mins from beginning of epoch till batch#131\n",
      "training loss has decreased---> reducing the best loss from 0.28 to 0.25 | throughput = 135969 tokens/second | norm = 0.0016 | learning rate = 7.72320e-06\n",
      "inside validation data for epoch 76\n",
      "Val loss has decreased -->reducing the global validation loss from 2.13 to 2.04\n",
      " validation loss for epoch = 76 is 2.0364\n",
      " epoch= 76 :  val loss is 2.0364 \n",
      "saving the model model2024-07-2203:47:56.pth\n",
      "*****LOGGING INFO IN distl_gpt2_Random_init_nll_and_cos_lossrandom_init_wts_True_2024-07-22.log*********\n",
      "executing epoch:78, it took 1.0511873284975688 mins from beginning of epoch till batch#131\n",
      "training loss has decreased---> reducing the best loss from 0.25 to 0.22 | throughput = 136044 tokens/second | norm = 0.0017 | learning rate = 7.64960e-06\n",
      "inside validation data for epoch 77\n",
      "Val loss has decreased -->reducing the global validation loss from 2.04 to 1.96\n",
      " validation loss for epoch = 77 is 1.9616\n",
      " epoch= 77 :  val loss is 1.9616 \n",
      "saving the model model2024-07-2203:50:53.pth\n",
      "*****LOGGING INFO IN distl_gpt2_Random_init_nll_and_cos_lossrandom_init_wts_True_2024-07-22.log*********\n",
      "executing epoch:79, it took 1.0517436504364013 mins from beginning of epoch till batch#131\n",
      "training loss has decreased---> reducing the best loss from 0.22 to 0.20 | throughput = 136028 tokens/second | norm = 0.0039 | learning rate = 7.57519e-06\n",
      "inside validation data for epoch 78\n",
      "Val loss has decreased -->reducing the global validation loss from 1.96 to 1.90\n",
      " validation loss for epoch = 78 is 1.8982\n",
      " epoch= 78 :  val loss is 1.8982 \n",
      "saving the model model2024-07-2203:53:50.pth\n",
      "*****LOGGING INFO IN distl_gpt2_Random_init_nll_and_cos_lossrandom_init_wts_True_2024-07-22.log*********\n",
      "executing epoch:80, it took 1.0507673223813374 mins from beginning of epoch till batch#131\n",
      "training loss has decreased---> reducing the best loss from 0.20 to 0.18 | throughput = 136070 tokens/second | norm = 0.0040 | learning rate = 7.50000e-06\n",
      "inside validation data for epoch 79\n",
      "Val loss has decreased -->reducing the global validation loss from 1.90 to 1.82\n",
      " validation loss for epoch = 79 is 1.8204\n",
      " epoch= 79 :  val loss is 1.8204 \n",
      "saving the model model2024-07-2203:56:47.pth\n",
      "*****LOGGING INFO IN distl_gpt2_Random_init_nll_and_cos_lossrandom_init_wts_True_2024-07-22.log*********\n",
      "executing epoch:81, it took 1.0518820961316426 mins from beginning of epoch till batch#131\n",
      "training loss has decreased---> reducing the best loss from 0.18 to 0.16 | throughput = 135977 tokens/second | norm = 0.0022 | learning rate = 7.42405e-06\n",
      "inside validation data for epoch 80\n",
      "Val loss has decreased -->reducing the global validation loss from 1.82 to 1.76\n",
      " validation loss for epoch = 80 is 1.7608\n",
      " epoch= 80 :  val loss is 1.7608 \n",
      "saving the model model2024-07-2203:59:44.pth\n",
      "*****LOGGING INFO IN distl_gpt2_Random_init_nll_and_cos_lossrandom_init_wts_True_2024-07-22.log*********\n",
      "executing epoch:82, it took 1.052303687731425 mins from beginning of epoch till batch#131\n",
      "training loss has decreased---> reducing the best loss from 0.16 to 0.15 | throughput = 135976 tokens/second | norm = 0.0040 | learning rate = 7.34736e-06\n",
      "inside validation data for epoch 81\n",
      "Val loss has decreased -->reducing the global validation loss from 1.76 to 1.71\n",
      " validation loss for epoch = 81 is 1.7053\n",
      " epoch= 81 :  val loss is 1.7053 \n",
      "saving the model model2024-07-2204:02:42.pth\n",
      "*****LOGGING INFO IN distl_gpt2_Random_init_nll_and_cos_lossrandom_init_wts_True_2024-07-22.log*********\n",
      "executing epoch:83, it took 1.052493699391683 mins from beginning of epoch till batch#131\n",
      "training loss has decreased---> reducing the best loss from 0.15 to 0.13 | throughput = 136014 tokens/second | norm = 0.0011 | learning rate = 7.26995e-06\n",
      "inside validation data for epoch 82\n",
      "Val loss has decreased -->reducing the global validation loss from 1.71 to 1.65\n",
      " validation loss for epoch = 82 is 1.6478\n",
      " epoch= 82 :  val loss is 1.6478 \n",
      "saving the model model2024-07-2204:05:39.pth\n",
      "*****LOGGING INFO IN distl_gpt2_Random_init_nll_and_cos_lossrandom_init_wts_True_2024-07-22.log*********\n",
      "executing epoch:84, it took 1.0513482252756754 mins from beginning of epoch till batch#131\n",
      "training loss has decreased---> reducing the best loss from 0.13 to 0.12 | throughput = 136013 tokens/second | norm = 0.0009 | learning rate = 7.19186e-06\n",
      "inside validation data for epoch 83\n",
      "Val loss has decreased -->reducing the global validation loss from 1.65 to 1.61\n",
      " validation loss for epoch = 83 is 1.6072\n",
      " epoch= 83 :  val loss is 1.6072 \n",
      "saving the model model2024-07-2204:08:36.pth\n",
      "*****LOGGING INFO IN distl_gpt2_Random_init_nll_and_cos_lossrandom_init_wts_True_2024-07-22.log*********\n",
      "executing epoch:85, it took 1.0528002301851909 mins from beginning of epoch till batch#131\n",
      "training loss has decreased---> reducing the best loss from 0.12 to 0.10 | throughput = 135955 tokens/second | norm = 0.0050 | learning rate = 7.11309e-06\n",
      "inside validation data for epoch 84\n",
      "Val loss has decreased -->reducing the global validation loss from 1.61 to 1.57\n",
      " validation loss for epoch = 84 is 1.5672\n",
      " epoch= 84 :  val loss is 1.5672 \n",
      "saving the model model2024-07-2204:11:33.pth\n",
      "*****LOGGING INFO IN distl_gpt2_Random_init_nll_and_cos_lossrandom_init_wts_True_2024-07-22.log*********\n",
      "executing epoch:86, it took 1.0522082050641377 mins from beginning of epoch till batch#131\n",
      "training loss has decreased---> reducing the best loss from 0.10 to 0.09 | throughput = 136131 tokens/second | norm = 0.0012 | learning rate = 7.03368e-06\n",
      "inside validation data for epoch 85\n",
      "Val loss has decreased -->reducing the global validation loss from 1.57 to 1.54\n",
      " validation loss for epoch = 85 is 1.5388\n",
      " epoch= 85 :  val loss is 1.5388 \n",
      "saving the model model2024-07-2204:14:30.pth\n",
      "*****LOGGING INFO IN distl_gpt2_Random_init_nll_and_cos_lossrandom_init_wts_True_2024-07-22.log*********\n",
      "executing epoch:87, it took 1.0494762102762858 mins from beginning of epoch till batch#131\n",
      "training loss has decreased---> reducing the best loss from 0.09 to 0.08 | throughput = 136182 tokens/second | norm = 0.0010 | learning rate = 6.95366e-06\n",
      "inside validation data for epoch 86\n",
      "Val loss has decreased -->reducing the global validation loss from 1.54 to 1.49\n",
      " validation loss for epoch = 86 is 1.4949\n",
      " epoch= 86 :  val loss is 1.4949 \n",
      "saving the model model2024-07-2204:17:27.pth\n",
      "*****LOGGING INFO IN distl_gpt2_Random_init_nll_and_cos_lossrandom_init_wts_True_2024-07-22.log*********\n",
      "executing epoch:88, it took 1.0523597955703736 mins from beginning of epoch till batch#131\n",
      "training loss has decreased---> reducing the best loss from 0.08 to 0.07 | throughput = 135991 tokens/second | norm = 0.0022 | learning rate = 6.87303e-06\n",
      "inside validation data for epoch 87\n",
      "Val loss has decreased -->reducing the global validation loss from 1.49 to 1.47\n",
      " validation loss for epoch = 87 is 1.4688\n",
      " epoch= 87 :  val loss is 1.4688 \n",
      "saving the model model2024-07-2204:20:24.pth\n",
      "*****LOGGING INFO IN distl_gpt2_Random_init_nll_and_cos_lossrandom_init_wts_True_2024-07-22.log*********\n",
      "executing epoch:89, it took 1.052225923538208 mins from beginning of epoch till batch#131\n",
      "training loss has decreased---> reducing the best loss from 0.07 to 0.07 | throughput = 135946 tokens/second | norm = 0.0055 | learning rate = 6.79184e-06\n",
      "inside validation data for epoch 88\n",
      "Val loss has decreased -->reducing the global validation loss from 1.47 to 1.44\n",
      " validation loss for epoch = 88 is 1.4428\n",
      " epoch= 88 :  val loss is 1.4428 \n",
      "saving the model model2024-07-2204:23:21.pth\n",
      "*****LOGGING INFO IN distl_gpt2_Random_init_nll_and_cos_lossrandom_init_wts_True_2024-07-22.log*********\n",
      "executing epoch:90, it took 1.0526522278785706 mins from beginning of epoch till batch#131\n",
      "training loss has decreased---> reducing the best loss from 0.07 to 0.06 | throughput = 136032 tokens/second | norm = 0.0010 | learning rate = 6.71010e-06\n",
      "inside validation data for epoch 89\n",
      "Val loss has decreased -->reducing the global validation loss from 1.44 to 1.42\n",
      " validation loss for epoch = 89 is 1.4219\n",
      " epoch= 89 :  val loss is 1.4219 \n",
      "saving the model model2024-07-2204:26:18.pth\n",
      "*****LOGGING INFO IN distl_gpt2_Random_init_nll_and_cos_lossrandom_init_wts_True_2024-07-22.log*********\n",
      "executing epoch:91, it took 1.0503145615259806 mins from beginning of epoch till batch#131\n",
      "training loss has decreased---> reducing the best loss from 0.06 to 0.05 | throughput = 136127 tokens/second | norm = 0.0012 | learning rate = 6.62784e-06\n",
      "inside validation data for epoch 90\n",
      "Val loss has decreased -->reducing the global validation loss from 1.42 to 1.41\n",
      " validation loss for epoch = 90 is 1.4057\n",
      " epoch= 90 :  val loss is 1.4057 \n",
      "saving the model model2024-07-2204:29:14.pth\n",
      "*****LOGGING INFO IN distl_gpt2_Random_init_nll_and_cos_lossrandom_init_wts_True_2024-07-22.log*********\n",
      "executing epoch:92, it took 1.0523791909217834 mins from beginning of epoch till batch#131\n",
      "training loss has decreased---> reducing the best loss from 0.05 to 0.05 | throughput = 135925 tokens/second | norm = 0.0024 | learning rate = 6.54508e-06\n",
      "inside validation data for epoch 91\n",
      "Val loss has decreased -->reducing the global validation loss from 1.41 to 1.38\n",
      " validation loss for epoch = 91 is 1.3835\n",
      " epoch= 91 :  val loss is 1.3835 \n",
      "saving the model model2024-07-2204:32:12.pth\n",
      "*****LOGGING INFO IN distl_gpt2_Random_init_nll_and_cos_lossrandom_init_wts_True_2024-07-22.log*********\n",
      "executing epoch:93, it took 1.0512350916862487 mins from beginning of epoch till batch#131\n",
      "training loss has decreased---> reducing the best loss from 0.05 to 0.05 | throughput = 136033 tokens/second | norm = 0.0043 | learning rate = 6.46186e-06\n",
      "inside validation data for epoch 92\n",
      "Val loss has decreased -->reducing the global validation loss from 1.38 to 1.37\n",
      " validation loss for epoch = 92 is 1.3693\n",
      " epoch= 92 :  val loss is 1.3693 \n",
      "saving the model model2024-07-2204:35:09.pth\n",
      "*****LOGGING INFO IN distl_gpt2_Random_init_nll_and_cos_lossrandom_init_wts_True_2024-07-22.log*********\n",
      "executing epoch:94, it took 1.0525013287862142 mins from beginning of epoch till batch#131\n",
      "training loss has decreased---> reducing the best loss from 0.05 to 0.04 | throughput = 135957 tokens/second | norm = 0.0037 | learning rate = 6.37819e-06\n",
      "inside validation data for epoch 93\n",
      "Val loss has decreased -->reducing the global validation loss from 1.37 to 1.36\n",
      " validation loss for epoch = 93 is 1.3589\n",
      " epoch= 93 :  val loss is 1.3589 \n",
      "saving the model model2024-07-2204:38:06.pth\n",
      "*****LOGGING INFO IN distl_gpt2_Random_init_nll_and_cos_lossrandom_init_wts_True_2024-07-22.log*********\n",
      "executing epoch:95, it took 1.050813905398051 mins from beginning of epoch till batch#131\n",
      "training loss has decreased---> reducing the best loss from 0.04 to 0.04 | throughput = 136142 tokens/second | norm = 0.0008 | learning rate = 6.29410e-06\n",
      "inside validation data for epoch 94\n",
      "Val loss has decreased -->reducing the global validation loss from 1.36 to 1.34\n",
      " validation loss for epoch = 94 is 1.3444\n",
      " epoch= 94 :  val loss is 1.3444 \n",
      "saving the model model2024-07-2204:41:03.pth\n",
      "*****LOGGING INFO IN distl_gpt2_Random_init_nll_and_cos_lossrandom_init_wts_True_2024-07-22.log*********\n",
      "executing epoch:96, it took 1.051195454597473 mins from beginning of epoch till batch#131\n",
      "training loss has decreased---> reducing the best loss from 0.04 to 0.03 | throughput = 136040 tokens/second | norm = 0.0005 | learning rate = 6.20961e-06\n",
      "inside validation data for epoch 95\n",
      "Val loss has decreased -->reducing the global validation loss from 1.34 to 1.33\n",
      " validation loss for epoch = 95 is 1.3282\n",
      " epoch= 95 :  val loss is 1.3282 \n",
      "saving the model model2024-07-2204:44:00.pth\n",
      "*****LOGGING INFO IN distl_gpt2_Random_init_nll_and_cos_lossrandom_init_wts_True_2024-07-22.log*********\n",
      "executing epoch:97, it took 1.0527734557787578 mins from beginning of epoch till batch#131\n",
      "training loss has decreased---> reducing the best loss from 0.03 to 0.03 | throughput = 135899 tokens/second | norm = 0.0024 | learning rate = 6.12476e-06\n",
      "inside validation data for epoch 96\n",
      "Val loss has decreased -->reducing the global validation loss from 1.33 to 1.32\n",
      " validation loss for epoch = 96 is 1.3223\n",
      " epoch= 96 :  val loss is 1.3223 \n",
      "saving the model model2024-07-2204:46:57.pth\n",
      "*****LOGGING INFO IN distl_gpt2_Random_init_nll_and_cos_lossrandom_init_wts_True_2024-07-22.log*********\n",
      "executing epoch:98, it took 1.0527098814646403 mins from beginning of epoch till batch#131\n",
      "training loss has decreased---> reducing the best loss from 0.03 to 0.03 | throughput = 135999 tokens/second | norm = 0.0028 | learning rate = 6.03956e-06\n",
      "inside validation data for epoch 97\n",
      "Val loss has decreased -->reducing the global validation loss from 1.32 to 1.31\n",
      " validation loss for epoch = 97 is 1.3141\n",
      " epoch= 97 :  val loss is 1.3141 \n",
      "saving the model model2024-07-2204:49:54.pth\n",
      "*****LOGGING INFO IN distl_gpt2_Random_init_nll_and_cos_lossrandom_init_wts_True_2024-07-22.log*********\n",
      "executing epoch:99, it took 1.048819069067637 mins from beginning of epoch till batch#131\n",
      "training loss has decreased---> reducing the best loss from 0.03 to 0.03 | throughput = 136218 tokens/second | norm = 0.0003 | learning rate = 5.95404e-06\n",
      "inside validation data for epoch 98\n",
      "Val loss has decreased -->reducing the global validation loss from 1.31 to 1.31\n",
      " validation loss for epoch = 98 is 1.3091\n",
      " epoch= 98 :  val loss is 1.3091 \n",
      "saving the model model2024-07-2204:52:50.pth\n",
      "*****LOGGING INFO IN distl_gpt2_Random_init_nll_and_cos_lossrandom_init_wts_True_2024-07-22.log*********\n",
      "executing epoch:100, it took 1.0523645440737406 mins from beginning of epoch till batch#131\n",
      "training loss has decreased---> reducing the best loss from 0.03 to 0.02 | throughput = 135967 tokens/second | norm = 0.0006 | learning rate = 5.86824e-06\n",
      "inside validation data for epoch 99\n",
      "Val loss has decreased -->reducing the global validation loss from 1.31 to 1.30\n",
      " validation loss for epoch = 99 is 1.2992\n",
      " epoch= 99 :  val loss is 1.2992 \n",
      "saving the model model2024-07-2204:55:48.pth\n",
      "*****LOGGING INFO IN distl_gpt2_Random_init_nll_and_cos_lossrandom_init_wts_True_2024-07-22.log*********\n",
      "executing epoch:101, it took 0.3824817657470703 mins from beginning of epoch till batch#131\n",
      "No improvement in training  loss-->epoch= 100 and best loss is 0.02|current_loss = 222.51024162769318|counter = 1\n",
      "executing epoch:102, it took 0.37944764693578087 mins from beginning of epoch till batch#131\n",
      "No improvement in training  loss-->epoch= 101 and best loss is 0.02|current_loss = 208.49416041374207|counter = 2\n",
      "executing epoch:103, it took 0.37947624921798706 mins from beginning of epoch till batch#131\n",
      "No improvement in training  loss-->epoch= 102 and best loss is 0.02|current_loss = 198.9793557524681|counter = 3\n",
      "executing epoch:104, it took 0.37955340147018435 mins from beginning of epoch till batch#131\n",
      "No improvement in training  loss-->epoch= 103 and best loss is 0.02|current_loss = 191.03794687986374|counter = 4\n",
      "executing epoch:105, it took 0.3789956291516622 mins from beginning of epoch till batch#131\n",
      "No improvement in training  loss-->epoch= 104 and best loss is 0.02|current_loss = 183.993095099926|counter = 5\n",
      "executing epoch:106, it took 0.3793605208396912 mins from beginning of epoch till batch#131\n",
      "No improvement in training  loss-->epoch= 105 and best loss is 0.02|current_loss = 177.54308646917343|counter = 6\n",
      "executing epoch:107, it took 0.37949447631835936 mins from beginning of epoch till batch#131\n",
      "No improvement in training  loss-->epoch= 106 and best loss is 0.02|current_loss = 171.5413807630539|counter = 7\n",
      "executing epoch:108, it took 0.37908567984898883 mins from beginning of epoch till batch#131\n",
      "No improvement in training  loss-->epoch= 107 and best loss is 0.02|current_loss = 165.89385551214218|counter = 8\n",
      "executing epoch:109, it took 0.3796298662821452 mins from beginning of epoch till batch#131\n",
      "No improvement in training  loss-->epoch= 108 and best loss is 0.02|current_loss = 160.5420667529106|counter = 9\n",
      "executing epoch:110, it took 0.37936127980550133 mins from beginning of epoch till batch#131\n",
      "No improvement in training  loss-->epoch= 109 and best loss is 0.02|current_loss = 155.4439092874527|counter = 10\n",
      "executing epoch:111, it took 0.3792347311973572 mins from beginning of epoch till batch#131\n",
      "No improvement in training  loss-->epoch= 110 and best loss is 0.02|current_loss = 150.57166224718094|counter = 11\n",
      "executing epoch:112, it took 0.37927222649256387 mins from beginning of epoch till batch#131\n",
      "No improvement in training  loss-->epoch= 111 and best loss is 0.02|current_loss = 145.9083063006401|counter = 12\n",
      "executing epoch:113, it took 0.37923761606216433 mins from beginning of epoch till batch#131\n",
      "No improvement in training  loss-->epoch= 112 and best loss is 0.02|current_loss = 141.4365758895874|counter = 13\n",
      "executing epoch:114, it took 0.3793247739473979 mins from beginning of epoch till batch#131\n",
      "No improvement in training  loss-->epoch= 113 and best loss is 0.02|current_loss = 137.14139729738235|counter = 14\n",
      "executing epoch:115, it took 0.37909626166025795 mins from beginning of epoch till batch#131\n",
      "No improvement in training  loss-->epoch= 114 and best loss is 0.02|current_loss = 133.01376575231552|counter = 15\n",
      "executing epoch:116, it took 0.3794427990913391 mins from beginning of epoch till batch#131\n",
      "No improvement in training  loss-->epoch= 115 and best loss is 0.02|current_loss = 129.04852932691574|counter = 16\n",
      "executing epoch:117, it took 0.3791500846544901 mins from beginning of epoch till batch#131\n",
      "No improvement in training  loss-->epoch= 116 and best loss is 0.02|current_loss = 125.23738098144531|counter = 17\n",
      "executing epoch:118, it took 0.3795880476633708 mins from beginning of epoch till batch#131\n",
      "No improvement in training  loss-->epoch= 117 and best loss is 0.02|current_loss = 121.57235181331635|counter = 18\n",
      "executing epoch:119, it took 0.37928484280904134 mins from beginning of epoch till batch#131\n",
      "No improvement in training  loss-->epoch= 118 and best loss is 0.02|current_loss = 118.05070620775223|counter = 19\n",
      "executing epoch:120, it took 0.3788585901260376 mins from beginning of epoch till batch#131\n",
      "No improvement in training  loss-->epoch= 119 and best loss is 0.02|current_loss = 114.66609293222427|counter = 20\n",
      "executing epoch:121, it took 0.37931425174077354 mins from beginning of epoch till batch#131\n",
      "No improvement in training  loss-->epoch= 120 and best loss is 0.02|current_loss = 111.41334128379822|counter = 21\n",
      "executing epoch:122, it took 0.37942262093226115 mins from beginning of epoch till batch#131\n",
      "No improvement in training  loss-->epoch= 121 and best loss is 0.02|current_loss = 108.29005253314972|counter = 22\n",
      "executing epoch:123, it took 0.3791417439778646 mins from beginning of epoch till batch#131\n",
      "No improvement in training  loss-->epoch= 122 and best loss is 0.02|current_loss = 105.29165387153625|counter = 23\n",
      "executing epoch:124, it took 0.3795174320538839 mins from beginning of epoch till batch#131\n",
      "No improvement in training  loss-->epoch= 123 and best loss is 0.02|current_loss = 102.41284012794495|counter = 24\n",
      "executing epoch:125, it took 0.37935820817947385 mins from beginning of epoch till batch#131\n",
      "No improvement in training  loss-->epoch= 124 and best loss is 0.02|current_loss = 99.64935117959976|counter = 25\n",
      "executing epoch:126, it took 0.3787891228993734 mins from beginning of epoch till batch#131\n",
      "No improvement in training  loss-->epoch= 125 and best loss is 0.02|current_loss = 96.99844294786453|counter = 26\n",
      "executing epoch:127, it took 0.37976956367492676 mins from beginning of epoch till batch#131\n",
      "No improvement in training  loss-->epoch= 126 and best loss is 0.02|current_loss = 94.45673960447311|counter = 27\n",
      "executing epoch:128, it took 0.379876963297526 mins from beginning of epoch till batch#131\n",
      "No improvement in training  loss-->epoch= 127 and best loss is 0.02|current_loss = 92.0196316242218|counter = 28\n",
      "executing epoch:129, it took 0.3792119065920512 mins from beginning of epoch till batch#131\n",
      "No improvement in training  loss-->epoch= 128 and best loss is 0.02|current_loss = 89.68482387065887|counter = 29\n",
      "executing epoch:130, it took 0.3793589075406392 mins from beginning of epoch till batch#131\n",
      "No improvement in training  loss-->epoch= 129 and best loss is 0.02|current_loss = 87.44526320695877|counter = 30\n",
      "executing epoch:131, it took 0.3794228951136271 mins from beginning of epoch till batch#131\n",
      "No improvement in training  loss-->epoch= 130 and best loss is 0.02|current_loss = 85.3007361292839|counter = 31\n",
      "executing epoch:132, it took 0.37921460072199503 mins from beginning of epoch till batch#131\n",
      "No improvement in training  loss-->epoch= 131 and best loss is 0.02|current_loss = 83.2480776309967|counter = 32\n",
      "executing epoch:133, it took 0.37897161245346067 mins from beginning of epoch till batch#131\n",
      "No improvement in training  loss-->epoch= 132 and best loss is 0.02|current_loss = 81.28427201509476|counter = 33\n",
      "executing epoch:134, it took 0.37915761868158976 mins from beginning of epoch till batch#131\n",
      "No improvement in training  loss-->epoch= 133 and best loss is 0.02|current_loss = 79.40395718812943|counter = 34\n",
      "executing epoch:135, it took 0.3794521967569987 mins from beginning of epoch till batch#131\n",
      "No improvement in training  loss-->epoch= 134 and best loss is 0.02|current_loss = 77.60509485006332|counter = 35\n",
      "executing epoch:136, it took 0.37927539745966593 mins from beginning of epoch till batch#131\n",
      "No improvement in training  loss-->epoch= 135 and best loss is 0.02|current_loss = 75.88439679145813|counter = 36\n",
      "executing epoch:137, it took 0.37934009631474813 mins from beginning of epoch till batch#131\n",
      "No improvement in training  loss-->epoch= 136 and best loss is 0.02|current_loss = 74.23965722322464|counter = 37\n",
      "executing epoch:138, it took 0.37925286690394083 mins from beginning of epoch till batch#131\n",
      "No improvement in training  loss-->epoch= 137 and best loss is 0.02|current_loss = 72.66616332530975|counter = 38\n",
      "executing epoch:139, it took 0.37908864418665567 mins from beginning of epoch till batch#131\n",
      "No improvement in training  loss-->epoch= 138 and best loss is 0.02|current_loss = 71.16348731517792|counter = 39\n",
      "executing epoch:140, it took 0.3791948715845744 mins from beginning of epoch till batch#131\n",
      "No improvement in training  loss-->epoch= 139 and best loss is 0.02|current_loss = 69.72710186243057|counter = 40\n",
      "executing epoch:141, it took 0.37875744501749675 mins from beginning of epoch till batch#131\n",
      "No improvement in training  loss-->epoch= 140 and best loss is 0.02|current_loss = 68.35630333423615|counter = 41\n",
      "executing epoch:142, it took 0.37888028621673586 mins from beginning of epoch till batch#131\n",
      "No improvement in training  loss-->epoch= 141 and best loss is 0.02|current_loss = 67.046706199646|counter = 42\n",
      "executing epoch:143, it took 0.3788205822308858 mins from beginning of epoch till batch#131\n",
      "No improvement in training  loss-->epoch= 142 and best loss is 0.02|current_loss = 65.79677259922028|counter = 43\n",
      "executing epoch:144, it took 0.37849165201187135 mins from beginning of epoch till batch#131\n",
      "No improvement in training  loss-->epoch= 143 and best loss is 0.02|current_loss = 64.60391581058502|counter = 44\n",
      "executing epoch:145, it took 0.37881468137105306 mins from beginning of epoch till batch#131\n",
      "No improvement in training  loss-->epoch= 144 and best loss is 0.02|current_loss = 63.46535587310791|counter = 45\n",
      "executing epoch:146, it took 0.3788072427113851 mins from beginning of epoch till batch#131\n",
      "No improvement in training  loss-->epoch= 145 and best loss is 0.02|current_loss = 62.38039427995682|counter = 46\n",
      "executing epoch:147, it took 0.37831424474716185 mins from beginning of epoch till batch#131\n",
      "No improvement in training  loss-->epoch= 146 and best loss is 0.02|current_loss = 61.346322655677795|counter = 47\n",
      "executing epoch:148, it took 0.3788778066635132 mins from beginning of epoch till batch#131\n",
      "No improvement in training  loss-->epoch= 147 and best loss is 0.02|current_loss = 60.36048573255539|counter = 48\n",
      "executing epoch:149, it took 0.37868237098058066 mins from beginning of epoch till batch#131\n",
      "No improvement in training  loss-->epoch= 148 and best loss is 0.02|current_loss = 59.42114984989166|counter = 49\n",
      "executing epoch:150, it took 0.378547465801239 mins from beginning of epoch till batch#131\n",
      "No improvement in training  loss-->epoch= 149 and best loss is 0.02|current_loss = 58.527640998363495|counter = 50\n",
      "executing epoch:151, it took 0.3786596655845642 mins from beginning of epoch till batch#131\n",
      "No improvement in training  loss-->epoch= 150 and best loss is 0.02|current_loss = 57.67705672979355|counter = 51\n",
      "executing epoch:152, it took 0.37831716537475585 mins from beginning of epoch till batch#131\n",
      "No improvement in training  loss-->epoch= 151 and best loss is 0.02|current_loss = 56.86752086877823|counter = 52\n",
      "executing epoch:153, it took 0.37819890181223553 mins from beginning of epoch till batch#131\n",
      "No improvement in training  loss-->epoch= 152 and best loss is 0.02|current_loss = 56.098140478134155|counter = 53\n",
      "executing epoch:154, it took 0.37824965715408326 mins from beginning of epoch till batch#131\n",
      "No improvement in training  loss-->epoch= 153 and best loss is 0.02|current_loss = 55.36647307872772|counter = 54\n",
      "executing epoch:155, it took 0.3786650021870931 mins from beginning of epoch till batch#131\n",
      "No improvement in training  loss-->epoch= 154 and best loss is 0.02|current_loss = 54.671760618686676|counter = 55\n",
      "executing epoch:156, it took 0.3784315824508667 mins from beginning of epoch till batch#131\n",
      "No improvement in training  loss-->epoch= 155 and best loss is 0.02|current_loss = 54.012623846530914|counter = 56\n",
      "executing epoch:157, it took 0.37830947240193685 mins from beginning of epoch till batch#131\n",
      "No improvement in training  loss-->epoch= 156 and best loss is 0.02|current_loss = 53.38745152950287|counter = 57\n",
      "executing epoch:158, it took 0.37854320208231607 mins from beginning of epoch till batch#131\n",
      "No improvement in training  loss-->epoch= 157 and best loss is 0.02|current_loss = 52.79477918148041|counter = 58\n",
      "executing epoch:159, it took 0.37814167737960813 mins from beginning of epoch till batch#131\n",
      "No improvement in training  loss-->epoch= 158 and best loss is 0.02|current_loss = 52.233114659786224|counter = 59\n",
      "executing epoch:160, it took 0.3781595826148987 mins from beginning of epoch till batch#131\n",
      "No improvement in training  loss-->epoch= 159 and best loss is 0.02|current_loss = 51.701519787311554|counter = 60\n",
      "executing epoch:161, it took 0.3784848690032959 mins from beginning of epoch till batch#131\n",
      "No improvement in training  loss-->epoch= 160 and best loss is 0.02|current_loss = 51.198836982250214|counter = 61\n",
      "executing epoch:162, it took 0.37814151843388877 mins from beginning of epoch till batch#131\n",
      "No improvement in training  loss-->epoch= 161 and best loss is 0.02|current_loss = 50.72394347190857|counter = 62\n",
      "executing epoch:163, it took 0.37839579582214355 mins from beginning of epoch till batch#131\n",
      "No improvement in training  loss-->epoch= 162 and best loss is 0.02|current_loss = 50.27599823474884|counter = 63\n",
      "executing epoch:164, it took 0.37858786185582477 mins from beginning of epoch till batch#131\n",
      "No improvement in training  loss-->epoch= 163 and best loss is 0.02|current_loss = 49.85337179899216|counter = 64\n",
      "executing epoch:165, it took 0.37805018822352093 mins from beginning of epoch till batch#131\n",
      "No improvement in training  loss-->epoch= 164 and best loss is 0.02|current_loss = 49.45546865463257|counter = 65\n",
      "executing epoch:166, it took 0.378310231367747 mins from beginning of epoch till batch#131\n",
      "No improvement in training  loss-->epoch= 165 and best loss is 0.02|current_loss = 49.08129417896271|counter = 66\n",
      "executing epoch:167, it took 0.3780924956003825 mins from beginning of epoch till batch#131\n",
      "No improvement in training  loss-->epoch= 166 and best loss is 0.02|current_loss = 48.72982633113861|counter = 67\n",
      "executing epoch:168, it took 0.3782283465067546 mins from beginning of epoch till batch#131\n",
      "No improvement in training  loss-->epoch= 167 and best loss is 0.02|current_loss = 48.40068620443344|counter = 68\n",
      "executing epoch:169, it took 0.37872864802678424 mins from beginning of epoch till batch#131\n",
      "No improvement in training  loss-->epoch= 168 and best loss is 0.02|current_loss = 48.09306889772415|counter = 69\n",
      "executing epoch:170, it took 0.37864203453063966 mins from beginning of epoch till batch#131\n",
      "No improvement in training  loss-->epoch= 169 and best loss is 0.02|current_loss = 47.80560851097107|counter = 70\n",
      "executing epoch:171, it took 0.3779866973559062 mins from beginning of epoch till batch#131\n",
      "No improvement in training  loss-->epoch= 170 and best loss is 0.02|current_loss = 47.53791183233261|counter = 71\n",
      "executing epoch:172, it took 0.3778800169626872 mins from beginning of epoch till batch#131\n",
      "No improvement in training  loss-->epoch= 171 and best loss is 0.02|current_loss = 47.28990149497986|counter = 72\n",
      "executing epoch:173, it took 0.3784192204475403 mins from beginning of epoch till batch#131\n",
      "No improvement in training  loss-->epoch= 172 and best loss is 0.02|current_loss = 47.061120092868805|counter = 73\n",
      "executing epoch:174, it took 0.37820892333984374 mins from beginning of epoch till batch#131\n",
      "No improvement in training  loss-->epoch= 173 and best loss is 0.02|current_loss = 46.85000967979431|counter = 74\n",
      "executing epoch:175, it took 0.3784924109776815 mins from beginning of epoch till batch#131\n",
      "No improvement in training  loss-->epoch= 174 and best loss is 0.02|current_loss = 46.656423449516296|counter = 75\n",
      "executing epoch:176, it took 0.3785821795463562 mins from beginning of epoch till batch#131\n",
      "No improvement in training  loss-->epoch= 175 and best loss is 0.02|current_loss = 46.48173898458481|counter = 76\n",
      "executing epoch:177, it took 0.3786370913187663 mins from beginning of epoch till batch#131\n",
      "No improvement in training  loss-->epoch= 176 and best loss is 0.02|current_loss = 46.32343512773514|counter = 77\n",
      "executing epoch:178, it took 0.3784246683120728 mins from beginning of epoch till batch#131\n",
      "No improvement in training  loss-->epoch= 177 and best loss is 0.02|current_loss = 46.182631969451904|counter = 78\n",
      "executing epoch:179, it took 0.378550386428833 mins from beginning of epoch till batch#131\n",
      "No improvement in training  loss-->epoch= 178 and best loss is 0.02|current_loss = 46.060998022556305|counter = 79\n",
      "executing epoch:180, it took 0.3785545428593953 mins from beginning of epoch till batch#131\n",
      "No improvement in training  loss-->epoch= 179 and best loss is 0.02|current_loss = 45.95358097553253|counter = 80\n",
      "executing epoch:181, it took 0.37852328618367515 mins from beginning of epoch till batch#131\n",
      "No improvement in training  loss-->epoch= 180 and best loss is 0.02|current_loss = 45.866291999816895|counter = 81\n",
      "executing epoch:182, it took 0.3782249689102173 mins from beginning of epoch till batch#131\n",
      "No improvement in training  loss-->epoch= 181 and best loss is 0.02|current_loss = 45.79544115066528|counter = 82\n",
      "executing epoch:183, it took 0.3783833066622416 mins from beginning of epoch till batch#131\n",
      "No improvement in training  loss-->epoch= 182 and best loss is 0.02|current_loss = 45.73202818632126|counter = 83\n",
      "executing epoch:184, it took 0.37833133538564045 mins from beginning of epoch till batch#131\n",
      "No improvement in training  loss-->epoch= 183 and best loss is 0.02|current_loss = 45.67595773935318|counter = 84\n",
      "executing epoch:185, it took 0.3782515287399292 mins from beginning of epoch till batch#131\n",
      "No improvement in training  loss-->epoch= 184 and best loss is 0.02|current_loss = 45.63751870393753|counter = 85\n",
      "executing epoch:186, it took 0.3783385197321574 mins from beginning of epoch till batch#131\n",
      "No improvement in training  loss-->epoch= 185 and best loss is 0.02|current_loss = 45.601084887981415|counter = 86\n",
      "executing epoch:187, it took 0.37807759443918865 mins from beginning of epoch till batch#131\n",
      "No improvement in training  loss-->epoch= 186 and best loss is 0.02|current_loss = 45.582900285720825|counter = 87\n",
      "executing epoch:188, it took 0.37875713109970094 mins from beginning of epoch till batch#131\n",
      "No improvement in training  loss-->epoch= 187 and best loss is 0.02|current_loss = 45.56266951560974|counter = 88\n",
      "executing epoch:189, it took 0.37842870950698854 mins from beginning of epoch till batch#131\n",
      "No improvement in training  loss-->epoch= 188 and best loss is 0.02|current_loss = 45.55365389585495|counter = 89\n",
      "executing epoch:190, it took 0.37835044860839845 mins from beginning of epoch till batch#131\n",
      "No improvement in training  loss-->epoch= 189 and best loss is 0.02|current_loss = 45.53782069683075|counter = 90\n",
      "executing epoch:191, it took 0.3786446849505107 mins from beginning of epoch till batch#131\n",
      "No improvement in training  loss-->epoch= 190 and best loss is 0.02|current_loss = 45.526018679142|counter = 91\n",
      "executing epoch:192, it took 0.3787160913149516 mins from beginning of epoch till batch#131\n",
      "No improvement in training  loss-->epoch= 191 and best loss is 0.02|current_loss = 45.5180162191391|counter = 92\n",
      "executing epoch:193, it took 0.3781683286031087 mins from beginning of epoch till batch#131\n",
      "No improvement in training  loss-->epoch= 192 and best loss is 0.02|current_loss = 45.5181422829628|counter = 93\n",
      "executing epoch:194, it took 0.3785484472910563 mins from beginning of epoch till batch#131\n",
      "No improvement in training  loss-->epoch= 193 and best loss is 0.02|current_loss = 45.51847559213638|counter = 94\n",
      "executing epoch:195, it took 0.3784657001495361 mins from beginning of epoch till batch#131\n",
      "No improvement in training  loss-->epoch= 194 and best loss is 0.02|current_loss = 45.51884925365448|counter = 95\n",
      "executing epoch:196, it took 0.3785302956899007 mins from beginning of epoch till batch#131\n",
      "No improvement in training  loss-->epoch= 195 and best loss is 0.02|current_loss = 45.51685148477554|counter = 96\n",
      "executing epoch:197, it took 0.378402841091156 mins from beginning of epoch till batch#131\n",
      "No improvement in training  loss-->epoch= 196 and best loss is 0.02|current_loss = 45.5132200717926|counter = 97\n",
      "executing epoch:198, it took 0.3786195119222005 mins from beginning of epoch till batch#131\n",
      "No improvement in training  loss-->epoch= 197 and best loss is 0.02|current_loss = 45.51058453321457|counter = 98\n",
      "executing epoch:199, it took 0.37836326360702516 mins from beginning of epoch till batch#131\n",
      "No improvement in training  loss-->epoch= 198 and best loss is 0.02|current_loss = 45.50936549901962|counter = 99\n",
      "executing epoch:200, it took 0.37863481044769287 mins from beginning of epoch till batch#131\n",
      "No improvement in training  loss-->epoch= 199 and best loss is 0.02|current_loss = 45.50932937860489|counter = 100\n",
      "executing epoch:201, it took 0.37860023180643715 mins from beginning of epoch till batch#131\n",
      "No improvement in training  loss-->epoch= 200 and best loss is 0.02|current_loss = 45.50929248332977|counter = 101\n",
      "executing epoch:202, it took 0.378087592124939 mins from beginning of epoch till batch#131\n",
      "No improvement in training  loss-->epoch= 201 and best loss is 0.02|current_loss = 45.509541392326355|counter = 102\n",
      "executing epoch:203, it took 0.37849308649698893 mins from beginning of epoch till batch#131\n",
      "No improvement in training  loss-->epoch= 202 and best loss is 0.02|current_loss = 45.509455025196075|counter = 103\n",
      "executing epoch:204, it took 0.37867504358291626 mins from beginning of epoch till batch#131\n",
      "No improvement in training  loss-->epoch= 203 and best loss is 0.02|current_loss = 45.50935620069504|counter = 104\n",
      "executing epoch:205, it took 0.3782214879989624 mins from beginning of epoch till batch#131\n",
      "No improvement in training  loss-->epoch= 204 and best loss is 0.02|current_loss = 45.50943964719772|counter = 105\n",
      "executing epoch:206, it took 0.37831114530563353 mins from beginning of epoch till batch#131\n",
      "No improvement in training  loss-->epoch= 205 and best loss is 0.02|current_loss = 45.50936883687973|counter = 106\n",
      "executing epoch:207, it took 0.37831531365712484 mins from beginning of epoch till batch#131\n",
      "No improvement in training  loss-->epoch= 206 and best loss is 0.02|current_loss = 45.50949311256409|counter = 107\n",
      "executing epoch:208, it took 0.3781758189201355 mins from beginning of epoch till batch#131\n",
      "No improvement in training  loss-->epoch= 207 and best loss is 0.02|current_loss = 45.50981104373932|counter = 108\n",
      "executing epoch:209, it took 0.3786350170771281 mins from beginning of epoch till batch#131\n",
      "No improvement in training  loss-->epoch= 208 and best loss is 0.02|current_loss = 45.509805619716644|counter = 109\n",
      "executing epoch:210, it took 0.3790860613187154 mins from beginning of epoch till batch#131\n",
      "No improvement in training  loss-->epoch= 209 and best loss is 0.02|current_loss = 45.50984412431717|counter = 110\n",
      "executing epoch:211, it took 0.3782717943191528 mins from beginning of epoch till batch#131\n",
      "No improvement in training  loss-->epoch= 210 and best loss is 0.02|current_loss = 45.50996744632721|counter = 111\n",
      "executing epoch:212, it took 0.3782322327295939 mins from beginning of epoch till batch#131\n",
      "No improvement in training  loss-->epoch= 211 and best loss is 0.02|current_loss = 45.51001971960068|counter = 112\n",
      "executing epoch:213, it took 0.378411336739858 mins from beginning of epoch till batch#131\n",
      "No improvement in training  loss-->epoch= 212 and best loss is 0.02|current_loss = 45.510033667087555|counter = 113\n",
      "executing epoch:214, it took 0.3785088260968526 mins from beginning of epoch till batch#131\n",
      "No improvement in training  loss-->epoch= 213 and best loss is 0.02|current_loss = 45.51011264324188|counter = 114\n",
      "executing epoch:215, it took 0.3784238378206889 mins from beginning of epoch till batch#131\n",
      "No improvement in training  loss-->epoch= 214 and best loss is 0.02|current_loss = 45.51009690761566|counter = 115\n",
      "executing epoch:216, it took 0.3786845723787943 mins from beginning of epoch till batch#131\n",
      "No improvement in training  loss-->epoch= 215 and best loss is 0.02|current_loss = 45.51012122631073|counter = 116\n",
      "executing epoch:217, it took 0.3784252603848775 mins from beginning of epoch till batch#131\n",
      "No improvement in training  loss-->epoch= 216 and best loss is 0.02|current_loss = 45.51012045145035|counter = 117\n",
      "executing epoch:218, it took 0.37820641199747723 mins from beginning of epoch till batch#131\n",
      "No improvement in training  loss-->epoch= 217 and best loss is 0.02|current_loss = 45.51013368368149|counter = 118\n",
      "executing epoch:219, it took 0.3783344586690267 mins from beginning of epoch till batch#131\n",
      "No improvement in training  loss-->epoch= 218 and best loss is 0.02|current_loss = 45.5101455450058|counter = 119\n",
      "executing epoch:220, it took 0.3786245067914327 mins from beginning of epoch till batch#131\n",
      "No improvement in training  loss-->epoch= 219 and best loss is 0.02|current_loss = 45.510158240795135|counter = 120\n"
     ]
    }
   ],
   "source": [
    "tr_model,epoch_train_log,epoch_val_log = train_model(train_loader, val_loader, model =  model,tokenizer = tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "70f210db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json , os\n",
    "path_var_train_log = os.path.join(\".\",\"Tokenizers_and_loss_vals\",\"epoch_train_plain_loss.json\")\n",
    "path_var_val_log = os.path.join(\".\",\"Tokenizers_and_loss_vals\",\"epoch_val_val_loss_.json\")\n",
    "\n",
    "#print(path_var)\n",
    "#Write the list to a JSON file\n",
    "with open(path_var_train_log, \"w\") as file:\n",
    "    json.dump(epoch_train_log, file)\n",
    "\n",
    "with open(path_var_val_log, \"w\") as file:\n",
    "    json.dump(epoch_val_log, file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "02c864b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path_var_train_log, \"r\") as file:\n",
    "    train_loss = json.load(file)\n",
    "with open(path_var_val_log, \"r\") as file:\n",
    "    val_loss = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "93ea26a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "220\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "print(len(train_loss))\n",
    "print(len(val_loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "339ee3ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f8ce720c640>]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_values = range(len(train_loss))\n",
    "plt.plot(x_values, train_loss, label='Train_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6c51ae82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f8ce728d100>]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD4CAYAAAAAczaOAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8/fFQqAAAACXBIWXMAAAsTAAALEwEAmpwYAAAcr0lEQVR4nO3de3AdZ5nn8e9zjnSOrtbFkhXZVmITK4CBxA7OhSSQLAy5MbOBZZaNZwpMNruGmaQGWKp2A7O1zKWoYliuATY7gRiSKSaZTAgTL5UhJM5tMhMSy8EYx05iJZhYji+yZUuWbF3Ps3+cln3kSLZkXVrq9/epOnW63+4+erra9ev2e97Tbe6OiIiEIRV3ASIiMnMU+iIiAVHoi4gERKEvIhIQhb6ISECK4i7gVOrq6nzJkiVxlyEiMqds2rTpgLvXj7ZsVof+kiVLaGlpibsMEZE5xcx+N9Yyde+IiAREoS8iEhCFvohIQBT6IiIBUeiLiAREoS8iEhCFvohIQBIZ+l29A3zrsVfYvOtw3KWIiMwqiQx9z8G3HttBy86OuEsREZlVEhn680qLyKRTtB/pi7sUEZFZJZGhb2bUV2Zp71boi4gUSmToA9RVZHSlLyJyksSGfn1llgPd/XGXISIyqyQ69HWlLyIyUmJDv64iS0dPH0M5j7sUEZFZI7GhX1+ZJefQ0aMuHhGRYckN/YosgLp4REQKJDb06yqj0NewTRGR4xIb+sNX+gd0pS8iclxyQ19X+iIib5LY0C/PFlFanFafvohIgcSGPgz/QEuhLyIyLPGhryt9EZETEh36dRUZXemLiBQ4beibWZOZPWFm28zsRTP7TNT+F2a228w2R6/rC7b5gpm1mtnLZnZNQfu1UVurmd02Pbt0gq70RURGKhrHOoPA5939BTOrBDaZ2aPRsm+6+9cKVzaz5cCNwDuAhcBjZnZetPh7wAeBNmCjma13921TsSOjqa8o4dDRAQaGchSnE/2fGhGRcTltErr7Hnd/IZo+AmwHFp1ikxuA+9y9z91/C7QCF0evVnd/zd37gfuidadNXWUGgIO626aICDDBPn0zWwKsBJ6Lmm41sy1mts7MaqK2RcCugs3aorax2k/+G2vNrMXMWtrb2ydS3pvoVgwiIiONO/TNrAL4CfBZd+8C7gDOBVYAe4CvT0VB7n6nu69y91X19fWT+qwTP9DqnYrSRETmvPH06WNmxeQD/8fu/iCAu+8rWP594GfR7G6gqWDzxVEbp2ifFnXHb8Wg7h0RERjf6B0D7gK2u/s3CtobC1b7CLA1ml4P3GhmWTNbCjQDzwMbgWYzW2pmGfJf9q6fmt0YnW7FICIy0niu9C8HPg78xsw2R21fBFab2QrAgZ3ApwDc/UUzux/YRn7kzy3uPgRgZrcCjwBpYJ27vzhlezKKkuI0lSVF6tMXEYmcNvTd/RnARln08Cm2+TLw5VHaHz7VdtOhviKrK30RkUjiB6/X6QdaIiLHJT70ddM1EZETkh/6FbrSFxEZlvzQr8xypHeQ3oGhuEsREYld8kN/eKy+unhERJIf+sP331EXj4hIAKFfX1ECwAHddE1EJIDQr9RN10REhiU+9OdXqHtHRGRY4kO/OJ1iXkkRh46qe0dEJPGhD1BbnuFgj0JfRCSY0D+k0BcRCSX0s7rSFxEhmNAv1pW+iAjBhH6Wjp5+3D3uUkREYhVI6BfTP5Sjp1/33xGRsAUS+vkfaHXoV7kiErhAQr8YgA6N1ReRwAUS+tGVfo9+lSsiYQsj9Mvyt2Lo6BmIuRIRkXiFEfoVw6GvK30RCVsQoV+eSZNJp3SlLyLBCyL0zYza8oyu9EUkeEGEPkBNeUZX+iISvGBCf76u9EVEwgn9mvIMh47qSl9EwhZM6M8vz3CwW1f6IhK204a+mTWZ2RNmts3MXjSzz0TttWb2qJntiN5ronYzs9vNrNXMtpjZhQWftSZaf4eZrZm+3XqzmrIMXb2DDAzlZvLPiojMKuO50h8EPu/uy4FLgVvMbDlwG7DB3ZuBDdE8wHVAc/RaC9wB+ZME8CXgEuBi4EvDJ4qZMDxWX49NFJGQnTb03X2Pu78QTR8BtgOLgBuAu6PV7gY+HE3fANzjeb8Eqs2sEbgGeNTdO9z9EPAocO1U7sypDP8q95BG8IhIwCbUp29mS4CVwHNAg7vviRbtBRqi6UXAroLN2qK2sdpP/htrzazFzFra29snUt4p1ZbnQ/+gRvCISMDGHfpmVgH8BPisu3cVLvP800mm5Akl7n6nu69y91X19fVT8ZHAidDXlb6IhGxcoW9mxeQD/8fu/mDUvC/qtiF63x+17waaCjZfHLWN1T4jhkNfY/VFJGTjGb1jwF3Adnf/RsGi9cDwCJw1wEMF7Z+IRvFcCnRG3UCPAFebWU30Be7VUduMqC6L7qmvK30RCVjRONa5HPg48Bsz2xy1fRH4CnC/md0M/A74WLTsYeB6oBU4CtwE4O4dZvbXwMZovb9y946p2InxKE6nqCot1pW+iATttKHv7s8ANsbiD4yyvgO3jPFZ64B1EylwKtWWZ+jQr3JFJGDB/CIX0J02RSR4QYV+TZnutCkiYQsq9HWnTREJXVChn7+nfj/5rx1ERMITVOjPL88wMOR09w3GXYqISCyCCv2a4z/Q0k3XRCRMQYX+fIW+iAQuqNDXlb6IhC6o0NeVvoiELqjQ15W+iIQuqNAvz6TJFKUU+iISrKBC38xYUJllX1dv3KWIiMQiqNAHWFhVyhudCn0RCVN4oV9dwp7OY3GXISISi+BCv7G6lL2dveRyuhWDiIQnuNBfWFXCwJBzoFs3XhOR8IQX+tWlAOrXF5EgBRf6jVVR6B9Wv76IhCe40F9YXQIo9EUkTMGFflVpMWWZNG8cVveOiIQnuNA3MxqrNGxTRMIUXOhD/stcfZErIiEKM/SrStWnLyJBCjL0G6tLONDdR/9gLu5SRERmVJChv7C6FHd04zURCU6YoR+N1d+tLh4RCcxpQ9/M1pnZfjPbWtD2F2a228w2R6/rC5Z9wcxazexlM7umoP3aqK3VzG6b+l0Zv8ZorL5G8IhIaMZzpf8j4NpR2r/p7iui18MAZrYcuBF4R7TN/zGztJmlge8B1wHLgdXRurFYePxXuereEZGwFJ1uBXd/2syWjPPzbgDuc/c+4Ldm1gpcHC1rdffXAMzsvmjdbRMvefJKM2lqyoo1gkdEgjOZPv1bzWxL1P1TE7UtAnYVrNMWtY3VHpvGqlL2aKy+iATmTEP/DuBcYAWwB/j6VBVkZmvNrMXMWtrb26fqY99kYbXG6otIeM4o9N19n7sPuXsO+D4nunB2A00Fqy6O2sZqH+2z73T3Ve6+qr6+/kzKG5eF1SUKfREJzhmFvpk1Fsx+BBge2bMeuNHMsma2FGgGngc2As1mttTMMuS/7F1/5mVP3sLqUrp6B+nuG4yzDBGRGXXaL3LN7F7gKqDOzNqALwFXmdkKwIGdwKcA3P1FM7uf/Be0g8At7j4Ufc6twCNAGljn7i9O9c5MRGNVNGzz8DGaGyrjLEVEZMaMZ/TO6lGa7zrF+l8GvjxK+8PAwxOqbhoVPkFLoS8ioQjyF7lwIvT3qF9fRAISbOg3VGZJmZ6gJSJhCTb0i9IpGuaV0KbQF5GABBv6AEvrynmtvSfuMkREZkzQod+8oILW/d24e9yliIjMiKBDf1lDJd19g+zVffVFJBBBh37zggoAduzrjrkSEZGZodAHduxX6ItIGIIO/fkVWeaXZ9ix70jcpYiIzIigQx9g2YIKXemLSDCCD/3mhgp27DuiETwiEgSF/oJKunoHaT/SF3cpIiLTTqGvL3NFJCDBh/6yhuFhm/oyV0SSL/jQr6/IUlVarCt9EQlC8KFvZpzXoBE8IhKG4EMfYNmCSo3gEZEgKPTJf5l76OgAB3v64y5FRGRaKfTJj9UH3YNHRJJPoU9+rD5A636N4BGRZFPoAw3zslRmi/RlrogknkKfaATPWZVs39MVdykiItNKoR85f3EVW3d3MTiUi7sUEZFpo9CPrGiq5tjAEK/oy1wRSTCFfmRFUzUAm3cdjrUOEZHppNCPnF1bRk1ZMZt3HYq7FBGRaaPQj5gZFzRV8+tdnXGXIiIybU4b+ma2zsz2m9nWgrZaM3vUzHZE7zVRu5nZ7WbWamZbzOzCgm3WROvvMLM107M7k7OiqZpX9h+hu28w7lJERKbFeK70fwRce1LbbcAGd28GNkTzANcBzdFrLXAH5E8SwJeAS4CLgS8NnyhmkxVN1bjDlrbDcZciIjItThv67v400HFS8w3A3dH03cCHC9rv8bxfAtVm1ghcAzzq7h3ufgh4lDefSGJ3weJqAHXxiEhinWmffoO774mm9wIN0fQiYFfBem1R21jtb2Jma82sxcxa2tvbz7C8M1NTnmHJ/DJ9mSsiiTXpL3I9fz/iKbsnsbvf6e6r3H1VfX39VH3suK1oqtawTRFJrDMN/X1Rtw3R+/6ofTfQVLDe4qhtrPZZ54KmavZ19bG3szfuUkREptyZhv56YHgEzhrgoYL2T0SjeC4FOqNuoEeAq82sJvoC9+qobdY58SMtdfGISPKMZ8jmvcCzwFvNrM3Mbga+AnzQzHYAvxfNAzwMvAa0At8H/hTA3TuAvwY2Rq+/itpmneUL51GcNn6lLh4RSaCi063g7qvHWPSBUdZ14JYxPmcdsG5C1cUgW5RmeeM8fvX64bhLERGZcvpF7iguPXc+v3r9ED36kZaIJIxCfxRXnlfPwJDz7KsH4y5FRGRKKfRHseqcWsoyaZ56ZWZ/JyAiMt0U+qPIFKW47Nw6nnxlP/mvKUREkkGhP4Yr31rPro5j7Dx4NO5SRESmjEJ/DFc2538N/NTL+0+zpojI3KHQH8PZ88t4S125+vVFJFEU+qfwvvPqefa1g/QODMVdiojIlFDon8KVb62ndyDHxp2z8sfDIiITptA/hUuXzidTlOKpl9XFIyLJoNA/hdJMmkuW1vL4Sxq6KSLJoNA/jQ+9q5HXDvTw6zY9TUtE5j6F/mlcf34j2aIUP9nUFncpIiKTptA/jXklxVzzjrNY/+s36BvUKB4RmdsU+uPw0XcvpvPYAI9v1w+1RGRuU+iPwxXL6miYl+UBdfGIyByn0B+HdMr4yMrFPPlKO+1H+uIuR0TkjCn0x+kP372IoZzz0OZZ+Tx3EZFxUeiP07IFlVywuIoHNrVpzL6IzFkK/Qn4o0vO5qW9R/iXHQfiLkVE5Iwo9CfgIysX01hVwnefaI27FBGRM6LQn4BMUYr/+t638PxvO3QTNhGZkxT6E7T64rOZX57hu4/ral9E5h6F/gSVZtL85yuW8tQr7WzdrfvxiMjcotA/Ax9/zzlUlhTxPfXti8gco9A/A/NKivnkZUv45617eeH1Q3GXIyIybgr9M/SpK8+lYV6W//XQVoZyGrcvInPDpELfzHaa2W/MbLOZtURttWb2qJntiN5ronYzs9vNrNXMtpjZhVOxA3GpyBbx5x9aztbdXdz7/OtxlyMiMi5TcaX/79x9hbuviuZvAza4ezOwIZoHuA5ojl5rgTum4G/H6g/Ob+Q9b5nP/37kZTp6+uMuR0TktKaje+cG4O5o+m7gwwXt93jeL4FqM2uchr8/Y8yMv7zhHfT0DfLVn78UdzkiIqc12dB34BdmtsnM1kZtDe6+J5reCzRE04uAXQXbtkVtI5jZWjNrMbOW9vbZ/0Dy8xoquenyJdy3cRfP6PYMIjLLTTb0r3D3C8l33dxiZu8rXOj5O5NN6FtOd7/T3Ve5+6r6+vpJljczPvfB8zi3vpzP3b+ZA9269bKIzF6TCn133x297wd+ClwM7Bvutonehx83tRtoKth8cdQ255VlivjuH11I57EBPn//r8lpNI+IzFJnHPpmVm5mlcPTwNXAVmA9sCZabQ3wUDS9HvhENIrnUqCzoBtoznt74zz+54fezlOvtHPXM7+NuxwRkVEVTWLbBuCnZjb8OX/v7j83s43A/WZ2M/A74GPR+g8D1wOtwFHgpkn87Vnp45eewzM7DvDVR17i/MVVXPKW+XGXJCIygs3mB4KsWrXKW1pa4i5jQjqPDvAf7vhXDnT385M/uYxlCyriLklEAmNmmwqG0Y+gX+ROsaqyYn5008UUp41P/vB5PVNXRGYVhf40aKot4641F3Ggu4+b795IT99g3CWJiAAK/WlzQVM131l9IVt3d3LTDxX8IjI7KPSn0QeXN/DtG1ey6fVDfPKHz9Ot4BeRmCn0p9kfXLCQ229cyQuvH+aT657nSO9A3CWJSMAU+jPgQ+c38p3VK9m86zD/8f8+y57OY3GXJCKBUujPkOvf1cgPb7qItkPH+Mj3/o1tb3TFXZKIBEihP4Pe21zPP376PQB87G+fZcP2fTFXJCKhUejPsLc3zuOnt1zGkroybr67ha898rKevCUiM0ahH4PGqlIe+PRl3HhRE999opVPrHtOd+cUkRmh0I9JSXGar3z0fL760fNp2XmIa7/1NI9tU3ePiEwvhX7MPnZRE+tvvYL6yhL+yz0tfOHBLRrPLyLTRqE/C7z1rEr+6ZbL+PSV53Lfxl1c/Y2n+MWLe+MuS0QSSKE/S2SL0tx23dt44NOXUVlSzNq/28Tae1rYfVhj+kVk6ij0Z5l3n1PDz/7sCv7HtW/j6R3tvP9rT/L1X7yse/eIyJRQ6M9CxekUf3LVuTz2367kmnecxXceb+Wqrz3J3z/3OgNDubjLE5E5TKE/iy2uKeP21St58E8vo6mmlC/+9De8/+tPcn/LLgYV/iJyBvTkrDnC3Xny5Xa++dgrbGnrZHFNKTddvpT/dFETFdnJPPVSRJLmVE/OUujPMe7Ohu37+dunX2XjzkNUlhRx40VN/PEl57Ckrjzu8kRkFlDoJ9TmXYf5/r+8xs+37mUo57y3uY7VF5/N+9+2gJLidNzliUhMFPoJt6+rl3/YuIt7n3+dPZ29VGSLuHp5A79/QSOXnVunE4BIYBT6gRgcyvHsawf5f79+g3/eupcjvYOUFqe5fFkdH3j7Aq5YVkdTbVncZYrINFPoB6hvcIhnXz3I4y/tZ8P2/cd/5HXO/DIuO7eOi5fWsOqcWhbXlGJmMVcrIlNJoR84d6d1fzfPtB7gX1sP8MvXOo7f36e+MssFi6t456Iq3rWoiuUL53HWvBKdCETmsFOFvsb6BcDMaG6opLmhkpsuX8pQznl57xE2/a6DF14/zJa2w2x4aT/D5//qsmLedlYl5zVUsmxBBcvqK1haX05DZQmplE4GInOZQj9A6ZSxfOE8li+cx8fzD/Kiu2+Q7Xu6jr+27TnCgy/sHnHHz5LiFOfUltNUW0ZTbSlNNWUsrC6lsaqExuoS6sqzOimIzHIKfQGgIlvERUtquWhJ7fE2d2dfVx+t+7vZebCHnQd62Hmwh10dR/m3Vw9wtH9oxGcUpYz6yiwLKrPUV2aZX55lfkWG+RVZ6ioy1FVkqS3PUF1WTHVphpLilLqRRGbYjIe+mV0LfBtIAz9w96/MdA0yPmbGWVUlnFVVwhXNdSOWuTuHjg7wxuFj7OnsZU/nMfZ19bKvq499Xb20HTrGlrZODvb0j/k4yEw6xbzSYqpKi5hXWkxlSTGV2SIqS4qoyBbl50uKKM+mKcvk30uLiyjLpCnLpCkpTlMavWeLUhSlTCcRkdOY0dA3szTwPeCDQBuw0czWu/u2maxDJs/MqC3PUFue4Z2LqsZcL5dzunoHONDdz8HuPjp6+jl8bIDDRwc4fKyfrmODdB0boKt3gM5jA7QdOsqR3kG6ewc5NjA05ueOJmX5W1Rni1OUFp84GWSL02TTKTJF+Vdx2ihOp8ikUxRF08XpVP7EkTaKUvn1ilJGOmX59/SJ+eK0kbKCZan8slTKSJuRSkE6Wm7Re8o4vk3KovnCaTMseh+eHp43KGjLr3/8nXx7/picmB/ebngbkWEzfaV/MdDq7q8BmNl9wA2AQj+hUimjuixDdVmGZQsqJrTt4FCO7r5BjvYPcbR/kO6+/Pux/iF6+ofoHci/jvUP0T+Yo28wR9/gEH2DuXz7QI6+gRPzR/sHOXwsR/9gjsEhZyCXY2DQGRjK0T+UY2Aox8CQJ/ZB9fmTQv4kcPyEQL7RRqxjI9aFaPnwyWX4M05a5/jfiT5neM4KthtZix2fHrntm09SI9YZsb6N0V64/tgnvTGXjLFgoqfPyZxw3944j++sXnnG249lpkN/EbCrYL4NuKRwBTNbC6wFOPvss2euMpl1itKp6IQxs383l3MGc85gLsdQzo+fIIanB3P5E0PO8/M5H27LMZTj+LKcn1gvl4Mhd3I5J+ccX+7R9FDOcfLdZkM5cE4sc8+3D28Hw9uTXxatS7R9vq1g2/yC6PNPrF84n9+Y4zUc3+6kdUb8nYL5YSNqGbH8xIruBZ9zcnvBdOFnFsyMNknh0POR7YxprEVjDWOf8KXAJK8dmmpKJ/cBY5h1X+S6+53AnZAfpx9zORKgVMrIpIyM7jwuCTTT/6p3A00F84ujNhERmQEzHfobgWYzW2pmGeBGYP0M1yAiEqwZ7d5x90EzuxV4hPyQzXXu/uJM1iAiErIZ79N394eBh2f674qIiJ6RKyISFIW+iEhAFPoiIgFR6IuIBGRWP0TFzNqB303iI+qAA1NUzlwR4j5DmPsd4j5DmPs90X0+x93rR1swq0N/ssysZaynxyRViPsMYe53iPsMYe73VO6zundERAKi0BcRCUjSQ//OuAuIQYj7DGHud4j7DGHu95Ttc6L79EVEZKSkX+mLiEgBhb6ISEASGfpmdq2ZvWxmrWZ2W9z1TBczazKzJ8xsm5m9aGafidprzexRM9sRvdfEXetUM7O0mf3KzH4WzS81s+eiY/4P0a27E8XMqs3sATN7ycy2m9l7kn6szexz0b/trWZ2r5mVJPFYm9k6M9tvZlsL2kY9tpZ3e7T/W8zswon8rcSFfsHD168DlgOrzWx5vFVNm0Hg8+6+HLgUuCXa19uADe7eDGyI5pPmM8D2gvm/Ab7p7suAQ8DNsVQ1vb4N/Nzd3wZcQH7/E3uszWwR8GfAKnd/J/nbsd9IMo/1j4BrT2ob69heBzRHr7XAHRP5Q4kLfQoevu7u/cDww9cTx933uPsL0fQR8iGwiPz+3h2tdjfw4VgKnCZmthj4EPCDaN6A9wMPRKskcZ+rgPcBdwG4e7+7Hybhx5r87d9LzawIKAP2kMBj7e5PAx0nNY91bG8A7vG8XwLVZtY43r+VxNAf7eHri2KqZcaY2RJgJfAc0ODue6JFe4GGuOqaJt8C/juQi+bnA4fdfTCaT+IxXwq0Az+MurV+YGblJPhYu/tu4GvA6+TDvhPYRPKP9bCxju2kMi6JoR8cM6sAfgJ81t27Cpd5fkxuYsblmtnvA/vdfVPctcywIuBC4A53Xwn0cFJXTgKPdQ35q9qlwEKgnDd3gQRhKo9tEkM/qIevm1kx+cD/sbs/GDXvG/7vXvS+P676psHlwL83s53ku+7eT76vuzrqAoBkHvM2oM3dn4vmHyB/Ekjysf494Lfu3u7uA8CD5I9/0o/1sLGO7aQyLomhH8zD16O+7LuA7e7+jYJF64E10fQa4KGZrm26uPsX3H2xuy8hf2wfd/c/Bp4A/jBaLVH7DODue4FdZvbWqOkDwDYSfKzJd+tcamZl0b/14X1O9LEuMNaxXQ98IhrFcynQWdANdHrunrgXcD3wCvAq8Odx1zON+3kF+f/ybQE2R6/ryfdxbwB2AI8BtXHXOk37fxXws2j6LcDzQCvwj0A27vqmYX9XAC3R8f4noCbpxxr4S+AlYCvwd0A2iccauJf89xYD5P9Xd/NYxxYw8iMUXwV+Q35007j/lm7DICISkCR274iIyBgU+iIiAVHoi4gERKEvIhIQhb6ISEAU+iIiAVHoi4gE5P8DFNW8NJfoz78AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_values_val = range(len(val_loss))\n",
    "plt.plot(x_values_val, val_loss, label='val_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "37a5b406",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f8ce72b7490>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "min_length = min(len(train_loss), len(val_loss))\n",
    "list1 = train_loss[:min_length]\n",
    "list2 = val_loss[:min_length]\n",
    "\n",
    "# Create x-axis values\n",
    "x_values = range(min_length)\n",
    "\n",
    "# Plot the lists\n",
    "plt.plot(x_values, list1, label='Train_loss')\n",
    "plt.plot(x_values, list2, label='Val_loss')\n",
    "\n",
    "# Adding labels and title\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Train Loss and Val_Loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a63ca9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5272964",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955bd02f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2512b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0208a83",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
