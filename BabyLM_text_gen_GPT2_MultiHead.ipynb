{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49641a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "## reference https://huggingface.co/learn/nlp-course/en/chapter7/6?fw=pt#training-a-causal-language-model-from-scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9932cfe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: datasets in /opt/conda/lib/python3.8/site-packages (2.20.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.8/site-packages (from datasets) (21.3)\n",
      "Requirement already satisfied: fsspec[http]<=2024.5.0,>=2023.1.0 in /opt/conda/lib/python3.8/site-packages (from datasets) (2024.5.0)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.8/site-packages (from datasets) (1.3.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.8/site-packages (from datasets) (1.21.4)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.2 in /opt/conda/lib/python3.8/site-packages (from datasets) (0.23.4)\n",
      "Requirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.8/site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.8/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.8/site-packages (from datasets) (6.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.8/site-packages (from datasets) (16.1.0)\n",
      "Requirement already satisfied: requests>=2.32.2 in /opt/conda/lib/python3.8/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.8/site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.8/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /opt/conda/lib/python3.8/site-packages (from datasets) (4.66.4)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.8/site-packages (from datasets) (3.9.5)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.8/site-packages (from datasets) (3.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets) (21.2.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.8/site-packages (from huggingface-hub>=0.21.2->datasets) (4.12.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging->datasets) (3.0.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests>=2.32.2->datasets) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests>=2.32.2->datasets) (1.26.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests>=2.32.2->datasets) (3.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.8/site-packages (from requests>=2.32.2->datasets) (2.0.8)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.8/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.8/site-packages (from pandas->datasets) (2021.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: jupyter in /opt/conda/lib/python3.8/site-packages (1.0.0)\n",
      "Requirement already satisfied: ipywidgets in /opt/conda/lib/python3.8/site-packages (8.1.3)\n",
      "Requirement already satisfied: ipykernel in /opt/conda/lib/python3.8/site-packages (from jupyter) (6.29.5)\n",
      "Requirement already satisfied: qtconsole in /opt/conda/lib/python3.8/site-packages (from jupyter) (5.5.2)\n",
      "Requirement already satisfied: notebook in /opt/conda/lib/python3.8/site-packages (from jupyter) (6.4.1)\n",
      "Requirement already satisfied: jupyter-console in /opt/conda/lib/python3.8/site-packages (from jupyter) (6.6.3)\n",
      "Requirement already satisfied: nbconvert in /opt/conda/lib/python3.8/site-packages (from jupyter) (6.3.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (5.14.3)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.11 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (4.0.11)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (7.30.0)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.11 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (3.0.11)\n",
      "Requirement already satisfied: comm>=0.1.3 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: matplotlib-inline in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.3)\n",
      "Requirement already satisfied: decorator in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (5.1.0)\n",
      "Requirement already satisfied: setuptools>=18.5 in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (59.4.0)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.47)\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (4.8.0)\n",
      "Requirement already satisfied: backcall in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: pygments in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (2.10.0)\n",
      "Requirement already satisfied: pickleshare in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.7.5)\n",
      "Requirement already satisfied: jedi>=0.16 in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.18.1)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /opt/conda/lib/python3.8/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.8/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.8/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=6.1.0->ipywidgets) (0.2.5)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in /opt/conda/lib/python3.8/site-packages (from ipykernel->jupyter) (1.8.2)\n",
      "Requirement already satisfied: nest-asyncio in /opt/conda/lib/python3.8/site-packages (from ipykernel->jupyter) (1.5.4)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in /opt/conda/lib/python3.8/site-packages (from ipykernel->jupyter) (7.1.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.8/site-packages (from ipykernel->jupyter) (21.3)\n",
      "Requirement already satisfied: pyzmq>=24 in /opt/conda/lib/python3.8/site-packages (from ipykernel->jupyter) (26.0.3)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.8/site-packages (from ipykernel->jupyter) (5.8.0)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /opt/conda/lib/python3.8/site-packages (from ipykernel->jupyter) (5.7.2)\n",
      "Requirement already satisfied: tornado>=6.1 in /opt/conda/lib/python3.8/site-packages (from ipykernel->jupyter) (6.1)\n",
      "Requirement already satisfied: entrypoints in /opt/conda/lib/python3.8/site-packages (from jupyter-client>=6.1.12->ipykernel->jupyter) (0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /opt/conda/lib/python3.8/site-packages (from jupyter-client>=6.1.12->ipykernel->jupyter) (2.8.2)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /opt/conda/lib/python3.8/site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel->jupyter) (4.2.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.8/site-packages (from python-dateutil>=2.1->jupyter-client>=6.1.12->ipykernel->jupyter) (1.16.0)\n",
      "Requirement already satisfied: nbformat>=4.4 in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (5.1.3)\n",
      "Requirement already satisfied: testpath in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (0.5.0)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (1.5.0)\n",
      "Requirement already satisfied: bleach in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (4.1.0)\n",
      "Requirement already satisfied: jinja2>=2.4 in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (3.0.3)\n",
      "Requirement already satisfied: defusedxml in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (0.7.1)\n",
      "Requirement already satisfied: nbclient<0.6.0,>=0.5.0 in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (0.5.9)\n",
      "Requirement already satisfied: jupyterlab-pygments in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (0.1.2)\n",
      "Requirement already satisfied: mistune<2,>=0.8.1 in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (0.8.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.8/site-packages (from jinja2>=2.4->nbconvert->jupyter) (2.0.1)\n",
      "Requirement already satisfied: ipython-genutils in /opt/conda/lib/python3.8/site-packages (from nbformat>=4.4->nbconvert->jupyter) (0.2.0)\n",
      "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /opt/conda/lib/python3.8/site-packages (from nbformat>=4.4->nbconvert->jupyter) (4.2.1)\n",
      "Requirement already satisfied: importlib-resources>=1.4.0 in /opt/conda/lib/python3.8/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.4->nbconvert->jupyter) (5.4.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /opt/conda/lib/python3.8/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.4->nbconvert->jupyter) (21.2.0)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /opt/conda/lib/python3.8/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.4->nbconvert->jupyter) (0.18.0)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /opt/conda/lib/python3.8/site-packages (from importlib-resources>=1.4.0->jsonschema!=2.5.0,>=2.4->nbformat>=4.4->nbconvert->jupyter) (3.6.0)\n",
      "Requirement already satisfied: webencodings in /opt/conda/lib/python3.8/site-packages (from bleach->nbconvert->jupyter) (0.5.1)\n",
      "Requirement already satisfied: terminado>=0.8.3 in /opt/conda/lib/python3.8/site-packages (from notebook->jupyter) (0.12.1)\n",
      "Requirement already satisfied: Send2Trash>=1.5.0 in /opt/conda/lib/python3.8/site-packages (from notebook->jupyter) (1.8.0)\n",
      "Requirement already satisfied: prometheus-client in /opt/conda/lib/python3.8/site-packages (from notebook->jupyter) (0.12.0)\n",
      "Requirement already satisfied: argon2-cffi in /opt/conda/lib/python3.8/site-packages (from notebook->jupyter) (21.1.0)\n",
      "Requirement already satisfied: cffi>=1.0.0 in /opt/conda/lib/python3.8/site-packages (from argon2-cffi->notebook->jupyter) (1.15.0)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.8/site-packages (from cffi>=1.0.0->argon2-cffi->notebook->jupyter) (2.21)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging->ipykernel->jupyter) (3.0.6)\n",
      "Requirement already satisfied: qtpy>=2.4.0 in /opt/conda/lib/python3.8/site-packages (from qtconsole->jupyter) (2.4.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#installing some libraries\n",
    "!pip install datasets\n",
    "!pip install --upgrade jupyter ipywidgets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "452b4561",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, transformers\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import PreTrainedModel, PretrainedConfig\n",
    "from transformers import AutoModel, AutoConfig,AutoModelForCausalLM,AutoModelForTokenClassification\n",
    "\n",
    "import json\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "#from datasets import load_dataset\n",
    "import pandas as pd, numpy as np , random\n",
    "from torch import cuda\n",
    "import datetime\n",
    "import warnings,itertools\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "# Ignore all warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "#pip install transformers bitsandbytes>=0.39.0 -q\n",
    "import zipfile,logging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66aa4cc3",
   "metadata": {},
   "source": [
    "## Global Variables for training. Use the cell below to determing the following:\n",
    "- training batch size\n",
    "- Training with random weights or not\n",
    "- The replacement strategy for replacing the token categoty at runtime\n",
    "- The # of epochs for training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ad4f9b",
   "metadata": {},
   "source": [
    "## Here is the description of the various tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b2bfc5",
   "metadata": {},
   "source": [
    "- O\tOutside of a named entity\n",
    "- B-MISC\tBeginning of a miscellaneous entity right after another miscellaneous entity\n",
    "- I-MISC\tMiscellaneous entity\n",
    "- B-PER\tBeginning of a person’s name right after another person’s name\n",
    "- I-PER\tPerson’s name\n",
    "- B-ORG\tBeginning of an organization right after another organization\n",
    "- I-ORG\torganization\n",
    "- B-LOC\tBeginning of a location right after another location\n",
    "- I-LOC\tLocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19e69d02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model\n"
     ]
    }
   ],
   "source": [
    "#global params for training\n",
    "# hardcoding the T dimension to 512 as the BERT model processes sequences of length 512\n",
    "B,T = 128,512\n",
    "epoch = 100\n",
    "random_init_wts = True\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda:0')\n",
    "    print(device)\n",
    "else:\n",
    "    device = 'cpu'\n",
    "#print(device)\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "#os.environ[\"MKL_DEBUG_CPU_TYPE\"] = \"5\"\n",
    "global_tr_loss = torch.inf\n",
    "global_val_loss = torch.inf\n",
    "ner_model = AutoModelForTokenClassification.from_pretrained(\"dslim/bert-base-NER\")\n",
    "#print(global_tr_loss)\n",
    "model_path = os.path.join(\"model\")\n",
    "print(model_path)\n",
    "value_to_find = 6\n",
    "pre_train = True\n",
    "num_heads = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b80cab3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./data/unzip_text_10M'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "directory = os.path.join('.','data','unzip_text_10M')  # Replace with your directory path\n",
    "directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a6a2660",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_text(directory):\n",
    "    \"\"\"This function looks at the specified directory and unzips the files, reads the text provided in the file and finallly concatenates all the text in a single flattended list\"\"\"\n",
    "    directory = os.path.join('.','data','unzip_text_10M',str(directory))  # Replace with your directory path\n",
    "    print(f\"directory :{directory}\")\n",
    "    # List all files in the directory\n",
    "    files = [f for f in os.listdir(directory) if os.path.isfile(os.path.join(directory, f))]\n",
    "    print(f\"files:{files}\")\n",
    "    text_content = []\n",
    "    # Read each file\n",
    "    total_lines = 0\n",
    "    for filenum,filename in enumerate(files):\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            text = file.read()\n",
    "            text_content.append(text)\n",
    "            print(f\"the file:{filename} has been appeneded to the uber list and its length is {len(text_content)} \")\n",
    "            \n",
    "    \n",
    "    flattened_list = ''.join(text_content)\n",
    "    assert (len(flattened_list) == total_lines , f\"Expected {len(flattened_list)} to be equal to {total_lines}\" )\n",
    "    \n",
    "    return flattened_list\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1700611f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "directory :./data/unzip_text_10M/train_10M\n",
      "files:['switchboard.train', 'simple_wiki.train', 'open_subtitles.train', 'gutenberg.train', 'childes.train', 'bnc_spoken.train']\n",
      "the file:switchboard.train has been appeneded to the uber list and its length is 1 \n",
      "the file:simple_wiki.train has been appeneded to the uber list and its length is 2 \n",
      "the file:open_subtitles.train has been appeneded to the uber list and its length is 3 \n",
      "the file:gutenberg.train has been appeneded to the uber list and its length is 4 \n",
      "the file:childes.train has been appeneded to the uber list and its length is 5 \n",
      "the file:bnc_spoken.train has been appeneded to the uber list and its length is 6 \n"
     ]
    }
   ],
   "source": [
    "train_list = read_text(\"train_10M\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "656342d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54215049"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c90488c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "827\n"
     ]
    }
   ],
   "source": [
    "chunks = len(train_list)//(B*T)\n",
    "print(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b54a0053",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15756896 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertLMHeadModel, BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"dslim/bert-base-NER\",return_tensors = \"pt\" , truncate = True, return_overflowing_tokens=True , padding = False,)\n",
    "enc_train = tokenizer(train_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "83391cb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.440718844625236"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comp_ratio = len(train_list)/len(enc_train['input_ids'])\n",
    "comp_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2c3df73a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "directory :./data/unzip_text_10M/dev\n",
      "files:['switchboard.dev', 'simple_wiki.dev', 'open_subtitles.dev', 'gutenberg.dev', 'childes.dev', 'bnc_spoken.dev']\n",
      "the file:switchboard.dev has been appeneded to the uber list and its length is 1 \n",
      "the file:simple_wiki.dev has been appeneded to the uber list and its length is 2 \n",
      "the file:open_subtitles.dev has been appeneded to the uber list and its length is 3 \n",
      "the file:gutenberg.dev has been appeneded to the uber list and its length is 4 \n",
      "the file:childes.dev has been appeneded to the uber list and its length is 5 \n",
      "the file:bnc_spoken.dev has been appeneded to the uber list and its length is 6 \n"
     ]
    }
   ],
   "source": [
    "val_list = read_text(\"dev\")\n",
    "enc_val = tokenizer(val_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a858048",
   "metadata": {},
   "source": [
    "### our strategy for bacthed training is to divide the tokenized text into chunks. The idea is to store these chunks in a dataframe so that each row stores a B*T number of tokens.At traning time, we'll use a batch size of 1 and feed each row (B*T) to the model to simulate batched training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e63af1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df_from_list(enc , B= B, T = T, val= False):\n",
    "    \"\"\"This function splits the tokeinzed text into chunks of B*T where each row in the dataset becomes a batch for the model\"\"\"\n",
    "    chunk_size = B*T\n",
    "        \n",
    "    long_list_inp = enc['input_ids']\n",
    "    long_list_attention = enc['attention_mask']\n",
    "\n",
    "    # Step 3: Split the list into chunks and pad the last chunk if necessary\n",
    "    chunks_inp = [long_list_inp[i:i + chunk_size] for i in range(0, len(long_list_inp), chunk_size)]\n",
    "    chunks_att = [long_list_attention[i:i + chunk_size] for i in range(0, len(long_list_attention), chunk_size)]\n",
    "    df = pd.DataFrame({'input_ids': chunks_inp,'attention_mask':chunks_att})\n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ba7bc305",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the dataframe is = 241\n"
     ]
    }
   ],
   "source": [
    "df_train_temp = get_df_from_list(enc_train)\n",
    "# Display the DataFrame\n",
    "df_train_temp.head()\n",
    "print(f\"Length of the dataframe is = {len(df_train_temp)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b065ae6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the VALDATION dataframe is = 245\n"
     ]
    }
   ],
   "source": [
    "df_val_temp = get_df_from_list(enc_val)\n",
    "# Display the DataFrame\n",
    "df_val_temp.head()\n",
    "print(f\"Length of the VALDATION dataframe is = {len(df_val_temp)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c70341ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_df(df,eos_char = tokenizer.pad_token_id,B = B, T = T,val = False):\n",
    "    \"\"\"This function pads the rows with tokeizer's attribute to ensure that all the rows of the dataframe are of the same length\n",
    "    Using this approach prevents any unnessary padding and a custom collate funtion for model training and reduces memory footrprint.\"\"\"\n",
    "    if val:\n",
    "        B = 2*B\n",
    "    for ind,row in df.iterrows():\n",
    "        if len(row['input_ids']) != B*T :\n",
    "            print(f\"row = {ind} and input_id length = {len(row['input_ids'])}\")\n",
    "            print(f\"row = {ind} and attention length = {len(row['attention_mask'])}\")\n",
    "            pad_len = B*T - len(row['input_ids'])\n",
    "            print(f\"padding the row index {ind} with {pad_len} character\")\n",
    "            row['input_ids'] = row['input_ids']+ [eos_char] * pad_len\n",
    "            #attention mask should be padded to 0\n",
    "            row['attention_mask'] = row['attention_mask']+ [0] * pad_len\n",
    "            print(\"#### POST CONCAT####\")\n",
    "            print(f\"row = {ind} and input_id length = {len(row['input_ids'])}\")\n",
    "            print(f\"row = {ind} and attention length ={len(row['attention_mask'])}\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def verify_len(df,val = False, B= B):\n",
    "    row_ind = []\n",
    "    if val:\n",
    "        B = 2*B\n",
    "    for ind,row in df.iterrows():\n",
    "        if len(row['input_ids']) != B*T :\n",
    "            row_ind.append(ind)\n",
    "        else:\n",
    "            continue\n",
    "    if len(row_ind) !=0:\n",
    "        print(\"CONCATENATION Did not work\")\n",
    "    else:\n",
    "        print(\"CONCATENATION worked\")\n",
    "        \n",
    "            \n",
    "        \n",
    "    \n",
    "        \n",
    "        \n",
    "   \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "519c8f2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row = 240 and input_id length = 28256\n",
      "row = 240 and attention length = 28256\n",
      "padding the row index 240 with 37280 character\n",
      "#### POST CONCAT####\n",
      "row = 240 and input_id length = 65536\n",
      "row = 240 and attention length =65536\n",
      "CONCATENATION worked\n"
     ]
    }
   ],
   "source": [
    "df_train  = pad_df(df_train_temp)\n",
    "verify_len(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3d07b5af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row = 244 and input_id length = 32623\n",
      "row = 244 and attention length = 32623\n",
      "padding the row index 244 with 32913 character\n",
      "#### POST CONCAT####\n",
      "row = 244 and input_id length = 65536\n",
      "row = 244 and attention length =65536\n",
      "CONCATENATION worked\n"
     ]
    }
   ],
   "source": [
    "df_val  = pad_df(df_val_temp)\n",
    "verify_len(df_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f8d06913",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_ids</th>\n",
       "      <th>attention_mask</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[101, 138, 131, 146, 112, 182, 1612, 1152, 113...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[117, 1681, 11764, 5558, 119, 139, 131, 11205,...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[117, 138, 131, 1103, 2548, 117, 138, 131, 146...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[1107, 20119, 119, 138, 131, 2048, 117, 1674, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[1107, 1103, 8851, 16174, 10840, 18847, 1105, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           input_ids  \\\n",
       "0  [101, 138, 131, 146, 112, 182, 1612, 1152, 113...   \n",
       "1  [117, 1681, 11764, 5558, 119, 139, 131, 11205,...   \n",
       "2  [117, 138, 131, 1103, 2548, 117, 138, 131, 146...   \n",
       "3  [1107, 20119, 119, 138, 131, 2048, 117, 1674, ...   \n",
       "4  [1107, 1103, 8851, 16174, 10840, 18847, 1105, ...   \n",
       "\n",
       "                                      attention_mask  \n",
       "0  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "1  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "2  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "3  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "4  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61d2588",
   "metadata": {},
   "source": [
    "### In the cells below we classify all the tokens in the training data with specific NER tags. The idea is to create a repository of tokens that can be used to replace tokens during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3c33a563",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keys_from_value(dictionary, value):\n",
    "    for key, val in dictionary.items():\n",
    "        if val == value:\n",
    "            return key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d68e32db",
   "metadata": {},
   "outputs": [],
   "source": [
    "uber_token_list_input = np.concatenate(df_train['input_ids']).tolist()\n",
    "uber_token_list_att = np.concatenate(df_train['attention_mask']).tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d306138",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0beb3297",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_token(df):\n",
    "    \"\"\"This function processes the tokenized dataframe and produces a dictionay of classified tokens.\n",
    "    Each key in the dictionary corresponds to a class of NER supported by the NER model and the values are the input_ids for the tokens.\"\"\"\n",
    "    \n",
    "    ner_model = AutoModelForTokenClassification.from_pretrained(\"dslim/bert-base-NER\")\n",
    "    ner_model = torch.compile(ner_model)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    ner_model = ner_model.to(device)\n",
    "    label_list = ner_model.config.id2label\n",
    "    id_list = ner_model.config.label2id\n",
    "    #print(f\"id_list = {id_list}\")\n",
    "    classified_tokens = {value: [] for value in label_list.values()}\n",
    "    #print(f\"at the very beginnning the classified_tokens is {classified_tokens}\")\n",
    "    for ind,row in df.iterrows():\n",
    "        #print(f\"ind = {ind}| row = {row}\")\n",
    "        input_ids = row['input_ids']\n",
    "        attention_mask = row['attention_mask']\n",
    "        input_ids = torch.tensor(input_ids).view(B,T)\n",
    "        attention_mask = torch.tensor(attention_mask).view(B,T)\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        input_ids = torch.squeeze(input_ids,dim =0)\n",
    "        attention_mask = torch.squeeze(attention_mask,dim = 0)\n",
    "        #print(f\"before performing NER shapes-> input_ids ={input_ids.shape}| attention_mask = {attention_mask.shape}\")\n",
    "        max_id = torch.max(input_ids)\n",
    "        #print(f\"Max token ID: {max_id}, Vocab size: {ner_model.config.vocab_size}\")\n",
    "        assert max_id < ner_model.config.vocab_size, \"Token ID out of range\"\n",
    "        #print(f\"shape of input going into the model |input_ids = {input_ids.shape}| attention_mask = {attention_mask.shape}\")\n",
    "        with torch.no_grad():\n",
    "            outputs = ner_model(input_ids, attention_mask=attention_mask)\n",
    "            \n",
    "\n",
    "        # Get the predictions\n",
    "        predictions = torch.argmax(outputs.logits, dim=2)\n",
    "        #print(f\"predictions shape = {predictions.shape}\")\n",
    "        predictions = list(torch.squeeze(predictions.view(B*T), dim = 0))\n",
    "        #print(f\"post conversion to list . len of prediction = {len(predictions)}\")\n",
    "        \n",
    "        #print(f\"predictions are {predictions}\")\n",
    "        #handling the case where a single row of the dataframe may have duplicate tokens\n",
    "        list_of_token = list(input_ids.view(B*T))\n",
    "        #print(f\"length of list of tokens = {len(list_of_token)}\")\n",
    "        #print(f\"shape of pred = {predictions.shape }| shape of input = {list_of_token.shape}\")\n",
    "        counter = 0\n",
    "        for idx,elem in enumerate(list_of_token):\n",
    "            pred = predictions[idx].item()\n",
    "            #print(f\"for token # {idx} prediction is {pred}| token = {elem.item()}\")\n",
    "            key = get_keys_from_value(id_list, pred)\n",
    "            #print(f\"This token {elem.item()} is being added to the key:{key}\")\n",
    "            classified_tokens[key].append(elem.item())\n",
    "            counter+=1\n",
    "            \n",
    "            \n",
    "        \n",
    "        #print(f\"processed row - {ind} and the classified toekn dict is - {classified_tokens}\")    \n",
    "    \n",
    "    #the dataframe has been processed, now remove duplicates from the classified tokens dict\n",
    "    #print(f\"before removing duplicates |counter = {counter}\")\n",
    "    len_ch = 0\n",
    "    for key,val in classified_tokens.items():\n",
    "        print(len(classified_tokens[key]))\n",
    "        len_ch+= len(classified_tokens[key])\n",
    "    #print(f\"before removing duplicates |len_ch = {len_ch}\")\n",
    "    \n",
    "    \n",
    "    assert len_ch == counter, \"something went wrong. TRY again\"\n",
    "\n",
    "          \n",
    "    \n",
    "    for key,val in classified_tokens.items():\n",
    "        classified_tokens[key] = list(set(val)) \n",
    "    ner_model.to(\"cpu\")\n",
    "    return classified_tokens\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5fa8fae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if pre_train == False:\n",
    "    classified_tokens = classify_token(df_train)\n",
    "    \n",
    "# classified tokens\n",
    "    with open('classified_tokens.json', 'w') as f:\n",
    "        json.dump(classified_tokens, f)\n",
    "else:\n",
    "    with open('classified_tokens.json', 'r') as f:\n",
    "        classified_tokens = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f1fae4e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27108\n",
      "354\n",
      "416\n",
      "730\n",
      "272\n",
      "934\n",
      "1793\n",
      "569\n",
      "219\n",
      "32395\n"
     ]
    }
   ],
   "source": [
    "uniq_token = 0\n",
    "for key,val in classified_tokens.items():\n",
    "    print(len(classified_tokens[key]))\n",
    "    uniq_token+=len(classified_tokens[key])\n",
    "\n",
    "print(uniq_token)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8770ffc4",
   "metadata": {},
   "source": [
    "## Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a84704f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cc97afec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n"
     ]
    }
   ],
   "source": [
    "# Test the tokenizer:\n",
    "model_name = \"bert-base-cased\"\n",
    "if random_init_wts:\n",
    "    config = AutoConfig.from_pretrained(model_name,vocab_size = 32768)\n",
    "    # Initialize the model with random weights\n",
    "    model = AutoModelForCausalLM.from_config(config)\n",
    "else:\n",
    "    #model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\"bert-base-cased\",is_decoder=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "faf580a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertWithCustomEmbeddings(BertLMHeadModel):\n",
    "    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None, inputs_embeds=None, encoder_hidden_states=None, encoder_attention_mask=None, labels=None, past_key_values=None, use_cache=None, output_attentions=None, output_hidden_states=None, return_dict=None):\n",
    "        if inputs_embeds is None and input_ids is None:\n",
    "            raise ValueError(\"You must pass either input_ids or inputs_embeds\")\n",
    "        \n",
    "        return super().forward(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            encoder_hidden_states=encoder_hidden_states,\n",
    "            encoder_attention_mask=encoder_attention_mask,\n",
    "            labels=labels,\n",
    "            past_key_values=past_key_values,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict\n",
    "        )\n",
    "\n",
    "# Instantiate the modified model\n",
    "custom_model = BertWithCustomEmbeddings.from_pretrained(model_name, is_decoder=True)\n",
    "model = torch.compile(custom_model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95165018",
   "metadata": {},
   "source": [
    "### Data loaders and Dataset for batched training. This is where we replace the tokens at runtime.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "568fe08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataset_pyt_train(Dataset):\n",
    "    def __init__(self, df, B = B, T = T,value_to_find = value_to_find, ner_model = ner_model,model = AutoModelForCausalLM.from_pretrained(model_name, is_decoder=True)):\n",
    "        self.df = df\n",
    "        print(f\"Value of B {B}\")\n",
    "    \"\"\"This is where we replace the tokens at runtime. The idea is to classify each token based on the same NER model and for an index that is even, if there is a token/set of tokens that belong to the specified category we replace it with the other tokens from the traiing data \"\"\"    \n",
    "    def __getitem__(self, idx):\n",
    "        id_list = ner_model.config.label2id\n",
    "        ner_label = [key for key, val in id_list.items() if val == value_to_find][0]                                \n",
    "        source_inp = self.df.iloc[idx]['input_ids']\n",
    "        source_att= torch.tensor(self.df.iloc[idx]['attention_mask'],dtype = torch.long)\n",
    "        #print(f\"ner_label = {ner_label}\")\n",
    "        ner_model.to(\"cuda\")\n",
    "        source_inp = torch.tensor(source_inp).to(\"cuda\").view(B,T)\n",
    "        source_att = torch.tensor(source_att).to(\"cuda\").view(B,T)\n",
    "        #print(f\"after view shape of the source_inp = {source_inp.shape}| source_att = {source_att.shape}\")\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = ner_model(input_ids = source_inp, attention_mask= source_att)\n",
    "\n",
    "        if idx %2 == 0:\n",
    "            # Get the predictions and \n",
    "            model.to(device)\n",
    "            predictions = torch.argmax(outputs.logits, dim=2)\n",
    "            #print(f\"predictions shape = {predictions.shape}\")\n",
    "            predictions = torch.squeeze(predictions, dim = 0)\n",
    "            #print(f\"predictions shape post squ= {predictions.shape}\")\n",
    "            #print(f\"inside even_index{idx} in data loader...\")\n",
    "\n",
    "            #code to check and replace the values\n",
    "            #first count occurences:\n",
    "            occurrences = torch.sum(predictions == value_to_find).item()\n",
    "            #print(f\"Number of occurrences of {value_to_find}: {occurrences}\")\n",
    "            #print(f\"indices.numel() = {indices.numel()}\")\n",
    "            if occurrences > 0:\n",
    "                #extract tokens from sample space\n",
    "                orig_tensor = source_inp.clone()\n",
    "                sample_token_space = classified_tokens[ner_label]\n",
    "                # this is wjere we implement multi-headed cloning\n",
    "                cloned_source_tensors = []\n",
    "                replacement_tok_ids_list = []\n",
    "                for i in range(num_heads):\n",
    "                    cloned_source_tensors.append(source_inp.clone())\n",
    "                    replacement_tok_ids = random.choices(sample_token_space ,k=  occurrences)\n",
    "                    replacement_tok_ids_list.append(replacement_tok_ids)\n",
    "                #print(f\"length of cloned_source_tensors = {len(cloned_source_tensors)}\")\n",
    "                for i in range(num_heads):\n",
    "                    mask = (predictions == value_to_find)\n",
    "                    cloned_source_tensors[i][mask] = torch.tensor(replacement_tok_ids_list[i],dtype = source_inp.dtype,device = source_inp.device)\n",
    "                #print(f\"post replacement - length of cloned_source_tensors = {len(cloned_source_tensors)}\")\n",
    "                \n",
    "                embed_tensor = []\n",
    "                for input_ids in cloned_source_tensors:\n",
    "                    inp = input_ids.to(device)\n",
    "                    #print(f\"inp device = {inp.device}\")\n",
    "                    embed_tensor.append(model.bert.embeddings(inp))\n",
    "                stack_tensor = torch.stack(embed_tensor).float()\n",
    "                mean_embedding = torch.mean(stack_tensor, dim = 0)\n",
    "                             \n",
    "                                \n",
    "                del embed_tensor\n",
    "                return mean_embedding,source_att,source_inp\n",
    "            else:\n",
    "                #print(f\"even index {idx} but did not find a toekn element to replace numel = 0\")\n",
    "                \n",
    "                inputs_embeds =   model.bert.embeddings(source_inp)\n",
    "                return inputs_embeds,source_att,source_inp\n",
    "\n",
    "\n",
    "        #this code executes when either the odd index is being processed or there is no element to replace it with.\n",
    "        inputs_embeds =   model.bert.embeddings(source_inp.to(\"cuda\"))\n",
    "        #print(f\" odd index {idx} inside dataloder|returning shape of input_id = {input_id.shape} | shape of attention  = {attention_mask.shape}\")\n",
    "        \n",
    "                \n",
    "                \n",
    "        return inputs_embeds,source_att,source_inp\n",
    "        \n",
    "    def __len__(self):\n",
    "        #return the length of the dataframe\n",
    "        return len(self.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "71d7cf35",
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataset_pyt_val(Dataset):\n",
    "    def __init__(self, df, B = B, T = T ,model = AutoModelForCausalLM.from_pretrained(model_name, is_decoder=True)):\n",
    "        self.df = df\n",
    "                                        \n",
    "    def __getitem__(self, idx):\n",
    "        model.to(device)\n",
    "        input_id_temp = torch.tensor(self.df.iloc[idx]['input_ids'],dtype = torch.long)\n",
    "        att_mask = torch.tensor(self.df.iloc[idx]['attention_mask'],dtype = torch.long)\n",
    "        labels =   input_id_temp.view(B,T).to(device)   \n",
    "        attention_mask = att_mask.view(B,T).to(device)\n",
    "        inputs_embeds = model.bert.embeddings(input_id_temp.to(device))\n",
    "        return inputs_embeds,attention_mask,labels\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        #return the length of the dataframe\n",
    "        return len(self.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "48afad59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value of B 128\n"
     ]
    }
   ],
   "source": [
    "train_dataset = dataset_pyt_train(df_train)\n",
    "val_dataset = dataset_pyt_val(df_val)\n",
    "\n",
    "train_loader = DataLoader(train_dataset,batch_size = 1, shuffle = True , num_workers = 0, pin_memory = False)\n",
    "val_loader = DataLoader(val_dataset,batch_size = 1, shuffle = True , num_workers = 0, pin_memory = False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9699d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5c900d62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the train loader is 241\n",
      "Length of the val loader is 245\n",
      "num_tokens= 15794176| unique tokens = 32395\n"
     ]
    }
   ],
   "source": [
    "print(f\"Length of the train loader is {len(train_loader)}\")\n",
    "print(f\"Length of the val loader is {len(val_loader)}\")\n",
    "print(f\"num_tokens= {B*T*len(train_loader)}| unique tokens = {uniq_token}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e20ae1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_file(log_message, model_name = model_name ,random_init_wts = random_init_wts,value_to_find = value_to_find ):\n",
    "    current_datetime = datetime.datetime.now()\n",
    "    # Extract date and time components\n",
    "    current_date = str(current_datetime.date())\n",
    "    log_file = model_name+\"_MULTI\" +'random_init_wts'+ '_'+str(random_init_wts)+'_' +'NER_'+ str(value_to_find)+\"_\" +current_date+'.log'\n",
    "    print(f\"*****LOGGING INFO IN {log_file}*********\")\n",
    "    filepath = os.path.join(\"model\",log_file)\n",
    "    logging.basicConfig(filename=filepath, \n",
    "                    filemode='a',  # Overwrite the log file each time\n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s', \n",
    "                    level=logging.DEBUG)\n",
    "    logger = logging.getLogger()\n",
    "    logger.info(log_message)\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "46c5c97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad\n",
    "def eval_model(val_loader, model, epoch , device = device,tokenizer = tokenizer):\n",
    "    global global_val_loss\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    e = epoch+1\n",
    "    val_loss_accum = 0.0\n",
    "    print(f\"inside validation data for epoch {e}\")\n",
    "    #y_hat_val_list = []\n",
    "    #y_val_list = []\n",
    "    for ind,(inputs_embed,attention_mask,labels) in enumerate(val_loader):\n",
    "        #print(f\"id_list{id_list}\")\n",
    "        inputs_embeds = torch.squeeze(inputs_embed.to(device=device, non_blocking=True),dim = 0)\n",
    "        attention_mask = torch.squeeze(attention_mask.to(device=device, non_blocking=True),dim = 0)\n",
    "        labels = torch.squeeze(labels.to(device=device, non_blocking=True),dim = 0)\n",
    "        print(f\"inputs_embeds device = {inputs_embeds.device}| attention_mask device = {attention_mask.device} |labels device = {labels.device}\")\n",
    "            \n",
    "        with autocast(dtype = torch.bfloat16):\n",
    "            model_output = model(inputs_embeds = inputs_embeds ,attention_mask = attention_mask ,labels = labels)\n",
    "            loss = model_output.loss\n",
    "        \n",
    "        val_loss_accum+=loss.detach().item()\n",
    "        del ids,att_mask,labels,loss\n",
    "    \n",
    "    if val_loss_accum < global_val_loss:\n",
    "        print(f\"Val loss has decreased -->reducing the global validation loss from {global_val_loss:.2f} to {val_loss_accum:.2f}\")\n",
    "        s1 = f\"Val loss has decreased -->reducing the global validation loss from {global_val_loss:.2f} to {val_loss_accum:.2f}\"\n",
    "        global_val_loss = val_loss_accum\n",
    "        print(f\" validation loss for epoch = {e} is {val_loss_accum:.4f}\")\n",
    "        s2 = f\" validation loss for epoch = {e} is {val_loss_accum:.4f}\"\n",
    "        #print metrics and save the model\n",
    "        #y_hat_val = torch.cat(y_hat_val_list)\n",
    "        #y_val = torch.cat(y_val_list)\n",
    "        #acc_val = accuracy_score(y_val.cpu().numpy(), y_hat_val.cpu().numpy())\n",
    "        #f1_val = f1_score(y_val.cpu().numpy(), y_hat_val.cpu().numpy(), average='micro')\n",
    "        print(f\" epoch= {e} :  val loss is {val_loss_accum:.4f} \")\n",
    "        s3 = f\" epoch= {e} :  val loss is {val_loss_accum:.4f} \"\n",
    "        #save the model\n",
    "        \n",
    "        # Get the current date and time\n",
    "        current_datetime = datetime.datetime.now()\n",
    "        # Extract date and time components\n",
    "        current_date = str(current_datetime.date())\n",
    "        current_time = str(current_datetime.time()).split('.')[0]\n",
    "        file_name = 'model'+ current_date+current_time+'.pth'\n",
    "        path = os.path.join(\"model\",file_name)\n",
    "        print(f\"saving the model {file_name}\")\n",
    "        s4 = f\"saving the model {file_name}\"\n",
    "        #torch.save(model.state_dict(), path)\n",
    "        model.save_pretrained(path)\n",
    "        tokenizer.save_pretrained(path)\n",
    "        log_message = s1+s2+s3+s4\n",
    "        write_file(log_message)\n",
    "        \n",
    "        #plot_confusion_matrix(y_val.cpu().numpy(), y_hat_val.cpu().numpy(), labels)\n",
    "    else:\n",
    "        print(f\"No improvement in validation loss-->epoch= {e} and global val loss is {global_val_loss:.2f}\")\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae80abe9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0c9b6d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_loader,val_loader,model,num_epoch = 100,device = device,tokenizer = tokenizer):\n",
    "    global global_tr_loss\n",
    "    model.train()\n",
    "    device = device\n",
    "    lr_custom = 5e-5\n",
    "    print(f\"inside train model. Device = {device}\")\n",
    "    #adding betas params per the paper\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.AdamW(params =  model.parameters(), lr= lr_custom,fused = True ,weight_decay = .1)\n",
    "    extra_train = .1*num_epoch\n",
    "    max_train_steps = int(num_epoch +extra_train )\n",
    "    import time\n",
    "    from transformers import get_linear_schedule_with_warmup\n",
    "    scheduler_cos = transformers.get_cosine_schedule_with_warmup( optimizer= optimizer, num_warmup_steps =num_epoch*.1 ,num_training_steps= num_epoch-1 ,last_epoch = -1 )\n",
    "    \n",
    "        \n",
    "    for i in range (max_train_steps):\n",
    "        epoch_start_time = time.time()\n",
    "        epoch_train_loss = 0.0\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        for ind,(inputs_embed,attention_mask,labels) in enumerate(train_loader):\n",
    "            if ind == int(len(train_loader)/2):\n",
    "                batch_time = time.time()\n",
    "                duration = batch_time - epoch_start_time\n",
    "                print(f\"executing epoch:{i+1}, it took {duration/60} mins from beginning of epoch till batch#{ind}\")\n",
    "            \n",
    "            inputs_embeds = torch.squeeze(inputs_embed.to(device=device, non_blocking=True),dim = 0)\n",
    "            attention_mask = torch.squeeze(attention_mask.to(device=device, non_blocking=True),dim = 0)\n",
    "            labels = torch.squeeze(labels.to(device=device, non_blocking=True),dim = 0)\n",
    "            \n",
    "            \n",
    "            with autocast(dtype = torch.bfloat16):\n",
    "                model_output = model(inputs_embeds = inputs_embeds ,attention_mask = attention_mask ,labels = labels)\n",
    "                loss = model_output.loss\n",
    "                        \n",
    "            loss.backward()\n",
    "            epoch_train_loss += loss.detach().item()\n",
    "            \n",
    "            norm = torch.nn.utils.clip_grad_norm(model.parameters() , 1.0)\n",
    "            if i <= num_epoch:\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "            else:\n",
    "                optimizer_reduced_lr.step()\n",
    "                optimizer_reduced_lr.zero_grad(set_to_none=True)\n",
    "                #print(f\"Gradient accum step = {ind}|batch_loss after averaging ={batch_loss} | epoch_train_loss = {epoch_train_loss}\")\n",
    "                #print(f\"Gradient accum step = {ind}|batch_loss after averaging ={batch_loss} | epoch_train_loss = {epoch_train_loss}\")\n",
    "                                \n",
    "            del inputs_embeds,labels,attention_mask\n",
    "            \n",
    "                \n",
    "        #batch processing complete \n",
    "        if i <= num_epoch:\n",
    "            current_lr = scheduler_cos.get_last_lr()[0]\n",
    "            scheduler_cos.step()\n",
    "        if i >= num_epoch:\n",
    "            optimizer_reduced_lr = torch.optim.AdamW(params =  model.parameters(), lr= current_lr ,fused = True , weight_decay=.1)\n",
    "            optimizer_reduced_lr.zero_grad(set_to_none=True)\n",
    "            scheduler_constant = transformers.get_constant_schedule_with_warmup( optimizer = optimizer_reduced_lr ,num_warmup_steps = 0, last_epoch = -1 )\n",
    "            scheduler_constant.step()\n",
    "                      \n",
    "        #mean_loss = torch.mean(torch.tensor(epoch_train_loss))\n",
    "        epoch_end_time = time.time()\n",
    "        epoch_durn = (epoch_end_time - epoch_start_time)\n",
    "        num_token = B*T*len(train_loader)\n",
    "        if (epoch_train_loss < global_tr_loss) :\n",
    "            print(f\"training loss has decreased---> reducing the global loss from {global_tr_loss:.2f} to {epoch_train_loss:.2f} | throughput = {int(num_token/epoch_durn)} tokens/second | norm = {norm:.4f} | learning rate = {current_lr:.5e}\")\n",
    "            global_tr_loss = epoch_train_loss\n",
    "            print(f\" epoch= {i+1} and  train loss is {epoch_train_loss:.2f}\")\n",
    "            #printing training metrices\n",
    "            #y_hat = torch.cat(y_hat_list)\n",
    "            #y = torch.cat(label_list)\n",
    "            #acc = accuracy_score(y.cpu().numpy(), y_hat.cpu().numpy())\n",
    "            #f1 = f1_score(y.cpu().numpy(), y_hat.cpu().numpy(), average='micro')\n",
    "            #checking validation metrices\n",
    "            if (i%4 == 0):\n",
    "                eval_model(val_loader, model, epoch = i , device = device,tokenizer = tokenizer)\n",
    "            \n",
    "        else:\n",
    "            print(f\"No improvement in training loss..the global training loss is -->{global_tr_loss:.2f} | epoch= {i+1} and mean train loss is {epoch_train_loss:.2f}\")\n",
    "            \n",
    "        \n",
    "        \n",
    "    \n",
    "    return model\n",
    "        \n",
    "            \n",
    "            \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0f30dfae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x,att_mask,lab = train_dataset[2]\n",
    "# print(x.shape)\n",
    "# print(att_mask.shape)\n",
    "# print(lab.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "317e041b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# out = model(inputs_embeds = x, attention_mask = att_mask, labels = lab )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0375aec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# for ind,(inputs_embeds,attention_mask,labels) in enumerate(train_loader):\n",
    "#     print(f\"ind = {ind}|inputs_embed shape = {inputs_embeds.shape}|attention_mask shape = {attention_mask.shape}|labels shape = {labels.shape}\")\n",
    "#     with autocast(dtype = torch.bfloat16):\n",
    "#         model_custom.to(device)\n",
    "#         inputs_embeds = torch.squeeze(inputs_embeds,dim = 0).to(device = device)\n",
    "#         attention_mask = torch.squeeze(attention_mask,dim = 0).to(device = device)\n",
    "#         labels = torch.squeeze(labels,dim = 0).to(device = device)\n",
    "        \n",
    "#         model_output = model_custom(inputs_embeds =inputs_embeds  ,attention_mask = attention_mask ,labels = labels)\n",
    "#         loss = model_output.loss\n",
    "#         print(f\"loss = {loss}\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d5fe5320",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inside train model. Device = cuda:0\n",
      "executing epoch:1, it took 3.018706687291463 mins from beginning of epoch till batch#120\n",
      "No improvement in training loss..the global training loss is -->inf | epoch= 1 and mean train loss is nan\n",
      "executing epoch:2, it took 2.3158984859784444 mins from beginning of epoch till batch#120\n",
      "No improvement in training loss..the global training loss is -->inf | epoch= 2 and mean train loss is nan\n",
      "executing epoch:3, it took 2.3127740303675335 mins from beginning of epoch till batch#120\n",
      "No improvement in training loss..the global training loss is -->inf | epoch= 3 and mean train loss is nan\n",
      "executing epoch:4, it took 2.3309409697850545 mins from beginning of epoch till batch#120\n",
      "No improvement in training loss..the global training loss is -->inf | epoch= 4 and mean train loss is nan\n",
      "executing epoch:5, it took 2.3317768812179565 mins from beginning of epoch till batch#120\n",
      "No improvement in training loss..the global training loss is -->inf | epoch= 5 and mean train loss is nan\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_6436/3322038378.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtr_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_6436/3104248095.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(train_loader, val_loader, model, num_epoch, device, tokenizer)\u001b[0m\n\u001b[1;32m     34\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m             \u001b[0mepoch_train_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    523\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m             )\n\u001b[0;32m--> 525\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    526\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m         )\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 267\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    268\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    743\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 744\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    745\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    746\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tr_model = train_model(train_loader, val_loader, model =  model,tokenizer = tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a98908",
   "metadata": {},
   "source": [
    "## Use this section if you want a model with Random weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08de48f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig\n",
    "# Define the model name\n",
    "model_name = \"distilgpt2\"\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# Load the model configuration\n",
    "config = AutoConfig.from_pretrained(model_name)\n",
    "\n",
    "# Initialize the model with random weights\n",
    "model = AutoModelForCausalLM.from_config(config)\n",
    "\n",
    "# Check the model\n",
    "#print(model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c4801b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdba63ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(df_train.iloc[178]['input_ids'], dtype = torch.long)\n",
    "att = torch.tensor(df_train.iloc[178]['attention_mask'], dtype = torch.long)\n",
    "random_out = model(input_ids = x.view(B,T) , attention_mask = att.view(B,T), labels =   x.view(B,T) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f799311",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d56976",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b99823b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7d810c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aae2ffc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29502bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b85e0c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e21bed2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0f2cc3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80dccd96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63da9aac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd551aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
