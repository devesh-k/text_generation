{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3613027",
   "metadata": {},
   "source": [
    "## In this notebook, we scale the nll and scale the cosine similaity with (1-lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83df7b3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "140801d9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: datasets in /opt/conda/lib/python3.8/site-packages (2.20.0)\n",
      "Requirement already satisfied: fsspec[http]<=2024.5.0,>=2023.1.0 in /opt/conda/lib/python3.8/site-packages (from datasets) (2024.5.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.8/site-packages (from datasets) (3.4.0)\n",
      "Requirement already satisfied: requests>=2.32.2 in /opt/conda/lib/python3.8/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.8/site-packages (from datasets) (1.3.4)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.2 in /opt/conda/lib/python3.8/site-packages (from datasets) (0.23.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.8/site-packages (from datasets) (1.21.4)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.8/site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.8/site-packages (from datasets) (3.9.5)\n",
      "Requirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.8/site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /opt/conda/lib/python3.8/site-packages (from datasets) (4.66.4)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.8/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.8/site-packages (from datasets) (6.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.8/site-packages (from datasets) (16.1.0)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.8/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.8/site-packages (from datasets) (21.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets) (21.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.8/site-packages (from huggingface-hub>=0.21.2->datasets) (4.11.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging->datasets) (3.0.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests>=2.32.2->datasets) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests>=2.32.2->datasets) (1.26.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests>=2.32.2->datasets) (3.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.8/site-packages (from requests>=2.32.2->datasets) (2.0.8)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.8/site-packages (from pandas->datasets) (2021.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.8/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: jupyter in /opt/conda/lib/python3.8/site-packages (1.0.0)\n",
      "Requirement already satisfied: ipywidgets in /opt/conda/lib/python3.8/site-packages (8.1.3)\n",
      "Requirement already satisfied: notebook in /opt/conda/lib/python3.8/site-packages (from jupyter) (6.4.1)\n",
      "Requirement already satisfied: ipykernel in /opt/conda/lib/python3.8/site-packages (from jupyter) (6.29.5)\n",
      "Requirement already satisfied: qtconsole in /opt/conda/lib/python3.8/site-packages (from jupyter) (5.5.2)\n",
      "Requirement already satisfied: jupyter-console in /opt/conda/lib/python3.8/site-packages (from jupyter) (6.6.3)\n",
      "Requirement already satisfied: nbconvert in /opt/conda/lib/python3.8/site-packages (from jupyter) (6.3.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (5.14.3)\n",
      "Requirement already satisfied: comm>=0.1.3 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.11 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (4.0.11)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (7.30.0)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.11 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (3.0.11)\n",
      "Requirement already satisfied: decorator in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (5.1.0)\n",
      "Requirement already satisfied: pygments in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (2.10.0)\n",
      "Requirement already satisfied: setuptools>=18.5 in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (59.4.0)\n",
      "Requirement already satisfied: matplotlib-inline in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.3)\n",
      "Requirement already satisfied: jedi>=0.16 in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.18.1)\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (4.8.0)\n",
      "Requirement already satisfied: pickleshare in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.7.5)\n",
      "Requirement already satisfied: backcall in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.47)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /opt/conda/lib/python3.8/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.8/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.8/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=6.1.0->ipywidgets) (0.2.5)\n",
      "Requirement already satisfied: pyzmq>=24 in /opt/conda/lib/python3.8/site-packages (from ipykernel->jupyter) (26.0.3)\n",
      "Requirement already satisfied: tornado>=6.1 in /opt/conda/lib/python3.8/site-packages (from ipykernel->jupyter) (6.1)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.8/site-packages (from ipykernel->jupyter) (21.3)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.8/site-packages (from ipykernel->jupyter) (5.8.0)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in /opt/conda/lib/python3.8/site-packages (from ipykernel->jupyter) (1.8.2)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in /opt/conda/lib/python3.8/site-packages (from ipykernel->jupyter) (7.1.0)\n",
      "Requirement already satisfied: nest-asyncio in /opt/conda/lib/python3.8/site-packages (from ipykernel->jupyter) (1.5.4)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /opt/conda/lib/python3.8/site-packages (from ipykernel->jupyter) (5.7.2)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /opt/conda/lib/python3.8/site-packages (from jupyter-client>=6.1.12->ipykernel->jupyter) (2.8.2)\n",
      "Requirement already satisfied: entrypoints in /opt/conda/lib/python3.8/site-packages (from jupyter-client>=6.1.12->ipykernel->jupyter) (0.3)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /opt/conda/lib/python3.8/site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel->jupyter) (4.2.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.8/site-packages (from python-dateutil>=2.1->jupyter-client>=6.1.12->ipykernel->jupyter) (1.16.0)\n",
      "Requirement already satisfied: mistune<2,>=0.8.1 in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (0.8.4)\n",
      "Requirement already satisfied: jupyterlab-pygments in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (0.1.2)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (1.5.0)\n",
      "Requirement already satisfied: jinja2>=2.4 in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (3.0.3)\n",
      "Requirement already satisfied: defusedxml in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (0.7.1)\n",
      "Requirement already satisfied: nbclient<0.6.0,>=0.5.0 in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (0.5.9)\n",
      "Requirement already satisfied: bleach in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (4.1.0)\n",
      "Requirement already satisfied: testpath in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (0.5.0)\n",
      "Requirement already satisfied: nbformat>=4.4 in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (5.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.8/site-packages (from jinja2>=2.4->nbconvert->jupyter) (2.0.1)\n",
      "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /opt/conda/lib/python3.8/site-packages (from nbformat>=4.4->nbconvert->jupyter) (4.2.1)\n",
      "Requirement already satisfied: ipython-genutils in /opt/conda/lib/python3.8/site-packages (from nbformat>=4.4->nbconvert->jupyter) (0.2.0)\n",
      "Requirement already satisfied: importlib-resources>=1.4.0 in /opt/conda/lib/python3.8/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.4->nbconvert->jupyter) (5.4.0)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /opt/conda/lib/python3.8/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.4->nbconvert->jupyter) (0.18.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /opt/conda/lib/python3.8/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.4->nbconvert->jupyter) (21.2.0)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /opt/conda/lib/python3.8/site-packages (from importlib-resources>=1.4.0->jsonschema!=2.5.0,>=2.4->nbformat>=4.4->nbconvert->jupyter) (3.6.0)\n",
      "Requirement already satisfied: webencodings in /opt/conda/lib/python3.8/site-packages (from bleach->nbconvert->jupyter) (0.5.1)\n",
      "Requirement already satisfied: terminado>=0.8.3 in /opt/conda/lib/python3.8/site-packages (from notebook->jupyter) (0.12.1)\n",
      "Requirement already satisfied: argon2-cffi in /opt/conda/lib/python3.8/site-packages (from notebook->jupyter) (21.1.0)\n",
      "Requirement already satisfied: prometheus-client in /opt/conda/lib/python3.8/site-packages (from notebook->jupyter) (0.12.0)\n",
      "Requirement already satisfied: Send2Trash>=1.5.0 in /opt/conda/lib/python3.8/site-packages (from notebook->jupyter) (1.8.0)\n",
      "Requirement already satisfied: cffi>=1.0.0 in /opt/conda/lib/python3.8/site-packages (from argon2-cffi->notebook->jupyter) (1.15.0)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.8/site-packages (from cffi>=1.0.0->argon2-cffi->notebook->jupyter) (2.21)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging->ipykernel->jupyter) (3.0.6)\n",
      "Requirement already satisfied: qtpy>=2.4.0 in /opt/conda/lib/python3.8/site-packages (from qtconsole->jupyter) (2.4.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#installing some libraries\n",
    "!pip install datasets\n",
    "!pip install --upgrade jupyter ipywidgets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5bd1df66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch, transformers\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import PreTrainedModel, PretrainedConfig\n",
    "from transformers import AutoModel, AutoConfig,AutoModelForCausalLM, AutoConfig\n",
    "\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "#from datasets import load_dataset\n",
    "import pandas as pd, numpy as np\n",
    "from torch import cuda\n",
    "import datetime\n",
    "import warnings,itertools\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import json,math\n",
    "pre_train = False\n",
    "\n",
    "# Ignore all warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "#pip install transformers bitsandbytes>=0.39.0 -q\n",
    "import zipfile,logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a1371f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "#global params for training\n",
    "\n",
    "B,T = 16,1024\n",
    "epoch = 100\n",
    "# this controls whether we are using pre-trained wts or not\n",
    "random_init_wts = True\n",
    "if cuda.is_available():\n",
    "    device = torch.device('cuda:0')\n",
    "    print(device)\n",
    "else:\n",
    "    device = 'cpu'\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "# these 2 global vars help track the training and val loss\n",
    "global_tr_loss = torch.inf\n",
    "global_val_loss = torch.inf\n",
    "#directory to save wts\n",
    "model_path = os.path.join(\"model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7014f31e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./data/unzip_text_10M'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "directory = os.path.join('.','data','unzip_text_10M')  # Replace with your directory path\n",
    "directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c3e9b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_text(directory):\n",
    "    \"\"\"This function loads the dataset,provided in the zipped format and stores all the text files in list\n",
    "    each file has its contents stored as an item in list and at the end we concatenate all the sub-lists to create a flattened list and return it\"\"\"\n",
    "    directory = os.path.join('.','data','unzip_text_10M',str(directory))  # Replace with your directory path\n",
    "    print(f\"directory :{directory}\")\n",
    "    # List all files in the directory\n",
    "    files = [f for f in os.listdir(directory) if os.path.isfile(os.path.join(directory, f))]\n",
    "    print(f\"files:{files}\")\n",
    "    text_content = []\n",
    "    # Read each file\n",
    "    total_lines = 0\n",
    "    for filenum,filename in enumerate(files):\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            text = file.read()\n",
    "            text_content.append(text)\n",
    "            print(f\"the file:{filename} has been appeneded to the uber list and its length is {len(text_content)} \")\n",
    "            \n",
    "    \n",
    "    flattened_list = ''.join(text_content)\n",
    "    assert (len(flattened_list) == total_lines , f\"Expected {len(flattened_list)} to be equal to {total_lines}\" )\n",
    "    \n",
    "    return flattened_list\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d659a7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "directory :./data/unzip_text_10M/train_10M\n",
      "files:['switchboard.train', 'simple_wiki.train', 'open_subtitles.train', 'gutenberg.train', 'childes.train', 'bnc_spoken.train']\n",
      "the file:switchboard.train has been appeneded to the uber list and its length is 1 \n",
      "the file:simple_wiki.train has been appeneded to the uber list and its length is 2 \n",
      "the file:open_subtitles.train has been appeneded to the uber list and its length is 3 \n",
      "the file:gutenberg.train has been appeneded to the uber list and its length is 4 \n",
      "the file:childes.train has been appeneded to the uber list and its length is 5 \n",
      "the file:bnc_spoken.train has been appeneded to the uber list and its length is 6 \n"
     ]
    }
   ],
   "source": [
    "#read the dataset and get a list\n",
    "train_list = read_text(\"train_10M\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e1893203",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54215049"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f9415882",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3309\n"
     ]
    }
   ],
   "source": [
    "chunks = len(train_list)//(B*T)\n",
    "print(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7b9ad7ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17191971 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "#tokeinze the training data\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\",return_tensors = \"pt\" , truncate = True, return_overflowing_tokens=True , padding = False,)\n",
    "enc_train = tokenizer(train_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ccc4c1dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.1535097982657136"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comp_ratio = len(train_list)/len(enc_train['input_ids'])\n",
    "comp_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6448975a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "directory :./data/unzip_text_10M/dev\n",
      "files:['switchboard.dev', 'simple_wiki.dev', 'open_subtitles.dev', 'gutenberg.dev', 'childes.dev', 'bnc_spoken.dev']\n",
      "the file:switchboard.dev has been appeneded to the uber list and its length is 1 \n",
      "the file:simple_wiki.dev has been appeneded to the uber list and its length is 2 \n",
      "the file:open_subtitles.dev has been appeneded to the uber list and its length is 3 \n",
      "the file:gutenberg.dev has been appeneded to the uber list and its length is 4 \n",
      "the file:childes.dev has been appeneded to the uber list and its length is 5 \n",
      "the file:bnc_spoken.dev has been appeneded to the uber list and its length is 6 \n"
     ]
    }
   ],
   "source": [
    "#read validation data and tokenize\n",
    "val_list = read_text(\"dev\")\n",
    "enc_val = tokenizer(val_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15038e15",
   "metadata": {},
   "source": [
    "### we read the tokenize data and store the input_ids and attention mask as a single long list at run time we reshape the single [1,B*T] tensor into a batched tensor of dimension [B,T].\n",
    "This approach turns out to be more efficient as it removes the need for any extra tokens in the form of padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f2a9fb47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df_from_list(enc , B= B, T = T, val= False):\n",
    "    \"\"\"This function takes the tokenized list and reated a dataframe where each row contains the input_ids and attention_mask from the tokenizer.\n",
    "    each row of the dataframe is a list with items B*T\"\"\"\n",
    "    chunk_size = B*T\n",
    "    if val:\n",
    "        chunk_size = 2*B*T\n",
    "        \n",
    "    long_list_inp = enc['input_ids']\n",
    "    long_list_attention = enc['attention_mask']\n",
    "\n",
    "    # Step 3: Split the list into chunks and pad the last chunk if necessary\n",
    "    chunks_inp = [long_list_inp[i:i + chunk_size] for i in range(0, len(long_list_inp), chunk_size)]\n",
    "    chunks_att = [long_list_attention[i:i + chunk_size] for i in range(0, len(long_list_attention), chunk_size)]\n",
    "    df = pd.DataFrame({'input_ids': chunks_inp,'attention_mask':chunks_att})\n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ef44b28d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the dataframe is = 1050\n"
     ]
    }
   ],
   "source": [
    "df_train_temp = get_df_from_list(enc_train)\n",
    "# Display the DataFrame\n",
    "df_train_temp.head()\n",
    "print(f\"Length of the dataframe is = {len(df_train_temp)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aedf5db4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the VALDATION dataframe is = 1063\n"
     ]
    }
   ],
   "source": [
    "df_val_temp = get_df_from_list(enc_val)\n",
    "# Display the DataFrame\n",
    "df_val_temp.head()\n",
    "print(f\"Length of the VALDATION dataframe is = {len(df_val_temp)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bcf166b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_df(df,eos_char = tokenizer.eos_token_id,B = B, T = T,val = False):\n",
    "    \"\"\"The 2 functions below makes sure that each row of the dataframe is of equal length and if not it adds the eos token to make it B*T\"\"\"\n",
    "    if val:\n",
    "        B = 2*B\n",
    "    for ind,row in df.iterrows():\n",
    "        if len(row['input_ids']) != B*T :\n",
    "            print(f\"row = {ind} and input_id length = {len(row['input_ids'])}\")\n",
    "            print(f\"row = {ind} and attention length = {len(row['attention_mask'])}\")\n",
    "            pad_len = B*T - len(row['input_ids'])\n",
    "            print(f\"padding the row index {ind} with {pad_len} character\")\n",
    "            row['input_ids'] = row['input_ids']+ [eos_char] * pad_len\n",
    "            #attention mask should be padded to 0\n",
    "            row['attention_mask'] = row['attention_mask']+ [0] * pad_len\n",
    "            print(\"#### POST CONCAT####\")\n",
    "            print(f\"row = {ind} and input_id length = {len(row['input_ids'])}\")\n",
    "            print(f\"row = {ind} and attention length ={len(row['attention_mask'])}\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def verify_len(df,val = False, B= B):\n",
    "    row_ind = []\n",
    "    if val:\n",
    "        B = 2*B\n",
    "    for ind,row in df.iterrows():\n",
    "        if len(row['input_ids']) != B*T :\n",
    "            row_ind.append(ind)\n",
    "        else:\n",
    "            continue\n",
    "    if len(row_ind) !=0:\n",
    "        print(\"CONCATENATION Did not work\")\n",
    "    else:\n",
    "        print(\"CONCATENATION worked\")\n",
    "        \n",
    "            \n",
    "        \n",
    "    \n",
    "        \n",
    "        \n",
    "   \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2807ce57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row = 1049 and input_id length = 5155\n",
      "row = 1049 and attention length = 5155\n",
      "padding the row index 1049 with 11229 character\n",
      "#### POST CONCAT####\n",
      "row = 1049 and input_id length = 16384\n",
      "row = 1049 and attention length =16384\n",
      "CONCATENATION worked\n"
     ]
    }
   ],
   "source": [
    "df_train  = pad_df(df_train_temp)\n",
    "verify_len(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "187b01d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row = 1062 and input_id length = 13588\n",
      "row = 1062 and attention length = 13588\n",
      "padding the row index 1062 with 2796 character\n",
      "#### POST CONCAT####\n",
      "row = 1062 and input_id length = 16384\n",
      "row = 1062 and attention length =16384\n",
      "CONCATENATION worked\n"
     ]
    }
   ],
   "source": [
    "df_val  = pad_df(df_val_temp)\n",
    "verify_len(df_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8dd7cab1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_ids</th>\n",
       "      <th>attention_mask</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[32, 25, 197, 40, 1101, 1654, 484, 389, 13, 19...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[290, 1243, 588, 326, 198, 33, 25, 197, 392, 1...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[6510, 11, 14104, 290, 27913, 13, 198, 32, 25,...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[314, 892, 340, 6774, 503, 262, 5290, 287, 262...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[314, 1612, 11, 340, 338, 1611, 286, 43244, 11...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           input_ids  \\\n",
       "0  [32, 25, 197, 40, 1101, 1654, 484, 389, 13, 19...   \n",
       "1  [290, 1243, 588, 326, 198, 33, 25, 197, 392, 1...   \n",
       "2  [6510, 11, 14104, 290, 27913, 13, 198, 32, 25,...   \n",
       "3  [314, 892, 340, 6774, 503, 262, 5290, 287, 262...   \n",
       "4  [314, 1612, 11, 340, 338, 1611, 286, 43244, 11...   \n",
       "\n",
       "                                      attention_mask  \n",
       "0  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "1  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "2  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "3  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "4  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2d5d0c",
   "metadata": {},
   "source": [
    "## Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "caac79ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLoss(nn.Module):\n",
    "    def __init__(self, num_epochs):\n",
    "        super(CustomLoss, self).__init__()\n",
    "        #self.nll_loss = nn.CrossEntropyLoss()\n",
    "#        self.cosine_similarity = nn.CosineSimilarity(dim=2)\n",
    "        self.num_epochs = num_epochs\n",
    "        self.current_epoch = 0\n",
    "        self.nll_scale = 1.0  # Initialize with a default value\n",
    "\n",
    "    def forward(self, nll,cosine_sim):\n",
    "               \n",
    "        nll_normalized = nll / self.nll_scale         \n",
    "        # Calculate lambda value\n",
    "        lambda_val = self.get_lambda()\n",
    "        # Compute total loss\n",
    "        total_loss = nll_normalized + lambda_val * cosine_sim\n",
    "        #print(f\"inside CustomLoss->nll = {nll}|nll_scale = {self.nll_scale}|nll_normalized= {nll_normalized}|cosine_sim = {cosine_sim} | lambda val = {lambda_val}|total_loss = {total_loss}\")\n",
    "        return total_loss,lambda_val,self.nll_scale\n",
    "\n",
    "    def get_lambda(self):\n",
    "        # Exponential increase from 0 to 1\n",
    "        return 1 - math.exp(-5 * self.current_epoch / self.num_epochs)\n",
    "\n",
    "    def update_epoch(self, epoch):\n",
    "        self.current_epoch = epoch\n",
    "\n",
    "    def update_nll_scale(self, nll_value):\n",
    "        self.nll_scale = max(self.nll_scale, nll_value)\n",
    "    def update_epoch(self, epoch):\n",
    "        self.current_epoch = epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "053301ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the tokenizer:\n",
    "model_name = \"gpt2\"\n",
    "if random_init_wts:\n",
    "    config = AutoConfig.from_pretrained(model_name, vocab_size = 50304)\n",
    "    # Initialize the model with random weights\n",
    "    model = AutoModelForCausalLM.from_config(config)\n",
    "else:\n",
    "    #model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "\n",
    "#model = torch.compile(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8c0e92d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1050"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9611156b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# device = \"cuda\"\n",
    "# inp = torch.tensor(df_train.iloc[5]['input_ids']).view(B,T).to(device)\n",
    "# att = torch.tensor(df_train.iloc[5]['attention_mask']).view(B,T).to(device)\n",
    "# lab = inp.clone()\n",
    "# # lab = F.pad(lab[:, 1:], (0, 1), value=-100)  # Shift labels to the left and pad with -100\n",
    "# # lab[:, -1] = -100\n",
    "# model.to(device)\n",
    "# model_out = model(input_ids = inp, attention_mask = att , labels = lab)\n",
    "# assert not torch.isnan(inp).any(), \"Input contains NaN\"\n",
    "# assert not torch.isinf(inp).any(), \"Input contains inf\"\n",
    "# model_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7ce401cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"Input shape: {inp.shape}\")\n",
    "# print(f\"Attention mask shape: {att.shape}\")\n",
    "# print(f\"Labels shape: {lab.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02278b85",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9ad74ade",
   "metadata": {},
   "source": [
    "### Data loaders and Dataset for batched training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e5e09e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataset_pyt_val(Dataset):\n",
    "    \"\"\"Dataset for validation data\"\"\"\n",
    "    def __init__(self, df, B = B, T = T ):\n",
    "        self.df = df\n",
    "        print(f\"Value of B {B}\")\n",
    "                                        \n",
    "    def __getitem__(self, idx):\n",
    "        #print(f\"inside loader...idx ->{idx}\")\n",
    "        input_id_temp = torch.tensor(self.df.iloc[idx]['input_ids'],dtype = torch.long)\n",
    "        att_mask = torch.tensor(self.df.iloc[idx]['attention_mask'],dtype = torch.long)\n",
    "        input_id =   input_id_temp.view(B,T)    \n",
    "        attention_mask = att_mask.view(B,T)\n",
    "        \n",
    "        \n",
    "        return input_id, attention_mask\n",
    "        \n",
    "    def __len__(self):\n",
    "        #return the length of the dataframe\n",
    "        return len(self.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "380992e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataset_pyt_train(Dataset):\n",
    "    def __init__(self, df, B = B, T = T ):\n",
    "        self.df = df\n",
    "                                        \n",
    "    def __getitem__(self, idx):\n",
    "        #print(f\"************* idx for dataloader = {idx}\")\n",
    "        input_id_temp = torch.tensor(self.df.iloc[idx]['input_ids'],dtype = torch.long)\n",
    "        att_mask = torch.tensor(self.df.iloc[idx]['attention_mask'],dtype = torch.long)\n",
    "        input_id =   input_id_temp.view(B,T)    \n",
    "        attention_mask = att_mask.view(B,T)   \n",
    "        \n",
    "        return input_id, attention_mask\n",
    "        \n",
    "    def __len__(self):\n",
    "        #return the length of the dataframe\n",
    "        return len(self.df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411128fc",
   "metadata": {},
   "source": [
    "### Note - batch_size is 1 here because we reshape our input data in the shape [B,T] for a batched training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "db7299d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value of B 16\n"
     ]
    }
   ],
   "source": [
    "#train_dataset = dataset_pyt(filtered_df,tokenizer = tokenizer)\n",
    "train_dataset = dataset_pyt_train(df_train)\n",
    "val_dataset = dataset_pyt_val(df_val)\n",
    "#test_dataset = dataset_pyt(df_test,tokenizer = tokenizer)\n",
    "\n",
    "train_loader = DataLoader(train_dataset,batch_size = 1, shuffle = True , num_workers = 0, pin_memory = True)\n",
    "val_loader = DataLoader(val_dataset,batch_size = 1, shuffle = True , num_workers = 0, pin_memory = True)\n",
    "#test_loader = DataLoader(test_dataset,batch_size = batch_size, shuffle = False, collate_fn = custom_collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b080a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ad543fc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the train loader is 1050\n",
      "Length of the val loader is 1063\n",
      "num_tokens= 17203200\n"
     ]
    }
   ],
   "source": [
    "print(f\"Length of the train loader is {len(train_loader)}\")\n",
    "print(f\"Length of the val loader is {len(val_loader)}\")\n",
    "print(f\"num_tokens= {B*T*len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8c1203f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_file(log_message, model_name = model_name ,random_init_wts = random_init_wts ):\n",
    "    \"\"\"This function logs the training performance for future reference\"\"\"\n",
    "    current_datetime = datetime.datetime.now()\n",
    "    # Extract date and time components\n",
    "    current_date = str(current_datetime.date())\n",
    "    log_file = model_name +'_COS_SIM*lambda_'+'random_init_wts'+ '_'+str(random_init_wts)+'_' +current_date+'.log'\n",
    "    print(f\"*****LOGGING INFO IN {log_file}*********\")\n",
    "    filepath = os.path.join(\"model\",log_file)\n",
    "    logging.basicConfig(filename=filepath, \n",
    "                    filemode='a',  # Overwrite the log file each time\n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s', \n",
    "                    level=logging.DEBUG)\n",
    "    logger = logging.getLogger()\n",
    "    logger.info(log_message)\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "be2b5dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad\n",
    "\n",
    "def eval_model(val_loader, model, lambda_val ,nll_scale,epoch , device = device,tokenizer = tokenizer):\n",
    "    global global_val_loss\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    e = epoch+1\n",
    "    embedding_layer = model.transformer.wte\n",
    "    val_loss_accum = 0.0\n",
    "    print(f\"inside validation data for epoch {e}\")\n",
    "    for ind,(input_id,attention_mask) in enumerate(val_loader):\n",
    "        ids = input_id.to(device=device, non_blocking=True)\n",
    "        att_mask = attention_mask.to(device=device, non_blocking=True)\n",
    "        labels = ids.clone()\n",
    "        \n",
    "        with autocast(dtype = torch.bfloat16):\n",
    "            input_embeddings = embedding_layer(ids)\n",
    "            model_output = model(input_ids = ids ,attention_mask = att_mask, labels = labels)\n",
    "            logits = model_output.logits\n",
    "            predictions = torch.argmax(logits, dim=-1)\n",
    "            #print(f\"predictions = {predictions}\")\n",
    "            prediction_embeddings = embedding_layer(predictions)\n",
    "            cos_sim = F.cosine_similarity(torch.squeeze(prediction_embeddings,dim = 0), torch.squeeze(input_embeddings,dim = 0), dim=1)\n",
    "            cos_loss = 1- cos_sim.mean()\n",
    "            total_loss = cos_loss + model_output.loss\n",
    "        \n",
    "        val_loss_accum+=total_loss.detach().item()\n",
    "        del ids,att_mask,labels,input_embeddings,model_output,logits,predictions,prediction_embeddings,cos_sim,cos_loss\n",
    "    \n",
    "    #logging and saving if the validation loss has decreased.\n",
    "    if val_loss_accum < global_val_loss:\n",
    "        print(f\"Val loss has decreased -->reducing the global validation loss from {global_val_loss:.2f} to {val_loss_accum:.2f}\")\n",
    "        s1 = f\"Val loss has decreased -->reducing the global validation loss from {global_val_loss:.2f} to {val_loss_accum:.2f}\"\n",
    "        global_val_loss = val_loss_accum\n",
    "        print(f\" validation loss for epoch = {e} is {val_loss_accum:.4f}\")\n",
    "        s2 = f\" validation loss for epoch = {e} is {val_loss_accum:.4f}\"\n",
    "        print(f\" epoch= {e} :  val loss is {val_loss_accum:.4f} \")\n",
    "        s3 = f\" epoch= {e} :  val loss is {val_loss_accum:.4f} \"\n",
    "        #save the model\n",
    "        \n",
    "        # Get the current date and time\n",
    "        current_datetime = datetime.datetime.now()\n",
    "        # Extract date and time components\n",
    "        current_date = str(current_datetime.date())\n",
    "        current_time = str(current_datetime.time()).split('.')[0]\n",
    "        file_name = 'model'+ current_date+current_time+'.pth'\n",
    "        path = os.path.join(\"model\",file_name)\n",
    "        print(f\"saving the model {file_name}\")\n",
    "        s4 = f\"saving the model {file_name}\"\n",
    "        #torch.save(model.state_dict(), path)\n",
    "        model.save_pretrained(path)\n",
    "        tokenizer.save_pretrained(path)\n",
    "        log_message = s1+s2+s3+s4\n",
    "        write_file(log_message)\n",
    "        \n",
    "        #plot_confusion_matrix(y_val.cpu().numpy(), y_hat_val.cpu().numpy(), labels)\n",
    "    else:\n",
    "        print(f\"No improvement in validation loss-->epoch= {e} and global val loss is {global_val_loss:.2f}|current_Val loss = {val_loss_accum}\")\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cb61e4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_loader,val_loader,model,num_epoch = 100,device = device,tokenizer = tokenizer):\n",
    "    global global_tr_loss\n",
    "    model.train()\n",
    "    device = device\n",
    "    lr_custom = 2e-5\n",
    "    print(f\"inside train model. Device = {device}\")\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.AdamW(params =  model.parameters(), lr= lr_custom,fused = True ,weight_decay = .1)\n",
    "    total_batch_size = 2**19\n",
    "    grad_accum_step = total_batch_size//(B*T)\n",
    "    \n",
    "    extra_train = .1*num_epoch\n",
    "    max_train_steps = int(num_epoch +extra_train )\n",
    "    import time\n",
    "    from transformers import get_linear_schedule_with_warmup\n",
    "    total_steps = len(train_loader) * num_epoch\n",
    "    scheduler_cos = transformers.get_cosine_schedule_with_warmup( optimizer= optimizer, num_warmup_steps =int(total_steps * 0.1) ,num_training_steps= total_steps )\n",
    "    embedding_layer = model.transformer.wte\n",
    "    criterion = CustomLoss(num_epoch)\n",
    "        \n",
    "    for i in range (max_train_steps):\n",
    "        criterion.update_epoch(i)\n",
    "        epoch_start_time = time.time()\n",
    "        epoch_train_loss = 0.0\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        # we use 2 schedulers - the first LR scheduler uses a cosine decay for 100 epochs the second scheduler takes the last LR from cosine scheduler and then maintains that LR for the next 10 epochs\n",
    "        if i >= num_epoch:\n",
    "            optimizer_reduced_lr = torch.optim.AdamW(params =  model.parameters(), lr= current_lr ,fused = True , weight_decay=.1)\n",
    "            scheduler_constant = transformers.get_constant_schedule_with_warmup( optimizer = optimizer_reduced_lr ,num_warmup_steps = 0, last_epoch = -1 )\n",
    "        \n",
    "        lambda_val = None\n",
    "        nll_scale = None\n",
    "        for ind,(input_id,attention_mask) in enumerate(train_loader):\n",
    "            if ind == int(len(train_loader)/2):\n",
    "                batch_time = time.time()\n",
    "                duration = batch_time - epoch_start_time\n",
    "                print(f\"executing epoch:{i+1}, it took {duration/60} mins from beginning of epoch till batch#{ind}\")\n",
    "            \n",
    "            ids = input_id.to(device=device, non_blocking=True)\n",
    "            att_mask = attention_mask.to(device=device, non_blocking=True)\n",
    "            labels = ids.clone()\n",
    "            assert not torch.isnan(ids).any(), \"Input contains NaN values\"\n",
    "            assert not torch.isinf(ids).any(), \"Input contains infinite values\"\n",
    "            \n",
    "            with autocast(dtype = torch.bfloat16):\n",
    "                input_embeddings = embedding_layer(ids)\n",
    "                model_output = model(input_ids = ids ,attention_mask = att_mask, labels = labels)\n",
    "                logits = model_output.logits\n",
    "                predictions = torch.argmax(logits, dim=-1)\n",
    "                prediction_embeddings = embedding_layer(predictions)\n",
    "                #print(f\"model_output.logits = {model_output.logits}|model_output.loss = {model_output.loss}\")\n",
    "                cos_sim = F.cosine_similarity(torch.squeeze(prediction_embeddings,dim = 0), torch.squeeze(input_embeddings,dim = 0), dim=1)\n",
    "                cos_loss = 1- cos_sim.mean()\n",
    "                #print(f\"inside train_vale before calc custom loss|ind = {ind}|model_output.loss = {model_output.loss}|coss_loss = {cos_loss}\")\n",
    "                #print(f\"inside train_vale before calc custom loss|ind = {ind}|model_output.loss = {model_output.loss}|coss_loss = {cos_loss}|learning rate = {scheduler_cos.get_last_lr()[0]:.3e}\")\n",
    "                if np.isnan(model_output.loss.item()):\n",
    "                    print(\"f nan values encountered..\")\n",
    "                    decoded_texts = [tokenizer.decode(ids, skip_special_tokens=True) for ids in ids]\n",
    "                    print(f\"*********$$$$$$$$$ decoded_texts = {decoded_texts}*************\")\n",
    "\n",
    "\n",
    "            assert not np.isnan(model_output.loss.item()), \"NaN value found\"\n",
    "            #total_loss = cos_loss + model_output.loss.item()\n",
    "            \n",
    "            total_loss,lambda_val,nll_scale = criterion(nll = model_output.loss.item() , cosine_sim = cos_loss)\n",
    "            if ind == (len(train_loader) - 1):\n",
    "                lambda_val = lambda_val\n",
    "                nll_scale = nll_scale\n",
    "                \n",
    "            criterion.update_nll_scale(model_output.loss.item())                           \n",
    "            total_loss.backward()\n",
    "            epoch_train_loss += total_loss.detach().item()\n",
    "            norm = torch.nn.utils.clip_grad_norm(model.parameters() , 1.0)\n",
    "            if i <= num_epoch:\n",
    "                optimizer.step()\n",
    "                scheduler_cos.step()\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "            else:\n",
    "                optimizer_reduced_lr.step()\n",
    "                optimizer_reduced_lr.zero_grad(set_to_none=True)\n",
    "                scheduler_constant.step()\n",
    "                \n",
    "                                \n",
    "            del ids,att_mask,labels,input_embeddings,model_output,logits,predictions,prediction_embeddings,cos_sim\n",
    "            \n",
    "        \n",
    "        \n",
    "        #batch processing complete \n",
    "        if i <= num_epoch:\n",
    "            current_lr = scheduler_cos.get_last_lr()[0]\n",
    "        #This code below compares the training loss during the current epoch with the global loss and if updates the global loss if there is an improvement.\n",
    "        # at every 4 epoch we evluate the model on the validation data or when \n",
    "        epoch_end_time = time.time()\n",
    "        epoch_durn = (epoch_end_time - epoch_start_time)\n",
    "        num_token = B*T*len(train_loader)\n",
    "        if (epoch_train_loss < global_tr_loss) :\n",
    "            improvement_percent = (abs(epoch_train_loss-global_tr_loss)/global_tr_loss)*100\n",
    "            print(f\"training loss has decreased---> reducing the global loss from {global_tr_loss:.2f} to {epoch_train_loss:.2f} | throughput = {int(num_token/epoch_durn)} tokens/second | norm = {norm:.4f} | learning rate = {current_lr:.5e}\")\n",
    "            global_tr_loss = epoch_train_loss\n",
    "            print(f\" epoch= {i+1} and  train loss is {epoch_train_loss:.2f}\")\n",
    "            if (i%4 == 0 or improvement_percent > 5):\n",
    "                eval_model(val_loader, model, lambda_val ,nll_scale,epoch = i, device = device,tokenizer = tokenizer)\n",
    "            \n",
    "        else:\n",
    "            print(f\"No improvement in training loss..the global training loss is -->{global_tr_loss:.2f}|epoch train_loss = {epoch_train_loss:.2f} \")\n",
    "            #print(f\" epoch= {i+1} and mean train loss is {epoch_train_loss:.2f}\")\n",
    "        \n",
    "        \n",
    "    \n",
    "    return model\n",
    "        \n",
    "            \n",
    "            \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "289b1dda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inside train model. Device = cuda:0\n",
      "executing epoch:1, it took 1.4362579107284545 mins from beginning of epoch till batch#525\n",
      "training loss has decreased---> reducing the global loss from inf to 1053.42 | throughput = 99729 tokens/second | norm = 0.0000 | learning rate = 2.00000e-06\n",
      " epoch= 1 and  train loss is 1053.42\n",
      "inside validation data for epoch 1\n",
      "Val loss has decreased -->reducing the global validation loss from inf to 12675.04\n",
      " validation loss for epoch = 1 is 12675.0433\n",
      " epoch= 1 :  val loss is 12675.0433 \n",
      "saving the model model2024-07-1404:55:42.pth\n",
      "*****LOGGING INFO IN gpt2_COS_SIM*lambda_random_init_wts_True_2024-07-14.log*********\n",
      "executing epoch:2, it took 1.358099397023519 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->1053.42|epoch train_loss = 1067.74 \n",
      "executing epoch:3, it took 1.3596592982610067 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->1053.42|epoch train_loss = 1053.47 \n",
      "executing epoch:4, it took 1.3585092941919963 mins from beginning of epoch till batch#525\n",
      "training loss has decreased---> reducing the global loss from 1053.42 to 1033.57 | throughput = 105479 tokens/second | norm = 0.0163 | learning rate = 8.00000e-06\n",
      " epoch= 4 and  train loss is 1033.57\n",
      "executing epoch:5, it took 1.3592471758524576 mins from beginning of epoch till batch#525\n",
      "training loss has decreased---> reducing the global loss from 1033.57 to 1007.41 | throughput = 105495 tokens/second | norm = 0.0206 | learning rate = 1.00000e-05\n",
      " epoch= 5 and  train loss is 1007.41\n",
      "inside validation data for epoch 5\n",
      "Val loss has decreased -->reducing the global validation loss from 12675.04 to 10667.89\n",
      " validation loss for epoch = 5 is 10667.8851\n",
      " epoch= 5 :  val loss is 10667.8851 \n",
      "saving the model model2024-07-1405:09:16.pth\n",
      "*****LOGGING INFO IN gpt2_COS_SIM*lambda_random_init_wts_True_2024-07-14.log*********\n",
      "executing epoch:6, it took 1.3589450279871622 mins from beginning of epoch till batch#525\n",
      "training loss has decreased---> reducing the global loss from 1007.41 to 966.90 | throughput = 105492 tokens/second | norm = 0.0180 | learning rate = 1.20000e-05\n",
      " epoch= 6 and  train loss is 966.90\n",
      "executing epoch:7, it took 1.3584938009579977 mins from beginning of epoch till batch#525\n",
      "training loss has decreased---> reducing the global loss from 966.90 to 930.35 | throughput = 105538 tokens/second | norm = 0.0144 | learning rate = 1.40000e-05\n",
      " epoch= 7 and  train loss is 930.35\n",
      "executing epoch:8, it took 1.3585863669713338 mins from beginning of epoch till batch#525\n",
      "training loss has decreased---> reducing the global loss from 930.35 to 913.12 | throughput = 105558 tokens/second | norm = 0.0167 | learning rate = 1.60000e-05\n",
      " epoch= 8 and  train loss is 913.12\n",
      "executing epoch:9, it took 1.3582460165023804 mins from beginning of epoch till batch#525\n",
      "training loss has decreased---> reducing the global loss from 913.12 to 898.68 | throughput = 105538 tokens/second | norm = 0.0085 | learning rate = 1.80000e-05\n",
      " epoch= 9 and  train loss is 898.68\n",
      "inside validation data for epoch 9\n",
      "Val loss has decreased -->reducing the global validation loss from 10667.89 to 9691.48\n",
      " validation loss for epoch = 9 is 9691.4751\n",
      " epoch= 9 :  val loss is 9691.4751 \n",
      "saving the model model2024-07-1405:22:50.pth\n",
      "*****LOGGING INFO IN gpt2_COS_SIM*lambda_random_init_wts_True_2024-07-14.log*********\n",
      "executing epoch:10, it took 1.3581636746724446 mins from beginning of epoch till batch#525\n",
      "training loss has decreased---> reducing the global loss from 898.68 to 882.47 | throughput = 105545 tokens/second | norm = 0.0121 | learning rate = 2.00000e-05\n",
      " epoch= 10 and  train loss is 882.47\n",
      "executing epoch:11, it took 1.3584796786308289 mins from beginning of epoch till batch#525\n",
      "training loss has decreased---> reducing the global loss from 882.47 to 873.93 | throughput = 105517 tokens/second | norm = 0.0157 | learning rate = 1.99939e-05\n",
      " epoch= 11 and  train loss is 873.93\n",
      "executing epoch:12, it took 1.3574238578478495 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->873.93|epoch train_loss = 878.26 \n",
      "executing epoch:13, it took 1.3579907059669494 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->873.93|epoch train_loss = 888.31 \n",
      "executing epoch:14, it took 1.3585028370221457 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->873.93|epoch train_loss = 901.10 \n",
      "executing epoch:15, it took 1.357434864838918 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->873.93|epoch train_loss = 916.95 \n",
      "executing epoch:16, it took 1.3578092416127523 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->873.93|epoch train_loss = 925.88 \n",
      "executing epoch:17, it took 1.3574880242347718 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->873.93|epoch train_loss = 933.30 \n",
      "executing epoch:18, it took 1.3560314814249674 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->873.93|epoch train_loss = 940.21 \n",
      "executing epoch:19, it took 1.3559920032819113 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->873.93|epoch train_loss = 946.38 \n",
      "executing epoch:20, it took 1.3556569894154866 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->873.93|epoch train_loss = 950.77 \n",
      "executing epoch:21, it took 1.355007290840149 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->873.93|epoch train_loss = 955.71 \n",
      "executing epoch:22, it took 1.3551398913065593 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->873.93|epoch train_loss = 959.58 \n",
      "executing epoch:23, it took 1.3543335914611816 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->873.93|epoch train_loss = 964.22 \n",
      "executing epoch:24, it took 1.3545846501986185 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->873.93|epoch train_loss = 966.15 \n",
      "executing epoch:25, it took 1.3551967819531758 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->873.93|epoch train_loss = 980.84 \n",
      "executing epoch:26, it took 1.3543211460113525 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->873.93|epoch train_loss = 982.23 \n",
      "executing epoch:27, it took 1.354184345404307 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->873.93|epoch train_loss = 991.78 \n",
      "executing epoch:28, it took 1.3535701950391135 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->873.93|epoch train_loss = 969.99 \n",
      "executing epoch:29, it took 1.3534994880358378 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->873.93|epoch train_loss = 961.12 \n",
      "executing epoch:30, it took 1.3533298095067343 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->873.93|epoch train_loss = 955.50 \n",
      "executing epoch:31, it took 1.3529957095781961 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->873.93|epoch train_loss = 947.50 \n",
      "executing epoch:32, it took 1.3520405451456705 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->873.93|epoch train_loss = 946.02 \n",
      "executing epoch:33, it took 1.3522256374359132 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->873.93|epoch train_loss = 946.62 \n",
      "executing epoch:34, it took 1.351976458231608 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->873.93|epoch train_loss = 946.88 \n",
      "executing epoch:35, it took 1.3508914947509765 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->873.93|epoch train_loss = 950.06 \n",
      "executing epoch:36, it took 1.3516353170077007 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->873.93|epoch train_loss = 950.13 \n",
      "executing epoch:37, it took 1.3512843251228333 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->873.93|epoch train_loss = 959.40 \n",
      "executing epoch:38, it took 1.3509105682373046 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->873.93|epoch train_loss = 956.77 \n",
      "executing epoch:39, it took 1.3511499047279358 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->873.93|epoch train_loss = 947.99 \n",
      "executing epoch:40, it took 1.351378325621287 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->873.93|epoch train_loss = 947.77 \n",
      "executing epoch:41, it took 1.3511099060376486 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->873.93|epoch train_loss = 948.00 \n",
      "executing epoch:42, it took 1.3514012296994526 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->873.93|epoch train_loss = 949.75 \n",
      "executing epoch:43, it took 1.351458990573883 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->873.93|epoch train_loss = 951.99 \n",
      "executing epoch:44, it took 1.3511187354723613 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->873.93|epoch train_loss = 953.99 \n",
      "executing epoch:45, it took 1.3508558034896851 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->873.93|epoch train_loss = 955.62 \n",
      "executing epoch:46, it took 1.3510881582895915 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->873.93|epoch train_loss = 958.09 \n",
      "executing epoch:47, it took 1.3515985409418743 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->873.93|epoch train_loss = 959.69 \n",
      "executing epoch:48, it took 1.3505611101786295 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->873.93|epoch train_loss = 961.18 \n",
      "executing epoch:49, it took 1.3504692713419597 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->873.93|epoch train_loss = 962.33 \n",
      "executing epoch:50, it took 1.3505177656809488 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->873.93|epoch train_loss = 963.33 \n",
      "executing epoch:51, it took 1.3511264403661092 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->873.93|epoch train_loss = 964.02 \n",
      "executing epoch:52, it took 1.3504839380582174 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->873.93|epoch train_loss = 964.94 \n",
      "executing epoch:53, it took 1.3505520462989806 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->873.93|epoch train_loss = 965.94 \n",
      "executing epoch:54, it took 1.3506253679593405 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->873.93|epoch train_loss = 964.24 \n",
      "executing epoch:55, it took 1.3508441925048829 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->873.93|epoch train_loss = 964.05 \n",
      "executing epoch:56, it took 1.350782867272695 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->873.93|epoch train_loss = 965.20 \n",
      "executing epoch:57, it took 1.3511003216107687 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->873.93|epoch train_loss = 965.88 \n",
      "executing epoch:58, it took 1.3502805908521016 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->873.93|epoch train_loss = 966.16 \n",
      "executing epoch:59, it took 1.3501644651095073 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->873.93|epoch train_loss = 966.75 \n",
      "executing epoch:60, it took 1.3508964419364928 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->873.93|epoch train_loss = 967.79 \n",
      "executing epoch:61, it took 1.350712807973226 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->873.93|epoch train_loss = 968.84 \n",
      "executing epoch:62, it took 1.350753378868103 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->873.93|epoch train_loss = 969.81 \n",
      "executing epoch:63, it took 1.3509398142496745 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->873.93|epoch train_loss = 970.15 \n",
      "executing epoch:64, it took 1.351281468073527 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->873.93|epoch train_loss = 970.17 \n",
      "executing epoch:65, it took 1.3512378136316936 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->873.93|epoch train_loss = 970.19 \n",
      "executing epoch:66, it took 1.351141063372294 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->873.93|epoch train_loss = 970.30 \n",
      "executing epoch:67, it took 1.3514596859614054 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->873.93|epoch train_loss = 970.54 \n",
      "executing epoch:68, it took 1.3508790771166483 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->873.93|epoch train_loss = 970.85 \n",
      "executing epoch:69, it took 1.351224156220754 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->873.93|epoch train_loss = 971.31 \n",
      "executing epoch:70, it took 1.3509835561116537 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->873.93|epoch train_loss = 971.77 \n",
      "executing epoch:71, it took 1.3506980061531066 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->873.93|epoch train_loss = 972.01 \n",
      "executing epoch:72, it took 1.3504591266314188 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->873.93|epoch train_loss = 971.96 \n",
      "executing epoch:73, it took 1.3501548568407695 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->873.93|epoch train_loss = 971.89 \n",
      "executing epoch:74, it took 1.3499134302139282 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->873.93|epoch train_loss = 972.10 \n",
      "executing epoch:75, it took 1.3506646911303202 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->873.93|epoch train_loss = 972.33 \n",
      "executing epoch:76, it took 1.3506383061408997 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->873.93|epoch train_loss = 972.34 \n",
      "executing epoch:77, it took 1.3503300547599792 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->873.93|epoch train_loss = 971.87 \n",
      "executing epoch:78, it took 1.350373673439026 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->873.93|epoch train_loss = 971.69 \n",
      "executing epoch:79, it took 1.350013542175293 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->873.93|epoch train_loss = 971.62 \n",
      "executing epoch:80, it took 1.3513547937075296 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->873.93|epoch train_loss = 971.60 \n",
      "executing epoch:81, it took 1.3504849712053935 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->873.93|epoch train_loss = 971.58 \n",
      "executing epoch:82, it took 1.3504642446835835 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->873.93|epoch train_loss = 971.59 \n",
      "executing epoch:83, it took 1.3509843468666076 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->873.93|epoch train_loss = 971.63 \n",
      "executing epoch:84, it took 1.3509487112363179 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->873.93|epoch train_loss = 971.74 \n",
      "executing epoch:85, it took 1.3509629408518473 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->873.93|epoch train_loss = 971.83 \n",
      "executing epoch:86, it took 1.3505723079045613 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->873.93|epoch train_loss = 971.93 \n",
      "executing epoch:87, it took 1.350310782591502 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->873.93|epoch train_loss = 972.01 \n",
      "executing epoch:88, it took 1.3507824103037516 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->873.93|epoch train_loss = 972.11 \n",
      "executing epoch:89, it took 1.350673000017802 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->873.93|epoch train_loss = 972.22 \n",
      "executing epoch:90, it took 1.3501053730646768 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->873.93|epoch train_loss = 972.25 \n",
      "executing epoch:91, it took 1.3499773104985555 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->873.93|epoch train_loss = 972.31 \n",
      "executing epoch:92, it took 1.3501809557278952 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->873.93|epoch train_loss = 972.36 \n",
      "executing epoch:93, it took 1.3499733487764993 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->873.93|epoch train_loss = 972.42 \n",
      "executing epoch:94, it took 1.3500370224316915 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->873.93|epoch train_loss = 972.47 \n",
      "executing epoch:95, it took 1.35015918413798 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->873.93|epoch train_loss = 972.49 \n",
      "executing epoch:96, it took 1.3506010929743448 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->873.93|epoch train_loss = 972.51 \n",
      "executing epoch:97, it took 1.3504560271898904 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->873.93|epoch train_loss = 972.52 \n",
      "executing epoch:98, it took 1.3502014716466268 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->873.93|epoch train_loss = 972.53 \n",
      "executing epoch:99, it took 1.350032663345337 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->873.93|epoch train_loss = 972.54 \n",
      "executing epoch:100, it took 1.3503070871035259 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->873.93|epoch train_loss = 972.54 \n",
      "executing epoch:101, it took 1.3510117292404176 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->873.93|epoch train_loss = 972.54 \n",
      "executing epoch:102, it took 1.3501337846120198 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->873.93|epoch train_loss = 972.55 \n",
      "executing epoch:103, it took 1.351113001505534 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->873.93|epoch train_loss = 972.55 \n",
      "executing epoch:104, it took 1.3502579887708028 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->873.93|epoch train_loss = 972.55 \n",
      "executing epoch:105, it took 1.351082694530487 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->873.93|epoch train_loss = 972.55 \n",
      "executing epoch:106, it took 1.3507890542348227 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->873.93|epoch train_loss = 972.55 \n",
      "executing epoch:107, it took 1.3513890743255614 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->873.93|epoch train_loss = 972.56 \n",
      "executing epoch:108, it took 1.3505954662958781 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->873.93|epoch train_loss = 972.56 \n",
      "executing epoch:109, it took 1.3505477905273438 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->873.93|epoch train_loss = 972.56 \n",
      "executing epoch:110, it took 1.3500263293584187 mins from beginning of epoch till batch#525\n",
      "No improvement in training loss..the global training loss is -->873.93|epoch train_loss = 972.56 \n"
     ]
    }
   ],
   "source": [
    "tr_model = train_model(train_loader, val_loader, model =  model,tokenizer = tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb804637",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "68b3550f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'inp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_217/3314877709.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mids\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'inp' is not defined"
     ]
    }
   ],
   "source": [
    "for ids in inp:\n",
    "    print(tokenizer.decode(ids, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec4ec11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4580268f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc266b69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c31649",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d053070",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2cd2bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d90e13e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9e8660",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9450af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
