{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2bf00dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## reference https://huggingface.co/learn/nlp-course/en/chapter7/6?fw=pt#training-a-causal-language-model-from-scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62b61b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7eeba94",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting datasets\n",
      "  Downloading datasets-2.20.0-py3-none-any.whl (547 kB)\n",
      "\u001b[K     |████████████████████████████████| 547 kB 12.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting dill<0.3.9,>=0.3.0\n",
      "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "\u001b[K     |████████████████████████████████| 116 kB 107.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pyarrow>=15.0.0\n",
      "  Downloading pyarrow-16.1.0-cp38-cp38-manylinux_2_28_x86_64.whl (40.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 40.9 MB 98.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.8/site-packages (from datasets) (3.4.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.8/site-packages (from datasets) (1.21.4)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.8/site-packages (from datasets) (1.3.4)\n",
      "Collecting pyarrow-hotfix\n",
      "  Downloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
      "Collecting tqdm>=4.66.3\n",
      "  Downloading tqdm-4.66.4-py3-none-any.whl (78 kB)\n",
      "\u001b[K     |████████████████████████████████| 78 kB 107.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting multiprocess\n",
      "  Downloading multiprocess-0.70.16-py38-none-any.whl (132 kB)\n",
      "\u001b[K     |████████████████████████████████| 132 kB 120.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting aiohttp\n",
      "  Downloading aiohttp-3.9.5-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.3 MB 104.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: packaging in /opt/conda/lib/python3.8/site-packages (from datasets) (21.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.2 in /opt/conda/lib/python3.8/site-packages (from datasets) (0.23.4)\n",
      "Collecting xxhash\n",
      "  Downloading xxhash-3.4.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "\u001b[K     |████████████████████████████████| 194 kB 100.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting fsspec[http]<=2024.5.0,>=2023.1.0\n",
      "  Downloading fsspec-2024.5.0-py3-none-any.whl (316 kB)\n",
      "\u001b[K     |████████████████████████████████| 316 kB 128.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.8/site-packages (from datasets) (6.0)\n",
      "Collecting requests>=2.32.2\n",
      "  Downloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "\u001b[K     |████████████████████████████████| 64 kB 94.5 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting aiosignal>=1.1.2\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets) (21.2.0)\n",
      "Collecting multidict<7.0,>=4.5\n",
      "  Downloading multidict-6.0.5-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
      "\u001b[K     |████████████████████████████████| 129 kB 110.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting yarl<2.0,>=1.0\n",
      "  Downloading yarl-1.9.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (308 kB)\n",
      "\u001b[K     |████████████████████████████████| 308 kB 120.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting frozenlist>=1.1.1\n",
      "  Downloading frozenlist-1.4.1-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (240 kB)\n",
      "\u001b[K     |████████████████████████████████| 240 kB 116.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting async-timeout<5.0,>=4.0\n",
      "  Downloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.8/site-packages (from huggingface-hub>=0.21.2->datasets) (4.12.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging->datasets) (3.0.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.8/site-packages (from requests>=2.32.2->datasets) (2.0.8)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests>=2.32.2->datasets) (1.26.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests>=2.32.2->datasets) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests>=2.32.2->datasets) (3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.8/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.8/site-packages (from pandas->datasets) (2021.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.16.0)\n",
      "Installing collected packages: multidict, frozenlist, yarl, async-timeout, aiosignal, tqdm, requests, fsspec, dill, aiohttp, xxhash, pyarrow-hotfix, pyarrow, multiprocess, datasets\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.62.3\n",
      "    Uninstalling tqdm-4.62.3:\n",
      "      Successfully uninstalled tqdm-4.62.3\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.26.0\n",
      "    Uninstalling requests-2.26.0:\n",
      "      Successfully uninstalled requests-2.26.0\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2024.6.1\n",
      "    Uninstalling fsspec-2024.6.1:\n",
      "      Successfully uninstalled fsspec-2024.6.1\n",
      "  Attempting uninstall: pyarrow\n",
      "    Found existing installation: pyarrow 5.0.0\n",
      "    Uninstalling pyarrow-5.0.0:\n",
      "      Successfully uninstalled pyarrow-5.0.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchtext 0.12.0a0 requires torch==1.11.0a0+b6df043, but you have torch 2.3.1 which is incompatible.\u001b[0m\n",
      "Successfully installed aiohttp-3.9.5 aiosignal-1.3.1 async-timeout-4.0.3 datasets-2.20.0 dill-0.3.8 frozenlist-1.4.1 fsspec-2024.5.0 multidict-6.0.5 multiprocess-0.70.16 pyarrow-16.1.0 pyarrow-hotfix-0.6 requests-2.32.3 tqdm-4.66.4 xxhash-3.4.1 yarl-1.9.4\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting jupyter\n",
      "  Downloading jupyter-1.0.0-py2.py3-none-any.whl (2.7 kB)\n",
      "Collecting ipywidgets\n",
      "  Downloading ipywidgets-8.1.3-py3-none-any.whl (139 kB)\n",
      "\u001b[K     |████████████████████████████████| 139 kB 12.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: ipykernel in /opt/conda/lib/python3.8/site-packages (from jupyter) (6.6.0)\n",
      "Requirement already satisfied: nbconvert in /opt/conda/lib/python3.8/site-packages (from jupyter) (6.3.0)\n",
      "Collecting jupyter-console\n",
      "  Downloading jupyter_console-6.6.3-py3-none-any.whl (24 kB)\n",
      "Collecting qtconsole\n",
      "  Downloading qtconsole-5.5.2-py3-none-any.whl (123 kB)\n",
      "\u001b[K     |████████████████████████████████| 123 kB 90.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: notebook in /opt/conda/lib/python3.8/site-packages (from jupyter) (6.4.1)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (7.30.0)\n",
      "Collecting jupyterlab-widgets~=3.0.11\n",
      "  Downloading jupyterlab_widgets-3.0.11-py3-none-any.whl (214 kB)\n",
      "\u001b[K     |████████████████████████████████| 214 kB 161.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting widgetsnbextension~=4.0.11\n",
      "  Downloading widgetsnbextension-4.0.11-py3-none-any.whl (2.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.3 MB 114.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting comm>=0.1.3\n",
      "  Downloading comm-0.2.2-py3-none-any.whl (7.2 kB)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (4.8.0)\n",
      "Requirement already satisfied: setuptools>=18.5 in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (59.4.0)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.22)\n",
      "Requirement already satisfied: backcall in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: matplotlib-inline in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.3)\n",
      "Requirement already satisfied: pickleshare in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.7.5)\n",
      "Requirement already satisfied: jedi>=0.16 in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.18.1)\n",
      "Requirement already satisfied: decorator in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (5.1.0)\n",
      "Requirement already satisfied: pygments in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (2.10.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /opt/conda/lib/python3.8/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.8/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.8/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=6.1.0->ipywidgets) (0.2.5)\n",
      "Requirement already satisfied: debugpy<2.0,>=1.0.0 in /opt/conda/lib/python3.8/site-packages (from ipykernel->jupyter) (1.5.1)\n",
      "Requirement already satisfied: jupyter-client<8.0 in /opt/conda/lib/python3.8/site-packages (from ipykernel->jupyter) (7.1.0)\n",
      "Requirement already satisfied: tornado<7.0,>=4.2 in /opt/conda/lib/python3.8/site-packages (from ipykernel->jupyter) (6.1)\n",
      "Requirement already satisfied: jupyter-core>=4.6.0 in /opt/conda/lib/python3.8/site-packages (from jupyter-client<8.0->ipykernel->jupyter) (4.9.1)\n",
      "Requirement already satisfied: entrypoints in /opt/conda/lib/python3.8/site-packages (from jupyter-client<8.0->ipykernel->jupyter) (0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /opt/conda/lib/python3.8/site-packages (from jupyter-client<8.0->ipykernel->jupyter) (2.8.2)\n",
      "Requirement already satisfied: pyzmq>=13 in /opt/conda/lib/python3.8/site-packages (from jupyter-client<8.0->ipykernel->jupyter) (22.3.0)\n",
      "Requirement already satisfied: nest-asyncio>=1.5 in /opt/conda/lib/python3.8/site-packages (from jupyter-client<8.0->ipykernel->jupyter) (1.5.4)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.8/site-packages (from python-dateutil>=2.1->jupyter-client<8.0->ipykernel->jupyter) (1.16.0)\n",
      "Collecting prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0\n",
      "  Downloading prompt_toolkit-3.0.47-py3-none-any.whl (386 kB)\n",
      "\u001b[K     |████████████████████████████████| 386 kB 104.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting jupyter-core>=4.6.0\n",
      "  Downloading jupyter_core-5.7.2-py3-none-any.whl (28 kB)\n",
      "Collecting ipykernel\n",
      "  Downloading ipykernel-6.29.5-py3-none-any.whl (117 kB)\n",
      "\u001b[K     |████████████████████████████████| 117 kB 118.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting traitlets>=4.3.1\n",
      "  Downloading traitlets-5.14.3-py3-none-any.whl (85 kB)\n",
      "\u001b[K     |████████████████████████████████| 85 kB 82.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting debugpy<2.0,>=1.0.0\n",
      "  Downloading debugpy-1.8.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.1 MB 134.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pyzmq>=13\n",
      "  Downloading pyzmq-26.0.3-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (912 kB)\n",
      "\u001b[K     |████████████████████████████████| 912 kB 104.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: psutil in /opt/conda/lib/python3.8/site-packages (from ipykernel->jupyter) (5.8.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.8/site-packages (from ipykernel->jupyter) (21.3)\n",
      "Collecting platformdirs>=2.5\n",
      "  Downloading platformdirs-4.2.2-py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: bleach in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (4.1.0)\n",
      "Requirement already satisfied: defusedxml in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (0.7.1)\n",
      "Requirement already satisfied: testpath in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (0.5.0)\n",
      "Requirement already satisfied: jupyterlab-pygments in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (0.1.2)\n",
      "Requirement already satisfied: mistune<2,>=0.8.1 in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (0.8.4)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (1.5.0)\n",
      "Requirement already satisfied: nbclient<0.6.0,>=0.5.0 in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (0.5.9)\n",
      "Requirement already satisfied: nbformat>=4.4 in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (5.1.3)\n",
      "Requirement already satisfied: jinja2>=2.4 in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (3.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.8/site-packages (from jinja2>=2.4->nbconvert->jupyter) (2.0.1)\n",
      "Requirement already satisfied: ipython-genutils in /opt/conda/lib/python3.8/site-packages (from nbformat>=4.4->nbconvert->jupyter) (0.2.0)\n",
      "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /opt/conda/lib/python3.8/site-packages (from nbformat>=4.4->nbconvert->jupyter) (4.2.1)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /opt/conda/lib/python3.8/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.4->nbconvert->jupyter) (0.18.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /opt/conda/lib/python3.8/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.4->nbconvert->jupyter) (21.2.0)\n",
      "Requirement already satisfied: importlib-resources>=1.4.0 in /opt/conda/lib/python3.8/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.4->nbconvert->jupyter) (5.4.0)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /opt/conda/lib/python3.8/site-packages (from importlib-resources>=1.4.0->jsonschema!=2.5.0,>=2.4->nbformat>=4.4->nbconvert->jupyter) (3.6.0)\n",
      "Requirement already satisfied: webencodings in /opt/conda/lib/python3.8/site-packages (from bleach->nbconvert->jupyter) (0.5.1)\n",
      "Requirement already satisfied: Send2Trash>=1.5.0 in /opt/conda/lib/python3.8/site-packages (from notebook->jupyter) (1.8.0)\n",
      "Requirement already satisfied: terminado>=0.8.3 in /opt/conda/lib/python3.8/site-packages (from notebook->jupyter) (0.12.1)\n",
      "Requirement already satisfied: prometheus-client in /opt/conda/lib/python3.8/site-packages (from notebook->jupyter) (0.12.0)\n",
      "Requirement already satisfied: argon2-cffi in /opt/conda/lib/python3.8/site-packages (from notebook->jupyter) (21.1.0)\n",
      "Requirement already satisfied: cffi>=1.0.0 in /opt/conda/lib/python3.8/site-packages (from argon2-cffi->notebook->jupyter) (1.15.0)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.8/site-packages (from cffi>=1.0.0->argon2-cffi->notebook->jupyter) (2.21)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging->ipykernel->jupyter) (3.0.6)\n",
      "Collecting qtpy>=2.4.0\n",
      "  Downloading QtPy-2.4.1-py3-none-any.whl (93 kB)\n",
      "\u001b[K     |████████████████████████████████| 93 kB 77.1 MB/s  eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: traitlets, platformdirs, pyzmq, jupyter-core, prompt-toolkit, debugpy, comm, widgetsnbextension, qtpy, jupyterlab-widgets, ipykernel, qtconsole, jupyter-console, ipywidgets, jupyter\n",
      "  Attempting uninstall: traitlets\n",
      "    Found existing installation: traitlets 5.1.1\n",
      "    Uninstalling traitlets-5.1.1:\n",
      "      Successfully uninstalled traitlets-5.1.1\n",
      "  Attempting uninstall: pyzmq\n",
      "    Found existing installation: pyzmq 22.3.0\n",
      "    Uninstalling pyzmq-22.3.0:\n",
      "      Successfully uninstalled pyzmq-22.3.0\n",
      "  Attempting uninstall: jupyter-core\n",
      "    Found existing installation: jupyter-core 4.9.1\n",
      "    Uninstalling jupyter-core-4.9.1:\n",
      "      Successfully uninstalled jupyter-core-4.9.1\n",
      "  Attempting uninstall: prompt-toolkit\n",
      "    Found existing installation: prompt-toolkit 3.0.22\n",
      "    Uninstalling prompt-toolkit-3.0.22:\n",
      "      Successfully uninstalled prompt-toolkit-3.0.22\n",
      "  Attempting uninstall: debugpy\n",
      "    Found existing installation: debugpy 1.5.1\n",
      "    Uninstalling debugpy-1.5.1:\n",
      "      Successfully uninstalled debugpy-1.5.1\n",
      "  Attempting uninstall: ipykernel\n",
      "    Found existing installation: ipykernel 6.6.0\n",
      "    Uninstalling ipykernel-6.6.0:\n",
      "      Successfully uninstalled ipykernel-6.6.0\n",
      "Successfully installed comm-0.2.2 debugpy-1.8.2 ipykernel-6.29.5 ipywidgets-8.1.3 jupyter-1.0.0 jupyter-console-6.6.3 jupyter-core-5.7.2 jupyterlab-widgets-3.0.11 platformdirs-4.2.2 prompt-toolkit-3.0.47 pyzmq-26.0.3 qtconsole-5.5.2 qtpy-2.4.1 traitlets-5.14.3 widgetsnbextension-4.0.11\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#installing some libraries\n",
    "!pip install datasets\n",
    "!pip install --upgrade jupyter ipywidgets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0a81c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch, transformers\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import PreTrainedModel, PretrainedConfig\n",
    "from transformers import AutoModel, AutoConfig,AutoModelForCausalLM, AutoConfig\n",
    "\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "#from datasets import load_dataset\n",
    "import pandas as pd, numpy as np\n",
    "from torch import cuda\n",
    "import datetime\n",
    "import warnings,itertools\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import json\n",
    "pre_train = True\n",
    "\n",
    "# Ignore all warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "#pip install transformers bitsandbytes>=0.39.0 -q\n",
    "import zipfile,logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a46d4388",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, GPT2Config\n",
    "from nltk import pos_tag, word_tokenize\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "import random\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09a322df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cc32acd9a4d465499732ff4a009f628",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d53459a1bd04b33997ccdc1d7c734f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2c0bf9746074a3ba1bc08a979932081",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9276b000db0b40129cec70d872614c48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9bb794ec5834b999a03d6076f7cfc50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d41cd3ec709499dab28dd4d7c8d5850",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1734e3e5a27049598ba546cad7a36d1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import nltk\n",
    "from nltk import pos_tag, word_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "import random\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Load model and tokenizer\n",
    "model_name = 'gpt2'\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "167e01d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "model\n"
     ]
    }
   ],
   "source": [
    "#global params for training\n",
    "\n",
    "B,T = 32,1024\n",
    "epoch = 100\n",
    "random_init_wts = True\n",
    "min_text_len = 0\n",
    "# train_loss_list = []\n",
    "# val_loss_list =[]\n",
    "if cuda.is_available():\n",
    "    device = torch.device('cuda:0')\n",
    "    print(device)\n",
    "else:\n",
    "    device = 'cpu'\n",
    "#print(device)\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "#os.environ[\"MKL_DEBUG_CPU_TYPE\"] = \"5\"\n",
    "context_length = None\n",
    "global_tr_loss = torch.inf\n",
    "global_val_loss = torch.inf\n",
    "#print(global_tr_loss)\n",
    "model_path = os.path.join(\"model\")\n",
    "print(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c28f573",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./data/unzip_text_10M'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "directory = os.path.join('.','data','unzip_text_10M')  # Replace with your directory path\n",
    "directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc43e775",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_text(directory):\n",
    "    directory = os.path.join('.','data','unzip_text_10M',str(directory))  # Replace with your directory path\n",
    "    print(f\"directory :{directory}\")\n",
    "    # List all files in the directory\n",
    "    files = [f for f in os.listdir(directory) if os.path.isfile(os.path.join(directory, f))]\n",
    "    print(f\"files:{files}\")\n",
    "    text_content = []\n",
    "    # Read each file\n",
    "    total_lines = 0\n",
    "    for filenum,filename in enumerate(files):\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            #first_line = file.read()\n",
    "            #print(f\"filename :{filename}->first few lines {first_line}\")\n",
    "            #continue\n",
    "            #lines_list = [line.strip() for line in open(file_path, 'r')]\n",
    "            text = file.read()\n",
    "            text_content.append(text)\n",
    "            print(f\"the file:{filename} has been appeneded to the uber list and its length is {len(text_content)} \")\n",
    "            #total_lines+=len(lines_list)\n",
    "            #text_content.append(lines_list)\n",
    "    \n",
    "    flattened_list = ''.join(text_content)\n",
    "    assert (len(flattened_list) == total_lines , f\"Expected {len(flattened_list)} to be equal to {total_lines}\" )\n",
    "    \n",
    "    return flattened_list\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "87c53634",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "directory :./data/unzip_text_10M/train_10M\n",
      "files:['switchboard.train', 'simple_wiki.train', 'open_subtitles.train', 'gutenberg.train', 'childes.train', 'bnc_spoken.train']\n",
      "the file:switchboard.train has been appeneded to the uber list and its length is 1 \n",
      "the file:simple_wiki.train has been appeneded to the uber list and its length is 2 \n",
      "the file:open_subtitles.train has been appeneded to the uber list and its length is 3 \n",
      "the file:gutenberg.train has been appeneded to the uber list and its length is 4 \n",
      "the file:childes.train has been appeneded to the uber list and its length is 5 \n",
      "the file:bnc_spoken.train has been appeneded to the uber list and its length is 6 \n"
     ]
    }
   ],
   "source": [
    "train_list = read_text(\"train_10M\")\n",
    "#print(train_dict)\n",
    "#val_list = read_text(\"dev\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aa8c4b33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54215049"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c58c9597",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1654\n"
     ]
    }
   ],
   "source": [
    "chunks = len(train_list)//(B*T)\n",
    "print(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "11d99691",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = 'gpt2'\n",
    "# model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "# tokenizer = GPT2Tokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "717996b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17191971 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\",return_tensors = \"pt\" , truncate = True, return_overflowing_tokens=True , padding = False,)\n",
    "enc_train = tokenizer(train_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "effe28a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.1535097982657136"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comp_ratio = len(train_list)/len(enc_train['input_ids'])\n",
    "comp_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8229f307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "directory :./data/unzip_text_10M/dev\n",
      "files:['switchboard.dev', 'simple_wiki.dev', 'open_subtitles.dev', 'gutenberg.dev', 'childes.dev', 'bnc_spoken.dev']\n",
      "the file:switchboard.dev has been appeneded to the uber list and its length is 1 \n",
      "the file:simple_wiki.dev has been appeneded to the uber list and its length is 2 \n",
      "the file:open_subtitles.dev has been appeneded to the uber list and its length is 3 \n",
      "the file:gutenberg.dev has been appeneded to the uber list and its length is 4 \n",
      "the file:childes.dev has been appeneded to the uber list and its length is 5 \n",
      "the file:bnc_spoken.dev has been appeneded to the uber list and its length is 6 \n"
     ]
    }
   ],
   "source": [
    "val_list = read_text(\"dev\")\n",
    "enc_val = tokenizer(val_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "71d5b442",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df_from_list(enc , B= B, T = T, val= False):\n",
    "    chunk_size = B*T\n",
    "    if val:\n",
    "        chunk_size = 2*B*T\n",
    "        \n",
    "    long_list_inp = enc['input_ids']\n",
    "    long_list_attention = enc['attention_mask']\n",
    "\n",
    "    # Step 3: Split the list into chunks and pad the last chunk if necessary\n",
    "    chunks_inp = [long_list_inp[i:i + chunk_size] for i in range(0, len(long_list_inp), chunk_size)]\n",
    "    chunks_att = [long_list_attention[i:i + chunk_size] for i in range(0, len(long_list_attention), chunk_size)]\n",
    "    df = pd.DataFrame({'input_ids': chunks_inp,'attention_mask':chunks_att})\n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c444da1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the dataframe is = 525\n"
     ]
    }
   ],
   "source": [
    "df_train_temp = get_df_from_list(enc_train)\n",
    "# Display the DataFrame\n",
    "df_train_temp.head()\n",
    "print(f\"Length of the dataframe is = {len(df_train_temp)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "11611d82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the VALDATION dataframe is = 532\n"
     ]
    }
   ],
   "source": [
    "df_val_temp = get_df_from_list(enc_val)\n",
    "# Display the DataFrame\n",
    "df_val_temp.head()\n",
    "print(f\"Length of the VALDATION dataframe is = {len(df_val_temp)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a29bd9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_df(df,eos_char = tokenizer.eos_token_id,B = B, T = T,val = False):\n",
    "    if val:\n",
    "        B = 2*B\n",
    "    for ind,row in df.iterrows():\n",
    "        if len(row['input_ids']) != B*T :\n",
    "            print(f\"row = {ind} and input_id length = {len(row['input_ids'])}\")\n",
    "            print(f\"row = {ind} and attention length = {len(row['attention_mask'])}\")\n",
    "            pad_len = B*T - len(row['input_ids'])\n",
    "            print(f\"padding the row index {ind} with {pad_len} character\")\n",
    "            row['input_ids'] = row['input_ids']+ [eos_char] * pad_len\n",
    "            #attention mask should be padded to 0\n",
    "            row['attention_mask'] = row['attention_mask']+ [0] * pad_len\n",
    "            print(\"#### POST CONCAT####\")\n",
    "            print(f\"row = {ind} and input_id length = {len(row['input_ids'])}\")\n",
    "            print(f\"row = {ind} and attention length ={len(row['attention_mask'])}\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def verify_len(df,val = False, B= B):\n",
    "    row_ind = []\n",
    "    if val:\n",
    "        B = 2*B\n",
    "    for ind,row in df.iterrows():\n",
    "        if len(row['input_ids']) != B*T :\n",
    "            row_ind.append(ind)\n",
    "        else:\n",
    "            continue\n",
    "    if len(row_ind) !=0:\n",
    "        print(\"CONCATENATION Did not work\")\n",
    "    else:\n",
    "        print(\"CONCATENATION worked\")\n",
    "        \n",
    "            \n",
    "        \n",
    "    \n",
    "        \n",
    "        \n",
    "   \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6c1f457f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row = 524 and input_id length = 21539\n",
      "row = 524 and attention length = 21539\n",
      "padding the row index 524 with 11229 character\n",
      "#### POST CONCAT####\n",
      "row = 524 and input_id length = 32768\n",
      "row = 524 and attention length =32768\n",
      "CONCATENATION worked\n"
     ]
    }
   ],
   "source": [
    "df_train  = pad_df(df_train_temp)\n",
    "verify_len(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "77f1988e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row = 531 and input_id length = 13588\n",
      "row = 531 and attention length = 13588\n",
      "padding the row index 531 with 19180 character\n",
      "#### POST CONCAT####\n",
      "row = 531 and input_id length = 32768\n",
      "row = 531 and attention length =32768\n",
      "CONCATENATION worked\n"
     ]
    }
   ],
   "source": [
    "df_val  = pad_df(df_val_temp)\n",
    "verify_len(df_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2bd35aa3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_ids</th>\n",
       "      <th>attention_mask</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[32, 25, 197, 40, 1101, 1654, 484, 389, 13, 19...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[6510, 11, 14104, 290, 27913, 13, 198, 32, 25,...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[314, 1612, 11, 340, 338, 1611, 286, 43244, 11...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[318, 407, 922, 329, 262, 1200, 2035, 13, 198,...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[198, 32, 25, 197, 1870, 11, 21480, 11, 314, 4...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           input_ids  \\\n",
       "0  [32, 25, 197, 40, 1101, 1654, 484, 389, 13, 19...   \n",
       "1  [6510, 11, 14104, 290, 27913, 13, 198, 32, 25,...   \n",
       "2  [314, 1612, 11, 340, 338, 1611, 286, 43244, 11...   \n",
       "3  [318, 407, 922, 329, 262, 1200, 2035, 13, 198,...   \n",
       "4  [198, 32, 25, 197, 1870, 11, 21480, 11, 314, 4...   \n",
       "\n",
       "                                      attention_mask  \n",
       "0  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "1  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "2  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "3  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "4  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "31d478f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_token_pos_mapping(tokenizer):\n",
    "    token_pos_map = {}\n",
    "    for word, token_id in tokenizer.get_vocab().items():\n",
    "        if word.startswith('Ġ'):  # GPT-2 uses 'Ġ' to denote word beginnings\n",
    "            word = word[1:]\n",
    "            #print(f\"word = {word}|token_id = {token_id}\")\n",
    "        pos = pos_tag([word])[0][1]\n",
    "        #print(f\"pos = {pos}\")\n",
    "        token_pos_map[token_id] = pos\n",
    "    return token_pos_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8761c97e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "token_pos_map = create_token_pos_mapping(tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d6b32f6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NNS'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_pos_map[50000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5729afb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_verb_synonyms(word):\n",
    "    synsets = wordnet.synsets(word, pos=wordnet.VERB)\n",
    "    synonyms = list(set([lemma.name() for synset in synsets for lemma in synset.lemmas() if lemma.name() != word]))\n",
    "    return synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "46990b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to get verb synonyms\n",
    "\n",
    "\n",
    "# Function to prepare input with synonym embeddings\n",
    "def prepare_input_with_synonym_embeddings(input_ids, model = model, tokenizer = tokenizer, token_pos_map = create_token_pos_mapping(tokenizer), replacement_prob=0.5):\n",
    "    with torch.no_grad():\n",
    "        original_embeddings = model.transformer.wte(input_ids).clone()\n",
    "        print(f\"original ebedding shape = {original_embeddings.shape}\")\n",
    "        \n",
    "    embed_list = []\n",
    "    for i, token_id in enumerate(input_ids):\n",
    "        #print(f\"i = {i} | token_id = {token_id}\")\n",
    "        if token_pos_map.get(token_id.item(), '').startswith('VB') and random.random() < replacement_prob:\n",
    "            word = tokenizer.decode([token_id])\n",
    "            synonyms = get_verb_synonyms(word)\n",
    "            #print(f\"the {token_pos_map.get(token_id.item())}| verb found ...word = {word}| synonym = {synonyms}\")\n",
    "            \n",
    "            if synonyms:\n",
    "                print(f\"inside synonym |verb  = {word}| {token_pos_map.get(token_id.item())}|synonym = {synonyms}\")\n",
    "                synonym = random.choice(synonyms)\n",
    "                print(f\"post randomly choosing synonym... the value chosen is {synonym}\")\n",
    "                synonym_tokens = tokenizer.encode(synonym, add_special_tokens=False)\n",
    "                print(f\"synonym_tokens = {synonym_tokens}\" )\n",
    "                with torch.no_grad():\n",
    "                    synonym_embeddings = model.transformer.wte(torch.tensor([synonym_tokens]))\n",
    "                    print(f\"synonym_embeddings shape = {synonym_embeddings.shape}\")\n",
    "\n",
    "                mean_synonym_embedding = synonym_embeddings.mean(dim=1)\n",
    "                embed_list.append(torch.squeeze(mean_synonym_embedding,dim = 0))\n",
    "                \n",
    "                print(f\"mean_synonym_embedding shape = {mean_synonym_embedding.shape}\")\n",
    "\n",
    "            else:\n",
    "                embed_list.append(torch.squeeze(model.transformer.wte(token_id),dim = 0))\n",
    "        else:\n",
    "            embed_list.append(torch.squeeze(model.transformer.wte(token_id),dim = 0))\n",
    "                    \n",
    "    \n",
    "    \n",
    "    return torch.stack(embed_list)\n",
    "\n",
    "# Rest of the code (GPT2WithCustomEmbeddings class and training loop) remains the same\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "fcc1f9e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32768"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp = torch.tensor(df_train.iloc[4]['input_ids'])\n",
    "len(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a15b411c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original ebedding shape = torch.Size([32768, 768])\n",
      "inside synonym |verb  = See| VB|synonym = ['visualise', 'ensure', 'find_out', 'see_to_it', 'experience', 'fancy', 'assure', 'look', 'ascertain', 'catch', 'interpret', 'regard', 'construe', 'watch', 'date', 'visualize', 'attend', 'project', 'figure', 'learn', 'determine', 'realise', 'take_in', 'run_across', 'see', 'reckon', 'get_word', 'realize', 'get_wind', 'meet', 'examine', 'check', 'image', 'consider', 'envision', 'witness', 'visit', 'insure', 'pick_up', 'go_through', 'come_across', 'go_out', 'understand', 'discover', 'run_into', 'control', 'go_steady', 'take_care', 'escort', 'get_a_line', 'view', 'encounter', 'find', 'hear', 'picture']\n",
      "post randomly choosing synonym... the value chosen is interpret\n",
      "synonym_tokens = [27381]\n",
      "synonym_embeddings shape = torch.Size([1, 1, 768])\n",
      "mean_synonym_embedding shape = torch.Size([1, 768])\n",
      "inside synonym |verb  = make| VB|synonym = ['get', 'make_water', 'urinate', 'wee-wee', 'wee', 'have', 'work', 'stool', 'ready', 'puddle', 'seduce', 'earn', 'pretend', 'name', 'arrive_at', 'produce', 'take', 'pass_water', 'pee-pee', 'prepare', 'create', 'take_in', 'realise', 'crap', 'shit', 'realize', 'pull_in', 'throw', 'progress_to', 'construct', 'make_believe', 'reach', 'fix', 'form', 'make_up', 'induce', 'nominate', 'piddle', 'do', 'draw', 'lay_down', 'clear', 'constitute', 'cook', 'build', 'take_a_shit', 'pee', 'spend_a_penny', 'gain', 'piss', 'relieve_oneself', 'take_a_crap', 'defecate', 'attain', 'cause', 'score', 'ca-ca', 'take_a_leak', 'give', 'bring_in', 'establish', 'get_to', 'hold', 'stimulate', 'hit', 'micturate']\n",
      "post randomly choosing synonym... the value chosen is attain\n",
      "synonym_tokens = [1078, 391]\n",
      "synonym_embeddings shape = torch.Size([1, 2, 768])\n",
      "mean_synonym_embedding shape = torch.Size([1, 768])\n",
      "inside synonym |verb  = is| VBZ|synonym = ['be', 'live', 'embody', 'exist', 'follow', 'represent', 'equal', 'constitute', 'comprise', 'cost', 'personify', 'make_up']\n",
      "post randomly choosing synonym... the value chosen is embody\n",
      "synonym_tokens = [368, 2618]\n",
      "synonym_embeddings shape = torch.Size([1, 2, 768])\n",
      "mean_synonym_embedding shape = torch.Size([1, 768])\n",
      "inside synonym |verb  = See| VB|synonym = ['visualise', 'ensure', 'find_out', 'see_to_it', 'experience', 'fancy', 'assure', 'look', 'ascertain', 'catch', 'interpret', 'regard', 'construe', 'watch', 'date', 'visualize', 'attend', 'project', 'figure', 'learn', 'determine', 'realise', 'take_in', 'run_across', 'see', 'reckon', 'get_word', 'realize', 'get_wind', 'meet', 'examine', 'check', 'image', 'consider', 'envision', 'witness', 'visit', 'insure', 'pick_up', 'go_through', 'come_across', 'go_out', 'understand', 'discover', 'run_into', 'control', 'go_steady', 'take_care', 'escort', 'get_a_line', 'view', 'encounter', 'find', 'hear', 'picture']\n",
      "post randomly choosing synonym... the value chosen is come_across\n",
      "synonym_tokens = [2958, 62, 330, 1214]\n",
      "synonym_embeddings shape = torch.Size([1, 4, 768])\n",
      "mean_synonym_embedding shape = torch.Size([1, 768])\n",
      "inside synonym |verb  = Getting| VBG|synonym = ['set_out', 'get', 'experience', 'arrest', 'sustain', 'have', 'puzzle', 'amaze', 'get_down', 'drive', 'commence', 'gravel', 'beat', 'catch', 'dumbfound', 'start_out', 'start', 'produce', 'bewilder', 'sire', 'come', 'contract', 'acquire', 'take', 'stupefy', 'bring_forth', 'beget', 'go', 'buzz_off', 'make', 'perplex', 'nonplus', 'father', 'generate', 'let', 'fuck_off', 'find', 'fix', 'vex', 'convey', 'incur', 'become', 'induce', 'pay_off', 'aim', 'arrive', 'bring', 'draw', 'suffer', \"get_under_one's_skin\", 'pay_back', 'capture', 'mother', 'scram', 'begin', 'baffle', 'set_about', 'receive', 'cause', 'grow', 'develop', 'stick', 'bugger_off', 'mystify', 'flummox', 'obtain', 'fetch', 'stimulate', 'pose', 'engender']\n",
      "post randomly choosing synonym... the value chosen is begin\n",
      "synonym_tokens = [27471]\n",
      "synonym_embeddings shape = torch.Size([1, 1, 768])\n",
      "mean_synonym_embedding shape = torch.Size([1, 768])\n",
      "inside synonym |verb  = see| VB|synonym = ['visualise', 'ensure', 'find_out', 'see_to_it', 'experience', 'fancy', 'assure', 'look', 'ascertain', 'catch', 'interpret', 'regard', 'construe', 'watch', 'date', 'visualize', 'attend', 'project', 'figure', 'learn', 'determine', 'realise', 'take_in', 'run_across', 'reckon', 'get_word', 'get_wind', 'realize', 'meet', 'examine', 'check', 'image', 'consider', 'envision', 'witness', 'visit', 'insure', 'pick_up', 'go_through', 'come_across', 'go_out', 'understand', 'discover', 'run_into', 'control', 'go_steady', 'take_care', 'escort', 'get_a_line', 'view', 'encounter', 'find', 'hear', 'picture']\n",
      "post randomly choosing synonym... the value chosen is run_into\n",
      "synonym_tokens = [5143, 62, 20424]\n",
      "synonym_embeddings shape = torch.Size([1, 3, 768])\n",
      "mean_synonym_embedding shape = torch.Size([1, 768])\n",
      "inside synonym |verb  = are| VBP|synonym = ['be', 'live', 'embody', 'exist', 'follow', 'represent', 'equal', 'constitute', 'comprise', 'cost', 'personify', 'make_up']\n",
      "post randomly choosing synonym... the value chosen is make_up\n",
      "synonym_tokens = [15883, 62, 929]\n",
      "synonym_embeddings shape = torch.Size([1, 3, 768])\n",
      "mean_synonym_embedding shape = torch.Size([1, 768])\n",
      "inside synonym |verb  = let| VB|synonym = ['lease', 'get', 'countenance', 'have', 'permit', 'allow', 'rent']\n",
      "post randomly choosing synonym... the value chosen is get\n",
      "synonym_tokens = [1136]\n",
      "synonym_embeddings shape = torch.Size([1, 1, 768])\n",
      "mean_synonym_embedding shape = torch.Size([1, 768])\n",
      "inside synonym |verb  = is| VBZ|synonym = ['be', 'live', 'embody', 'exist', 'follow', 'represent', 'equal', 'constitute', 'comprise', 'cost', 'personify', 'make_up']\n",
      "post randomly choosing synonym... the value chosen is exist\n",
      "synonym_tokens = [38476]\n",
      "synonym_embeddings shape = torch.Size([1, 1, 768])\n",
      "mean_synonym_embedding shape = torch.Size([1, 768])\n",
      "inside synonym |verb  = is| VBZ|synonym = ['be', 'live', 'embody', 'exist', 'follow', 'represent', 'equal', 'constitute', 'comprise', 'cost', 'personify', 'make_up']\n",
      "post randomly choosing synonym... the value chosen is comprise\n",
      "synonym_tokens = [785, 7919]\n",
      "synonym_embeddings shape = torch.Size([1, 2, 768])\n",
      "mean_synonym_embedding shape = torch.Size([1, 768])\n",
      "inside synonym |verb  = do| VB|synonym = ['set', 'serve', 'coiffe', 'practise', 'suffice', 'come', 'coiffure', 'get_along', 'exercise', 'make', 'perform', 'execute', 'behave', 'answer', 'arrange', 'practice', 'manage', 'fare', 'act', 'make_out', 'cause', 'dress', 'coif']\n",
      "post randomly choosing synonym... the value chosen is coif\n",
      "synonym_tokens = [1073, 361]\n",
      "synonym_embeddings shape = torch.Size([1, 2, 768])\n",
      "mean_synonym_embedding shape = torch.Size([1, 768])\n",
      "inside synonym |verb  = is| VBZ|synonym = ['be', 'live', 'embody', 'exist', 'follow', 'represent', 'equal', 'constitute', 'comprise', 'cost', 'personify', 'make_up']\n",
      "post randomly choosing synonym... the value chosen is exist\n",
      "synonym_tokens = [38476]\n",
      "synonym_embeddings shape = torch.Size([1, 1, 768])\n",
      "mean_synonym_embedding shape = torch.Size([1, 768])\n",
      "inside synonym |verb  = is| VBZ|synonym = ['be', 'live', 'embody', 'exist', 'follow', 'represent', 'equal', 'constitute', 'comprise', 'cost', 'personify', 'make_up']\n",
      "post randomly choosing synonym... the value chosen is be\n",
      "synonym_tokens = [1350]\n",
      "synonym_embeddings shape = torch.Size([1, 1, 768])\n",
      "mean_synonym_embedding shape = torch.Size([1, 768])\n",
      "inside synonym |verb  = is| VBZ|synonym = ['be', 'live', 'embody', 'exist', 'follow', 'represent', 'equal', 'constitute', 'comprise', 'cost', 'personify', 'make_up']\n",
      "post randomly choosing synonym... the value chosen is make_up\n",
      "synonym_tokens = [15883, 62, 929]\n",
      "synonym_embeddings shape = torch.Size([1, 3, 768])\n",
      "mean_synonym_embedding shape = torch.Size([1, 768])\n",
      "inside synonym |verb  = Be| VB|synonym = ['be', 'live', 'embody', 'exist', 'follow', 'represent', 'equal', 'constitute', 'comprise', 'cost', 'personify', 'make_up']\n",
      "post randomly choosing synonym... the value chosen is comprise\n",
      "synonym_tokens = [785, 7919]\n",
      "synonym_embeddings shape = torch.Size([1, 2, 768])\n",
      "mean_synonym_embedding shape = torch.Size([1, 768])\n",
      "inside synonym |verb  = Do| VB|synonym = ['set', 'serve', 'coiffe', 'practise', 'suffice', 'come', 'coiffure', 'get_along', 'exercise', 'make', 'perform', 'execute', 'behave', 'answer', 'arrange', 'do', 'practice', 'manage', 'fare', 'act', 'make_out', 'cause', 'dress', 'coif']\n",
      "post randomly choosing synonym... the value chosen is coif\n",
      "synonym_tokens = [1073, 361]\n",
      "synonym_embeddings shape = torch.Size([1, 2, 768])\n",
      "mean_synonym_embedding shape = torch.Size([1, 768])\n",
      "inside synonym |verb  = is| VBZ|synonym = ['be', 'live', 'embody', 'exist', 'follow', 'represent', 'equal', 'constitute', 'comprise', 'cost', 'personify', 'make_up']\n",
      "post randomly choosing synonym... the value chosen is constitute\n",
      "synonym_tokens = [9979, 3678]\n",
      "synonym_embeddings shape = torch.Size([1, 2, 768])\n",
      "mean_synonym_embedding shape = torch.Size([1, 768])\n",
      "inside synonym |verb  = Do| VB|synonym = ['set', 'serve', 'coiffe', 'practise', 'suffice', 'come', 'coiffure', 'get_along', 'exercise', 'make', 'perform', 'execute', 'behave', 'answer', 'arrange', 'do', 'practice', 'manage', 'fare', 'act', 'make_out', 'cause', 'dress', 'coif']\n",
      "post randomly choosing synonym... the value chosen is cause\n",
      "synonym_tokens = [25587]\n",
      "synonym_embeddings shape = torch.Size([1, 1, 768])\n",
      "mean_synonym_embedding shape = torch.Size([1, 768])\n",
      "inside synonym |verb  = Do| VB|synonym = ['set', 'serve', 'coiffe', 'practise', 'suffice', 'come', 'coiffure', 'get_along', 'exercise', 'make', 'perform', 'execute', 'behave', 'answer', 'arrange', 'do', 'practice', 'manage', 'fare', 'act', 'make_out', 'cause', 'dress', 'coif']\n",
      "post randomly choosing synonym... the value chosen is practise\n",
      "synonym_tokens = [29152, 786]\n",
      "synonym_embeddings shape = torch.Size([1, 2, 768])\n",
      "mean_synonym_embedding shape = torch.Size([1, 768])\n",
      "inside synonym |verb  = Let| VB|synonym = ['lease', 'get', 'countenance', 'have', 'let', 'permit', 'allow', 'rent']\n",
      "post randomly choosing synonym... the value chosen is permit\n",
      "synonym_tokens = [525, 2781]\n",
      "synonym_embeddings shape = torch.Size([1, 2, 768])\n",
      "mean_synonym_embedding shape = torch.Size([1, 768])\n",
      "inside synonym |verb  = is| VBZ|synonym = ['be', 'live', 'embody', 'exist', 'follow', 'represent', 'equal', 'constitute', 'comprise', 'cost', 'personify', 'make_up']\n",
      "post randomly choosing synonym... the value chosen is embody\n",
      "synonym_tokens = [368, 2618]\n",
      "synonym_embeddings shape = torch.Size([1, 2, 768])\n",
      "mean_synonym_embedding shape = torch.Size([1, 768])\n",
      "inside synonym |verb  = Do| VB|synonym = ['set', 'serve', 'coiffe', 'practise', 'suffice', 'come', 'coiffure', 'get_along', 'exercise', 'make', 'perform', 'execute', 'behave', 'answer', 'arrange', 'do', 'practice', 'manage', 'fare', 'act', 'make_out', 'cause', 'dress', 'coif']\n",
      "post randomly choosing synonym... the value chosen is do\n",
      "synonym_tokens = [4598]\n",
      "synonym_embeddings shape = torch.Size([1, 1, 768])\n",
      "mean_synonym_embedding shape = torch.Size([1, 768])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "embed = prepare_input_with_synonym_embeddings(input_ids = inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "48a649d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32768, 768])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "32fac24d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32768"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B*T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbbad812",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f693a93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3c637e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e4ae3cf9",
   "metadata": {},
   "source": [
    "## Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "338357e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the tokenizer:\n",
    "model_name = \"gpt2\"\n",
    "if random_init_wts:\n",
    "    config = AutoConfig.from_pretrained(model_name, vocab_size = 50304)\n",
    "    # Initialize the model with random weights\n",
    "    model = AutoModelForCausalLM.from_config(config)\n",
    "else:\n",
    "    #model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "\n",
    "#model = torch.compile(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0900757b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertWithCustomEmbeddings(BertLMHeadModel):\n",
    "    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None, inputs_embeds=None, encoder_hidden_states=None, encoder_attention_mask=None, labels=None, past_key_values=None, use_cache=None, output_attentions=None, output_hidden_states=None, return_dict=None):\n",
    "        if inputs_embeds is None and input_ids is None:\n",
    "            raise ValueError(\"You must pass either input_ids or inputs_embeds\")\n",
    "        \n",
    "        return super().forward(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            encoder_hidden_states=encoder_hidden_states,\n",
    "            encoder_attention_mask=encoder_attention_mask,\n",
    "            labels=labels,\n",
    "            past_key_values=past_key_values,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict\n",
    "        )\n",
    "\n",
    "# Instantiate the modified model\n",
    "custom_model = BertWithCustomEmbeddings.from_pretrained(model_name, is_decoder=True)\n",
    "custom_model = torch.compile(custom_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5feb262",
   "metadata": {},
   "source": [
    "### Data loaders and Dataset for batched training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b1311889",
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataset_pyt_val(Dataset):\n",
    "    def __init__(self, df, B = B, T = T ):\n",
    "        self.df = df\n",
    "        print(f\"Value of B {B}\")\n",
    "                                        \n",
    "    def __getitem__(self, idx):\n",
    "        print(f\"inside loader...idx ->{idx}\")\n",
    "        input_id_temp = torch.tensor(self.df.iloc[idx]['input_ids'],dtype = torch.long)\n",
    "        \n",
    "        \n",
    "        att_mask = torch.tensor(self.df.iloc[idx]['attention_mask'],dtype = torch.long)\n",
    "        # check the length of the encoded list\n",
    "        \n",
    "        #x_dict['input_id'] = input_ids_list\n",
    "        #x_dict['attention_mask'] = input_ids_list\n",
    "                \n",
    "        #print(f\"x_dict = {x_dict}\")             \n",
    "#       print(f\"inside the loader and input_id = {input_ids} and its shape is {input_ids.shape}\")\n",
    "        #labels = input_id_list\n",
    "        #input_ids = torch.tensor(input_id_list)\n",
    "        #attention_mask = torch.tensor(attention_mask_list)\n",
    "        #labels = torch.tensor(labels)\n",
    "        #print(f\"inside the loader and input_id shape= {input_ids.shape} attention_mask_shape is {attention_mask.shape} and label shape is {labels.shape}\")\n",
    "        #print(f\"encoding = {encodings}\")\n",
    "        input_id =   input_id_temp.view(B,T)    \n",
    "        attention_mask = att_mask.view(B,T)   \n",
    "        \n",
    "        return input_id, attention_mask\n",
    "        \n",
    "    def __len__(self):\n",
    "        #return the length of the dataframe\n",
    "        return len(self.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f9f20770",
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataset_pyt_train(Dataset):\n",
    "    def __init__(self, df, B = B, T = T ):\n",
    "        self.df = df\n",
    "                                        \n",
    "    def __getitem__(self, idx):\n",
    "        print(f\"inside loader...idx ->{idx}\")\n",
    "        input_id_temp = torch.tensor(self.df.iloc[idx]['input_ids'],dtype = torch.long)\n",
    "        embeddings = prepare_input_with_synonym_embeddings(input_id_temp)\n",
    "        #print(f\"text ->{text}\")\n",
    "        \n",
    "        \n",
    "        \n",
    "        att_mask = torch.tensor(self.df.iloc[idx]['attention_mask'],dtype = torch.long)\n",
    "        \n",
    "        input_id =   input_id_temp.view(B,T)    \n",
    "        attention_mask = att_mask.view(B,T)   \n",
    "        \n",
    "        return input_id, attention_mask\n",
    "        \n",
    "    def __len__(self):\n",
    "        #return the length of the dataframe\n",
    "        return len(self.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "01f05a3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value of B 32\n"
     ]
    }
   ],
   "source": [
    "#train_dataset = dataset_pyt(filtered_df,tokenizer = tokenizer)\n",
    "train_dataset = dataset_pyt_train(df_train)\n",
    "val_dataset = dataset_pyt_val(df_val)\n",
    "#test_dataset = dataset_pyt(df_test,tokenizer = tokenizer)\n",
    "\n",
    "train_loader = DataLoader(train_dataset,batch_size = 1, shuffle = True , num_workers = 4, pin_memory = True)\n",
    "val_loader = DataLoader(val_dataset,batch_size = 1, shuffle = True , num_workers = 4, pin_memory = True)\n",
    "#test_loader = DataLoader(test_dataset,batch_size = batch_size, shuffle = False, collate_fn = custom_collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2385a046",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c0823b3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the train loader is 525\n",
      "Length of the val loader is 532\n",
      "num_tokens= 17203200\n"
     ]
    }
   ],
   "source": [
    "print(f\"Length of the train loader is {len(train_loader)}\")\n",
    "print(f\"Length of the val loader is {len(val_loader)}\")\n",
    "print(f\"num_tokens= {B*T*len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e646634a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_output = model(input_ids = inp ,attention_mask = att, labels = inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0438e689",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_file(log_message, model_name = model_name ,random_init_wts = random_init_wts ):\n",
    "    current_datetime = datetime.datetime.now()\n",
    "    # Extract date and time components\n",
    "    current_date = str(current_datetime.date())\n",
    "    log_file = model_name +'_COS_SIM_'+'random_init_wts'+ '_'+str(random_init_wts)+'_' +current_date+'.log'\n",
    "    print(f\"*****LOGGING INFO IN {log_file}*********\")\n",
    "    filepath = os.path.join(\"model\",log_file)\n",
    "    logging.basicConfig(filename=filepath, \n",
    "                    filemode='a',  # Overwrite the log file each time\n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s', \n",
    "                    level=logging.DEBUG)\n",
    "    logger = logging.getLogger()\n",
    "    logger.info(log_message)\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ff3d96d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad\n",
    "def eval_model(val_loader, model, epoch , device = device,tokenizer = tokenizer):\n",
    "    global global_val_loss\n",
    "    #m = nn.Softmax()\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    e = epoch+1\n",
    "    embedding_layer = model.transformer.wte\n",
    "    val_loss_accum = 0.0\n",
    "    #criterion = torch.nn.BCEWithLogitsLoss()\n",
    "    print(f\"inside validation data for epoch {e}\")\n",
    "    #y_hat_val_list = []\n",
    "    #y_val_list = []\n",
    "    for ind,(input_id,attention_mask) in enumerate(val_loader):\n",
    "        #print(f\"id_list{id_list}\")\n",
    "        ids = input_id.to(device=device, non_blocking=True)\n",
    "        att_mask = attention_mask.to(device=device, non_blocking=True)\n",
    "        labels = ids.clone()\n",
    "        #predictions\n",
    "        #print(f\"input_ids device = {input_ids.device}\")\n",
    "        with autocast(dtype = torch.bfloat16):\n",
    "            input_embeddings = embedding_layer(ids)\n",
    "            model_output = model(input_ids = ids ,attention_mask = att_mask)\n",
    "            logits = model_output.logits\n",
    "            predictions = torch.argmax(logits, dim=-1)\n",
    "            prediction_embeddings = embedding_layer(predictions)\n",
    "            cos_sim = F.cosine_similarity(torch.squeeze(prediction_embeddings,dim = 0), torch.squeeze(input_embeddings,dim = 0), dim=1)\n",
    "            cos_loss = 1- cos_sim.mean()\n",
    "        \n",
    "        val_loss_accum+=cos_loss.detach().item()\n",
    "        del ids,att_mask,labels,input_embeddings,model_output,logits,predictions,prediction_embeddings,cos_sim\n",
    "    \n",
    "    if val_loss_accum < global_val_loss:\n",
    "        print(f\"Val loss has decreased -->reducing the global validation loss from {global_val_loss:.2f} to {val_loss_accum:.2f}\")\n",
    "        s1 = f\"Val loss has decreased -->reducing the global validation loss from {global_val_loss:.2f} to {val_loss_accum:.2f}\"\n",
    "        global_val_loss = val_loss_accum\n",
    "        print(f\" validation loss for epoch = {e} is {val_loss_accum:.4f}\")\n",
    "        s2 = f\" validation loss for epoch = {e} is {val_loss_accum:.4f}\"\n",
    "        #print metrics and save the model\n",
    "        #y_hat_val = torch.cat(y_hat_val_list)\n",
    "        #y_val = torch.cat(y_val_list)\n",
    "        #acc_val = accuracy_score(y_val.cpu().numpy(), y_hat_val.cpu().numpy())\n",
    "        #f1_val = f1_score(y_val.cpu().numpy(), y_hat_val.cpu().numpy(), average='micro')\n",
    "        print(f\" epoch= {e} :  val loss is {val_loss_accum:.4f} \")\n",
    "        s3 = f\" epoch= {e} :  val loss is {val_loss_accum:.4f} \"\n",
    "        #save the model\n",
    "        \n",
    "        # Get the current date and time\n",
    "        current_datetime = datetime.datetime.now()\n",
    "        # Extract date and time components\n",
    "        current_date = str(current_datetime.date())\n",
    "        current_time = str(current_datetime.time()).split('.')[0]\n",
    "        file_name = 'model'+ current_date+current_time+'.pth'\n",
    "        path = os.path.join(\"model\",file_name)\n",
    "        print(f\"saving the model {file_name}\")\n",
    "        s4 = f\"saving the model {file_name}\"\n",
    "        #torch.save(model.state_dict(), path)\n",
    "        model.save_pretrained(path)\n",
    "        tokenizer.save_pretrained(path)\n",
    "        log_message = s1+s2+s3+s4\n",
    "        write_file(log_message)\n",
    "        \n",
    "        #plot_confusion_matrix(y_val.cpu().numpy(), y_hat_val.cpu().numpy(), labels)\n",
    "    else:\n",
    "        print(f\"No improvement in validation loss-->epoch= {e} and global val loss is {global_val_loss:.2f}\")\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "73185149",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_loader,val_loader,model,num_epoch = 100,device = device,tokenizer = tokenizer):\n",
    "    global global_tr_loss\n",
    "    model.train()\n",
    "    device = device\n",
    "    lr_custom = 5e-5\n",
    "    print(f\"inside train model. Device = {device}\")\n",
    "    #adding betas params per the paper\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.AdamW(params =  model.parameters(), lr= lr_custom,fused = True ,weight_decay = .1)\n",
    "    total_batch_size = 2**19\n",
    "    grad_accum_step = total_batch_size//(B*T)\n",
    "    \n",
    "    extra_train = .1*num_epoch\n",
    "    #m = nn.Softmax()\n",
    "    max_train_steps = int(num_epoch +extra_train )\n",
    "    import time\n",
    "    from transformers import get_linear_schedule_with_warmup\n",
    "    total_steps = len(train_loader) * num_epoch\n",
    "    scheduler_cos = transformers.get_cosine_schedule_with_warmup( optimizer= optimizer, num_warmup_steps =int(total_steps * 0.1) ,num_training_steps= total_steps ,last_epoch = -1 )\n",
    "    embedding_layer = model.transformer.wte\n",
    "        \n",
    "    for i in range (max_train_steps):\n",
    "        epoch_start_time = time.time()\n",
    "        epoch_train_loss = 0.0\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        batch_loss = 0.0\n",
    "        if i >= num_epoch:\n",
    "            optimizer_reduced_lr = torch.optim.AdamW(params =  model.parameters(), lr= current_lr ,fused = True , weight_decay=.1)\n",
    "            scheduler_constant = transformers.get_constant_schedule_with_warmup( optimizer = optimizer_reduced_lr ,num_warmup_steps = 0, last_epoch = -1 )\n",
    "            \n",
    "        \n",
    "        for ind,(input_id,attention_mask) in enumerate(train_loader):\n",
    "            if ind == int(len(train_loader)/2):\n",
    "                batch_time = time.time()\n",
    "                duration = batch_time - epoch_start_time\n",
    "                print(f\"executing epoch:{i+1}, it took {duration/60} mins from beginning of epoch till batch#{ind}\")\n",
    "            \n",
    "            ids = input_id.to(device=device, non_blocking=True)\n",
    "            att_mask = attention_mask.to(device=device, non_blocking=True)\n",
    "            labels = ids.clone()\n",
    "            \n",
    "            with autocast(dtype = torch.bfloat16):\n",
    "                input_embeddings = embedding_layer(ids)\n",
    "                model_output = model(input_ids = ids ,attention_mask = att_mask)\n",
    "                logits = model_output.logits\n",
    "                predictions = torch.argmax(logits, dim=-1)\n",
    "                #print(f\"predictions = {predictions}\")\n",
    "                prediction_embeddings = embedding_layer(predictions)\n",
    "                cos_sim = F.cosine_similarity(torch.squeeze(prediction_embeddings,dim = 0), torch.squeeze(input_embeddings,dim = 0), dim=1)\n",
    "                cos_loss = 1- cos_sim.mean()\n",
    "                #print(f\"ind = {ind}|cos_loss = {cos_loss.detach().item()}\")\n",
    "                         \n",
    "            cos_loss.backward()\n",
    "            epoch_train_loss += cos_loss.detach().item()\n",
    "            #print(f\"Gradient accum step = {ind}|batch_loss before averaging ={batch_loss} | epoch_train_loss = {epoch_train_loss}\")\n",
    "            norm = torch.nn.utils.clip_grad_norm(model.parameters() , 1.0)\n",
    "            if i <= num_epoch:\n",
    "                optimizer.step()\n",
    "                scheduler_cos.step()\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "            else:\n",
    "                optimizer_reduced_lr.step()\n",
    "                optimizer_reduced_lr.zero_grad(set_to_none=True)\n",
    "                scheduler_constant.step()\n",
    "                \n",
    "                                \n",
    "            del ids,att_mask,labels,input_embeddings,model_output,logits,predictions,prediction_embeddings,cos_sim\n",
    "            \n",
    "        \n",
    "        \n",
    "        #batch processing complete \n",
    "        if i <= num_epoch:\n",
    "            current_lr = scheduler_cos.get_last_lr()[0]\n",
    "                    \n",
    "        #mean_loss = torch.mean(torch.tensor(epoch_train_loss))\n",
    "        epoch_end_time = time.time()\n",
    "        epoch_durn = (epoch_end_time - epoch_start_time)\n",
    "        num_token = B*T*len(train_loader)\n",
    "        if (epoch_train_loss < global_tr_loss) :\n",
    "            improvement = (abs(global_tr_loss - epoch_train_loss)/(global_tr_loss))*100\n",
    "            print(f\"training loss has decreased--->improvement = {improvement}%| reducing the global loss from {global_tr_loss:.2f} to {epoch_train_loss:.2f} | throughput = {int(num_token/epoch_durn)} tokens/second | norm = {norm:.4f} | learning rate = {current_lr:.5e}\")\n",
    "            global_tr_loss = epoch_train_loss\n",
    "            print(f\" epoch= {i+1} and  train loss is {epoch_train_loss:.2f}\")\n",
    "            \n",
    "            #printing training metrices\n",
    "            #y_hat = torch.cat(y_hat_list)\n",
    "            #y = torch.cat(label_list)\n",
    "            #acc = accuracy_score(y.cpu().numpy(), y_hat.cpu().numpy())\n",
    "            #f1 = f1_score(y.cpu().numpy(), y_hat.cpu().numpy(), average='micro')\n",
    "            #checking validation metrices\n",
    "            if (i%4 == 0 or (improvement > 15)):\n",
    "                eval_model(val_loader, model, epoch = i , device = device,tokenizer = tokenizer)\n",
    "            \n",
    "        else:\n",
    "            print(f\"No improvement in training loss..the global training loss is -->{global_tr_loss:.2f} \")\n",
    "            print(f\" epoch= {i+1} and mean train loss is {epoch_train_loss:.2f}\")\n",
    "        \n",
    "        \n",
    "    \n",
    "    return model\n",
    "        \n",
    "            \n",
    "            \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f47b7895",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inside train model. Device = cuda:0\n",
      "executing epoch:1, it took 0.7804126977920532 mins from beginning of epoch till batch#262\n",
      "training loss has decreased--->improvement = nan%| reducing the global loss from inf to 511.25 | throughput = 233563 tokens/second | norm = 0.0535 | learning rate = 5.00000e-06\n",
      " epoch= 1 and  train loss is 511.25\n",
      "inside validation data for epoch 1\n",
      "Val loss has decreased -->reducing the global validation loss from inf to 504.22\n",
      " validation loss for epoch = 1 is 504.2208\n",
      " epoch= 1 :  val loss is 504.2208 \n",
      "saving the model model2024-07-1114:29:35.pth\n",
      "[2024-07-11 14:29:35,947] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "df: /root/.triton/autotune: No such file or directory\n",
      "df: /root/.triton/autotune: No such file or directory\n",
      "/opt/conda/compiler_compat/ld: cannot find -laio\n",
      "collect2: error: ld returned 1 exit status\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93m [WARNING] \u001b[0m async_io requires the dev libaio .so object and headers but these were not found.\n",
      "\u001b[93m [WARNING] \u001b[0m async_io: please install the libaio-dev package with apt\n",
      "\u001b[93m [WARNING] \u001b[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\n",
      "\u001b[93m [WARNING] \u001b[0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH\n",
      "\u001b[93m [WARNING] \u001b[0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3\n",
      "\u001b[93m [WARNING] \u001b[0m using untested triton version (2.3.1), only 1.0.0 is known to be compatible\n",
      "*****LOGGING INFO IN gpt2_COS_SIM_random_init_wts_True_2024-07-11.log*********\n",
      "executing epoch:2, it took 0.5944814840952556 mins from beginning of epoch till batch#262\n",
      "training loss has decreased--->improvement = 11.505462627948608%| reducing the global loss from 511.25 to 452.43 | throughput = 293566 tokens/second | norm = 0.1436 | learning rate = 1.00000e-05\n",
      " epoch= 2 and  train loss is 452.43\n",
      "executing epoch:3, it took 0.38579709132512413 mins from beginning of epoch till batch#262\n",
      "training loss has decreased--->improvement = 31.361265727895727%| reducing the global loss from 452.43 to 310.54 | throughput = 372468 tokens/second | norm = 0.1465 | learning rate = 1.50000e-05\n",
      " epoch= 3 and  train loss is 310.54\n",
      "inside validation data for epoch 3\n",
      "Val loss has decreased -->reducing the global validation loss from 504.22 to 266.04\n",
      " validation loss for epoch = 3 is 266.0400\n",
      " epoch= 3 :  val loss is 266.0400 \n",
      "saving the model model2024-07-1114:32:12.pth\n",
      "*****LOGGING INFO IN gpt2_COS_SIM_random_init_wts_True_2024-07-11.log*********\n",
      "executing epoch:4, it took 0.3862829526265462 mins from beginning of epoch till batch#262\n",
      "training loss has decreased--->improvement = 28.65120072300692%| reducing the global loss from 310.54 to 221.57 | throughput = 372079 tokens/second | norm = 0.1369 | learning rate = 2.00000e-05\n",
      " epoch= 4 and  train loss is 221.57\n",
      "inside validation data for epoch 4\n",
      "Val loss has decreased -->reducing the global validation loss from 266.04 to 189.20\n",
      " validation loss for epoch = 4 is 189.2017\n",
      " epoch= 4 :  val loss is 189.2017 \n",
      "saving the model model2024-07-1114:33:45.pth\n",
      "*****LOGGING INFO IN gpt2_COS_SIM_random_init_wts_True_2024-07-11.log*********\n",
      "executing epoch:5, it took 0.3850069244702657 mins from beginning of epoch till batch#262\n",
      "training loss has decreased--->improvement = 29.463608077418137%| reducing the global loss from 221.57 to 156.29 | throughput = 373201 tokens/second | norm = 0.0769 | learning rate = 2.50000e-05\n",
      " epoch= 5 and  train loss is 156.29\n",
      "inside validation data for epoch 5\n",
      "Val loss has decreased -->reducing the global validation loss from 189.20 to 133.85\n",
      " validation loss for epoch = 5 is 133.8473\n",
      " epoch= 5 :  val loss is 133.8473 \n",
      "saving the model model2024-07-1114:35:18.pth\n",
      "*****LOGGING INFO IN gpt2_COS_SIM_random_init_wts_True_2024-07-11.log*********\n",
      "executing epoch:6, it took 0.38609134356180824 mins from beginning of epoch till batch#262\n",
      "training loss has decreased--->improvement = 29.20247678118609%| reducing the global loss from 156.29 to 110.65 | throughput = 372915 tokens/second | norm = 0.0831 | learning rate = 3.00000e-05\n",
      " epoch= 6 and  train loss is 110.65\n",
      "inside validation data for epoch 6\n",
      "Val loss has decreased -->reducing the global validation loss from 133.85 to 94.81\n",
      " validation loss for epoch = 6 is 94.8087\n",
      " epoch= 6 :  val loss is 94.8087 \n",
      "saving the model model2024-07-1114:36:50.pth\n",
      "*****LOGGING INFO IN gpt2_COS_SIM_random_init_wts_True_2024-07-11.log*********\n",
      "executing epoch:7, it took 0.38454258839289346 mins from beginning of epoch till batch#262\n",
      "training loss has decreased--->improvement = 27.591116101381903%| reducing the global loss from 110.65 to 80.12 | throughput = 373627 tokens/second | norm = 0.0716 | learning rate = 3.50000e-05\n",
      " epoch= 7 and  train loss is 80.12\n",
      "inside validation data for epoch 7\n",
      "Val loss has decreased -->reducing the global validation loss from 94.81 to 70.11\n",
      " validation loss for epoch = 7 is 70.1109\n",
      " epoch= 7 :  val loss is 70.1109 \n",
      "saving the model model2024-07-1114:38:23.pth\n",
      "*****LOGGING INFO IN gpt2_COS_SIM_random_init_wts_True_2024-07-11.log*********\n",
      "executing epoch:8, it took 0.3862093885739644 mins from beginning of epoch till batch#262\n",
      "training loss has decreased--->improvement = 24.666572854310704%| reducing the global loss from 80.12 to 60.36 | throughput = 372485 tokens/second | norm = 0.0519 | learning rate = 4.00000e-05\n",
      " epoch= 8 and  train loss is 60.36\n",
      "inside validation data for epoch 8\n",
      "Val loss has decreased -->reducing the global validation loss from 70.11 to 56.52\n",
      " validation loss for epoch = 8 is 56.5225\n",
      " epoch= 8 :  val loss is 56.5225 \n",
      "saving the model model2024-07-1114:39:56.pth\n",
      "*****LOGGING INFO IN gpt2_COS_SIM_random_init_wts_True_2024-07-11.log*********\n",
      "executing epoch:9, it took 0.385414997736613 mins from beginning of epoch till batch#262\n",
      "training loss has decreased--->improvement = 22.215704866978786%| reducing the global loss from 60.36 to 46.95 | throughput = 372534 tokens/second | norm = 0.0496 | learning rate = 4.50000e-05\n",
      " epoch= 9 and  train loss is 46.95\n",
      "inside validation data for epoch 9\n",
      "Val loss has decreased -->reducing the global validation loss from 56.52 to 38.93\n",
      " validation loss for epoch = 9 is 38.9255\n",
      " epoch= 9 :  val loss is 38.9255 \n",
      "saving the model model2024-07-1114:41:28.pth\n",
      "*****LOGGING INFO IN gpt2_COS_SIM_random_init_wts_True_2024-07-11.log*********\n",
      "executing epoch:10, it took 0.386030920346578 mins from beginning of epoch till batch#262\n",
      "training loss has decreased--->improvement = 33.724192074418454%| reducing the global loss from 46.95 to 31.11 | throughput = 372055 tokens/second | norm = 0.0439 | learning rate = 5.00000e-05\n",
      " epoch= 10 and  train loss is 31.11\n",
      "inside validation data for epoch 10\n",
      "Val loss has decreased -->reducing the global validation loss from 38.93 to 25.44\n",
      " validation loss for epoch = 10 is 25.4388\n",
      " epoch= 10 :  val loss is 25.4388 \n",
      "saving the model model2024-07-1114:43:01.pth\n",
      "*****LOGGING INFO IN gpt2_COS_SIM_random_init_wts_True_2024-07-11.log*********\n",
      "executing epoch:11, it took 0.3860042611757914 mins from beginning of epoch till batch#262\n",
      "training loss has decreased--->improvement = 32.60235284145479%| reducing the global loss from 31.11 to 20.97 | throughput = 372341 tokens/second | norm = 0.0180 | learning rate = 4.99848e-05\n",
      " epoch= 11 and  train loss is 20.97\n",
      "inside validation data for epoch 11\n",
      "Val loss has decreased -->reducing the global validation loss from 25.44 to 18.27\n",
      " validation loss for epoch = 11 is 18.2656\n",
      " epoch= 11 :  val loss is 18.2656 \n",
      "saving the model model2024-07-1114:44:34.pth\n",
      "*****LOGGING INFO IN gpt2_COS_SIM_random_init_wts_True_2024-07-11.log*********\n",
      "executing epoch:12, it took 0.38560360272725425 mins from beginning of epoch till batch#262\n",
      "training loss has decreased--->improvement = 22.587137188676117%| reducing the global loss from 20.97 to 16.23 | throughput = 373299 tokens/second | norm = 0.0257 | learning rate = 4.99391e-05\n",
      " epoch= 12 and  train loss is 16.23\n",
      "inside validation data for epoch 12\n",
      "Val loss has decreased -->reducing the global validation loss from 18.27 to 13.43\n",
      " validation loss for epoch = 12 is 13.4251\n",
      " epoch= 12 :  val loss is 13.4251 \n",
      "saving the model model2024-07-1114:46:06.pth\n",
      "*****LOGGING INFO IN gpt2_COS_SIM_random_init_wts_True_2024-07-11.log*********\n",
      "executing epoch:13, it took 0.3860967755317688 mins from beginning of epoch till batch#262\n",
      "training loss has decreased--->improvement = 23.357905279050534%| reducing the global loss from 16.23 to 12.44 | throughput = 372107 tokens/second | norm = 0.0217 | learning rate = 4.98630e-05\n",
      " epoch= 13 and  train loss is 12.44\n",
      "inside validation data for epoch 13\n",
      "Val loss has decreased -->reducing the global validation loss from 13.43 to 10.89\n",
      " validation loss for epoch = 13 is 10.8885\n",
      " epoch= 13 :  val loss is 10.8885 \n",
      "saving the model model2024-07-1114:47:39.pth\n",
      "*****LOGGING INFO IN gpt2_COS_SIM_random_init_wts_True_2024-07-11.log*********\n",
      "executing epoch:14, it took 0.3849053462346395 mins from beginning of epoch till batch#262\n",
      "training loss has decreased--->improvement = 26.854247412079957%| reducing the global loss from 12.44 to 9.10 | throughput = 372715 tokens/second | norm = 0.0157 | learning rate = 4.97567e-05\n",
      " epoch= 14 and  train loss is 9.10\n",
      "inside validation data for epoch 14\n",
      "Val loss has decreased -->reducing the global validation loss from 10.89 to 7.95\n",
      " validation loss for epoch = 14 is 7.9513\n",
      " epoch= 14 :  val loss is 7.9513 \n",
      "saving the model model2024-07-1114:49:12.pth\n",
      "*****LOGGING INFO IN gpt2_COS_SIM_random_init_wts_True_2024-07-11.log*********\n",
      "executing epoch:15, it took 0.38574262062708536 mins from beginning of epoch till batch#262\n",
      "training loss has decreased--->improvement = 27.82639288114846%| reducing the global loss from 9.10 to 6.57 | throughput = 373833 tokens/second | norm = 0.1665 | learning rate = 4.96202e-05\n",
      " epoch= 15 and  train loss is 6.57\n",
      "inside validation data for epoch 15\n",
      "Val loss has decreased -->reducing the global validation loss from 7.95 to 6.03\n",
      " validation loss for epoch = 15 is 6.0300\n",
      " epoch= 15 :  val loss is 6.0300 \n",
      "saving the model model2024-07-1114:50:44.pth\n",
      "*****LOGGING INFO IN gpt2_COS_SIM_random_init_wts_True_2024-07-11.log*********\n",
      "executing epoch:16, it took 0.383408526579539 mins from beginning of epoch till batch#262\n",
      "training loss has decreased--->improvement = 23.477971109335616%| reducing the global loss from 6.57 to 5.03 | throughput = 374249 tokens/second | norm = 0.0124 | learning rate = 4.94537e-05\n",
      " epoch= 16 and  train loss is 5.03\n",
      "inside validation data for epoch 16\n",
      "Val loss has decreased -->reducing the global validation loss from 6.03 to 4.80\n",
      " validation loss for epoch = 16 is 4.8006\n",
      " epoch= 16 :  val loss is 4.8006 \n",
      "saving the model model2024-07-1114:52:16.pth\n",
      "*****LOGGING INFO IN gpt2_COS_SIM_random_init_wts_True_2024-07-11.log*********\n",
      "executing epoch:17, it took 0.38584762016932167 mins from beginning of epoch till batch#262\n",
      "training loss has decreased--->improvement = 18.329956678533826%| reducing the global loss from 5.03 to 4.10 | throughput = 372447 tokens/second | norm = 0.0196 | learning rate = 4.92574e-05\n",
      " epoch= 17 and  train loss is 4.10\n",
      "inside validation data for epoch 17\n",
      "Val loss has decreased -->reducing the global validation loss from 4.80 to 4.21\n",
      " validation loss for epoch = 17 is 4.2079\n",
      " epoch= 17 :  val loss is 4.2079 \n",
      "saving the model model2024-07-1114:53:49.pth\n",
      "*****LOGGING INFO IN gpt2_COS_SIM_random_init_wts_True_2024-07-11.log*********\n",
      "executing epoch:18, it took 0.3853196859359741 mins from beginning of epoch till batch#262\n",
      "training loss has decreased--->improvement = 7.438752235495016%| reducing the global loss from 4.10 to 3.80 | throughput = 372859 tokens/second | norm = 0.0097 | learning rate = 4.90315e-05\n",
      " epoch= 18 and  train loss is 3.80\n",
      "executing epoch:19, it took 0.3859020988146464 mins from beginning of epoch till batch#262\n",
      "training loss has decreased--->improvement = 27.397458684006182%| reducing the global loss from 3.80 to 2.76 | throughput = 372467 tokens/second | norm = 0.0100 | learning rate = 4.87764e-05\n",
      " epoch= 19 and  train loss is 2.76\n",
      "inside validation data for epoch 19\n",
      "Val loss has decreased -->reducing the global validation loss from 4.21 to 2.72\n",
      " validation loss for epoch = 19 is 2.7176\n",
      " epoch= 19 :  val loss is 2.7176 \n",
      "saving the model model2024-07-1114:56:08.pth\n",
      "*****LOGGING INFO IN gpt2_COS_SIM_random_init_wts_True_2024-07-11.log*********\n",
      "executing epoch:20, it took 0.3842745661735535 mins from beginning of epoch till batch#262\n",
      "training loss has decreased--->improvement = 17.626969900918894%| reducing the global loss from 2.76 to 2.27 | throughput = 373791 tokens/second | norm = 0.0051 | learning rate = 4.84923e-05\n",
      " epoch= 20 and  train loss is 2.27\n",
      "inside validation data for epoch 20\n",
      "Val loss has decreased -->reducing the global validation loss from 2.72 to 2.31\n",
      " validation loss for epoch = 20 is 2.3148\n",
      " epoch= 20 :  val loss is 2.3148 \n",
      "saving the model model2024-07-1114:57:40.pth\n",
      "*****LOGGING INFO IN gpt2_COS_SIM_random_init_wts_True_2024-07-11.log*********\n",
      "executing epoch:21, it took 0.3850181063016256 mins from beginning of epoch till batch#262\n",
      "training loss has decreased--->improvement = 5.704011372081695%| reducing the global loss from 2.27 to 2.14 | throughput = 372832 tokens/second | norm = 0.0033 | learning rate = 4.81796e-05\n",
      " epoch= 21 and  train loss is 2.14\n",
      "inside validation data for epoch 21\n",
      "Val loss has decreased -->reducing the global validation loss from 2.31 to 1.99\n",
      " validation loss for epoch = 21 is 1.9874\n",
      " epoch= 21 :  val loss is 1.9874 \n",
      "saving the model model2024-07-1114:59:12.pth\n",
      "*****LOGGING INFO IN gpt2_COS_SIM_random_init_wts_True_2024-07-11.log*********\n",
      "executing epoch:22, it took 0.3845439076423645 mins from beginning of epoch till batch#262\n",
      "training loss has decreased--->improvement = 9.016734499861025%| reducing the global loss from 2.14 to 1.95 | throughput = 373351 tokens/second | norm = 0.0485 | learning rate = 4.78386e-05\n",
      " epoch= 22 and  train loss is 1.95\n",
      "executing epoch:23, it took 0.38578895330429075 mins from beginning of epoch till batch#262\n",
      "No improvement in training loss..the global training loss is -->1.95 \n",
      " epoch= 23 and mean train loss is 2.24\n",
      "executing epoch:24, it took 0.3850311994552612 mins from beginning of epoch till batch#262\n",
      "No improvement in training loss..the global training loss is -->1.95 \n",
      " epoch= 24 and mean train loss is 2.18\n",
      "executing epoch:25, it took 0.38412840366363527 mins from beginning of epoch till batch#262\n",
      "No improvement in training loss..the global training loss is -->1.95 \n",
      " epoch= 25 and mean train loss is 5.63\n",
      "executing epoch:26, it took 0.383681054910024 mins from beginning of epoch till batch#262\n",
      "No improvement in training loss..the global training loss is -->1.95 \n",
      " epoch= 26 and mean train loss is 21.85\n",
      "executing epoch:27, it took 0.3836249033610026 mins from beginning of epoch till batch#262\n",
      "No improvement in training loss..the global training loss is -->1.95 \n",
      " epoch= 27 and mean train loss is 12.82\n",
      "executing epoch:28, it took 0.3852158705393473 mins from beginning of epoch till batch#262\n",
      "No improvement in training loss..the global training loss is -->1.95 \n",
      " epoch= 28 and mean train loss is 4.74\n",
      "executing epoch:29, it took 0.3848294973373413 mins from beginning of epoch till batch#262\n",
      "No improvement in training loss..the global training loss is -->1.95 \n",
      " epoch= 29 and mean train loss is 2.82\n",
      "executing epoch:30, it took 0.38385237455368043 mins from beginning of epoch till batch#262\n",
      "No improvement in training loss..the global training loss is -->1.95 \n",
      " epoch= 30 and mean train loss is 2.45\n",
      "executing epoch:31, it took 0.38363873163859047 mins from beginning of epoch till batch#262\n",
      "No improvement in training loss..the global training loss is -->1.95 \n",
      " epoch= 31 and mean train loss is 3.11\n",
      "executing epoch:32, it took 0.3837253451347351 mins from beginning of epoch till batch#262\n",
      "No improvement in training loss..the global training loss is -->1.95 \n",
      " epoch= 32 and mean train loss is 2.52\n",
      "executing epoch:33, it took 0.38375242153803507 mins from beginning of epoch till batch#262\n",
      "training loss has decreased--->improvement = 27.083750859320077%| reducing the global loss from 1.95 to 1.42 | throughput = 374058 tokens/second | norm = 0.0094 | learning rate = 4.23665e-05\n",
      " epoch= 33 and  train loss is 1.42\n",
      "inside validation data for epoch 33\n",
      "Val loss has decreased -->reducing the global validation loss from 1.99 to 1.69\n",
      " validation loss for epoch = 33 is 1.6872\n",
      " epoch= 33 :  val loss is 1.6872 \n",
      "saving the model model2024-07-1115:09:11.pth\n",
      "*****LOGGING INFO IN gpt2_COS_SIM_random_init_wts_True_2024-07-11.log*********\n",
      "executing epoch:34, it took 0.3847636580467224 mins from beginning of epoch till batch#262\n",
      "No improvement in training loss..the global training loss is -->1.42 \n",
      " epoch= 34 and mean train loss is 1.59\n",
      "executing epoch:35, it took 0.3855524778366089 mins from beginning of epoch till batch#262\n",
      "training loss has decreased--->improvement = 24.64882659167856%| reducing the global loss from 1.42 to 1.07 | throughput = 372653 tokens/second | norm = 0.0025 | learning rate = 4.10697e-05\n",
      " epoch= 35 and  train loss is 1.07\n",
      "inside validation data for epoch 35\n",
      "Val loss has decreased -->reducing the global validation loss from 1.69 to 1.23\n",
      " validation loss for epoch = 35 is 1.2280\n",
      " epoch= 35 :  val loss is 1.2280 \n",
      "saving the model model2024-07-1115:11:29.pth\n",
      "*****LOGGING INFO IN gpt2_COS_SIM_random_init_wts_True_2024-07-11.log*********\n",
      "executing epoch:36, it took 0.38374247153600055 mins from beginning of epoch till batch#262\n",
      "training loss has decreased--->improvement = 22.331511912687553%| reducing the global loss from 1.07 to 0.83 | throughput = 374290 tokens/second | norm = 0.0036 | learning rate = 4.03915e-05\n",
      " epoch= 36 and  train loss is 0.83\n",
      "inside validation data for epoch 36\n",
      "Val loss has decreased -->reducing the global validation loss from 1.23 to 1.05\n",
      " validation loss for epoch = 36 is 1.0466\n",
      " epoch= 36 :  val loss is 1.0466 \n",
      "saving the model model2024-07-1115:13:02.pth\n",
      "*****LOGGING INFO IN gpt2_COS_SIM_random_init_wts_True_2024-07-11.log*********\n",
      "executing epoch:37, it took 0.3849614222844442 mins from beginning of epoch till batch#262\n",
      "training loss has decreased--->improvement = 14.456172760076244%| reducing the global loss from 0.83 to 0.71 | throughput = 373225 tokens/second | norm = 0.0038 | learning rate = 3.96946e-05\n",
      " epoch= 37 and  train loss is 0.71\n",
      "inside validation data for epoch 37\n",
      "Val loss has decreased -->reducing the global validation loss from 1.05 to 0.93\n",
      " validation loss for epoch = 37 is 0.9298\n",
      " epoch= 37 :  val loss is 0.9298 \n",
      "saving the model model2024-07-1115:14:34.pth\n",
      "*****LOGGING INFO IN gpt2_COS_SIM_random_init_wts_True_2024-07-11.log*********\n",
      "executing epoch:38, it took 0.3843938787778219 mins from beginning of epoch till batch#262\n",
      "training loss has decreased--->improvement = 16.43851311658872%| reducing the global loss from 0.71 to 0.59 | throughput = 373354 tokens/second | norm = 0.0042 | learning rate = 3.89798e-05\n",
      " epoch= 38 and  train loss is 0.59\n",
      "inside validation data for epoch 38\n",
      "Val loss has decreased -->reducing the global validation loss from 0.93 to 0.89\n",
      " validation loss for epoch = 38 is 0.8879\n",
      " epoch= 38 :  val loss is 0.8879 \n",
      "saving the model model2024-07-1115:16:07.pth\n",
      "*****LOGGING INFO IN gpt2_COS_SIM_random_init_wts_True_2024-07-11.log*********\n",
      "executing epoch:39, it took 0.38448853890101115 mins from beginning of epoch till batch#262\n",
      "training loss has decreased--->improvement = 27.021104242678646%| reducing the global loss from 0.59 to 0.43 | throughput = 373633 tokens/second | norm = 0.0028 | learning rate = 3.82480e-05\n",
      " epoch= 39 and  train loss is 0.43\n",
      "inside validation data for epoch 39\n",
      "Val loss has decreased -->reducing the global validation loss from 0.89 to 0.62\n",
      " validation loss for epoch = 39 is 0.6164\n",
      " epoch= 39 :  val loss is 0.6164 \n",
      "saving the model model2024-07-1115:17:39.pth\n",
      "*****LOGGING INFO IN gpt2_COS_SIM_random_init_wts_True_2024-07-11.log*********\n",
      "executing epoch:40, it took 0.384096089998881 mins from beginning of epoch till batch#262\n",
      "training loss has decreased--->improvement = 22.48750126697228%| reducing the global loss from 0.43 to 0.34 | throughput = 373873 tokens/second | norm = 0.0007 | learning rate = 3.75000e-05\n",
      " epoch= 40 and  train loss is 0.34\n",
      "inside validation data for epoch 40\n",
      "No improvement in validation loss-->epoch= 40 and global val loss is 0.62\n",
      "executing epoch:41, it took 0.38483206431070965 mins from beginning of epoch till batch#262\n",
      "No improvement in training loss..the global training loss is -->0.34 \n",
      " epoch= 41 and mean train loss is 0.39\n",
      "executing epoch:42, it took 0.38364760875701903 mins from beginning of epoch till batch#262\n",
      "training loss has decreased--->improvement = 34.16151764629171%| reducing the global loss from 0.34 to 0.22 | throughput = 374968 tokens/second | norm = 0.0007 | learning rate = 3.59593e-05\n",
      " epoch= 42 and  train loss is 0.22\n",
      "inside validation data for epoch 42\n",
      "Val loss has decreased -->reducing the global validation loss from 0.62 to 0.45\n",
      " validation loss for epoch = 42 is 0.4469\n",
      " epoch= 42 :  val loss is 0.4469 \n",
      "saving the model model2024-07-1115:21:27.pth\n",
      "*****LOGGING INFO IN gpt2_COS_SIM_random_init_wts_True_2024-07-11.log*********\n",
      "executing epoch:43, it took 0.38561007181803386 mins from beginning of epoch till batch#262\n",
      "training loss has decreased--->improvement = 13.788330708945374%| reducing the global loss from 0.22 to 0.19 | throughput = 372737 tokens/second | norm = 0.0045 | learning rate = 3.51684e-05\n",
      " epoch= 43 and  train loss is 0.19\n",
      "executing epoch:44, it took 0.3849528511365255 mins from beginning of epoch till batch#262\n",
      "training loss has decreased--->improvement = 10.92403108905983%| reducing the global loss from 0.19 to 0.17 | throughput = 373902 tokens/second | norm = 0.0008 | learning rate = 3.43652e-05\n",
      " epoch= 44 and  train loss is 0.17\n",
      "executing epoch:45, it took 0.38407657941182455 mins from beginning of epoch till batch#262\n",
      "training loss has decreased--->improvement = 13.608740433229825%| reducing the global loss from 0.17 to 0.15 | throughput = 374168 tokens/second | norm = 0.0007 | learning rate = 3.35505e-05\n",
      " epoch= 45 and  train loss is 0.15\n",
      "inside validation data for epoch 45\n",
      "Val loss has decreased -->reducing the global validation loss from 0.45 to 0.38\n",
      " validation loss for epoch = 45 is 0.3777\n",
      " epoch= 45 :  val loss is 0.3777 \n",
      "saving the model model2024-07-1115:24:31.pth\n",
      "*****LOGGING INFO IN gpt2_COS_SIM_random_init_wts_True_2024-07-11.log*********\n",
      "executing epoch:46, it took 0.38366952737172444 mins from beginning of epoch till batch#262\n",
      "training loss has decreased--->improvement = 10.510301106778757%| reducing the global loss from 0.15 to 0.13 | throughput = 374194 tokens/second | norm = 0.0004 | learning rate = 3.27254e-05\n",
      " epoch= 46 and  train loss is 0.13\n",
      "executing epoch:47, it took 0.38484957615534465 mins from beginning of epoch till batch#262\n",
      "training loss has decreased--->improvement = 11.89188944165543%| reducing the global loss from 0.13 to 0.12 | throughput = 373510 tokens/second | norm = 0.0006 | learning rate = 3.18909e-05\n",
      " epoch= 47 and  train loss is 0.12\n",
      "executing epoch:48, it took 0.38480600516001384 mins from beginning of epoch till batch#262\n",
      "training loss has decreased--->improvement = 12.17503862417176%| reducing the global loss from 0.12 to 0.10 | throughput = 373651 tokens/second | norm = 0.0011 | learning rate = 3.10480e-05\n",
      " epoch= 48 and  train loss is 0.10\n",
      "executing epoch:49, it took 0.38429905970891315 mins from beginning of epoch till batch#262\n",
      "training loss has decreased--->improvement = 5.3477837314105185%| reducing the global loss from 0.10 to 0.10 | throughput = 374334 tokens/second | norm = 0.0005 | learning rate = 3.01978e-05\n",
      " epoch= 49 and  train loss is 0.10\n",
      "inside validation data for epoch 49\n",
      "Val loss has decreased -->reducing the global validation loss from 0.38 to 0.31\n",
      " validation loss for epoch = 49 is 0.3127\n",
      " epoch= 49 :  val loss is 0.3127 \n",
      "saving the model model2024-07-1115:28:22.pth\n",
      "*****LOGGING INFO IN gpt2_COS_SIM_random_init_wts_True_2024-07-11.log*********\n",
      "executing epoch:50, it took 0.3854800303777059 mins from beginning of epoch till batch#262\n",
      "training loss has decreased--->improvement = 12.25072565122943%| reducing the global loss from 0.10 to 0.08 | throughput = 373059 tokens/second | norm = 0.0009 | learning rate = 2.93412e-05\n",
      " epoch= 50 and  train loss is 0.08\n",
      "executing epoch:51, it took 0.3845313549041748 mins from beginning of epoch till batch#262\n",
      "training loss has decreased--->improvement = 7.791830412936824%| reducing the global loss from 0.08 to 0.08 | throughput = 374145 tokens/second | norm = 0.0003 | learning rate = 2.84793e-05\n",
      " epoch= 51 and  train loss is 0.08\n",
      "executing epoch:52, it took 0.38420013984044393 mins from beginning of epoch till batch#262\n",
      "training loss has decreased--->improvement = 7.380477417223001%| reducing the global loss from 0.08 to 0.07 | throughput = 374308 tokens/second | norm = 0.0005 | learning rate = 2.76132e-05\n",
      " epoch= 52 and  train loss is 0.07\n",
      "executing epoch:53, it took 0.3828291336695353 mins from beginning of epoch till batch#262\n",
      "training loss has decreased--->improvement = 8.075380013510857%| reducing the global loss from 0.07 to 0.07 | throughput = 375965 tokens/second | norm = 0.0008 | learning rate = 2.67439e-05\n",
      " epoch= 53 and  train loss is 0.07\n",
      "inside validation data for epoch 53\n",
      "Val loss has decreased -->reducing the global validation loss from 0.31 to 0.27\n",
      " validation loss for epoch = 53 is 0.2700\n",
      " epoch= 53 :  val loss is 0.2700 \n",
      "saving the model model2024-07-1115:32:12.pth\n",
      "*****LOGGING INFO IN gpt2_COS_SIM_random_init_wts_True_2024-07-11.log*********\n",
      "executing epoch:54, it took 0.38539470434188844 mins from beginning of epoch till batch#262\n",
      "training loss has decreased--->improvement = 11.521388560724825%| reducing the global loss from 0.07 to 0.06 | throughput = 373033 tokens/second | norm = 0.0013 | learning rate = 2.58725e-05\n",
      " epoch= 54 and  train loss is 0.06\n",
      "executing epoch:55, it took 0.3843714912732442 mins from beginning of epoch till batch#262\n",
      "training loss has decreased--->improvement = 6.104172103191244%| reducing the global loss from 0.06 to 0.06 | throughput = 373712 tokens/second | norm = 0.0013 | learning rate = 2.50000e-05\n",
      " epoch= 55 and  train loss is 0.06\n",
      "executing epoch:56, it took 0.3838527321815491 mins from beginning of epoch till batch#262\n",
      "training loss has decreased--->improvement = 8.674170085667878%| reducing the global loss from 0.06 to 0.05 | throughput = 374933 tokens/second | norm = 0.0007 | learning rate = 2.41275e-05\n",
      " epoch= 56 and  train loss is 0.05\n",
      "executing epoch:57, it took 0.38397223552068077 mins from beginning of epoch till batch#262\n",
      "training loss has decreased--->improvement = 4.761634109749787%| reducing the global loss from 0.05 to 0.05 | throughput = 373540 tokens/second | norm = 0.0020 | learning rate = 2.32561e-05\n",
      " epoch= 57 and  train loss is 0.05\n",
      "inside validation data for epoch 57\n",
      "No improvement in validation loss-->epoch= 57 and global val loss is 0.27\n",
      "executing epoch:58, it took 0.3848939180374146 mins from beginning of epoch till batch#262\n",
      "training loss has decreased--->improvement = 5.455124156417691%| reducing the global loss from 0.05 to 0.05 | throughput = 373861 tokens/second | norm = 0.0005 | learning rate = 2.23868e-05\n",
      " epoch= 58 and  train loss is 0.05\n",
      "executing epoch:59, it took 0.38405689398447673 mins from beginning of epoch till batch#262\n",
      "training loss has decreased--->improvement = 3.9828676331348496%| reducing the global loss from 0.05 to 0.04 | throughput = 374818 tokens/second | norm = 0.0003 | learning rate = 2.15207e-05\n",
      " epoch= 59 and  train loss is 0.04\n",
      "executing epoch:60, it took 0.3834505081176758 mins from beginning of epoch till batch#262\n",
      "training loss has decreased--->improvement = 3.417129474282506%| reducing the global loss from 0.04 to 0.04 | throughput = 374149 tokens/second | norm = 0.0005 | learning rate = 2.06588e-05\n",
      " epoch= 60 and  train loss is 0.04\n",
      "executing epoch:61, it took 0.38485329151153563 mins from beginning of epoch till batch#262\n",
      "training loss has decreased--->improvement = 5.161551060345011%| reducing the global loss from 0.04 to 0.04 | throughput = 373767 tokens/second | norm = 0.0007 | learning rate = 1.98022e-05\n",
      " epoch= 61 and  train loss is 0.04\n",
      "inside validation data for epoch 61\n",
      "Val loss has decreased -->reducing the global validation loss from 0.27 to 0.27\n",
      " validation loss for epoch = 61 is 0.2669\n",
      " epoch= 61 :  val loss is 0.2669 \n",
      "saving the model model2024-07-1115:39:50.pth\n",
      "*****LOGGING INFO IN gpt2_COS_SIM_random_init_wts_True_2024-07-11.log*********\n",
      "executing epoch:62, it took 0.38453232844670615 mins from beginning of epoch till batch#262\n",
      "No improvement in training loss..the global training loss is -->0.04 \n",
      " epoch= 62 and mean train loss is 1.13\n",
      "executing epoch:63, it took 0.38470020294189455 mins from beginning of epoch till batch#262\n",
      "No improvement in training loss..the global training loss is -->0.04 \n",
      " epoch= 63 and mean train loss is 0.57\n",
      "executing epoch:64, it took 0.38432492812474567 mins from beginning of epoch till batch#262\n",
      "No improvement in training loss..the global training loss is -->0.04 \n",
      " epoch= 64 and mean train loss is 1.45\n",
      "executing epoch:65, it took 0.3842532674471537 mins from beginning of epoch till batch#262\n",
      "No improvement in training loss..the global training loss is -->0.04 \n",
      " epoch= 65 and mean train loss is 0.67\n",
      "executing epoch:66, it took 0.381955087184906 mins from beginning of epoch till batch#262\n",
      "No improvement in training loss..the global training loss is -->0.04 \n",
      " epoch= 66 and mean train loss is 0.07\n",
      "executing epoch:67, it took 0.38226503133773804 mins from beginning of epoch till batch#262\n",
      "No improvement in training loss..the global training loss is -->0.04 \n",
      " epoch= 67 and mean train loss is 0.42\n",
      "executing epoch:68, it took 0.38267091910044354 mins from beginning of epoch till batch#262\n",
      "No improvement in training loss..the global training loss is -->0.04 \n",
      " epoch= 68 and mean train loss is 0.59\n",
      "executing epoch:69, it took 0.3845480720202128 mins from beginning of epoch till batch#262\n",
      "No improvement in training loss..the global training loss is -->0.04 \n",
      " epoch= 69 and mean train loss is 0.86\n",
      "executing epoch:70, it took 0.3837543725967407 mins from beginning of epoch till batch#262\n",
      "No improvement in training loss..the global training loss is -->0.04 \n",
      " epoch= 70 and mean train loss is 0.49\n",
      "executing epoch:71, it took 0.38295512199401854 mins from beginning of epoch till batch#262\n",
      "No improvement in training loss..the global training loss is -->0.04 \n",
      " epoch= 71 and mean train loss is 0.36\n",
      "executing epoch:72, it took 0.38281110525131223 mins from beginning of epoch till batch#262\n",
      "No improvement in training loss..the global training loss is -->0.04 \n",
      " epoch= 72 and mean train loss is 0.37\n",
      "executing epoch:73, it took 0.38221696615219114 mins from beginning of epoch till batch#262\n",
      "No improvement in training loss..the global training loss is -->0.04 \n",
      " epoch= 73 and mean train loss is 0.33\n",
      "executing epoch:74, it took 0.38291119734446205 mins from beginning of epoch till batch#262\n",
      "No improvement in training loss..the global training loss is -->0.04 \n",
      " epoch= 74 and mean train loss is 0.23\n",
      "executing epoch:75, it took 0.3857696413993835 mins from beginning of epoch till batch#262\n",
      "No improvement in training loss..the global training loss is -->0.04 \n",
      " epoch= 75 and mean train loss is 1.15\n",
      "executing epoch:76, it took 0.38493026892344157 mins from beginning of epoch till batch#262\n",
      "No improvement in training loss..the global training loss is -->0.04 \n",
      " epoch= 76 and mean train loss is 0.65\n",
      "executing epoch:77, it took 0.3843016227086385 mins from beginning of epoch till batch#262\n",
      "No improvement in training loss..the global training loss is -->0.04 \n",
      " epoch= 77 and mean train loss is 0.47\n",
      "executing epoch:78, it took 0.38417023022969565 mins from beginning of epoch till batch#262\n",
      "No improvement in training loss..the global training loss is -->0.04 \n",
      " epoch= 78 and mean train loss is 0.37\n",
      "executing epoch:79, it took 0.38566115697224934 mins from beginning of epoch till batch#262\n",
      "No improvement in training loss..the global training loss is -->0.04 \n",
      " epoch= 79 and mean train loss is 0.30\n",
      "executing epoch:80, it took 0.3847456455230713 mins from beginning of epoch till batch#262\n",
      "No improvement in training loss..the global training loss is -->0.04 \n",
      " epoch= 80 and mean train loss is 0.25\n",
      "executing epoch:81, it took 0.3845020333925883 mins from beginning of epoch till batch#262\n",
      "No improvement in training loss..the global training loss is -->0.04 \n",
      " epoch= 81 and mean train loss is 0.21\n",
      "executing epoch:82, it took 0.3844616492589315 mins from beginning of epoch till batch#262\n",
      "No improvement in training loss..the global training loss is -->0.04 \n",
      " epoch= 82 and mean train loss is 0.19\n",
      "executing epoch:83, it took 0.3848639686902364 mins from beginning of epoch till batch#262\n",
      "No improvement in training loss..the global training loss is -->0.04 \n",
      " epoch= 83 and mean train loss is 0.16\n",
      "executing epoch:84, it took 0.38546069860458376 mins from beginning of epoch till batch#262\n",
      "No improvement in training loss..the global training loss is -->0.04 \n",
      " epoch= 84 and mean train loss is 0.15\n",
      "executing epoch:85, it took 0.3849249005317688 mins from beginning of epoch till batch#262\n",
      "No improvement in training loss..the global training loss is -->0.04 \n",
      " epoch= 85 and mean train loss is 0.14\n",
      "executing epoch:86, it took 0.38538728952407836 mins from beginning of epoch till batch#262\n",
      "No improvement in training loss..the global training loss is -->0.04 \n",
      " epoch= 86 and mean train loss is 0.13\n",
      "executing epoch:87, it took 0.38529176314671837 mins from beginning of epoch till batch#262\n",
      "No improvement in training loss..the global training loss is -->0.04 \n",
      " epoch= 87 and mean train loss is 0.12\n",
      "executing epoch:88, it took 0.38392618894577024 mins from beginning of epoch till batch#262\n",
      "No improvement in training loss..the global training loss is -->0.04 \n",
      " epoch= 88 and mean train loss is 0.11\n",
      "executing epoch:89, it took 0.38383538722991944 mins from beginning of epoch till batch#262\n",
      "No improvement in training loss..the global training loss is -->0.04 \n",
      " epoch= 89 and mean train loss is 0.11\n",
      "executing epoch:90, it took 0.38415898084640504 mins from beginning of epoch till batch#262\n",
      "No improvement in training loss..the global training loss is -->0.04 \n",
      " epoch= 90 and mean train loss is 0.10\n",
      "executing epoch:91, it took 0.3852049628893534 mins from beginning of epoch till batch#262\n",
      "No improvement in training loss..the global training loss is -->0.04 \n",
      " epoch= 91 and mean train loss is 0.10\n",
      "executing epoch:92, it took 0.3856734911600749 mins from beginning of epoch till batch#262\n",
      "No improvement in training loss..the global training loss is -->0.04 \n",
      " epoch= 92 and mean train loss is 0.09\n",
      "executing epoch:93, it took 0.38484320243199666 mins from beginning of epoch till batch#262\n",
      "No improvement in training loss..the global training loss is -->0.04 \n",
      " epoch= 93 and mean train loss is 0.09\n",
      "executing epoch:94, it took 0.38595435619354246 mins from beginning of epoch till batch#262\n",
      "No improvement in training loss..the global training loss is -->0.04 \n",
      " epoch= 94 and mean train loss is 0.09\n",
      "executing epoch:95, it took 0.38507968187332153 mins from beginning of epoch till batch#262\n",
      "No improvement in training loss..the global training loss is -->0.04 \n",
      " epoch= 95 and mean train loss is 0.09\n",
      "executing epoch:96, it took 0.3845441977183024 mins from beginning of epoch till batch#262\n",
      "No improvement in training loss..the global training loss is -->0.04 \n",
      " epoch= 96 and mean train loss is 0.09\n",
      "executing epoch:97, it took 0.3849440614382426 mins from beginning of epoch till batch#262\n",
      "No improvement in training loss..the global training loss is -->0.04 \n",
      " epoch= 97 and mean train loss is 0.09\n",
      "executing epoch:98, it took 0.38529805739720663 mins from beginning of epoch till batch#262\n",
      "No improvement in training loss..the global training loss is -->0.04 \n",
      " epoch= 98 and mean train loss is 0.09\n",
      "executing epoch:99, it took 0.3833935379981995 mins from beginning of epoch till batch#262\n",
      "No improvement in training loss..the global training loss is -->0.04 \n",
      " epoch= 99 and mean train loss is 0.09\n",
      "executing epoch:100, it took 0.38294677734375 mins from beginning of epoch till batch#262\n",
      "No improvement in training loss..the global training loss is -->0.04 \n",
      " epoch= 100 and mean train loss is 0.09\n",
      "executing epoch:101, it took 0.38223414023717245 mins from beginning of epoch till batch#262\n",
      "No improvement in training loss..the global training loss is -->0.04 \n",
      " epoch= 101 and mean train loss is 0.09\n",
      "executing epoch:102, it took 0.3827599048614502 mins from beginning of epoch till batch#262\n",
      "No improvement in training loss..the global training loss is -->0.04 \n",
      " epoch= 102 and mean train loss is 0.09\n",
      "executing epoch:103, it took 0.38262346585591633 mins from beginning of epoch till batch#262\n",
      "No improvement in training loss..the global training loss is -->0.04 \n",
      " epoch= 103 and mean train loss is 0.09\n",
      "executing epoch:104, it took 0.3831476370493571 mins from beginning of epoch till batch#262\n",
      "No improvement in training loss..the global training loss is -->0.04 \n",
      " epoch= 104 and mean train loss is 0.09\n",
      "executing epoch:105, it took 0.3833043018976847 mins from beginning of epoch till batch#262\n",
      "No improvement in training loss..the global training loss is -->0.04 \n",
      " epoch= 105 and mean train loss is 0.09\n",
      "executing epoch:106, it took 0.3831343650817871 mins from beginning of epoch till batch#262\n",
      "No improvement in training loss..the global training loss is -->0.04 \n",
      " epoch= 106 and mean train loss is 0.09\n",
      "executing epoch:107, it took 0.3853941043217977 mins from beginning of epoch till batch#262\n",
      "No improvement in training loss..the global training loss is -->0.04 \n",
      " epoch= 107 and mean train loss is 0.09\n",
      "executing epoch:108, it took 0.3846086780230204 mins from beginning of epoch till batch#262\n",
      "No improvement in training loss..the global training loss is -->0.04 \n",
      " epoch= 108 and mean train loss is 0.09\n",
      "executing epoch:109, it took 0.38483301798502606 mins from beginning of epoch till batch#262\n",
      "No improvement in training loss..the global training loss is -->0.04 \n",
      " epoch= 109 and mean train loss is 0.09\n",
      "executing epoch:110, it took 0.383704408009847 mins from beginning of epoch till batch#262\n",
      "No improvement in training loss..the global training loss is -->0.04 \n",
      " epoch= 110 and mean train loss is 0.09\n"
     ]
    }
   ],
   "source": [
    "tr_model = train_model(train_loader, val_loader, model =  model,tokenizer = tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0634d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a447b8f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d6361d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b51ff1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dbf0193",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f041a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c72a297",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934c8539",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1be427",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31edc083",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b01264",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
