{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6038703",
   "metadata": {},
   "outputs": [],
   "source": [
    "## reference https://huggingface.co/learn/nlp-course/en/chapter7/6?fw=pt#training-a-causal-language-model-from-scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a09abd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a72532fc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: datasets in /opt/conda/lib/python3.8/site-packages (2.20.0)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.8/site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: fsspec[http]<=2024.5.0,>=2023.1.0 in /opt/conda/lib/python3.8/site-packages (from datasets) (2024.5.0)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.8/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.8/site-packages (from datasets) (1.21.4)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /opt/conda/lib/python3.8/site-packages (from datasets) (4.66.4)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.8/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.2 in /opt/conda/lib/python3.8/site-packages (from datasets) (0.23.4)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.8/site-packages (from datasets) (21.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /opt/conda/lib/python3.8/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.8/site-packages (from datasets) (16.1.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.8/site-packages (from datasets) (6.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.8/site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.8/site-packages (from datasets) (3.4.0)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.8/site-packages (from datasets) (3.9.5)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.8/site-packages (from datasets) (1.3.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets) (21.2.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.8/site-packages (from huggingface-hub>=0.21.2->datasets) (4.12.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging->datasets) (3.0.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests>=2.32.2->datasets) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests>=2.32.2->datasets) (3.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests>=2.32.2->datasets) (1.26.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.8/site-packages (from requests>=2.32.2->datasets) (2.0.8)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.8/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.8/site-packages (from pandas->datasets) (2021.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: jupyter in /opt/conda/lib/python3.8/site-packages (1.0.0)\n",
      "Requirement already satisfied: ipywidgets in /opt/conda/lib/python3.8/site-packages (8.1.3)\n",
      "Requirement already satisfied: notebook in /opt/conda/lib/python3.8/site-packages (from jupyter) (6.4.1)\n",
      "Requirement already satisfied: qtconsole in /opt/conda/lib/python3.8/site-packages (from jupyter) (5.5.2)\n",
      "Requirement already satisfied: nbconvert in /opt/conda/lib/python3.8/site-packages (from jupyter) (6.3.0)\n",
      "Requirement already satisfied: ipykernel in /opt/conda/lib/python3.8/site-packages (from jupyter) (6.29.5)\n",
      "Requirement already satisfied: jupyter-console in /opt/conda/lib/python3.8/site-packages (from jupyter) (6.6.3)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (7.30.0)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.11 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (3.0.11)\n",
      "Requirement already satisfied: comm>=0.1.3 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.11 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (4.0.11)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (5.14.3)\n",
      "Requirement already satisfied: jedi>=0.16 in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.18.1)\n",
      "Requirement already satisfied: backcall in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: decorator in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (5.1.0)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.47)\n",
      "Requirement already satisfied: pickleshare in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.7.5)\n",
      "Requirement already satisfied: setuptools>=18.5 in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (59.4.0)\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (4.8.0)\n",
      "Requirement already satisfied: pygments in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (2.10.0)\n",
      "Requirement already satisfied: matplotlib-inline in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.3)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /opt/conda/lib/python3.8/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.8/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.8/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=6.1.0->ipywidgets) (0.2.5)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in /opt/conda/lib/python3.8/site-packages (from ipykernel->jupyter) (7.1.0)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in /opt/conda/lib/python3.8/site-packages (from ipykernel->jupyter) (1.8.2)\n",
      "Requirement already satisfied: pyzmq>=24 in /opt/conda/lib/python3.8/site-packages (from ipykernel->jupyter) (26.0.3)\n",
      "Requirement already satisfied: nest-asyncio in /opt/conda/lib/python3.8/site-packages (from ipykernel->jupyter) (1.5.4)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /opt/conda/lib/python3.8/site-packages (from ipykernel->jupyter) (5.7.2)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.8/site-packages (from ipykernel->jupyter) (21.3)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.8/site-packages (from ipykernel->jupyter) (5.8.0)\n",
      "Requirement already satisfied: tornado>=6.1 in /opt/conda/lib/python3.8/site-packages (from ipykernel->jupyter) (6.1)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /opt/conda/lib/python3.8/site-packages (from jupyter-client>=6.1.12->ipykernel->jupyter) (2.8.2)\n",
      "Requirement already satisfied: entrypoints in /opt/conda/lib/python3.8/site-packages (from jupyter-client>=6.1.12->ipykernel->jupyter) (0.3)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /opt/conda/lib/python3.8/site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel->jupyter) (4.2.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.8/site-packages (from python-dateutil>=2.1->jupyter-client>=6.1.12->ipykernel->jupyter) (1.16.0)\n",
      "Requirement already satisfied: nbclient<0.6.0,>=0.5.0 in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (0.5.9)\n",
      "Requirement already satisfied: defusedxml in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (0.7.1)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (1.5.0)\n",
      "Requirement already satisfied: testpath in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (0.5.0)\n",
      "Requirement already satisfied: bleach in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (4.1.0)\n",
      "Requirement already satisfied: jupyterlab-pygments in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (0.1.2)\n",
      "Requirement already satisfied: jinja2>=2.4 in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (3.0.3)\n",
      "Requirement already satisfied: nbformat>=4.4 in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (5.1.3)\n",
      "Requirement already satisfied: mistune<2,>=0.8.1 in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (0.8.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.8/site-packages (from jinja2>=2.4->nbconvert->jupyter) (2.0.1)\n",
      "Requirement already satisfied: ipython-genutils in /opt/conda/lib/python3.8/site-packages (from nbformat>=4.4->nbconvert->jupyter) (0.2.0)\n",
      "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /opt/conda/lib/python3.8/site-packages (from nbformat>=4.4->nbconvert->jupyter) (4.2.1)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /opt/conda/lib/python3.8/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.4->nbconvert->jupyter) (0.18.0)\n",
      "Requirement already satisfied: importlib-resources>=1.4.0 in /opt/conda/lib/python3.8/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.4->nbconvert->jupyter) (5.4.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /opt/conda/lib/python3.8/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.4->nbconvert->jupyter) (21.2.0)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /opt/conda/lib/python3.8/site-packages (from importlib-resources>=1.4.0->jsonschema!=2.5.0,>=2.4->nbformat>=4.4->nbconvert->jupyter) (3.6.0)\n",
      "Requirement already satisfied: webencodings in /opt/conda/lib/python3.8/site-packages (from bleach->nbconvert->jupyter) (0.5.1)\n",
      "Requirement already satisfied: Send2Trash>=1.5.0 in /opt/conda/lib/python3.8/site-packages (from notebook->jupyter) (1.8.0)\n",
      "Requirement already satisfied: argon2-cffi in /opt/conda/lib/python3.8/site-packages (from notebook->jupyter) (21.1.0)\n",
      "Requirement already satisfied: terminado>=0.8.3 in /opt/conda/lib/python3.8/site-packages (from notebook->jupyter) (0.12.1)\n",
      "Requirement already satisfied: prometheus-client in /opt/conda/lib/python3.8/site-packages (from notebook->jupyter) (0.12.0)\n",
      "Requirement already satisfied: cffi>=1.0.0 in /opt/conda/lib/python3.8/site-packages (from argon2-cffi->notebook->jupyter) (1.15.0)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.8/site-packages (from cffi>=1.0.0->argon2-cffi->notebook->jupyter) (2.21)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging->ipykernel->jupyter) (3.0.6)\n",
      "Requirement already satisfied: qtpy>=2.4.0 in /opt/conda/lib/python3.8/site-packages (from qtconsole->jupyter) (2.4.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#installing some libraries\n",
    "!pip install datasets\n",
    "!pip install --upgrade jupyter ipywidgets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f9510f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch, transformers\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import PreTrainedModel, PretrainedConfig\n",
    "from transformers import AutoModel, AutoConfig,AutoModelForCausalLM, AutoConfig\n",
    "\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "#from datasets import load_dataset\n",
    "import pandas as pd, numpy as np\n",
    "from torch import cuda\n",
    "import datetime\n",
    "import warnings,itertools\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import json\n",
    "pre_train = False\n",
    "\n",
    "# Ignore all warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "#pip install transformers bitsandbytes>=0.39.0 -q\n",
    "import zipfile,logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86bc7624",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "#global params for training\n",
    "\n",
    "B,T = 32,1024\n",
    "epoch = 100\n",
    "# this controls whether we are using pre-trained wts or not\n",
    "random_init_wts = False\n",
    "if cuda.is_available():\n",
    "    device = torch.device('cuda:0')\n",
    "    print(device)\n",
    "else:\n",
    "    device = 'cpu'\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "# these 2 global vars help track the training and val loss\n",
    "global_tr_loss = torch.inf\n",
    "global_val_loss = torch.inf\n",
    "#directory to save wts\n",
    "model_path = os.path.join(\"model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e00505e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./data/unzip_text_10M'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "directory = os.path.join('.','data','unzip_text_10M')  # Replace with your directory path\n",
    "directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fabef65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_text(directory):\n",
    "    \"\"\"This function loads the dataset,provided in the zipped format and stores all the text files in list\n",
    "    each file has its contents stored as an item in list and at the end we concatenate all the sub-lists to create a flattened list and return it\"\"\"\n",
    "    directory = os.path.join('.','data','unzip_text_10M',str(directory))  # Replace with your directory path\n",
    "    print(f\"directory :{directory}\")\n",
    "    # List all files in the directory\n",
    "    files = [f for f in os.listdir(directory) if os.path.isfile(os.path.join(directory, f))]\n",
    "    print(f\"files:{files}\")\n",
    "    text_content = []\n",
    "    # Read each file\n",
    "    total_lines = 0\n",
    "    for filenum,filename in enumerate(files):\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            text = file.read()\n",
    "            text_content.append(text)\n",
    "            print(f\"the file:{filename} has been appeneded to the uber list and its length is {len(text_content)} \")\n",
    "            \n",
    "    \n",
    "    flattened_list = ''.join(text_content)\n",
    "    assert (len(flattened_list) == total_lines , f\"Expected {len(flattened_list)} to be equal to {total_lines}\" )\n",
    "    \n",
    "    return flattened_list\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9ed2e8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "directory :./data/unzip_text_10M/train_10M\n",
      "files:['switchboard.train', 'simple_wiki.train', 'open_subtitles.train', 'gutenberg.train', 'childes.train', 'bnc_spoken.train']\n",
      "the file:switchboard.train has been appeneded to the uber list and its length is 1 \n",
      "the file:simple_wiki.train has been appeneded to the uber list and its length is 2 \n",
      "the file:open_subtitles.train has been appeneded to the uber list and its length is 3 \n",
      "the file:gutenberg.train has been appeneded to the uber list and its length is 4 \n",
      "the file:childes.train has been appeneded to the uber list and its length is 5 \n",
      "the file:bnc_spoken.train has been appeneded to the uber list and its length is 6 \n"
     ]
    }
   ],
   "source": [
    "#read the dataset and get a list\n",
    "train_list = read_text(\"train_10M\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a682b9a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54215049"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "863408a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1654\n"
     ]
    }
   ],
   "source": [
    "chunks = len(train_list)//(B*T)\n",
    "print(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1ae66d32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c8958bd189b45afae28f5176562bbb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc5a48e6fa534208b414c84384b36c15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4189e9c87ba403d81afd9de6f115cbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "395a0dbe4bf446798638181ba7f59301",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a907a85e7a324b4c96dde99034d4d5f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17191971 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "#tokeinze the training data\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\",return_tensors = \"pt\" , truncate = True, return_overflowing_tokens=True , padding = False,)\n",
    "enc_train = tokenizer(train_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "496765e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.1535097982657136"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comp_ratio = len(train_list)/len(enc_train['input_ids'])\n",
    "comp_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ad041ad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "directory :./data/unzip_text_10M/dev\n",
      "files:['switchboard.dev', 'simple_wiki.dev', 'open_subtitles.dev', 'gutenberg.dev', 'childes.dev', 'bnc_spoken.dev']\n",
      "the file:switchboard.dev has been appeneded to the uber list and its length is 1 \n",
      "the file:simple_wiki.dev has been appeneded to the uber list and its length is 2 \n",
      "the file:open_subtitles.dev has been appeneded to the uber list and its length is 3 \n",
      "the file:gutenberg.dev has been appeneded to the uber list and its length is 4 \n",
      "the file:childes.dev has been appeneded to the uber list and its length is 5 \n",
      "the file:bnc_spoken.dev has been appeneded to the uber list and its length is 6 \n"
     ]
    }
   ],
   "source": [
    "#read validation data and tokenize\n",
    "val_list = read_text(\"dev\")\n",
    "enc_val = tokenizer(val_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8456fa05",
   "metadata": {},
   "source": [
    "### we read the tokenize data and store the input_ids and attention mask as a single long list at run time we reshape the single [1,B*T] tensor into a batched tensor of dimension [B,T].\n",
    "This approach turns out to be more efficient as it removes the need for any extra tokens in the form of padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d8f5712c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df_from_list(enc , B= B, T = T, val= False):\n",
    "    \"\"\"This function takes the tokenized list and reated a dataframe where each row contains the input_ids and attention_mask from the tokenizer.\n",
    "    each row of the dataframe is a list with items B*T\"\"\"\n",
    "    chunk_size = B*T\n",
    "    if val:\n",
    "        chunk_size = 2*B*T\n",
    "        \n",
    "    long_list_inp = enc['input_ids']\n",
    "    long_list_attention = enc['attention_mask']\n",
    "\n",
    "    # Step 3: Split the list into chunks and pad the last chunk if necessary\n",
    "    chunks_inp = [long_list_inp[i:i + chunk_size] for i in range(0, len(long_list_inp), chunk_size)]\n",
    "    chunks_att = [long_list_attention[i:i + chunk_size] for i in range(0, len(long_list_attention), chunk_size)]\n",
    "    df = pd.DataFrame({'input_ids': chunks_inp,'attention_mask':chunks_att})\n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5cd0be5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the dataframe is = 525\n"
     ]
    }
   ],
   "source": [
    "df_train_temp = get_df_from_list(enc_train)\n",
    "# Display the DataFrame\n",
    "df_train_temp.head()\n",
    "print(f\"Length of the dataframe is = {len(df_train_temp)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f6023ec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the VALDATION dataframe is = 532\n"
     ]
    }
   ],
   "source": [
    "df_val_temp = get_df_from_list(enc_val)\n",
    "# Display the DataFrame\n",
    "df_val_temp.head()\n",
    "print(f\"Length of the VALDATION dataframe is = {len(df_val_temp)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "64cbb694",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_df(df,eos_char = tokenizer.eos_token_id,B = B, T = T,val = False):\n",
    "    \"\"\"The 2 functions below makes sure that each row of the dataframe is of equal length and if not it adds the eos token to make it B*T\"\"\"\n",
    "    if val:\n",
    "        B = 2*B\n",
    "    for ind,row in df.iterrows():\n",
    "        if len(row['input_ids']) != B*T :\n",
    "            print(f\"row = {ind} and input_id length = {len(row['input_ids'])}\")\n",
    "            print(f\"row = {ind} and attention length = {len(row['attention_mask'])}\")\n",
    "            pad_len = B*T - len(row['input_ids'])\n",
    "            print(f\"padding the row index {ind} with {pad_len} character\")\n",
    "            row['input_ids'] = row['input_ids']+ [eos_char] * pad_len\n",
    "            #attention mask should be padded to 0\n",
    "            row['attention_mask'] = row['attention_mask']+ [0] * pad_len\n",
    "            print(\"#### POST CONCAT####\")\n",
    "            print(f\"row = {ind} and input_id length = {len(row['input_ids'])}\")\n",
    "            print(f\"row = {ind} and attention length ={len(row['attention_mask'])}\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def verify_len(df,val = False, B= B):\n",
    "    row_ind = []\n",
    "    if val:\n",
    "        B = 2*B\n",
    "    for ind,row in df.iterrows():\n",
    "        if len(row['input_ids']) != B*T :\n",
    "            row_ind.append(ind)\n",
    "        else:\n",
    "            continue\n",
    "    if len(row_ind) !=0:\n",
    "        print(\"CONCATENATION Did not work\")\n",
    "    else:\n",
    "        print(\"CONCATENATION worked\")\n",
    "        \n",
    "            \n",
    "        \n",
    "    \n",
    "        \n",
    "        \n",
    "   \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f552c674",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row = 524 and input_id length = 21539\n",
      "row = 524 and attention length = 21539\n",
      "padding the row index 524 with 11229 character\n",
      "#### POST CONCAT####\n",
      "row = 524 and input_id length = 32768\n",
      "row = 524 and attention length =32768\n",
      "CONCATENATION worked\n"
     ]
    }
   ],
   "source": [
    "df_train  = pad_df(df_train_temp)\n",
    "verify_len(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "545745ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row = 531 and input_id length = 13588\n",
      "row = 531 and attention length = 13588\n",
      "padding the row index 531 with 19180 character\n",
      "#### POST CONCAT####\n",
      "row = 531 and input_id length = 32768\n",
      "row = 531 and attention length =32768\n",
      "CONCATENATION worked\n"
     ]
    }
   ],
   "source": [
    "df_val  = pad_df(df_val_temp)\n",
    "verify_len(df_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c66a48eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_ids</th>\n",
       "      <th>attention_mask</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[32, 25, 197, 40, 1101, 1654, 484, 389, 13, 19...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[6510, 11, 14104, 290, 27913, 13, 198, 32, 25,...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[314, 1612, 11, 340, 338, 1611, 286, 43244, 11...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[318, 407, 922, 329, 262, 1200, 2035, 13, 198,...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[198, 32, 25, 197, 1870, 11, 21480, 11, 314, 4...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           input_ids  \\\n",
       "0  [32, 25, 197, 40, 1101, 1654, 484, 389, 13, 19...   \n",
       "1  [6510, 11, 14104, 290, 27913, 13, 198, 32, 25,...   \n",
       "2  [314, 1612, 11, 340, 338, 1611, 286, 43244, 11...   \n",
       "3  [318, 407, 922, 329, 262, 1200, 2035, 13, 198,...   \n",
       "4  [198, 32, 25, 197, 1870, 11, 21480, 11, 314, 4...   \n",
       "\n",
       "                                      attention_mask  \n",
       "0  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "1  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "2  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "3  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "4  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc741a1",
   "metadata": {},
   "source": [
    "## Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5f30d284",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7681607682674c39a6bfa8e55e7cf784",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec378524777e45da8da024d11a517e18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test the tokenizer:\n",
    "model_name = \"gpt2\"\n",
    "if random_init_wts:\n",
    "    config = AutoConfig.from_pretrained(model_name, vocab_size = 50304)\n",
    "    # Initialize the model with random weights\n",
    "    model = AutoModelForCausalLM.from_config(config)\n",
    "else:\n",
    "    #model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "\n",
    "model = torch.compile(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed5f21c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4f0c55d0",
   "metadata": {},
   "source": [
    "### Data loaders and Dataset for batched training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "89e8aa11",
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataset_pyt_val(Dataset):\n",
    "    \"\"\"Dataset for validation data\"\"\"\n",
    "    def __init__(self, df, B = B, T = T ):\n",
    "        self.df = df\n",
    "        print(f\"Value of B {B}\")\n",
    "                                        \n",
    "    def __getitem__(self, idx):\n",
    "        #print(f\"inside loader...idx ->{idx}\")\n",
    "        input_id_temp = torch.tensor(self.df.iloc[idx]['input_ids'],dtype = torch.long)\n",
    "        att_mask = torch.tensor(self.df.iloc[idx]['attention_mask'],dtype = torch.long)\n",
    "        input_id =   input_id_temp.view(B,T)    \n",
    "        attention_mask = att_mask.view(B,T)   \n",
    "        \n",
    "        return input_id, attention_mask\n",
    "        \n",
    "    def __len__(self):\n",
    "        #return the length of the dataframe\n",
    "        return len(self.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "32e04748",
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataset_pyt_train(Dataset):\n",
    "    def __init__(self, df, B = B, T = T ):\n",
    "        self.df = df\n",
    "                                        \n",
    "    def __getitem__(self, idx):\n",
    "        input_id_temp = torch.tensor(self.df.iloc[idx]['input_ids'],dtype = torch.long)\n",
    "        att_mask = torch.tensor(self.df.iloc[idx]['attention_mask'],dtype = torch.long)\n",
    "        input_id =   input_id_temp.view(B,T)    \n",
    "        attention_mask = att_mask.view(B,T)   \n",
    "        \n",
    "        return input_id, attention_mask\n",
    "        \n",
    "    def __len__(self):\n",
    "        #return the length of the dataframe\n",
    "        return len(self.df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a346af87",
   "metadata": {},
   "source": [
    "### Note - batch_size is 1 here because we reshape our input data in the shape [B,T] for a batched training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cf90af9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value of B 32\n"
     ]
    }
   ],
   "source": [
    "#train_dataset = dataset_pyt(filtered_df,tokenizer = tokenizer)\n",
    "train_dataset = dataset_pyt_train(df_train)\n",
    "val_dataset = dataset_pyt_val(df_val)\n",
    "#test_dataset = dataset_pyt(df_test,tokenizer = tokenizer)\n",
    "\n",
    "train_loader = DataLoader(train_dataset,batch_size = 1, shuffle = True , num_workers = 4, pin_memory = True)\n",
    "val_loader = DataLoader(val_dataset,batch_size = 1, shuffle = True , num_workers = 4, pin_memory = True)\n",
    "#test_loader = DataLoader(test_dataset,batch_size = batch_size, shuffle = False, collate_fn = custom_collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88720776",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "979de3cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the train loader is 525\n",
      "Length of the val loader is 532\n",
      "num_tokens= 17203200\n"
     ]
    }
   ],
   "source": [
    "print(f\"Length of the train loader is {len(train_loader)}\")\n",
    "print(f\"Length of the val loader is {len(val_loader)}\")\n",
    "print(f\"num_tokens= {B*T*len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "33848ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_file(log_message, model_name = model_name ,random_init_wts = random_init_wts ):\n",
    "    \"\"\"This function logs the training performance for future reference\"\"\"\n",
    "    current_datetime = datetime.datetime.now()\n",
    "    # Extract date and time components\n",
    "    current_date = str(current_datetime.date())\n",
    "    log_file = model_name +'_COS_SIM_'+'random_init_wts'+ '_'+str(random_init_wts)+'_' +current_date+'.log'\n",
    "    print(f\"*****LOGGING INFO IN {log_file}*********\")\n",
    "    filepath = os.path.join(\"model\",log_file)\n",
    "    logging.basicConfig(filename=filepath, \n",
    "                    filemode='a',  # Overwrite the log file each time\n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s', \n",
    "                    level=logging.DEBUG)\n",
    "    logger = logging.getLogger()\n",
    "    logger.info(log_message)\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "34e9170e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad\n",
    "def eval_model(val_loader, model, epoch , device = device,tokenizer = tokenizer):\n",
    "    global global_val_loss\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    e = epoch+1\n",
    "    embedding_layer = model.transformer.wte\n",
    "    val_loss_accum = 0.0\n",
    "    print(f\"inside validation data for epoch {e}\")\n",
    "    for ind,(input_id,attention_mask) in enumerate(val_loader):\n",
    "        ids = input_id.to(device=device, non_blocking=True)\n",
    "        att_mask = attention_mask.to(device=device, non_blocking=True)\n",
    "        labels = ids.clone()\n",
    "        \n",
    "        with autocast(dtype = torch.bfloat16):\n",
    "            input_embeddings = embedding_layer(ids)\n",
    "            model_output = model(input_ids = ids ,attention_mask = att_mask, labels = labels)\n",
    "            logits = model_output.logits\n",
    "            predictions = torch.argmax(logits, dim=-1)\n",
    "            #print(f\"predictions = {predictions}\")\n",
    "            prediction_embeddings = embedding_layer(predictions)\n",
    "            cos_sim = F.cosine_similarity(torch.squeeze(prediction_embeddings,dim = 0), torch.squeeze(input_embeddings,dim = 0), dim=1)\n",
    "            cos_loss = 1- cos_sim.mean()\n",
    "            total_loss = cos_loss + model_output.loss\n",
    "        \n",
    "        val_loss_accum+=total_loss.detach().item()\n",
    "        del ids,att_mask,labels,input_embeddings,model_output,logits,predictions,prediction_embeddings,cos_sim,cos_loss\n",
    "    \n",
    "    #logging and saving if the validation loss has decreased.\n",
    "    if val_loss_accum < global_val_loss:\n",
    "        print(f\"Val loss has decreased -->reducing the global validation loss from {global_val_loss:.2f} to {val_loss_accum:.2f}\")\n",
    "        s1 = f\"Val loss has decreased -->reducing the global validation loss from {global_val_loss:.2f} to {val_loss_accum:.2f}\"\n",
    "        global_val_loss = val_loss_accum\n",
    "        print(f\" validation loss for epoch = {e} is {val_loss_accum:.4f}\")\n",
    "        s2 = f\" validation loss for epoch = {e} is {val_loss_accum:.4f}\"\n",
    "        print(f\" epoch= {e} :  val loss is {val_loss_accum:.4f} \")\n",
    "        s3 = f\" epoch= {e} :  val loss is {val_loss_accum:.4f} \"\n",
    "        #save the model\n",
    "        \n",
    "        # Get the current date and time\n",
    "        current_datetime = datetime.datetime.now()\n",
    "        # Extract date and time components\n",
    "        current_date = str(current_datetime.date())\n",
    "        current_time = str(current_datetime.time()).split('.')[0]\n",
    "        file_name = 'model'+ current_date+current_time+'.pth'\n",
    "        path = os.path.join(\"model\",file_name)\n",
    "        print(f\"saving the model {file_name}\")\n",
    "        s4 = f\"saving the model {file_name}\"\n",
    "        #torch.save(model.state_dict(), path)\n",
    "        model.save_pretrained(path)\n",
    "        tokenizer.save_pretrained(path)\n",
    "        log_message = s1+s2+s3+s4\n",
    "        write_file(log_message)\n",
    "        \n",
    "        #plot_confusion_matrix(y_val.cpu().numpy(), y_hat_val.cpu().numpy(), labels)\n",
    "    else:\n",
    "        print(f\"No improvement in validation loss-->epoch= {e} and global val loss is {global_val_loss:.2f}\")\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2da3c189",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_loader,val_loader,model,num_epoch = 100,device = device,tokenizer = tokenizer):\n",
    "    global global_tr_loss\n",
    "    model.train()\n",
    "    device = device\n",
    "    lr_custom = 5e-5\n",
    "    print(f\"inside train model. Device = {device}\")\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.AdamW(params =  model.parameters(), lr= lr_custom,fused = True ,weight_decay = .1)\n",
    "    total_batch_size = 2**19\n",
    "    grad_accum_step = total_batch_size//(B*T)\n",
    "    \n",
    "    extra_train = .1*num_epoch\n",
    "    max_train_steps = int(num_epoch +extra_train )\n",
    "    import time\n",
    "    from transformers import get_linear_schedule_with_warmup\n",
    "    total_steps = len(train_loader) * num_epoch\n",
    "    scheduler_cos = transformers.get_cosine_schedule_with_warmup( optimizer= optimizer, num_warmup_steps =int(total_steps * 0.1) ,num_training_steps= total_steps ,last_epoch = -1 )\n",
    "    embedding_layer = model.transformer.wte\n",
    "        \n",
    "    for i in range (max_train_steps):\n",
    "        epoch_start_time = time.time()\n",
    "        epoch_train_loss = 0.0\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        batch_loss = 0.0\n",
    "        # we use 2 schedulers - the first LR scheduler uses a cosine decay for 100 epochs the second scheduler takes the last LR from cosine scheduler and then maintains that LR for the next 10 epochs\n",
    "        if i >= num_epoch:\n",
    "            optimizer_reduced_lr = torch.optim.AdamW(params =  model.parameters(), lr= current_lr ,fused = True , weight_decay=.1)\n",
    "            scheduler_constant = transformers.get_constant_schedule_with_warmup( optimizer = optimizer_reduced_lr ,num_warmup_steps = 0, last_epoch = -1 )\n",
    "                \n",
    "        for ind,(input_id,attention_mask) in enumerate(train_loader):\n",
    "            if ind == int(len(train_loader)/2):\n",
    "                batch_time = time.time()\n",
    "                duration = batch_time - epoch_start_time\n",
    "                print(f\"executing epoch:{i+1}, it took {duration/60} mins from beginning of epoch till batch#{ind}\")\n",
    "            \n",
    "            ids = input_id.to(device=device, non_blocking=True)\n",
    "            att_mask = attention_mask.to(device=device, non_blocking=True)\n",
    "            labels = ids.clone()\n",
    "            \n",
    "            with autocast(dtype = torch.bfloat16):\n",
    "                input_embeddings = embedding_layer(ids)\n",
    "                model_output = model(input_ids = ids ,attention_mask = att_mask, labels = labels)\n",
    "                logits = model_output.logits\n",
    "                predictions = torch.argmax(logits, dim=-1)\n",
    "                prediction_embeddings = embedding_layer(predictions)\n",
    "                cos_sim = F.cosine_similarity(torch.squeeze(prediction_embeddings,dim = 0), torch.squeeze(input_embeddings,dim = 0), dim=1)\n",
    "                cos_loss = 1- cos_sim.mean()\n",
    "                total_loss = cos_loss + model_output.loss\n",
    "                                        \n",
    "            total_loss.backward()\n",
    "            epoch_train_loss += total_loss.detach().item()\n",
    "            norm = torch.nn.utils.clip_grad_norm(model.parameters() , 1.0)\n",
    "            if i <= num_epoch:\n",
    "                optimizer.step()\n",
    "                scheduler_cos.step()\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "            else:\n",
    "                optimizer_reduced_lr.step()\n",
    "                optimizer_reduced_lr.zero_grad(set_to_none=True)\n",
    "                scheduler_constant.step()\n",
    "                \n",
    "                                \n",
    "            del ids,att_mask,labels,input_embeddings,model_output,logits,predictions,prediction_embeddings,cos_sim\n",
    "            \n",
    "        \n",
    "        \n",
    "        #batch processing complete \n",
    "        if i <= num_epoch:\n",
    "            current_lr = scheduler_cos.get_last_lr()[0]\n",
    "                    \n",
    "        #This code below compares the training loss during the current epoch with the global loss and if updates the global loss if there is an improvement.\n",
    "        # at every 4 epoch we evluate the model on the validation data or when \n",
    "        epoch_end_time = time.time()\n",
    "        epoch_durn = (epoch_end_time - epoch_start_time)\n",
    "        num_token = B*T*len(train_loader)\n",
    "        if (epoch_train_loss < global_tr_loss) :\n",
    "            print(f\"training loss has decreased---> reducing the global loss from {global_tr_loss:.2f} to {epoch_train_loss:.2f} | throughput = {int(num_token/epoch_durn)} tokens/second | norm = {norm:.4f} | learning rate = {current_lr:.5e}\")\n",
    "            global_tr_loss = epoch_train_loss\n",
    "            print(f\" epoch= {i+1} and  train loss is {epoch_train_loss:.2f}\")\n",
    "            if (i%4 == 0):\n",
    "                eval_model(val_loader, model, epoch = i , device = device,tokenizer = tokenizer)\n",
    "            \n",
    "        else:\n",
    "            print(f\"No improvement in training loss..the global training loss is -->{global_tr_loss:.2f} \")\n",
    "            print(f\" epoch= {i+1} and mean train loss is {epoch_train_loss:.2f}\")\n",
    "        \n",
    "        \n",
    "    \n",
    "    return model\n",
    "        \n",
    "            \n",
    "            \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3f4a22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inside train model. Device = cuda:0\n",
      "executing epoch:1, it took 3.026550054550171 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the global loss from inf to 2034.24 | throughput = 53582 tokens/second | norm = 14.0004 | learning rate = 5.00000e-06\n",
      " epoch= 1 and  train loss is 2034.24\n",
      "inside validation data for epoch 1\n",
      "Val loss has decreased -->reducing the global validation loss from inf to 1926.11\n",
      " validation loss for epoch = 1 is 1926.1128\n",
      " epoch= 1 :  val loss is 1926.1128 \n",
      "saving the model model2024-07-1119:19:49.pth\n",
      "[2024-07-11 19:19:50,703] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "df: /root/.triton/autotune: No such file or directory\n",
      "df: /root/.triton/autotune: No such file or directory\n",
      "/opt/conda/compiler_compat/ld: cannot find -laio\n",
      "collect2: error: ld returned 1 exit status\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93m [WARNING] \u001b[0m async_io requires the dev libaio .so object and headers but these were not found.\n",
      "\u001b[93m [WARNING] \u001b[0m async_io: please install the libaio-dev package with apt\n",
      "\u001b[93m [WARNING] \u001b[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\n",
      "\u001b[93m [WARNING] \u001b[0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH\n",
      "\u001b[93m [WARNING] \u001b[0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3\n",
      "\u001b[93m [WARNING] \u001b[0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible\n",
      "*****LOGGING INFO IN gpt2_COS_SIM_random_init_wts_False_2024-07-11.log*********\n",
      "executing epoch:2, it took 2.59167503118515 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the global loss from 2034.24 to 1794.01 | throughput = 60052 tokens/second | norm = 3.3484 | learning rate = 1.00000e-05\n",
      " epoch= 2 and  train loss is 1794.01\n",
      "executing epoch:3, it took 2.206708101431529 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the global loss from 1794.01 to 1754.83 | throughput = 64796 tokens/second | norm = 3.7710 | learning rate = 1.50000e-05\n",
      " epoch= 3 and  train loss is 1754.83\n",
      "executing epoch:4, it took 2.2088915586471556 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the global loss from 1754.83 to 1732.04 | throughput = 64861 tokens/second | norm = 5.2447 | learning rate = 2.00000e-05\n",
      " epoch= 4 and  train loss is 1732.04\n",
      "executing epoch:5, it took 2.2450008114178974 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the global loss from 1732.04 to 1710.83 | throughput = 71408 tokens/second | norm = 11.9882 | learning rate = 2.50000e-05\n",
      " epoch= 5 and  train loss is 1710.83\n",
      "inside validation data for epoch 5\n",
      "Val loss has decreased -->reducing the global validation loss from 1926.11 to 1806.84\n",
      " validation loss for epoch = 5 is 1806.8403\n",
      " epoch= 5 :  val loss is 1806.8403 \n",
      "saving the model model2024-07-1119:38:45.pth\n",
      "*****LOGGING INFO IN gpt2_COS_SIM_random_init_wts_False_2024-07-11.log*********\n",
      "executing epoch:6, it took 1.5827991525332132 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the global loss from 1710.83 to 1690.00 | throughput = 90538 tokens/second | norm = 6.5550 | learning rate = 3.00000e-05\n",
      " epoch= 6 and  train loss is 1690.00\n",
      "executing epoch:7, it took 1.5821642637252809 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the global loss from 1690.00 to 1668.21 | throughput = 90563 tokens/second | norm = 10.0303 | learning rate = 3.50000e-05\n",
      " epoch= 7 and  train loss is 1668.21\n",
      "executing epoch:8, it took 1.5848619103431703 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the global loss from 1668.21 to 1644.51 | throughput = 90447 tokens/second | norm = 4.9987 | learning rate = 4.00000e-05\n",
      " epoch= 8 and  train loss is 1644.51\n",
      "executing epoch:9, it took 1.582370678583781 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the global loss from 1644.51 to 1619.09 | throughput = 90564 tokens/second | norm = 4.6023 | learning rate = 4.50000e-05\n",
      " epoch= 9 and  train loss is 1619.09\n",
      "inside validation data for epoch 9\n",
      "Val loss has decreased -->reducing the global validation loss from 1806.84 to 1752.36\n",
      " validation loss for epoch = 9 is 1752.3587\n",
      " epoch= 9 :  val loss is 1752.3587 \n",
      "saving the model model2024-07-1119:52:34.pth\n",
      "*****LOGGING INFO IN gpt2_COS_SIM_random_init_wts_False_2024-07-11.log*********\n",
      "executing epoch:10, it took 1.5824767231941224 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the global loss from 1619.09 to 1591.37 | throughput = 90511 tokens/second | norm = 3.9770 | learning rate = 5.00000e-05\n",
      " epoch= 10 and  train loss is 1591.37\n",
      "executing epoch:11, it took 1.5838472088178 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the global loss from 1591.37 to 1562.63 | throughput = 90619 tokens/second | norm = 5.3083 | learning rate = 4.99848e-05\n",
      " epoch= 11 and  train loss is 1562.63\n",
      "executing epoch:12, it took 1.5818037788073223 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the global loss from 1562.63 to 1535.58 | throughput = 90486 tokens/second | norm = 10.7819 | learning rate = 4.99391e-05\n",
      " epoch= 12 and  train loss is 1535.58\n",
      "executing epoch:13, it took 1.5838537136713664 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the global loss from 1535.58 to 1503.55 | throughput = 90565 tokens/second | norm = 4.9251 | learning rate = 4.98630e-05\n",
      " epoch= 13 and  train loss is 1503.55\n",
      "inside validation data for epoch 13\n",
      "Val loss has decreased -->reducing the global validation loss from 1752.36 to 1707.91\n",
      " validation loss for epoch = 13 is 1707.9135\n",
      " epoch= 13 :  val loss is 1707.9135 \n",
      "saving the model model2024-07-1120:06:24.pth\n",
      "*****LOGGING INFO IN gpt2_COS_SIM_random_init_wts_False_2024-07-11.log*********\n",
      "executing epoch:14, it took 1.5847451051076253 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the global loss from 1503.55 to 1474.51 | throughput = 90442 tokens/second | norm = 3.1160 | learning rate = 4.97567e-05\n",
      " epoch= 14 and  train loss is 1474.51\n",
      "executing epoch:15, it took 1.5838276465733847 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the global loss from 1474.51 to 1446.64 | throughput = 90528 tokens/second | norm = 2.1141 | learning rate = 4.96202e-05\n",
      " epoch= 15 and  train loss is 1446.64\n",
      "executing epoch:16, it took 1.5837464809417725 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the global loss from 1446.64 to 1419.36 | throughput = 90515 tokens/second | norm = 4.0103 | learning rate = 4.94537e-05\n",
      " epoch= 16 and  train loss is 1419.36\n",
      "executing epoch:17, it took 1.5841648936271668 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the global loss from 1419.36 to 1394.53 | throughput = 90585 tokens/second | norm = 2.3991 | learning rate = 4.92574e-05\n",
      " epoch= 17 and  train loss is 1394.53\n",
      "inside validation data for epoch 17\n",
      "Val loss has decreased -->reducing the global validation loss from 1707.91 to 1698.40\n",
      " validation loss for epoch = 17 is 1698.3953\n",
      " epoch= 17 :  val loss is 1698.3953 \n",
      "saving the model model2024-07-1120:20:14.pth\n",
      "*****LOGGING INFO IN gpt2_COS_SIM_random_init_wts_False_2024-07-11.log*********\n",
      "executing epoch:18, it took 1.5852572997411092 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the global loss from 1394.53 to 1370.20 | throughput = 90395 tokens/second | norm = 4.2595 | learning rate = 4.90315e-05\n",
      " epoch= 18 and  train loss is 1370.20\n",
      "executing epoch:19, it took 1.5834181865056356 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the global loss from 1370.20 to 1344.40 | throughput = 90522 tokens/second | norm = 3.7976 | learning rate = 4.87764e-05\n",
      " epoch= 19 and  train loss is 1344.40\n",
      "executing epoch:20, it took 1.582509994506836 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the global loss from 1344.40 to 1322.13 | throughput = 90583 tokens/second | norm = 4.8590 | learning rate = 4.84923e-05\n",
      " epoch= 20 and  train loss is 1322.13\n",
      "executing epoch:21, it took 1.581472667058309 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the global loss from 1322.13 to 1298.59 | throughput = 90625 tokens/second | norm = 4.9292 | learning rate = 4.81796e-05\n",
      " epoch= 21 and  train loss is 1298.59\n",
      "inside validation data for epoch 21\n",
      "No improvement in validation loss-->epoch= 21 and global val loss is 1698.40\n",
      "executing epoch:22, it took 1.5816571315129597 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the global loss from 1298.59 to 1276.43 | throughput = 90623 tokens/second | norm = 3.4064 | learning rate = 4.78386e-05\n",
      " epoch= 22 and  train loss is 1276.43\n",
      "executing epoch:23, it took 1.5837431391080221 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the global loss from 1276.43 to 1256.11 | throughput = 90490 tokens/second | norm = 2.4872 | learning rate = 4.74699e-05\n",
      " epoch= 23 and  train loss is 1256.11\n",
      "executing epoch:24, it took 1.5852670272191365 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the global loss from 1256.11 to 1233.87 | throughput = 90514 tokens/second | norm = 6.0115 | learning rate = 4.70737e-05\n",
      " epoch= 24 and  train loss is 1233.87\n",
      "executing epoch:25, it took 1.5839678684870402 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the global loss from 1233.87 to 1214.78 | throughput = 90512 tokens/second | norm = 1.4620 | learning rate = 4.66506e-05\n",
      " epoch= 25 and  train loss is 1214.78\n",
      "inside validation data for epoch 25\n",
      "No improvement in validation loss-->epoch= 25 and global val loss is 1698.40\n",
      "executing epoch:26, it took 1.5841454227765401 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the global loss from 1214.78 to 1193.60 | throughput = 90537 tokens/second | norm = 3.6078 | learning rate = 4.62012e-05\n",
      " epoch= 26 and  train loss is 1193.60\n",
      "executing epoch:27, it took 1.584785803159078 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the global loss from 1193.60 to 1173.48 | throughput = 90433 tokens/second | norm = 2.0807 | learning rate = 4.57259e-05\n",
      " epoch= 27 and  train loss is 1173.48\n",
      "executing epoch:28, it took 1.5836694637934368 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the global loss from 1173.48 to 1152.82 | throughput = 90564 tokens/second | norm = 2.4291 | learning rate = 4.52254e-05\n",
      " epoch= 28 and  train loss is 1152.82\n",
      "executing epoch:29, it took 1.583192237218221 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the global loss from 1152.82 to 1132.98 | throughput = 90565 tokens/second | norm = 4.0096 | learning rate = 4.47003e-05\n",
      " epoch= 29 and  train loss is 1132.98\n",
      "inside validation data for epoch 29\n",
      "No improvement in validation loss-->epoch= 29 and global val loss is 1698.40\n",
      "executing epoch:30, it took 1.5858741799990337 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the global loss from 1132.98 to 1115.66 | throughput = 90485 tokens/second | norm = 4.4357 | learning rate = 4.41511e-05\n",
      " epoch= 30 and  train loss is 1115.66\n",
      "executing epoch:31, it took 1.5839922388394674 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the global loss from 1115.66 to 1095.54 | throughput = 90457 tokens/second | norm = 3.2975 | learning rate = 4.35786e-05\n",
      " epoch= 31 and  train loss is 1095.54\n",
      "executing epoch:32, it took 1.5853859782218933 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the global loss from 1095.54 to 1075.56 | throughput = 90543 tokens/second | norm = 3.4243 | learning rate = 4.29835e-05\n",
      " epoch= 32 and  train loss is 1075.56\n",
      "executing epoch:33, it took 1.5852779825528462 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the global loss from 1075.56 to 1055.68 | throughput = 90450 tokens/second | norm = 2.6917 | learning rate = 4.23665e-05\n",
      " epoch= 33 and  train loss is 1055.68\n",
      "inside validation data for epoch 33\n",
      "No improvement in validation loss-->epoch= 33 and global val loss is 1698.40\n",
      "executing epoch:34, it took 1.581036138534546 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the global loss from 1055.68 to 1038.62 | throughput = 90552 tokens/second | norm = 3.4399 | learning rate = 4.17283e-05\n",
      " epoch= 34 and  train loss is 1038.62\n",
      "executing epoch:35, it took 1.5842474937438964 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the global loss from 1038.62 to 1019.24 | throughput = 90491 tokens/second | norm = 3.1019 | learning rate = 4.10697e-05\n",
      " epoch= 35 and  train loss is 1019.24\n",
      "executing epoch:36, it took 1.58298978805542 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the global loss from 1019.24 to 1001.32 | throughput = 90481 tokens/second | norm = 2.4443 | learning rate = 4.03915e-05\n",
      " epoch= 36 and  train loss is 1001.32\n",
      "executing epoch:37, it took 1.5850986043612163 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the global loss from 1001.32 to 982.53 | throughput = 90454 tokens/second | norm = 2.6534 | learning rate = 3.96946e-05\n",
      " epoch= 37 and  train loss is 982.53\n",
      "inside validation data for epoch 37\n",
      "No improvement in validation loss-->epoch= 37 and global val loss is 1698.40\n",
      "executing epoch:38, it took 1.5848560730616252 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the global loss from 982.53 to 964.91 | throughput = 90419 tokens/second | norm = 3.0326 | learning rate = 3.89798e-05\n",
      " epoch= 38 and  train loss is 964.91\n",
      "executing epoch:39, it took 1.5847320000330607 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the global loss from 964.91 to 946.54 | throughput = 90568 tokens/second | norm = 1.1102 | learning rate = 3.82480e-05\n",
      " epoch= 39 and  train loss is 946.54\n",
      "executing epoch:40, it took 1.5834863821665446 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the global loss from 946.54 to 929.12 | throughput = 90465 tokens/second | norm = 1.7862 | learning rate = 3.75000e-05\n",
      " epoch= 40 and  train loss is 929.12\n",
      "executing epoch:41, it took 1.583830221494039 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the global loss from 929.12 to 911.88 | throughput = 90549 tokens/second | norm = 2.7263 | learning rate = 3.67368e-05\n",
      " epoch= 41 and  train loss is 911.88\n",
      "inside validation data for epoch 41\n",
      "No improvement in validation loss-->epoch= 41 and global val loss is 1698.40\n",
      "executing epoch:42, it took 1.5857457041740417 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the global loss from 911.88 to 895.34 | throughput = 90487 tokens/second | norm = 1.4929 | learning rate = 3.59593e-05\n",
      " epoch= 42 and  train loss is 895.34\n",
      "executing epoch:43, it took 1.5827720880508422 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the global loss from 895.34 to 879.01 | throughput = 90474 tokens/second | norm = 2.8983 | learning rate = 3.51684e-05\n",
      " epoch= 43 and  train loss is 879.01\n",
      "executing epoch:44, it took 1.584636648495992 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the global loss from 879.01 to 861.83 | throughput = 90549 tokens/second | norm = 3.8382 | learning rate = 3.43652e-05\n",
      " epoch= 44 and  train loss is 861.83\n",
      "executing epoch:45, it took 1.5851375222206117 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the global loss from 861.83 to 845.02 | throughput = 90501 tokens/second | norm = 2.8279 | learning rate = 3.35505e-05\n",
      " epoch= 45 and  train loss is 845.02\n",
      "inside validation data for epoch 45\n",
      "No improvement in validation loss-->epoch= 45 and global val loss is 1698.40\n",
      "executing epoch:46, it took 1.5838946104049683 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the global loss from 845.02 to 829.90 | throughput = 90458 tokens/second | norm = 4.2654 | learning rate = 3.27254e-05\n",
      " epoch= 46 and  train loss is 829.90\n",
      "executing epoch:47, it took 1.5855145851771038 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the global loss from 829.90 to 813.72 | throughput = 90505 tokens/second | norm = 1.3821 | learning rate = 3.18909e-05\n",
      " epoch= 47 and  train loss is 813.72\n",
      "executing epoch:48, it took 1.5849823196729025 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the global loss from 813.72 to 798.53 | throughput = 90453 tokens/second | norm = 6.9825 | learning rate = 3.10480e-05\n",
      " epoch= 48 and  train loss is 798.53\n",
      "executing epoch:49, it took 1.5809144973754883 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the global loss from 798.53 to 783.11 | throughput = 90594 tokens/second | norm = 3.0828 | learning rate = 3.01978e-05\n",
      " epoch= 49 and  train loss is 783.11\n",
      "inside validation data for epoch 49\n",
      "No improvement in validation loss-->epoch= 49 and global val loss is 1698.40\n",
      "executing epoch:50, it took 1.5853403449058532 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the global loss from 783.11 to 768.04 | throughput = 90580 tokens/second | norm = 2.7157 | learning rate = 2.93412e-05\n",
      " epoch= 50 and  train loss is 768.04\n",
      "executing epoch:51, it took 1.5852332830429077 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the global loss from 768.04 to 752.91 | throughput = 90398 tokens/second | norm = 2.9531 | learning rate = 2.84793e-05\n",
      " epoch= 51 and  train loss is 752.91\n",
      "executing epoch:52, it took 1.5851617892583212 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the global loss from 752.91 to 738.85 | throughput = 90508 tokens/second | norm = 4.1057 | learning rate = 2.76132e-05\n",
      " epoch= 52 and  train loss is 738.85\n",
      "executing epoch:53, it took 1.584958024819692 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the global loss from 738.85 to 724.56 | throughput = 88159 tokens/second | norm = 1.7550 | learning rate = 2.67439e-05\n",
      " epoch= 53 and  train loss is 724.56\n",
      "inside validation data for epoch 53\n",
      "No improvement in validation loss-->epoch= 53 and global val loss is 1698.40\n",
      "executing epoch:54, it took 2.1897591670354206 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the global loss from 724.56 to 710.34 | throughput = 65333 tokens/second | norm = 0.6673 | learning rate = 2.58725e-05\n",
      " epoch= 54 and  train loss is 710.34\n",
      "executing epoch:55, it took 2.1686238169670107 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the global loss from 710.34 to 696.10 | throughput = 65495 tokens/second | norm = 1.3827 | learning rate = 2.50000e-05\n",
      " epoch= 55 and  train loss is 696.10\n",
      "executing epoch:56, it took 2.215417222181956 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the global loss from 696.10 to 683.62 | throughput = 65426 tokens/second | norm = 2.5397 | learning rate = 2.41275e-05\n",
      " epoch= 56 and  train loss is 683.62\n",
      "executing epoch:57, it took 2.1722126841545104 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the global loss from 683.62 to 670.54 | throughput = 65427 tokens/second | norm = 2.2370 | learning rate = 2.32561e-05\n",
      " epoch= 57 and  train loss is 670.54\n",
      "inside validation data for epoch 57\n",
      "No improvement in validation loss-->epoch= 57 and global val loss is 1698.40\n",
      "executing epoch:58, it took 2.187644187609355 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the global loss from 670.54 to 658.16 | throughput = 65488 tokens/second | norm = 4.1958 | learning rate = 2.23868e-05\n",
      " epoch= 58 and  train loss is 658.16\n",
      "executing epoch:59, it took 2.222415824731191 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the global loss from 658.16 to 646.33 | throughput = 69249 tokens/second | norm = 2.2111 | learning rate = 2.15207e-05\n",
      " epoch= 59 and  train loss is 646.33\n",
      "executing epoch:60, it took 1.583108377456665 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the global loss from 646.33 to 633.69 | throughput = 90564 tokens/second | norm = 2.4278 | learning rate = 2.06588e-05\n",
      " epoch= 60 and  train loss is 633.69\n",
      "executing epoch:61, it took 1.584700628121694 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the global loss from 633.69 to 621.48 | throughput = 90420 tokens/second | norm = 3.2167 | learning rate = 1.98022e-05\n",
      " epoch= 61 and  train loss is 621.48\n",
      "inside validation data for epoch 61\n",
      "No improvement in validation loss-->epoch= 61 and global val loss is 1698.40\n",
      "executing epoch:62, it took 1.5819895307223002 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the global loss from 621.48 to 609.07 | throughput = 90524 tokens/second | norm = 1.7987 | learning rate = 1.89520e-05\n",
      " epoch= 62 and  train loss is 609.07\n",
      "executing epoch:63, it took 1.5845598101615905 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the global loss from 609.07 to 598.16 | throughput = 90553 tokens/second | norm = 2.4192 | learning rate = 1.81091e-05\n",
      " epoch= 63 and  train loss is 598.16\n",
      "executing epoch:64, it took 1.5842054764429727 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the global loss from 598.16 to 587.66 | throughput = 90462 tokens/second | norm = 2.8059 | learning rate = 1.72746e-05\n",
      " epoch= 64 and  train loss is 587.66\n",
      "executing epoch:65, it took 1.5839665492375692 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the global loss from 587.66 to 576.88 | throughput = 90543 tokens/second | norm = 3.4441 | learning rate = 1.64495e-05\n",
      " epoch= 65 and  train loss is 576.88\n",
      "inside validation data for epoch 65\n",
      "No improvement in validation loss-->epoch= 65 and global val loss is 1698.40\n",
      "executing epoch:66, it took 1.5861730734507242 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the global loss from 576.88 to 566.72 | throughput = 90465 tokens/second | norm = 2.0748 | learning rate = 1.56348e-05\n",
      " epoch= 66 and  train loss is 566.72\n",
      "executing epoch:67, it took 1.5833144426345824 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the global loss from 566.72 to 556.44 | throughput = 90458 tokens/second | norm = 4.5538 | learning rate = 1.48316e-05\n",
      " epoch= 67 and  train loss is 556.44\n",
      "executing epoch:68, it took 1.5849131306012472 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the global loss from 556.44 to 547.06 | throughput = 90511 tokens/second | norm = 2.3256 | learning rate = 1.40407e-05\n",
      " epoch= 68 and  train loss is 547.06\n",
      "executing epoch:69, it took 1.5845222234725953 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the global loss from 547.06 to 538.07 | throughput = 90482 tokens/second | norm = 1.2650 | learning rate = 1.32632e-05\n",
      " epoch= 69 and  train loss is 538.07\n",
      "inside validation data for epoch 69\n",
      "No improvement in validation loss-->epoch= 69 and global val loss is 1698.40\n",
      "executing epoch:70, it took 1.5843924641609193 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the global loss from 538.07 to 528.59 | throughput = 90453 tokens/second | norm = 1.9281 | learning rate = 1.25000e-05\n",
      " epoch= 70 and  train loss is 528.59\n",
      "executing epoch:71, it took 1.5868381261825562 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the global loss from 528.59 to 520.03 | throughput = 90465 tokens/second | norm = 2.2232 | learning rate = 1.17520e-05\n",
      " epoch= 71 and  train loss is 520.03\n",
      "executing epoch:72, it took 1.5839755336443584 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the global loss from 520.03 to 511.99 | throughput = 90441 tokens/second | norm = 2.3577 | learning rate = 1.10202e-05\n",
      " epoch= 72 and  train loss is 511.99\n",
      "executing epoch:73, it took 1.5863514343897502 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the global loss from 511.99 to 504.25 | throughput = 90436 tokens/second | norm = 1.9996 | learning rate = 1.03054e-05\n",
      " epoch= 73 and  train loss is 504.25\n",
      "inside validation data for epoch 73\n",
      "No improvement in validation loss-->epoch= 73 and global val loss is 1698.40\n",
      "executing epoch:74, it took 1.583847951889038 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the global loss from 504.25 to 496.79 | throughput = 90480 tokens/second | norm = 3.3730 | learning rate = 9.60846e-06\n",
      " epoch= 74 and  train loss is 496.79\n",
      "executing epoch:75, it took 1.585607941945394 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the global loss from 496.79 to 489.63 | throughput = 90458 tokens/second | norm = 1.9680 | learning rate = 8.93031e-06\n",
      " epoch= 75 and  train loss is 489.63\n",
      "executing epoch:76, it took 1.5836700399716694 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the global loss from 489.63 to 482.77 | throughput = 90498 tokens/second | norm = 1.5193 | learning rate = 8.27173e-06\n",
      " epoch= 76 and  train loss is 482.77\n",
      "executing epoch:77, it took 1.585742747783661 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the global loss from 482.77 to 476.67 | throughput = 90474 tokens/second | norm = 1.8992 | learning rate = 7.63354e-06\n",
      " epoch= 77 and  train loss is 476.67\n",
      "inside validation data for epoch 77\n",
      "No improvement in validation loss-->epoch= 77 and global val loss is 1698.40\n",
      "executing epoch:78, it took 1.5850395242373148 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the global loss from 476.67 to 470.74 | throughput = 90455 tokens/second | norm = 2.5810 | learning rate = 7.01650e-06\n",
      " epoch= 78 and  train loss is 470.74\n",
      "executing epoch:79, it took 1.5828100005785624 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the global loss from 470.74 to 465.15 | throughput = 90592 tokens/second | norm = 2.5628 | learning rate = 6.42138e-06\n",
      " epoch= 79 and  train loss is 465.15\n",
      "executing epoch:80, it took 1.5857540170351665 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the global loss from 465.15 to 459.82 | throughput = 90349 tokens/second | norm = 1.5912 | learning rate = 5.84889e-06\n",
      " epoch= 80 and  train loss is 459.82\n",
      "executing epoch:81, it took 1.58457803328832 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the global loss from 459.82 to 455.12 | throughput = 90489 tokens/second | norm = 0.3278 | learning rate = 5.29973e-06\n",
      " epoch= 81 and  train loss is 455.12\n",
      "inside validation data for epoch 81\n",
      "No improvement in validation loss-->epoch= 81 and global val loss is 1698.40\n",
      "executing epoch:82, it took 1.586243780454 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the global loss from 455.12 to 450.68 | throughput = 90418 tokens/second | norm = 2.1405 | learning rate = 4.77458e-06\n",
      " epoch= 82 and  train loss is 450.68\n",
      "executing epoch:83, it took 1.583108385403951 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the global loss from 450.68 to 446.69 | throughput = 90482 tokens/second | norm = 1.8815 | learning rate = 4.27406e-06\n",
      " epoch= 83 and  train loss is 446.69\n",
      "executing epoch:84, it took 1.5841641823450725 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the global loss from 446.69 to 442.96 | throughput = 90516 tokens/second | norm = 2.7350 | learning rate = 3.79880e-06\n",
      " epoch= 84 and  train loss is 442.96\n",
      "executing epoch:85, it took 1.5849656422932943 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the global loss from 442.96 to 439.66 | throughput = 90394 tokens/second | norm = 1.8772 | learning rate = 3.34936e-06\n",
      " epoch= 85 and  train loss is 439.66\n",
      "inside validation data for epoch 85\n",
      "No improvement in validation loss-->epoch= 85 and global val loss is 1698.40\n",
      "executing epoch:86, it took 1.5835835615793863 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the global loss from 439.66 to 436.62 | throughput = 90479 tokens/second | norm = 2.2308 | learning rate = 2.92631e-06\n",
      " epoch= 86 and  train loss is 436.62\n",
      "executing epoch:87, it took 1.583934986591339 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the global loss from 436.62 to 433.98 | throughput = 90556 tokens/second | norm = 2.1635 | learning rate = 2.53015e-06\n",
      " epoch= 87 and  train loss is 433.98\n",
      "executing epoch:88, it took 1.5832098007202149 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the global loss from 433.98 to 431.63 | throughput = 90572 tokens/second | norm = 2.1154 | learning rate = 2.16136e-06\n",
      " epoch= 88 and  train loss is 431.63\n",
      "executing epoch:89, it took 1.584583314259847 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the global loss from 431.63 to 429.52 | throughput = 90425 tokens/second | norm = 2.1916 | learning rate = 1.82040e-06\n",
      " epoch= 89 and  train loss is 429.52\n",
      "inside validation data for epoch 89\n",
      "No improvement in validation loss-->epoch= 89 and global val loss is 1698.40\n",
      "executing epoch:90, it took 1.581818425655365 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the global loss from 429.52 to 427.68 | throughput = 90510 tokens/second | norm = 1.4600 | learning rate = 1.50768e-06\n",
      " epoch= 90 and  train loss is 427.68\n",
      "executing epoch:91, it took 1.585625425974528 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the global loss from 427.68 to 426.12 | throughput = 90478 tokens/second | norm = 1.8650 | learning rate = 1.22359e-06\n",
      " epoch= 91 and  train loss is 426.12\n",
      "executing epoch:92, it took 1.5843737363815307 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the global loss from 426.12 to 424.74 | throughput = 90461 tokens/second | norm = 2.7889 | learning rate = 9.68458e-07\n",
      " epoch= 92 and  train loss is 424.74\n",
      "executing epoch:93, it took 1.586472737789154 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the global loss from 424.74 to 423.65 | throughput = 90499 tokens/second | norm = 1.6822 | learning rate = 7.42607e-07\n",
      " epoch= 93 and  train loss is 423.65\n",
      "inside validation data for epoch 93\n",
      "No improvement in validation loss-->epoch= 93 and global val loss is 1698.40\n",
      "executing epoch:94, it took 1.5863292654355368 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the global loss from 423.65 to 422.74 | throughput = 90429 tokens/second | norm = 1.2489 | learning rate = 5.46310e-07\n",
      " epoch= 94 and  train loss is 422.74\n",
      "executing epoch:95, it took 1.5847102363904317 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the global loss from 422.74 to 422.01 | throughput = 90512 tokens/second | norm = 1.7503 | learning rate = 3.79806e-07\n",
      " epoch= 95 and  train loss is 422.01\n",
      "executing epoch:96, it took 1.5850664655367532 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the global loss from 422.01 to 421.43 | throughput = 90433 tokens/second | norm = 1.0020 | learning rate = 2.43298e-07\n",
      " epoch= 96 and  train loss is 421.43\n",
      "executing epoch:97, it took 1.585416313012441 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the global loss from 421.43 to 420.99 | throughput = 90517 tokens/second | norm = 1.3443 | learning rate = 1.36953e-07\n",
      " epoch= 97 and  train loss is 420.99\n",
      "inside validation data for epoch 97\n",
      "No improvement in validation loss-->epoch= 97 and global val loss is 1698.40\n",
      "executing epoch:98, it took 1.5842858672142028 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the global loss from 420.99 to 420.66 | throughput = 90455 tokens/second | norm = 1.6202 | learning rate = 6.08987e-08\n",
      " epoch= 98 and  train loss is 420.66\n",
      "executing epoch:99, it took 1.585534922281901 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the global loss from 420.66 to 420.42 | throughput = 90524 tokens/second | norm = 1.3711 | learning rate = 1.52293e-08\n",
      " epoch= 99 and  train loss is 420.42\n",
      "executing epoch:100, it took 1.584875766436259 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the global loss from 420.42 to 420.35 | throughput = 90445 tokens/second | norm = 1.4776 | learning rate = 0.00000e+00\n",
      " epoch= 100 and  train loss is 420.35\n",
      "executing epoch:101, it took 1.5860449473063152 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the global loss from 420.35 to 420.34 | throughput = 90482 tokens/second | norm = 1.7575 | learning rate = 1.52293e-08\n",
      " epoch= 101 and  train loss is 420.34\n",
      "inside validation data for epoch 101\n",
      "No improvement in validation loss-->epoch= 101 and global val loss is 1698.40\n",
      "executing epoch:102, it took 1.586033082008362 mins from beginning of epoch till batch#262\n",
      "training loss has decreased---> reducing the global loss from 420.34 to 420.34 | throughput = 90403 tokens/second | norm = 1.3077 | learning rate = 1.52293e-08\n",
      " epoch= 102 and  train loss is 420.34\n",
      "executing epoch:103, it took 1.5830505649248758 mins from beginning of epoch till batch#262\n"
     ]
    }
   ],
   "source": [
    "tr_model = train_model(train_loader, val_loader, model =  model,tokenizer = tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ecebbf8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e23c85a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab610d65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e46c89d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19c2afa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a928301",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b59b7ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d9ecec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f3d708",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2c452d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3514f8f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
