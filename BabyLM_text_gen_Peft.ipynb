{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a733333c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## reference https://huggingface.co/learn/nlp-course/en/chapter7/6?fw=pt#training-a-causal-language-model-from-scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde4a587",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6918e52",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: datasets in /opt/conda/lib/python3.8/site-packages (2.20.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.8/site-packages (from datasets) (3.4.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.8/site-packages (from datasets) (21.3)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.8/site-packages (from datasets) (1.3.4)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /opt/conda/lib/python3.8/site-packages (from datasets) (4.66.4)\n",
      "Requirement already satisfied: requests>=2.32.2 in /opt/conda/lib/python3.8/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.8/site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.8/site-packages (from datasets) (6.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.8/site-packages (from datasets) (16.1.0)\n",
      "Requirement already satisfied: fsspec[http]<=2024.5.0,>=2023.1.0 in /opt/conda/lib/python3.8/site-packages (from datasets) (2024.5.0)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.8/site-packages (from datasets) (3.9.5)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.8/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.8/site-packages (from datasets) (1.21.4)\n",
      "Requirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.8/site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.8/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.2 in /opt/conda/lib/python3.8/site-packages (from datasets) (0.23.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets) (21.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.8/site-packages (from huggingface-hub>=0.21.2->datasets) (4.11.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging->datasets) (3.0.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.8/site-packages (from requests>=2.32.2->datasets) (2.0.8)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests>=2.32.2->datasets) (1.26.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests>=2.32.2->datasets) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests>=2.32.2->datasets) (3.1)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.8/site-packages (from pandas->datasets) (2021.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.8/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: jupyter in /opt/conda/lib/python3.8/site-packages (1.0.0)\n",
      "Requirement already satisfied: ipywidgets in /opt/conda/lib/python3.8/site-packages (8.1.3)\n",
      "Requirement already satisfied: qtconsole in /opt/conda/lib/python3.8/site-packages (from jupyter) (5.5.2)\n",
      "Requirement already satisfied: jupyter-console in /opt/conda/lib/python3.8/site-packages (from jupyter) (6.6.3)\n",
      "Requirement already satisfied: ipykernel in /opt/conda/lib/python3.8/site-packages (from jupyter) (6.29.4)\n",
      "Requirement already satisfied: nbconvert in /opt/conda/lib/python3.8/site-packages (from jupyter) (6.3.0)\n",
      "Requirement already satisfied: notebook in /opt/conda/lib/python3.8/site-packages (from jupyter) (6.4.1)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.11 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (4.0.11)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.11 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (3.0.11)\n",
      "Requirement already satisfied: comm>=0.1.3 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (7.30.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (5.14.3)\n",
      "Requirement already satisfied: decorator in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (5.1.0)\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (4.8.0)\n",
      "Requirement already satisfied: pickleshare in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.7.5)\n",
      "Requirement already satisfied: jedi>=0.16 in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.18.1)\n",
      "Requirement already satisfied: pygments in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (2.10.0)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.47)\n",
      "Requirement already satisfied: matplotlib-inline in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.3)\n",
      "Requirement already satisfied: backcall in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: setuptools>=18.5 in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (59.4.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /opt/conda/lib/python3.8/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.8/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.8/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=6.1.0->ipywidgets) (0.2.5)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.8/site-packages (from ipykernel->jupyter) (5.8.0)\n",
      "Requirement already satisfied: tornado>=6.1 in /opt/conda/lib/python3.8/site-packages (from ipykernel->jupyter) (6.1)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in /opt/conda/lib/python3.8/site-packages (from ipykernel->jupyter) (7.1.0)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in /opt/conda/lib/python3.8/site-packages (from ipykernel->jupyter) (1.8.1)\n",
      "Requirement already satisfied: nest-asyncio in /opt/conda/lib/python3.8/site-packages (from ipykernel->jupyter) (1.5.4)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.8/site-packages (from ipykernel->jupyter) (21.3)\n",
      "Requirement already satisfied: pyzmq>=24 in /opt/conda/lib/python3.8/site-packages (from ipykernel->jupyter) (26.0.3)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /opt/conda/lib/python3.8/site-packages (from ipykernel->jupyter) (5.7.2)\n",
      "Requirement already satisfied: entrypoints in /opt/conda/lib/python3.8/site-packages (from jupyter-client>=6.1.12->ipykernel->jupyter) (0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /opt/conda/lib/python3.8/site-packages (from jupyter-client>=6.1.12->ipykernel->jupyter) (2.8.2)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /opt/conda/lib/python3.8/site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel->jupyter) (4.2.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.8/site-packages (from python-dateutil>=2.1->jupyter-client>=6.1.12->ipykernel->jupyter) (1.16.0)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (1.5.0)\n",
      "Requirement already satisfied: bleach in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (4.1.0)\n",
      "Requirement already satisfied: mistune<2,>=0.8.1 in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (0.8.4)\n",
      "Requirement already satisfied: testpath in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (0.5.0)\n",
      "Requirement already satisfied: jinja2>=2.4 in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (3.0.3)\n",
      "Requirement already satisfied: defusedxml in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (0.7.1)\n",
      "Requirement already satisfied: nbclient<0.6.0,>=0.5.0 in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (0.5.9)\n",
      "Requirement already satisfied: jupyterlab-pygments in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (0.1.2)\n",
      "Requirement already satisfied: nbformat>=4.4 in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (5.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.8/site-packages (from jinja2>=2.4->nbconvert->jupyter) (2.0.1)\n",
      "Requirement already satisfied: ipython-genutils in /opt/conda/lib/python3.8/site-packages (from nbformat>=4.4->nbconvert->jupyter) (0.2.0)\n",
      "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /opt/conda/lib/python3.8/site-packages (from nbformat>=4.4->nbconvert->jupyter) (4.2.1)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /opt/conda/lib/python3.8/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.4->nbconvert->jupyter) (0.18.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /opt/conda/lib/python3.8/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.4->nbconvert->jupyter) (21.2.0)\n",
      "Requirement already satisfied: importlib-resources>=1.4.0 in /opt/conda/lib/python3.8/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.4->nbconvert->jupyter) (5.4.0)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /opt/conda/lib/python3.8/site-packages (from importlib-resources>=1.4.0->jsonschema!=2.5.0,>=2.4->nbformat>=4.4->nbconvert->jupyter) (3.6.0)\n",
      "Requirement already satisfied: webencodings in /opt/conda/lib/python3.8/site-packages (from bleach->nbconvert->jupyter) (0.5.1)\n",
      "Requirement already satisfied: argon2-cffi in /opt/conda/lib/python3.8/site-packages (from notebook->jupyter) (21.1.0)\n",
      "Requirement already satisfied: prometheus-client in /opt/conda/lib/python3.8/site-packages (from notebook->jupyter) (0.12.0)\n",
      "Requirement already satisfied: Send2Trash>=1.5.0 in /opt/conda/lib/python3.8/site-packages (from notebook->jupyter) (1.8.0)\n",
      "Requirement already satisfied: terminado>=0.8.3 in /opt/conda/lib/python3.8/site-packages (from notebook->jupyter) (0.12.1)\n",
      "Requirement already satisfied: cffi>=1.0.0 in /opt/conda/lib/python3.8/site-packages (from argon2-cffi->notebook->jupyter) (1.15.0)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.8/site-packages (from cffi>=1.0.0->argon2-cffi->notebook->jupyter) (2.21)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging->ipykernel->jupyter) (3.0.6)\n",
      "Requirement already satisfied: qtpy>=2.4.0 in /opt/conda/lib/python3.8/site-packages (from qtconsole->jupyter) (2.4.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#installing some libraries\n",
    "!pip install datasets\n",
    "!pip install --upgrade jupyter ipywidgets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b9f3c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch, transformers\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import PreTrainedModel, PretrainedConfig\n",
    "from transformers import AutoModel, AutoConfig,AutoModelForCausalLM, AutoConfig\n",
    "\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "#from datasets import load_dataset\n",
    "import pandas as pd, numpy as np\n",
    "from torch import cuda\n",
    "import datetime\n",
    "import warnings,itertools\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "# Ignore all warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "#pip install transformers bitsandbytes>=0.39.0 -q\n",
    "import zipfile,logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1d4c155",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "model\n"
     ]
    }
   ],
   "source": [
    "#global params for training\n",
    "\n",
    "B,T = 4,1024\n",
    "epoch = 100\n",
    "random_init_wts = False\n",
    "min_text_len = 0\n",
    "# train_loss_list = []\n",
    "# val_loss_list =[]\n",
    "if cuda.is_available():\n",
    "    device = torch.device('cuda:0')\n",
    "    print(device)\n",
    "else:\n",
    "    device = 'cpu'\n",
    "#print(device)\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "#os.environ[\"MKL_DEBUG_CPU_TYPE\"] = \"5\"\n",
    "context_length = None\n",
    "global_tr_loss = torch.inf\n",
    "global_val_loss = torch.inf\n",
    "#print(global_tr_loss)\n",
    "model_path = os.path.join(\"model\")\n",
    "print(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "158e56e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./data/unzip_text_10M'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "directory = os.path.join('.','data','unzip_text_10M')  # Replace with your directory path\n",
    "directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a31cd215",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_text(directory):\n",
    "    directory = os.path.join('.','data','unzip_text_10M',str(directory))  # Replace with your directory path\n",
    "    print(f\"directory :{directory}\")\n",
    "    # List all files in the directory\n",
    "    files = [f for f in os.listdir(directory) if os.path.isfile(os.path.join(directory, f))]\n",
    "    print(f\"files:{files}\")\n",
    "    text_content = []\n",
    "    # Read each file\n",
    "    total_lines = 0\n",
    "    for filenum,filename in enumerate(files):\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            #first_line = file.read()\n",
    "            #print(f\"filename :{filename}->first few lines {first_line}\")\n",
    "            #continue\n",
    "            #lines_list = [line.strip() for line in open(file_path, 'r')]\n",
    "            text = file.read()\n",
    "            text_content.append(text)\n",
    "            print(f\"the file:{filename} has been appeneded to the uber list and its length is {len(text_content)} \")\n",
    "            #total_lines+=len(lines_list)\n",
    "            #text_content.append(lines_list)\n",
    "    \n",
    "    flattened_list = ''.join(text_content)\n",
    "    assert (len(flattened_list) == total_lines , f\"Expected {len(flattened_list)} to be equal to {total_lines}\" )\n",
    "    \n",
    "    return flattened_list\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "914662d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "directory :./data/unzip_text_10M/train_10M\n",
      "files:['switchboard.train', 'simple_wiki.train', 'open_subtitles.train', 'gutenberg.train', 'childes.train', 'bnc_spoken.train']\n",
      "the file:switchboard.train has been appeneded to the uber list and its length is 1 \n",
      "the file:simple_wiki.train has been appeneded to the uber list and its length is 2 \n",
      "the file:open_subtitles.train has been appeneded to the uber list and its length is 3 \n",
      "the file:gutenberg.train has been appeneded to the uber list and its length is 4 \n",
      "the file:childes.train has been appeneded to the uber list and its length is 5 \n",
      "the file:bnc_spoken.train has been appeneded to the uber list and its length is 6 \n"
     ]
    }
   ],
   "source": [
    "train_list = read_text(\"train_10M\")\n",
    "#print(train_dict)\n",
    "#val_list = read_text(\"dev\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1fe69411",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54215049"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3345e08d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13236\n"
     ]
    }
   ],
   "source": [
    "chunks = len(train_list)//(B*T)\n",
    "print(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4241a6c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17191971 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2-large\",return_tensors = \"pt\" , truncate = True, return_overflowing_tokens=True , padding = False,)\n",
    "enc_train = tokenizer(train_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d51a4e50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.1535097982657136"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comp_ratio = len(train_list)/len(enc_train['input_ids'])\n",
    "comp_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6b845f7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "directory :./data/unzip_text_10M/dev\n",
      "files:['switchboard.dev', 'simple_wiki.dev', 'open_subtitles.dev', 'gutenberg.dev', 'childes.dev', 'bnc_spoken.dev']\n",
      "the file:switchboard.dev has been appeneded to the uber list and its length is 1 \n",
      "the file:simple_wiki.dev has been appeneded to the uber list and its length is 2 \n",
      "the file:open_subtitles.dev has been appeneded to the uber list and its length is 3 \n",
      "the file:gutenberg.dev has been appeneded to the uber list and its length is 4 \n",
      "the file:childes.dev has been appeneded to the uber list and its length is 5 \n",
      "the file:bnc_spoken.dev has been appeneded to the uber list and its length is 6 \n"
     ]
    }
   ],
   "source": [
    "val_list = read_text(\"dev\")\n",
    "enc_val = tokenizer(val_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550bd85a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f6d3f19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df_from_list(enc , B= B, T = T, val= False):\n",
    "    chunk_size = B*T\n",
    "    if val:\n",
    "        chunk_size = 2*B*T\n",
    "        \n",
    "    long_list_inp = enc['input_ids']\n",
    "    long_list_attention = enc['attention_mask']\n",
    "\n",
    "    # Step 3: Split the list into chunks and pad the last chunk if necessary\n",
    "    chunks_inp = [long_list_inp[i:i + chunk_size] for i in range(0, len(long_list_inp), chunk_size)]\n",
    "    chunks_att = [long_list_attention[i:i + chunk_size] for i in range(0, len(long_list_attention), chunk_size)]\n",
    "    df = pd.DataFrame({'input_ids': chunks_inp,'attention_mask':chunks_att})\n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "87ccf69a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the dataframe is = 4198\n"
     ]
    }
   ],
   "source": [
    "df_train_temp = get_df_from_list(enc_train)\n",
    "# Display the DataFrame\n",
    "df_train_temp.head()\n",
    "print(f\"Length of the dataframe is = {len(df_train_temp)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6e65939a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the VALDATION dataframe is = 4252\n"
     ]
    }
   ],
   "source": [
    "df_val_temp = get_df_from_list(enc_val)\n",
    "# Display the DataFrame\n",
    "df_val_temp.head()\n",
    "print(f\"Length of the VALDATION dataframe is = {len(df_val_temp)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ed927b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_df(df,eos_char = tokenizer.eos_token_id,B = B, T = T,val = False):\n",
    "    if val:\n",
    "        B = 2*B\n",
    "    for ind,row in df.iterrows():\n",
    "        if len(row['input_ids']) != B*T :\n",
    "            print(f\"row = {ind} and input_id length = {len(row['input_ids'])}\")\n",
    "            print(f\"row = {ind} and attention length = {len(row['attention_mask'])}\")\n",
    "            pad_len = B*T - len(row['input_ids'])\n",
    "            print(f\"padding the row index {ind} with {pad_len} character\")\n",
    "            row['input_ids'] = row['input_ids']+ [eos_char] * pad_len\n",
    "            #attention mask should be padded to 0\n",
    "            row['attention_mask'] = row['attention_mask']+ [0] * pad_len\n",
    "            print(\"#### POST CONCAT####\")\n",
    "            print(f\"row = {ind} and input_id length = {len(row['input_ids'])}\")\n",
    "            print(f\"row = {ind} and attention length ={len(row['attention_mask'])}\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def verify_len(df,val = False, B= B):\n",
    "    row_ind = []\n",
    "    if val:\n",
    "        B = 2*B\n",
    "    for ind,row in df.iterrows():\n",
    "        if len(row['input_ids']) != B*T :\n",
    "            row_ind.append(ind)\n",
    "        else:\n",
    "            continue\n",
    "    if len(row_ind) !=0:\n",
    "        print(\"CONCATENATION Did not work\")\n",
    "    else:\n",
    "        print(\"CONCATENATION worked\")\n",
    "        \n",
    "            \n",
    "        \n",
    "    \n",
    "        \n",
    "        \n",
    "   \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "deece116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row = 4197 and input_id length = 1059\n",
      "row = 4197 and attention length = 1059\n",
      "padding the row index 4197 with 3037 character\n",
      "#### POST CONCAT####\n",
      "row = 4197 and input_id length = 4096\n",
      "row = 4197 and attention length =4096\n",
      "CONCATENATION worked\n"
     ]
    }
   ],
   "source": [
    "df_train  = pad_df(df_train_temp)\n",
    "verify_len(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bb8ce924",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row = 4251 and input_id length = 1300\n",
      "row = 4251 and attention length = 1300\n",
      "padding the row index 4251 with 2796 character\n",
      "#### POST CONCAT####\n",
      "row = 4251 and input_id length = 4096\n",
      "row = 4251 and attention length =4096\n",
      "CONCATENATION worked\n"
     ]
    }
   ],
   "source": [
    "df_val  = pad_df(df_val_temp)\n",
    "verify_len(df_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "21a26cd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_ids</th>\n",
       "      <th>attention_mask</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[32, 25, 197, 40, 1101, 1654, 484, 389, 13, 19...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[198, 32, 25, 197, 40, 1239, 373, 1165, 47878,...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[4398, 470, 1775, 326, 13, 198, 32, 25, 197, 4...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[281, 12226, 329, 661, 11, 1165, 13, 198, 33, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[290, 1243, 588, 326, 198, 33, 25, 197, 392, 1...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           input_ids  \\\n",
       "0  [32, 25, 197, 40, 1101, 1654, 484, 389, 13, 19...   \n",
       "1  [198, 32, 25, 197, 40, 1239, 373, 1165, 47878,...   \n",
       "2  [4398, 470, 1775, 326, 13, 198, 32, 25, 197, 4...   \n",
       "3  [281, 12226, 329, 661, 11, 1165, 13, 198, 33, ...   \n",
       "4  [290, 1243, 588, 326, 198, 33, 25, 197, 392, 1...   \n",
       "\n",
       "                                      attention_mask  \n",
       "0  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "1  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "2  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "3  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "4  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c1525d",
   "metadata": {},
   "source": [
    "## Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "42b542b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, TaskType\n",
    "target_modules = [\"c_attn\",\"q_proj\", \"k_proj\", \"v_proj\", \"out_proj\", \"fc_in\", \"fc_out\", \"wte\"]\n",
    "peft_config = LoraConfig(r=4, lora_alpha=16, target_modules=target_modules, lora_dropout=0.1, bias=\"none\", task_type=\"CAUSAL_LM\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "93870edc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 943,428 || all params: 774,973,508 || trainable%: 0.1217\n"
     ]
    }
   ],
   "source": [
    "# Test the tokenizer:\n",
    "from peft import get_peft_model\n",
    "\n",
    "model_name = \"gpt2-large\"\n",
    "if random_init_wts:\n",
    "    config = AutoConfig.from_pretrained(model_name, vocab_size = 50304)\n",
    "    # Initialize the model with random weights\n",
    "    model = AutoModelForCausalLM.from_config(config)\n",
    "else:\n",
    "    #model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\"gpt2-large\")\n",
    "\n",
    "from peft import get_peft_model\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "model = torch.compile(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835f757f",
   "metadata": {},
   "source": [
    "### Data loaders and Dataset for batched training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "480d91c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataset_pyt_val(Dataset):\n",
    "    def __init__(self, df, B = B, T = T ):\n",
    "        self.df = df\n",
    "        print(f\"Value of B {B}\")\n",
    "                                        \n",
    "    def __getitem__(self, idx):\n",
    "        #print(f\"inside loader...idx ->{idx}\")\n",
    "        input_id_temp = torch.tensor(self.df.iloc[idx]['input_ids'],dtype = torch.long)\n",
    "        #print(f\"length of text ->{len(text)}\")\n",
    "        #print(f\"text ->{text}\")\n",
    "        #encodings = tokenizer(text, truncation=True, max_length= self.max_length, return_overflowing_tokens=True, padding = 'max_length',return_tensors='pt')\n",
    "        att_mask = torch.tensor(self.df.iloc[idx]['attention_mask'],dtype = torch.long)\n",
    "        # check the length of the encoded list\n",
    "        \n",
    "        #x_dict['input_id'] = input_ids_list\n",
    "        #x_dict['attention_mask'] = input_ids_list\n",
    "                \n",
    "        #print(f\"x_dict = {x_dict}\")             \n",
    "#       print(f\"inside the loader and input_id = {input_ids} and its shape is {input_ids.shape}\")\n",
    "        #labels = input_id_list\n",
    "        #input_ids = torch.tensor(input_id_list)\n",
    "        #attention_mask = torch.tensor(attention_mask_list)\n",
    "        #labels = torch.tensor(labels)\n",
    "        #print(f\"inside the loader and input_id shape= {input_ids.shape} attention_mask_shape is {attention_mask.shape} and label shape is {labels.shape}\")\n",
    "        #print(f\"encoding = {encodings}\")\n",
    "        input_id =   input_id_temp.view(B,T)    \n",
    "        attention_mask = att_mask.view(B,T)   \n",
    "        \n",
    "        return input_id, attention_mask\n",
    "        \n",
    "    def __len__(self):\n",
    "        #return the length of the dataframe\n",
    "        return len(self.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ae1ff815",
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataset_pyt_train(Dataset):\n",
    "    def __init__(self, df, B = B, T = T ):\n",
    "        self.df = df\n",
    "                                        \n",
    "    def __getitem__(self, idx):\n",
    "        #print(f\"inside loader...idx ->{idx}\")\n",
    "        input_id_temp = torch.tensor(self.df.iloc[idx]['input_ids'],dtype = torch.long)\n",
    "        #print(f\"length of text ->{len(text)}\")\n",
    "        #print(f\"text ->{text}\")\n",
    "        #encodings = tokenizer(text, truncation=True, max_length= self.max_length, return_overflowing_tokens=True, padding = 'max_length',return_tensors='pt')\n",
    "        att_mask = torch.tensor(self.df.iloc[idx]['attention_mask'],dtype = torch.long)\n",
    "        # check the length of the encoded list\n",
    "        \n",
    "        #x_dict['input_id'] = input_ids_list\n",
    "        #x_dict['attention_mask'] = input_ids_list\n",
    "                \n",
    "        #print(f\"x_dict = {x_dict}\")             \n",
    "#       print(f\"inside the loader and input_id = {input_ids} and its shape is {input_ids.shape}\")\n",
    "        #labels = input_id_list\n",
    "        #input_ids = torch.tensor(input_id_list)\n",
    "        #attention_mask = torch.tensor(attention_mask_list)\n",
    "        #labels = torch.tensor(labels)\n",
    "        #print(f\"inside the loader and input_id shape= {input_ids.shape} attention_mask_shape is {attention_mask.shape} and label shape is {labels.shape}\")\n",
    "        #print(f\"encoding = {encodings}\")\n",
    "        input_id =   input_id_temp.view(B,T)    \n",
    "        attention_mask = att_mask.view(B,T)   \n",
    "        \n",
    "        return input_id, attention_mask\n",
    "        \n",
    "    def __len__(self):\n",
    "        #return the length of the dataframe\n",
    "        return len(self.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6b4bd108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value of B 4\n"
     ]
    }
   ],
   "source": [
    "#train_dataset = dataset_pyt(filtered_df,tokenizer = tokenizer)\n",
    "train_dataset = dataset_pyt_train(df_train)\n",
    "val_dataset = dataset_pyt_val(df_val)\n",
    "#test_dataset = dataset_pyt(df_test,tokenizer = tokenizer)\n",
    "\n",
    "train_loader = DataLoader(train_dataset,batch_size = 1, shuffle = True , num_workers = 4, pin_memory = True)\n",
    "val_loader = DataLoader(val_dataset,batch_size = 1, shuffle = True , num_workers = 4, pin_memory = True)\n",
    "#test_loader = DataLoader(test_dataset,batch_size = batch_size, shuffle = False, collate_fn = custom_collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e78ed18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ac42d3b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the train loader is 4198\n",
      "Length of the val loader is 4252\n",
      "num_tokens= 17195008\n"
     ]
    }
   ],
   "source": [
    "print(f\"Length of the train loader is {len(train_loader)}\")\n",
    "print(f\"Length of the val loader is {len(val_loader)}\")\n",
    "print(f\"num_tokens= {B*T*len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4e3c83df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_output = model(input_ids = inp ,attention_mask = att, labels = inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e1fcc293",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_file(log_message, model_name = model_name ,random_init_wts = random_init_wts ):\n",
    "    current_datetime = datetime.datetime.now()\n",
    "    # Extract date and time components\n",
    "    current_date = str(current_datetime.date())\n",
    "    log_file = model_name +'_PEFT_'+'random_init_wts'+ '_'+str(random_init_wts)+'_' +current_date+'.log'\n",
    "    print(f\"*****LOGGING INFO IN {log_file}*********\")\n",
    "    filepath = os.path.join(\"model\",log_file)\n",
    "    logging.basicConfig(filename=filepath, \n",
    "                    filemode='a',  # Overwrite the log file each time\n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s', \n",
    "                    level=logging.DEBUG)\n",
    "    logger = logging.getLogger()\n",
    "    logger.info(log_message)\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cfa52a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad\n",
    "def eval_model(val_loader, model, epoch , device = device,tokenizer = tokenizer):\n",
    "    global global_val_loss\n",
    "    #m = nn.Softmax()\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    e = epoch+1\n",
    "    val_loss_accum = 0.0\n",
    "    #criterion = torch.nn.BCEWithLogitsLoss()\n",
    "    print(f\"inside validation data for epoch {e}\")\n",
    "    #y_hat_val_list = []\n",
    "    #y_val_list = []\n",
    "    for ind,(input_id,attention_mask) in enumerate(val_loader):\n",
    "        #print(f\"id_list{id_list}\")\n",
    "        ids = input_id.to(device=device, non_blocking=True)\n",
    "        att_mask = attention_mask.to(device=device, non_blocking=True)\n",
    "        labels = ids\n",
    "        #predictions\n",
    "        #print(f\"input_ids device = {input_ids.device}\")\n",
    "        with autocast(dtype = torch.bfloat16):\n",
    "            model_output = model(input_ids = ids ,attention_mask = att_mask, labels = labels)\n",
    "            act_loss = model_output.loss\n",
    "        \n",
    "        val_loss_accum+=act_loss.detach().item()\n",
    "        del ids,att_mask,labels,act_loss\n",
    "    \n",
    "    if val_loss_accum < global_val_loss:\n",
    "        print(f\"Val loss has decreased -->reducing the global validation loss from {global_val_loss:.2f} to {val_loss_accum:.2f}\")\n",
    "        s1 = f\"Val loss has decreased -->reducing the global validation loss from {global_val_loss:.2f} to {val_loss_accum:.2f}\"\n",
    "        global_val_loss = val_loss_accum\n",
    "        print(f\" validation loss for epoch = {e} is {val_loss_accum:.4f}\")\n",
    "        s2 = f\" validation loss for epoch = {e} is {val_loss_accum:.4f}\"\n",
    "        #print metrics and save the model\n",
    "        #y_hat_val = torch.cat(y_hat_val_list)\n",
    "        #y_val = torch.cat(y_val_list)\n",
    "        #acc_val = accuracy_score(y_val.cpu().numpy(), y_hat_val.cpu().numpy())\n",
    "        #f1_val = f1_score(y_val.cpu().numpy(), y_hat_val.cpu().numpy(), average='micro')\n",
    "        print(f\" epoch= {e} :  val loss is {val_loss_accum:.4f} \")\n",
    "        s3 = f\" epoch= {e} :  val loss is {val_loss_accum:.4f} \"\n",
    "        #save the model\n",
    "        \n",
    "        # Get the current date and time\n",
    "        current_datetime = datetime.datetime.now()\n",
    "        # Extract date and time components\n",
    "        current_date = str(current_datetime.date())\n",
    "        current_time = str(current_datetime.time()).split('.')[0]\n",
    "        file_name = 'model'+ current_date+current_time+'.pth'\n",
    "        path = os.path.join(\"model\",file_name)\n",
    "        print(f\"saving the model {file_name}\")\n",
    "        s4 = f\"saving the model {file_name}\"\n",
    "        #torch.save(model.state_dict(), path)\n",
    "        model.save_pretrained(path)\n",
    "        tokenizer.save_pretrained(path)\n",
    "        log_message = s1+s2+s3+s4\n",
    "        write_file(log_message)\n",
    "        \n",
    "        #plot_confusion_matrix(y_val.cpu().numpy(), y_hat_val.cpu().numpy(), labels)\n",
    "    else:\n",
    "        print(f\"No improvement in validation loss-->epoch= {e} and global val loss is {global_val_loss:.2f}\")\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "44ed047e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_loader,val_loader,model,num_epoch = 100,device = device,tokenizer = tokenizer):\n",
    "    global global_tr_loss\n",
    "    model.train()\n",
    "    device = device\n",
    "    lr_custom = 2e-5\n",
    "    print(f\"inside train model. Device = {device}\")\n",
    "    #adding betas params per the paper\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr= lr_custom,betas = (.9,.95),fused = True ,weight_decay = .1)\n",
    "    total_batch_size = 2**19\n",
    "    grad_accum_step = total_batch_size//(B*T)\n",
    "    \n",
    "    extra_train = .1*num_epoch\n",
    "    #m = nn.Softmax()\n",
    "    max_train_steps = int(num_epoch +extra_train )\n",
    "    import time\n",
    "    from transformers import get_linear_schedule_with_warmup\n",
    "    scheduler_cos = transformers.get_cosine_schedule_with_warmup( optimizer= optimizer, num_warmup_steps =num_epoch*.1 ,num_training_steps= num_epoch-1 ,last_epoch = -1 )\n",
    "    \n",
    "        \n",
    "    for i in range (max_train_steps):\n",
    "        epoch_start_time = time.time()\n",
    "        epoch_train_loss = 0.0\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        batch_loss = 0.0\n",
    "        for ind,(input_id,attention_mask) in enumerate(train_loader):\n",
    "            if ind == int(len(train_loader)/2):\n",
    "                batch_time = time.time()\n",
    "                duration = batch_time - epoch_start_time\n",
    "                print(f\"executing epoch:{i+1}, it took {duration/60} mins from beginning of epoch till batch#{ind}\")\n",
    "            \n",
    "            ids = input_id.to(device=device, non_blocking=True)\n",
    "            att_mask = attention_mask.to(device=device, non_blocking=True)\n",
    "            labels = ids\n",
    "            \n",
    "            with autocast(dtype = torch.bfloat16):\n",
    "                model_output = model(input_ids = ids ,attention_mask = att_mask, labels = labels)\n",
    "                act_loss = model_output.loss\n",
    "            #print(f\"inside batch_processing. ind = {ind}|loss for ind = {act_loss} |batch_loss ={batch_loss} | epoch_train_loss = {epoch_train_loss}\")\n",
    "            batch_loss +=act_loss.detach().item()\n",
    "            #print(f\"incrementing batch_loss. ind = {ind}|loss for ind = {act_loss} |batch_loss ={batch_loss} | epoch_train_loss = {epoch_train_loss}\")\n",
    "            act_loss = act_loss/grad_accum_step\n",
    "            if not act_loss.requires_grad:\n",
    "                raise RuntimeError(\"Loss tensor does not require gradients\")\n",
    "\n",
    "            act_loss.backward()\n",
    "            epoch_train_loss += act_loss.detach().item()\n",
    "            \n",
    "            if (ind + 1)% grad_accum_step == 0:\n",
    "                #print(f\"Gradient accum step = {ind}|batch_loss before averaging ={batch_loss} | epoch_train_loss = {epoch_train_loss}\")\n",
    "                norm = torch.nn.utils.clip_grad_norm(model.parameters() , 1.0)\n",
    "                if i <= num_epoch:\n",
    "                    optimizer.step()\n",
    "                    optimizer.zero_grad(set_to_none=True)\n",
    "                else:\n",
    "                    optimizer_reduced_lr.step()\n",
    "                    optimizer_reduced_lr.zero_grad(set_to_none=True)\n",
    "                #print(f\"Gradient accum step = {ind}|batch_loss after averaging ={batch_loss} | epoch_train_loss = {epoch_train_loss}\")\n",
    "                #print(f\"Gradient accum step = {ind}|batch_loss after averaging ={batch_loss} | epoch_train_loss = {epoch_train_loss}\")\n",
    "                                \n",
    "            del ids,att_mask,labels\n",
    "            \n",
    "        \n",
    "        \n",
    "        #batch processing complete \n",
    "        if i <= num_epoch:\n",
    "            current_lr = scheduler_cos.get_last_lr()[0]\n",
    "            scheduler_cos.step()\n",
    "        if i >= num_epoch:\n",
    "            optimizer_reduced_lr = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr= current_lr,betas = (.9,.95),fused = True ,weight_decay = .1)\n",
    "            optimizer_reduced_lr.zero_grad(set_to_none=True)\n",
    "            scheduler_constant = transformers.get_constant_schedule_with_warmup( optimizer = optimizer_reduced_lr ,num_warmup_steps = 0, last_epoch = -1 )\n",
    "            scheduler_constant.step()\n",
    "                      \n",
    "        #mean_loss = torch.mean(torch.tensor(epoch_train_loss))\n",
    "        epoch_end_time = time.time()\n",
    "        epoch_durn = (epoch_end_time - epoch_start_time)\n",
    "        num_token = B*T*len(train_loader)\n",
    "        if (epoch_train_loss < global_tr_loss) :\n",
    "            print(f\"training loss has decreased---> reducing the global loss from {global_tr_loss:.2f} to {epoch_train_loss:.2f} | throughput = {int(num_token/epoch_durn)} tokens/second | norm = {norm:.4f} | learning rate = {current_lr:.5e}\")\n",
    "            global_tr_loss = epoch_train_loss\n",
    "            print(f\" epoch= {i+1} and  train loss is {epoch_train_loss:.2f}\")\n",
    "            #printing training metrices\n",
    "            #y_hat = torch.cat(y_hat_list)\n",
    "            #y = torch.cat(label_list)\n",
    "            #acc = accuracy_score(y.cpu().numpy(), y_hat.cpu().numpy())\n",
    "            #f1 = f1_score(y.cpu().numpy(), y_hat.cpu().numpy(), average='micro')\n",
    "            #checking validation metrices\n",
    "            if (i%4 == 0):\n",
    "                eval_model(val_loader, model, epoch = i , device = device,tokenizer = tokenizer)\n",
    "            \n",
    "        else:\n",
    "            print(f\"No improvement in training loss..the global training loss is -->{global_tr_loss:.2f} \")\n",
    "            print(f\" epoch= {i+1} and mean train loss is {epoch_train_loss:.2f}\")\n",
    "        \n",
    "        \n",
    "    \n",
    "    return model\n",
    "        \n",
    "            \n",
    "            \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9a424af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if \"lora\" in name and not param.requires_grad:\n",
    "        print(f\"LoRA Parameter {name} does not require gradients.\")\n",
    "        param.requires_grad = True  # Explicitly set to require gradients if not\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1c7ed19f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_orig_mod.base_model.model.transformer.wte.lora_embedding_A.default: requires_grad=True\n",
      "_orig_mod.base_model.model.transformer.wte.lora_embedding_B.default: requires_grad=True\n",
      "_orig_mod.base_model.model.transformer.h.0.attn.c_attn.lora_A.default.weight: requires_grad=True\n",
      "_orig_mod.base_model.model.transformer.h.0.attn.c_attn.lora_B.default.weight: requires_grad=True\n",
      "_orig_mod.base_model.model.transformer.h.1.attn.c_attn.lora_A.default.weight: requires_grad=True\n",
      "_orig_mod.base_model.model.transformer.h.1.attn.c_attn.lora_B.default.weight: requires_grad=True\n",
      "_orig_mod.base_model.model.transformer.h.2.attn.c_attn.lora_A.default.weight: requires_grad=True\n",
      "_orig_mod.base_model.model.transformer.h.2.attn.c_attn.lora_B.default.weight: requires_grad=True\n",
      "_orig_mod.base_model.model.transformer.h.3.attn.c_attn.lora_A.default.weight: requires_grad=True\n",
      "_orig_mod.base_model.model.transformer.h.3.attn.c_attn.lora_B.default.weight: requires_grad=True\n",
      "_orig_mod.base_model.model.transformer.h.4.attn.c_attn.lora_A.default.weight: requires_grad=True\n",
      "_orig_mod.base_model.model.transformer.h.4.attn.c_attn.lora_B.default.weight: requires_grad=True\n",
      "_orig_mod.base_model.model.transformer.h.5.attn.c_attn.lora_A.default.weight: requires_grad=True\n",
      "_orig_mod.base_model.model.transformer.h.5.attn.c_attn.lora_B.default.weight: requires_grad=True\n",
      "_orig_mod.base_model.model.transformer.h.6.attn.c_attn.lora_A.default.weight: requires_grad=True\n",
      "_orig_mod.base_model.model.transformer.h.6.attn.c_attn.lora_B.default.weight: requires_grad=True\n",
      "_orig_mod.base_model.model.transformer.h.7.attn.c_attn.lora_A.default.weight: requires_grad=True\n",
      "_orig_mod.base_model.model.transformer.h.7.attn.c_attn.lora_B.default.weight: requires_grad=True\n",
      "_orig_mod.base_model.model.transformer.h.8.attn.c_attn.lora_A.default.weight: requires_grad=True\n",
      "_orig_mod.base_model.model.transformer.h.8.attn.c_attn.lora_B.default.weight: requires_grad=True\n",
      "_orig_mod.base_model.model.transformer.h.9.attn.c_attn.lora_A.default.weight: requires_grad=True\n",
      "_orig_mod.base_model.model.transformer.h.9.attn.c_attn.lora_B.default.weight: requires_grad=True\n",
      "_orig_mod.base_model.model.transformer.h.10.attn.c_attn.lora_A.default.weight: requires_grad=True\n",
      "_orig_mod.base_model.model.transformer.h.10.attn.c_attn.lora_B.default.weight: requires_grad=True\n",
      "_orig_mod.base_model.model.transformer.h.11.attn.c_attn.lora_A.default.weight: requires_grad=True\n",
      "_orig_mod.base_model.model.transformer.h.11.attn.c_attn.lora_B.default.weight: requires_grad=True\n",
      "_orig_mod.base_model.model.transformer.h.12.attn.c_attn.lora_A.default.weight: requires_grad=True\n",
      "_orig_mod.base_model.model.transformer.h.12.attn.c_attn.lora_B.default.weight: requires_grad=True\n",
      "_orig_mod.base_model.model.transformer.h.13.attn.c_attn.lora_A.default.weight: requires_grad=True\n",
      "_orig_mod.base_model.model.transformer.h.13.attn.c_attn.lora_B.default.weight: requires_grad=True\n",
      "_orig_mod.base_model.model.transformer.h.14.attn.c_attn.lora_A.default.weight: requires_grad=True\n",
      "_orig_mod.base_model.model.transformer.h.14.attn.c_attn.lora_B.default.weight: requires_grad=True\n",
      "_orig_mod.base_model.model.transformer.h.15.attn.c_attn.lora_A.default.weight: requires_grad=True\n",
      "_orig_mod.base_model.model.transformer.h.15.attn.c_attn.lora_B.default.weight: requires_grad=True\n",
      "_orig_mod.base_model.model.transformer.h.16.attn.c_attn.lora_A.default.weight: requires_grad=True\n",
      "_orig_mod.base_model.model.transformer.h.16.attn.c_attn.lora_B.default.weight: requires_grad=True\n",
      "_orig_mod.base_model.model.transformer.h.17.attn.c_attn.lora_A.default.weight: requires_grad=True\n",
      "_orig_mod.base_model.model.transformer.h.17.attn.c_attn.lora_B.default.weight: requires_grad=True\n",
      "_orig_mod.base_model.model.transformer.h.18.attn.c_attn.lora_A.default.weight: requires_grad=True\n",
      "_orig_mod.base_model.model.transformer.h.18.attn.c_attn.lora_B.default.weight: requires_grad=True\n",
      "_orig_mod.base_model.model.transformer.h.19.attn.c_attn.lora_A.default.weight: requires_grad=True\n",
      "_orig_mod.base_model.model.transformer.h.19.attn.c_attn.lora_B.default.weight: requires_grad=True\n",
      "_orig_mod.base_model.model.transformer.h.20.attn.c_attn.lora_A.default.weight: requires_grad=True\n",
      "_orig_mod.base_model.model.transformer.h.20.attn.c_attn.lora_B.default.weight: requires_grad=True\n",
      "_orig_mod.base_model.model.transformer.h.21.attn.c_attn.lora_A.default.weight: requires_grad=True\n",
      "_orig_mod.base_model.model.transformer.h.21.attn.c_attn.lora_B.default.weight: requires_grad=True\n",
      "_orig_mod.base_model.model.transformer.h.22.attn.c_attn.lora_A.default.weight: requires_grad=True\n",
      "_orig_mod.base_model.model.transformer.h.22.attn.c_attn.lora_B.default.weight: requires_grad=True\n",
      "_orig_mod.base_model.model.transformer.h.23.attn.c_attn.lora_A.default.weight: requires_grad=True\n",
      "_orig_mod.base_model.model.transformer.h.23.attn.c_attn.lora_B.default.weight: requires_grad=True\n",
      "_orig_mod.base_model.model.transformer.h.24.attn.c_attn.lora_A.default.weight: requires_grad=True\n",
      "_orig_mod.base_model.model.transformer.h.24.attn.c_attn.lora_B.default.weight: requires_grad=True\n",
      "_orig_mod.base_model.model.transformer.h.25.attn.c_attn.lora_A.default.weight: requires_grad=True\n",
      "_orig_mod.base_model.model.transformer.h.25.attn.c_attn.lora_B.default.weight: requires_grad=True\n",
      "_orig_mod.base_model.model.transformer.h.26.attn.c_attn.lora_A.default.weight: requires_grad=True\n",
      "_orig_mod.base_model.model.transformer.h.26.attn.c_attn.lora_B.default.weight: requires_grad=True\n",
      "_orig_mod.base_model.model.transformer.h.27.attn.c_attn.lora_A.default.weight: requires_grad=True\n",
      "_orig_mod.base_model.model.transformer.h.27.attn.c_attn.lora_B.default.weight: requires_grad=True\n",
      "_orig_mod.base_model.model.transformer.h.28.attn.c_attn.lora_A.default.weight: requires_grad=True\n",
      "_orig_mod.base_model.model.transformer.h.28.attn.c_attn.lora_B.default.weight: requires_grad=True\n",
      "_orig_mod.base_model.model.transformer.h.29.attn.c_attn.lora_A.default.weight: requires_grad=True\n",
      "_orig_mod.base_model.model.transformer.h.29.attn.c_attn.lora_B.default.weight: requires_grad=True\n",
      "_orig_mod.base_model.model.transformer.h.30.attn.c_attn.lora_A.default.weight: requires_grad=True\n",
      "_orig_mod.base_model.model.transformer.h.30.attn.c_attn.lora_B.default.weight: requires_grad=True\n",
      "_orig_mod.base_model.model.transformer.h.31.attn.c_attn.lora_A.default.weight: requires_grad=True\n",
      "_orig_mod.base_model.model.transformer.h.31.attn.c_attn.lora_B.default.weight: requires_grad=True\n",
      "_orig_mod.base_model.model.transformer.h.32.attn.c_attn.lora_A.default.weight: requires_grad=True\n",
      "_orig_mod.base_model.model.transformer.h.32.attn.c_attn.lora_B.default.weight: requires_grad=True\n",
      "_orig_mod.base_model.model.transformer.h.33.attn.c_attn.lora_A.default.weight: requires_grad=True\n",
      "_orig_mod.base_model.model.transformer.h.33.attn.c_attn.lora_B.default.weight: requires_grad=True\n",
      "_orig_mod.base_model.model.transformer.h.34.attn.c_attn.lora_A.default.weight: requires_grad=True\n",
      "_orig_mod.base_model.model.transformer.h.34.attn.c_attn.lora_B.default.weight: requires_grad=True\n",
      "_orig_mod.base_model.model.transformer.h.35.attn.c_attn.lora_A.default.weight: requires_grad=True\n",
      "_orig_mod.base_model.model.transformer.h.35.attn.c_attn.lora_B.default.weight: requires_grad=True\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if \"lora\" in name:  # Assuming LoRA parameters contain \"lora\" in their names\n",
    "        print(f\"{name}: requires_grad={param.requires_grad}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86fad5aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inside train model. Device = cuda:0\n",
      "executing epoch:1, it took 8.89516570965449 mins from beginning of epoch till batch#2099\n",
      "training loss has decreased---> reducing the global loss from inf to 115.23 | throughput = 17942 tokens/second | norm = 0.6124 | learning rate = 0.00000e+00\n",
      " epoch= 1 and  train loss is 115.23\n",
      "inside validation data for epoch 1\n",
      "Val loss has decreased -->reducing the global validation loss from inf to 14384.63\n",
      " validation loss for epoch = 1 is 14384.6327\n",
      " epoch= 1 :  val loss is 14384.6327 \n",
      "saving the model model2024-06-2022:33:02.pth\n",
      "*****LOGGING INFO IN gpt2-large_PEFT_random_init_wts_False_2024-06-20.log*********\n",
      "executing epoch:2, it took 7.762219254175822 mins from beginning of epoch till batch#2099\n",
      "training loss has decreased---> reducing the global loss from 115.23 to 111.74 | throughput = 20257 tokens/second | norm = 1.0857 | learning rate = 2.00000e-06\n",
      " epoch= 2 and  train loss is 111.74\n",
      "executing epoch:3, it took 6.382842453320821 mins from beginning of epoch till batch#2099\n",
      "training loss has decreased---> reducing the global loss from 111.74 to 110.98 | throughput = 22495 tokens/second | norm = 1.0179 | learning rate = 4.00000e-06\n",
      " epoch= 3 and  train loss is 110.98\n",
      "executing epoch:4, it took 6.384033707777659 mins from beginning of epoch till batch#2099\n",
      "training loss has decreased---> reducing the global loss from 110.98 to 109.36 | throughput = 22486 tokens/second | norm = 1.5675 | learning rate = 6.00000e-06\n",
      " epoch= 4 and  train loss is 109.36\n",
      "executing epoch:5, it took 6.386796104907989 mins from beginning of epoch till batch#2099\n",
      "training loss has decreased---> reducing the global loss from 109.36 to 106.17 | throughput = 22413 tokens/second | norm = 1.9755 | learning rate = 8.00000e-06\n",
      " epoch= 5 and  train loss is 106.17\n",
      "inside validation data for epoch 5\n",
      "Val loss has decreased -->reducing the global validation loss from 14384.63 to 13453.22\n",
      " validation loss for epoch = 5 is 13453.2154\n",
      " epoch= 5 :  val loss is 13453.2154 \n",
      "saving the model model2024-06-2023:31:05.pth\n",
      "*****LOGGING INFO IN gpt2-large_PEFT_random_init_wts_False_2024-06-20.log*********\n",
      "executing epoch:6, it took 6.374426376819611 mins from beginning of epoch till batch#2099\n",
      "training loss has decreased---> reducing the global loss from 106.17 to 99.98 | throughput = 22526 tokens/second | norm = 3.0291 | learning rate = 1.00000e-05\n",
      " epoch= 6 and  train loss is 99.98\n",
      "executing epoch:7, it took 6.386326396465302 mins from beginning of epoch till batch#2099\n",
      "training loss has decreased---> reducing the global loss from 99.98 to 91.60 | throughput = 22459 tokens/second | norm = 1.0975 | learning rate = 1.20000e-05\n",
      " epoch= 7 and  train loss is 91.60\n",
      "executing epoch:8, it took 6.380060493946075 mins from beginning of epoch till batch#2099\n",
      "training loss has decreased---> reducing the global loss from 91.60 to 86.80 | throughput = 22445 tokens/second | norm = 0.3400 | learning rate = 1.40000e-05\n",
      " epoch= 8 and  train loss is 86.80\n",
      "executing epoch:9, it took 6.384579642613729 mins from beginning of epoch till batch#2099\n",
      "training loss has decreased---> reducing the global loss from 86.80 to 84.92 | throughput = 22478 tokens/second | norm = 0.2801 | learning rate = 1.60000e-05\n",
      " epoch= 9 and  train loss is 84.92\n",
      "inside validation data for epoch 9\n",
      "Val loss has decreased -->reducing the global validation loss from 13453.22 to 11205.26\n",
      " validation loss for epoch = 9 is 11205.2608\n",
      " epoch= 9 :  val loss is 11205.2608 \n",
      "saving the model model2024-06-2100:27:42.pth\n",
      "*****LOGGING INFO IN gpt2-large_PEFT_random_init_wts_False_2024-06-21.log*********\n",
      "executing epoch:10, it took 6.392503174146016 mins from beginning of epoch till batch#2099\n",
      "training loss has decreased---> reducing the global loss from 84.92 to 83.93 | throughput = 22449 tokens/second | norm = 0.1774 | learning rate = 1.80000e-05\n",
      " epoch= 10 and  train loss is 83.93\n",
      "executing epoch:11, it took 6.4082798282305395 mins from beginning of epoch till batch#2099\n",
      "training loss has decreased---> reducing the global loss from 83.93 to 83.11 | throughput = 22460 tokens/second | norm = 0.1090 | learning rate = 2.00000e-05\n",
      " epoch= 11 and  train loss is 83.11\n",
      "executing epoch:12, it took 6.381408747037252 mins from beginning of epoch till batch#2099\n",
      "training loss has decreased---> reducing the global loss from 83.11 to 82.42 | throughput = 22458 tokens/second | norm = 0.0671 | learning rate = 1.99938e-05\n",
      " epoch= 12 and  train loss is 82.42\n",
      "executing epoch:13, it took 6.388257114092509 mins from beginning of epoch till batch#2099\n",
      "training loss has decreased---> reducing the global loss from 82.42 to 81.85 | throughput = 22491 tokens/second | norm = 0.0689 | learning rate = 1.99751e-05\n",
      " epoch= 13 and  train loss is 81.85\n",
      "inside validation data for epoch 13\n",
      "Val loss has decreased -->reducing the global validation loss from 11205.26 to 10870.88\n",
      " validation loss for epoch = 13 is 10870.8835\n",
      " epoch= 13 :  val loss is 10870.8835 \n",
      "saving the model model2024-06-2101:24:19.pth\n",
      "*****LOGGING INFO IN gpt2-large_PEFT_random_init_wts_False_2024-06-21.log*********\n",
      "executing epoch:14, it took 6.410916610558828 mins from beginning of epoch till batch#2099\n",
      "training loss has decreased---> reducing the global loss from 81.85 to 81.31 | throughput = 22464 tokens/second | norm = 0.0788 | learning rate = 1.99440e-05\n",
      " epoch= 14 and  train loss is 81.31\n",
      "executing epoch:15, it took 6.388139879703521 mins from beginning of epoch till batch#2099\n",
      "training loss has decreased---> reducing the global loss from 81.31 to 80.73 | throughput = 22408 tokens/second | norm = 0.1020 | learning rate = 1.99005e-05\n",
      " epoch= 15 and  train loss is 80.73\n",
      "executing epoch:16, it took 6.393777751922608 mins from beginning of epoch till batch#2099\n",
      "training loss has decreased---> reducing the global loss from 80.73 to 80.07 | throughput = 22457 tokens/second | norm = 0.1054 | learning rate = 1.98447e-05\n",
      " epoch= 16 and  train loss is 80.07\n",
      "executing epoch:17, it took 6.37116862932841 mins from beginning of epoch till batch#2099\n",
      "training loss has decreased---> reducing the global loss from 80.07 to 79.41 | throughput = 22503 tokens/second | norm = 0.0938 | learning rate = 1.97766e-05\n",
      " epoch= 17 and  train loss is 79.41\n",
      "inside validation data for epoch 17\n",
      "Val loss has decreased -->reducing the global validation loss from 10870.88 to 10580.15\n",
      " validation loss for epoch = 17 is 10580.1492\n",
      " epoch= 17 :  val loss is 10580.1492 \n",
      "saving the model model2024-06-2102:20:59.pth\n",
      "*****LOGGING INFO IN gpt2-large_PEFT_random_init_wts_False_2024-06-21.log*********\n",
      "executing epoch:18, it took 6.3851386706034345 mins from beginning of epoch till batch#2099\n",
      "training loss has decreased---> reducing the global loss from 79.41 to 78.83 | throughput = 22463 tokens/second | norm = 0.0634 | learning rate = 1.96963e-05\n",
      " epoch= 18 and  train loss is 78.83\n",
      "executing epoch:19, it took 6.378799871603648 mins from beginning of epoch till batch#2099\n",
      "training loss has decreased---> reducing the global loss from 78.83 to 78.34 | throughput = 22499 tokens/second | norm = 0.0772 | learning rate = 1.96039e-05\n",
      " epoch= 19 and  train loss is 78.34\n",
      "executing epoch:20, it took 6.390694959958394 mins from beginning of epoch till batch#2099\n",
      "training loss has decreased---> reducing the global loss from 78.34 to 77.93 | throughput = 22455 tokens/second | norm = 0.0666 | learning rate = 1.94996e-05\n",
      " epoch= 20 and  train loss is 77.93\n",
      "executing epoch:21, it took 6.408444158236185 mins from beginning of epoch till batch#2099\n",
      "training loss has decreased---> reducing the global loss from 77.93 to 77.58 | throughput = 22458 tokens/second | norm = 0.0522 | learning rate = 1.93834e-05\n",
      " epoch= 21 and  train loss is 77.58\n",
      "inside validation data for epoch 21\n",
      "Val loss has decreased -->reducing the global validation loss from 10580.15 to 10385.22\n",
      " validation loss for epoch = 21 is 10385.2224\n",
      " epoch= 21 :  val loss is 10385.2224 \n",
      "saving the model model2024-06-2103:17:38.pth\n",
      "*****LOGGING INFO IN gpt2-large_PEFT_random_init_wts_False_2024-06-21.log*********\n",
      "executing epoch:22, it took 6.389658367633819 mins from beginning of epoch till batch#2099\n",
      "training loss has decreased---> reducing the global loss from 77.58 to 77.27 | throughput = 22420 tokens/second | norm = 0.0593 | learning rate = 1.92556e-05\n",
      " epoch= 22 and  train loss is 77.27\n",
      "executing epoch:23, it took 6.3830245931943255 mins from beginning of epoch till batch#2099\n",
      "training loss has decreased---> reducing the global loss from 77.27 to 77.01 | throughput = 22474 tokens/second | norm = 0.0654 | learning rate = 1.91162e-05\n",
      " epoch= 23 and  train loss is 77.01\n",
      "executing epoch:24, it took 6.384485638141632 mins from beginning of epoch till batch#2099\n",
      "training loss has decreased---> reducing the global loss from 77.01 to 76.77 | throughput = 22468 tokens/second | norm = 0.0618 | learning rate = 1.89655e-05\n",
      " epoch= 24 and  train loss is 76.77\n",
      "executing epoch:25, it took 6.410970028241476 mins from beginning of epoch till batch#2099\n",
      "training loss has decreased---> reducing the global loss from 76.77 to 76.55 | throughput = 22410 tokens/second | norm = 0.0560 | learning rate = 1.88036e-05\n",
      " epoch= 25 and  train loss is 76.55\n",
      "inside validation data for epoch 25\n",
      "Val loss has decreased -->reducing the global validation loss from 10385.22 to 10272.56\n",
      " validation loss for epoch = 25 is 10272.5585\n",
      " epoch= 25 :  val loss is 10272.5585 \n",
      "saving the model model2024-06-2104:14:20.pth\n",
      "*****LOGGING INFO IN gpt2-large_PEFT_random_init_wts_False_2024-06-21.log*********\n",
      "executing epoch:26, it took 6.358841435114543 mins from beginning of epoch till batch#2099\n",
      "training loss has decreased---> reducing the global loss from 76.55 to 76.34 | throughput = 22481 tokens/second | norm = 0.0611 | learning rate = 1.86307e-05\n",
      " epoch= 26 and  train loss is 76.34\n",
      "executing epoch:27, it took 6.385605331261953 mins from beginning of epoch till batch#2099\n",
      "training loss has decreased---> reducing the global loss from 76.34 to 76.11 | throughput = 22467 tokens/second | norm = 0.0537 | learning rate = 1.84471e-05\n",
      " epoch= 27 and  train loss is 76.11\n",
      "executing epoch:28, it took 6.41360510190328 mins from beginning of epoch till batch#2099\n",
      "training loss has decreased---> reducing the global loss from 76.11 to 75.94 | throughput = 22460 tokens/second | norm = 0.0568 | learning rate = 1.82529e-05\n",
      " epoch= 28 and  train loss is 75.94\n",
      "executing epoch:29, it took 6.3973435441652935 mins from beginning of epoch till batch#2099\n",
      "training loss has decreased---> reducing the global loss from 75.94 to 75.79 | throughput = 22391 tokens/second | norm = 0.1215 | learning rate = 1.80485e-05\n",
      " epoch= 29 and  train loss is 75.79\n",
      "inside validation data for epoch 29\n",
      "Val loss has decreased -->reducing the global validation loss from 10272.56 to 10188.69\n",
      " validation loss for epoch = 29 is 10188.6909\n",
      " epoch= 29 :  val loss is 10188.6909 \n",
      "saving the model model2024-06-2105:11:02.pth\n",
      "*****LOGGING INFO IN gpt2-large_PEFT_random_init_wts_False_2024-06-21.log*********\n",
      "executing epoch:30, it took 6.38297092517217 mins from beginning of epoch till batch#2099\n",
      "training loss has decreased---> reducing the global loss from 75.79 to 75.67 | throughput = 22463 tokens/second | norm = 0.0679 | learning rate = 1.78340e-05\n",
      " epoch= 30 and  train loss is 75.67\n",
      "executing epoch:31, it took 6.395768336455027 mins from beginning of epoch till batch#2099\n",
      "training loss has decreased---> reducing the global loss from 75.67 to 75.55 | throughput = 22449 tokens/second | norm = 0.0590 | learning rate = 1.76098e-05\n",
      " epoch= 31 and  train loss is 75.55\n",
      "executing epoch:32, it took 6.385292180379232 mins from beginning of epoch till batch#2099\n",
      "training loss has decreased---> reducing the global loss from 75.55 to 75.44 | throughput = 22427 tokens/second | norm = 0.0577 | learning rate = 1.73761e-05\n",
      " epoch= 32 and  train loss is 75.44\n",
      "executing epoch:33, it took 6.395444039503733 mins from beginning of epoch till batch#2099\n",
      "training loss has decreased---> reducing the global loss from 75.44 to 75.34 | throughput = 22442 tokens/second | norm = 0.0535 | learning rate = 1.71332e-05\n",
      " epoch= 33 and  train loss is 75.34\n",
      "inside validation data for epoch 33\n",
      "Val loss has decreased -->reducing the global validation loss from 10188.69 to 10138.61\n",
      " validation loss for epoch = 33 is 10138.6135\n",
      " epoch= 33 :  val loss is 10138.6135 \n",
      "saving the model model2024-06-2106:07:43.pth\n",
      "*****LOGGING INFO IN gpt2-large_PEFT_random_init_wts_False_2024-06-21.log*********\n",
      "executing epoch:34, it took 6.391419883569082 mins from beginning of epoch till batch#2099\n",
      "training loss has decreased---> reducing the global loss from 75.34 to 75.24 | throughput = 22481 tokens/second | norm = 0.0487 | learning rate = 1.68814e-05\n",
      " epoch= 34 and  train loss is 75.24\n",
      "executing epoch:35, it took 6.413794294993083 mins from beginning of epoch till batch#2099\n",
      "training loss has decreased---> reducing the global loss from 75.24 to 75.16 | throughput = 22410 tokens/second | norm = 0.0642 | learning rate = 1.66211e-05\n",
      " epoch= 35 and  train loss is 75.16\n",
      "executing epoch:36, it took 6.381332544485728 mins from beginning of epoch till batch#2099\n",
      "training loss has decreased---> reducing the global loss from 75.16 to 75.09 | throughput = 22464 tokens/second | norm = 0.0539 | learning rate = 1.63525e-05\n",
      " epoch= 36 and  train loss is 75.09\n",
      "executing epoch:37, it took 6.389786148071289 mins from beginning of epoch till batch#2099\n",
      "training loss has decreased---> reducing the global loss from 75.09 to 75.03 | throughput = 22449 tokens/second | norm = 0.0703 | learning rate = 1.60759e-05\n",
      " epoch= 37 and  train loss is 75.03\n",
      "inside validation data for epoch 37\n",
      "Val loss has decreased -->reducing the global validation loss from 10138.61 to 10107.21\n",
      " validation loss for epoch = 37 is 10107.2103\n",
      " epoch= 37 :  val loss is 10107.2103 \n",
      "saving the model model2024-06-2107:04:24.pth\n",
      "*****LOGGING INFO IN gpt2-large_PEFT_random_init_wts_False_2024-06-21.log*********\n",
      "executing epoch:38, it took 6.399232184886932 mins from beginning of epoch till batch#2099\n",
      "training loss has decreased---> reducing the global loss from 75.03 to 74.97 | throughput = 22423 tokens/second | norm = 0.0675 | learning rate = 1.57919e-05\n",
      " epoch= 38 and  train loss is 74.97\n",
      "executing epoch:39, it took 6.387648852666219 mins from beginning of epoch till batch#2099\n",
      "training loss has decreased---> reducing the global loss from 74.97 to 74.91 | throughput = 22469 tokens/second | norm = 0.0641 | learning rate = 1.55006e-05\n",
      " epoch= 39 and  train loss is 74.91\n",
      "executing epoch:40, it took 6.389280283451081 mins from beginning of epoch till batch#2099\n",
      "training loss has decreased---> reducing the global loss from 74.91 to 74.86 | throughput = 22460 tokens/second | norm = 0.0572 | learning rate = 1.52024e-05\n",
      " epoch= 40 and  train loss is 74.86\n",
      "executing epoch:41, it took 6.392263464132944 mins from beginning of epoch till batch#2099\n",
      "training loss has decreased---> reducing the global loss from 74.86 to 74.81 | throughput = 22420 tokens/second | norm = 0.0689 | learning rate = 1.48978e-05\n",
      " epoch= 41 and  train loss is 74.81\n",
      "inside validation data for epoch 41\n",
      "Val loss has decreased -->reducing the global validation loss from 10107.21 to 10085.68\n",
      " validation loss for epoch = 41 is 10085.6833\n",
      " epoch= 41 :  val loss is 10085.6833 \n",
      "saving the model model2024-06-2108:01:06.pth\n",
      "*****LOGGING INFO IN gpt2-large_PEFT_random_init_wts_False_2024-06-21.log*********\n",
      "executing epoch:42, it took 6.402653606732686 mins from beginning of epoch till batch#2099\n",
      "training loss has decreased---> reducing the global loss from 74.81 to 74.77 | throughput = 22416 tokens/second | norm = 0.0581 | learning rate = 1.45870e-05\n",
      " epoch= 42 and  train loss is 74.77\n",
      "executing epoch:43, it took 6.396588679154714 mins from beginning of epoch till batch#2099\n",
      "training loss has decreased---> reducing the global loss from 74.77 to 74.73 | throughput = 22439 tokens/second | norm = 0.0687 | learning rate = 1.42706e-05\n",
      " epoch= 43 and  train loss is 74.73\n",
      "executing epoch:44, it took 6.387324484189351 mins from beginning of epoch till batch#2099\n",
      "training loss has decreased---> reducing the global loss from 74.73 to 74.69 | throughput = 22414 tokens/second | norm = 0.0626 | learning rate = 1.39488e-05\n",
      " epoch= 44 and  train loss is 74.69\n",
      "executing epoch:45, it took 6.369529342651367 mins from beginning of epoch till batch#2099\n",
      "training loss has decreased---> reducing the global loss from 74.69 to 74.65 | throughput = 22428 tokens/second | norm = 0.0751 | learning rate = 1.36221e-05\n",
      " epoch= 45 and  train loss is 74.65\n",
      "inside validation data for epoch 45\n",
      "Val loss has decreased -->reducing the global validation loss from 10085.68 to 10069.39\n",
      " validation loss for epoch = 45 is 10069.3930\n",
      " epoch= 45 :  val loss is 10069.3930 \n",
      "saving the model model2024-06-2108:57:50.pth\n",
      "*****LOGGING INFO IN gpt2-large_PEFT_random_init_wts_False_2024-06-21.log*********\n",
      "executing epoch:46, it took 6.397268052895864 mins from beginning of epoch till batch#2099\n",
      "training loss has decreased---> reducing the global loss from 74.65 to 74.62 | throughput = 22414 tokens/second | norm = 0.0615 | learning rate = 1.32909e-05\n",
      " epoch= 46 and  train loss is 74.62\n",
      "executing epoch:47, it took 6.3998509526252745 mins from beginning of epoch till batch#2099\n",
      "training loss has decreased---> reducing the global loss from 74.62 to 74.59 | throughput = 22422 tokens/second | norm = 0.0682 | learning rate = 1.29556e-05\n",
      " epoch= 47 and  train loss is 74.59\n",
      "executing epoch:48, it took 6.425050044059754 mins from beginning of epoch till batch#2099\n",
      "training loss has decreased---> reducing the global loss from 74.59 to 74.56 | throughput = 22397 tokens/second | norm = 0.0585 | learning rate = 1.26166e-05\n",
      " epoch= 48 and  train loss is 74.56\n",
      "executing epoch:49, it took 6.385566890239716 mins from beginning of epoch till batch#2099\n",
      "training loss has decreased---> reducing the global loss from 74.56 to 74.53 | throughput = 22435 tokens/second | norm = 0.0686 | learning rate = 1.22743e-05\n",
      " epoch= 49 and  train loss is 74.53\n",
      "inside validation data for epoch 49\n",
      "Val loss has decreased -->reducing the global validation loss from 10069.39 to 10057.32\n",
      " validation loss for epoch = 49 is 10057.3221\n",
      " epoch= 49 :  val loss is 10057.3221 \n",
      "saving the model model2024-06-2109:54:35.pth\n",
      "*****LOGGING INFO IN gpt2-large_PEFT_random_init_wts_False_2024-06-21.log*********\n",
      "executing epoch:50, it took 6.39285268386205 mins from beginning of epoch till batch#2099\n",
      "training loss has decreased---> reducing the global loss from 74.53 to 74.50 | throughput = 22468 tokens/second | norm = 0.0947 | learning rate = 1.19293e-05\n",
      " epoch= 50 and  train loss is 74.50\n",
      "executing epoch:51, it took 6.419100590546926 mins from beginning of epoch till batch#2099\n",
      "training loss has decreased---> reducing the global loss from 74.50 to 74.47 | throughput = 22386 tokens/second | norm = 0.0939 | learning rate = 1.15818e-05\n",
      " epoch= 51 and  train loss is 74.47\n",
      "executing epoch:52, it took 6.365698111057282 mins from beginning of epoch till batch#2099\n",
      "training loss has decreased---> reducing the global loss from 74.47 to 74.45 | throughput = 22443 tokens/second | norm = 0.0695 | learning rate = 1.12323e-05\n",
      " epoch= 52 and  train loss is 74.45\n",
      "executing epoch:53, it took 6.389206369717916 mins from beginning of epoch till batch#2099\n",
      "training loss has decreased---> reducing the global loss from 74.45 to 74.43 | throughput = 22467 tokens/second | norm = 0.0704 | learning rate = 1.08813e-05\n",
      " epoch= 53 and  train loss is 74.43\n",
      "inside validation data for epoch 53\n",
      "Val loss has decreased -->reducing the global validation loss from 10057.32 to 10047.42\n",
      " validation loss for epoch = 53 is 10047.4164\n",
      " epoch= 53 :  val loss is 10047.4164 \n",
      "saving the model model2024-06-2110:51:17.pth\n",
      "*****LOGGING INFO IN gpt2-large_PEFT_random_init_wts_False_2024-06-21.log*********\n",
      "executing epoch:54, it took 6.3977503657341 mins from beginning of epoch till batch#2099\n",
      "training loss has decreased---> reducing the global loss from 74.43 to 74.40 | throughput = 22429 tokens/second | norm = 0.0645 | learning rate = 1.05292e-05\n",
      " epoch= 54 and  train loss is 74.40\n",
      "executing epoch:55, it took 6.399961098035177 mins from beginning of epoch till batch#2099\n",
      "training loss has decreased---> reducing the global loss from 74.40 to 74.38 | throughput = 22390 tokens/second | norm = 0.0784 | learning rate = 1.01765e-05\n",
      " epoch= 55 and  train loss is 74.38\n",
      "executing epoch:56, it took 6.400874360402425 mins from beginning of epoch till batch#2099\n",
      "training loss has decreased---> reducing the global loss from 74.38 to 74.36 | throughput = 22447 tokens/second | norm = 0.0780 | learning rate = 9.82352e-06\n",
      " epoch= 56 and  train loss is 74.36\n",
      "executing epoch:57, it took 6.394241245587667 mins from beginning of epoch till batch#2099\n",
      "training loss has decreased---> reducing the global loss from 74.36 to 74.34 | throughput = 22434 tokens/second | norm = 0.0794 | learning rate = 9.47077e-06\n",
      " epoch= 57 and  train loss is 74.34\n",
      "inside validation data for epoch 57\n",
      "Val loss has decreased -->reducing the global validation loss from 10047.42 to 10039.24\n",
      " validation loss for epoch = 57 is 10039.2438\n",
      " epoch= 57 :  val loss is 10039.2438 \n",
      "saving the model model2024-06-2111:48:02.pth\n",
      "*****LOGGING INFO IN gpt2-large_PEFT_random_init_wts_False_2024-06-21.log*********\n",
      "executing epoch:58, it took 6.397839037577311 mins from beginning of epoch till batch#2099\n",
      "training loss has decreased---> reducing the global loss from 74.34 to 74.32 | throughput = 22393 tokens/second | norm = 0.0738 | learning rate = 9.11868e-06\n",
      " epoch= 58 and  train loss is 74.32\n",
      "executing epoch:59, it took 6.367770783106486 mins from beginning of epoch till batch#2099\n",
      "training loss has decreased---> reducing the global loss from 74.32 to 74.30 | throughput = 22442 tokens/second | norm = 0.1686 | learning rate = 8.76768e-06\n",
      " epoch= 59 and  train loss is 74.30\n",
      "executing epoch:60, it took 6.397570685545603 mins from beginning of epoch till batch#2099\n",
      "training loss has decreased---> reducing the global loss from 74.30 to 74.29 | throughput = 22437 tokens/second | norm = 0.0743 | learning rate = 8.41823e-06\n",
      " epoch= 60 and  train loss is 74.29\n",
      "executing epoch:61, it took 6.42100023428599 mins from beginning of epoch till batch#2099\n",
      "training loss has decreased---> reducing the global loss from 74.29 to 74.27 | throughput = 22421 tokens/second | norm = 0.0851 | learning rate = 8.07074e-06\n",
      " epoch= 61 and  train loss is 74.27\n",
      "inside validation data for epoch 61\n",
      "Val loss has decreased -->reducing the global validation loss from 10039.24 to 10032.93\n",
      " validation loss for epoch = 61 is 10032.9272\n",
      " epoch= 61 :  val loss is 10032.9272 \n",
      "saving the model model2024-06-2112:44:48.pth\n",
      "*****LOGGING INFO IN gpt2-large_PEFT_random_init_wts_False_2024-06-21.log*********\n",
      "executing epoch:62, it took 6.400636263688406 mins from beginning of epoch till batch#2099\n",
      "training loss has decreased---> reducing the global loss from 74.27 to 74.26 | throughput = 22404 tokens/second | norm = 0.0932 | learning rate = 7.72566e-06\n",
      " epoch= 62 and  train loss is 74.26\n",
      "executing epoch:63, it took 6.396576333045959 mins from beginning of epoch till batch#2099\n",
      "training loss has decreased---> reducing the global loss from 74.26 to 74.24 | throughput = 22439 tokens/second | norm = 0.0677 | learning rate = 7.38341e-06\n",
      " epoch= 63 and  train loss is 74.24\n",
      "executing epoch:64, it took 6.385190773010254 mins from beginning of epoch till batch#2099\n",
      "training loss has decreased---> reducing the global loss from 74.24 to 74.23 | throughput = 22458 tokens/second | norm = 0.0835 | learning rate = 7.04442e-06\n",
      " epoch= 64 and  train loss is 74.23\n",
      "executing epoch:65, it took 6.4188319404919945 mins from beginning of epoch till batch#2099\n",
      "training loss has decreased---> reducing the global loss from 74.23 to 74.22 | throughput = 22424 tokens/second | norm = 0.0829 | learning rate = 6.70911e-06\n",
      " epoch= 65 and  train loss is 74.22\n",
      "inside validation data for epoch 65\n",
      "Val loss has decreased -->reducing the global validation loss from 10032.93 to 10027.57\n",
      " validation loss for epoch = 65 is 10027.5683\n",
      " epoch= 65 :  val loss is 10027.5683 \n",
      "saving the model model2024-06-2113:41:32.pth\n",
      "*****LOGGING INFO IN gpt2-large_PEFT_random_init_wts_False_2024-06-21.log*********\n",
      "executing epoch:66, it took 6.363577779134115 mins from beginning of epoch till batch#2099\n",
      "training loss has decreased---> reducing the global loss from 74.22 to 74.20 | throughput = 22443 tokens/second | norm = 0.0723 | learning rate = 6.37790e-06\n",
      " epoch= 66 and  train loss is 74.20\n",
      "executing epoch:67, it took 6.406067430973053 mins from beginning of epoch till batch#2099\n",
      "training loss has decreased---> reducing the global loss from 74.20 to 74.19 | throughput = 22406 tokens/second | norm = 0.0704 | learning rate = 6.05121e-06\n",
      " epoch= 67 and  train loss is 74.19\n",
      "executing epoch:68, it took 6.393825892607371 mins from beginning of epoch till batch#2099\n"
     ]
    }
   ],
   "source": [
    "tr_model = train_model(train_loader, val_loader, model =  model,tokenizer = tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0a4261",
   "metadata": {},
   "source": [
    "## Use this section if you want a model with Random weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872e4ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig\n",
    "# Define the model name\n",
    "model_name = \"distilgpt2\"\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# Load the model configuration\n",
    "config = AutoConfig.from_pretrained(model_name)\n",
    "\n",
    "# Initialize the model with random weights\n",
    "model = AutoModelForCausalLM.from_config(config)\n",
    "\n",
    "# Check the model\n",
    "#print(model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9955dfe8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24dc5fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(df_train.iloc[178]['input_ids'], dtype = torch.long)\n",
    "att = torch.tensor(df_train.iloc[178]['attention_mask'], dtype = torch.long)\n",
    "random_out = model(input_ids = x.view(B,T) , attention_mask = att.view(B,T), labels =   x.view(B,T) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aeafe71",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498b4417",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba55e5bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872b6804",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45273273",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ce20cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329bcbe7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f81b0ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a43d89c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f2e5eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f159692",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6607b259",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
