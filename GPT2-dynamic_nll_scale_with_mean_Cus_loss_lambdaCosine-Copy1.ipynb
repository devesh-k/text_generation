{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "adcf999b",
   "metadata": {},
   "source": [
    "## In this notebook, we scale the nll with the mean of the nll of the previous epoch and scale the cosine similaity with (1-lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ec6e29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2c7c41f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: datasets in /opt/conda/lib/python3.8/site-packages (2.20.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.8/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: fsspec[http]<=2024.5.0,>=2023.1.0 in /opt/conda/lib/python3.8/site-packages (from datasets) (2024.5.0)\n",
      "Requirement already satisfied: requests>=2.32.2 in /opt/conda/lib/python3.8/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.8/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.8/site-packages (from datasets) (6.0)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /opt/conda/lib/python3.8/site-packages (from datasets) (4.66.4)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.8/site-packages (from datasets) (3.9.5)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.8/site-packages (from datasets) (16.1.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.2 in /opt/conda/lib/python3.8/site-packages (from datasets) (0.23.4)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.8/site-packages (from datasets) (1.3.4)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.8/site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.8/site-packages (from datasets) (1.21.4)\n",
      "Requirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.8/site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.8/site-packages (from datasets) (21.3)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.8/site-packages (from datasets) (3.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets) (21.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.8/site-packages (from huggingface-hub>=0.21.2->datasets) (4.12.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging->datasets) (3.0.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.8/site-packages (from requests>=2.32.2->datasets) (2.0.8)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests>=2.32.2->datasets) (1.26.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests>=2.32.2->datasets) (3.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests>=2.32.2->datasets) (2021.10.8)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.8/site-packages (from pandas->datasets) (2021.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.8/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: jupyter in /opt/conda/lib/python3.8/site-packages (1.0.0)\n",
      "Requirement already satisfied: ipywidgets in /opt/conda/lib/python3.8/site-packages (8.1.3)\n",
      "Requirement already satisfied: qtconsole in /opt/conda/lib/python3.8/site-packages (from jupyter) (5.5.2)\n",
      "Requirement already satisfied: ipykernel in /opt/conda/lib/python3.8/site-packages (from jupyter) (6.29.5)\n",
      "Requirement already satisfied: nbconvert in /opt/conda/lib/python3.8/site-packages (from jupyter) (6.3.0)\n",
      "Requirement already satisfied: jupyter-console in /opt/conda/lib/python3.8/site-packages (from jupyter) (6.6.3)\n",
      "Requirement already satisfied: notebook in /opt/conda/lib/python3.8/site-packages (from jupyter) (6.4.1)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (7.30.0)\n",
      "Requirement already satisfied: comm>=0.1.3 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.11 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (3.0.11)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.11 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (4.0.11)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (5.14.3)\n",
      "Requirement already satisfied: pygments in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (2.10.0)\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (4.8.0)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.47)\n",
      "Requirement already satisfied: matplotlib-inline in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.3)\n",
      "Requirement already satisfied: pickleshare in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.7.5)\n",
      "Requirement already satisfied: decorator in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (5.1.0)\n",
      "Requirement already satisfied: jedi>=0.16 in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.18.1)\n",
      "Requirement already satisfied: backcall in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: setuptools>=18.5 in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (59.4.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /opt/conda/lib/python3.8/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.8/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.8/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=6.1.0->ipywidgets) (0.2.5)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /opt/conda/lib/python3.8/site-packages (from ipykernel->jupyter) (5.7.2)\n",
      "Requirement already satisfied: tornado>=6.1 in /opt/conda/lib/python3.8/site-packages (from ipykernel->jupyter) (6.1)\n",
      "Requirement already satisfied: pyzmq>=24 in /opt/conda/lib/python3.8/site-packages (from ipykernel->jupyter) (26.0.3)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.8/site-packages (from ipykernel->jupyter) (5.8.0)\n",
      "Requirement already satisfied: nest-asyncio in /opt/conda/lib/python3.8/site-packages (from ipykernel->jupyter) (1.5.4)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.8/site-packages (from ipykernel->jupyter) (21.3)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in /opt/conda/lib/python3.8/site-packages (from ipykernel->jupyter) (7.1.0)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in /opt/conda/lib/python3.8/site-packages (from ipykernel->jupyter) (1.8.2)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /opt/conda/lib/python3.8/site-packages (from jupyter-client>=6.1.12->ipykernel->jupyter) (2.8.2)\n",
      "Requirement already satisfied: entrypoints in /opt/conda/lib/python3.8/site-packages (from jupyter-client>=6.1.12->ipykernel->jupyter) (0.3)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /opt/conda/lib/python3.8/site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel->jupyter) (4.2.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.8/site-packages (from python-dateutil>=2.1->jupyter-client>=6.1.12->ipykernel->jupyter) (1.16.0)\n",
      "Requirement already satisfied: jinja2>=2.4 in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (3.0.3)\n",
      "Requirement already satisfied: nbformat>=4.4 in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (5.1.3)\n",
      "Requirement already satisfied: bleach in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (4.1.0)\n",
      "Requirement already satisfied: defusedxml in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (0.7.1)\n",
      "Requirement already satisfied: testpath in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (0.5.0)\n",
      "Requirement already satisfied: jupyterlab-pygments in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (0.1.2)\n",
      "Requirement already satisfied: mistune<2,>=0.8.1 in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (0.8.4)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (1.5.0)\n",
      "Requirement already satisfied: nbclient<0.6.0,>=0.5.0 in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (0.5.9)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.8/site-packages (from jinja2>=2.4->nbconvert->jupyter) (2.0.1)\n",
      "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /opt/conda/lib/python3.8/site-packages (from nbformat>=4.4->nbconvert->jupyter) (4.2.1)\n",
      "Requirement already satisfied: ipython-genutils in /opt/conda/lib/python3.8/site-packages (from nbformat>=4.4->nbconvert->jupyter) (0.2.0)\n",
      "Requirement already satisfied: importlib-resources>=1.4.0 in /opt/conda/lib/python3.8/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.4->nbconvert->jupyter) (5.4.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /opt/conda/lib/python3.8/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.4->nbconvert->jupyter) (21.2.0)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /opt/conda/lib/python3.8/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.4->nbconvert->jupyter) (0.18.0)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /opt/conda/lib/python3.8/site-packages (from importlib-resources>=1.4.0->jsonschema!=2.5.0,>=2.4->nbformat>=4.4->nbconvert->jupyter) (3.6.0)\n",
      "Requirement already satisfied: webencodings in /opt/conda/lib/python3.8/site-packages (from bleach->nbconvert->jupyter) (0.5.1)\n",
      "Requirement already satisfied: terminado>=0.8.3 in /opt/conda/lib/python3.8/site-packages (from notebook->jupyter) (0.12.1)\n",
      "Requirement already satisfied: prometheus-client in /opt/conda/lib/python3.8/site-packages (from notebook->jupyter) (0.12.0)\n",
      "Requirement already satisfied: argon2-cffi in /opt/conda/lib/python3.8/site-packages (from notebook->jupyter) (21.1.0)\n",
      "Requirement already satisfied: Send2Trash>=1.5.0 in /opt/conda/lib/python3.8/site-packages (from notebook->jupyter) (1.8.0)\n",
      "Requirement already satisfied: cffi>=1.0.0 in /opt/conda/lib/python3.8/site-packages (from argon2-cffi->notebook->jupyter) (1.15.0)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.8/site-packages (from cffi>=1.0.0->argon2-cffi->notebook->jupyter) (2.21)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging->ipykernel->jupyter) (3.0.6)\n",
      "Requirement already satisfied: qtpy>=2.4.0 in /opt/conda/lib/python3.8/site-packages (from qtconsole->jupyter) (2.4.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#installing some libraries\n",
    "!pip install datasets\n",
    "!pip install --upgrade jupyter ipywidgets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14cb5577",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch, transformers\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import PreTrainedModel, PretrainedConfig\n",
    "from transformers import AutoModel, AutoConfig,AutoModelForCausalLM, AutoConfig\n",
    "\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "#from datasets import load_dataset\n",
    "import pandas as pd, numpy as np\n",
    "from torch import cuda\n",
    "import datetime\n",
    "import warnings,itertools\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import json,math\n",
    "pre_train = False\n",
    "\n",
    "# Ignore all warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "#pip install transformers bitsandbytes>=0.39.0 -q\n",
    "import zipfile,logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "396fb8b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "#global params for training\n",
    "\n",
    "B,T = 32,1024\n",
    "epoch = 100\n",
    "# this controls whether we are using pre-trained wts or not\n",
    "random_init_wts = False\n",
    "if cuda.is_available():\n",
    "    device = torch.device('cuda:0')\n",
    "    print(device)\n",
    "else:\n",
    "    device = 'cpu'\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "# these 2 global vars help track the training and val loss\n",
    "global_tr_loss = torch.inf\n",
    "global_val_loss = torch.inf\n",
    "#directory to save wts\n",
    "model_path = os.path.join(\"model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3323a525",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./data/unzip_text_10M'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "directory = os.path.join('.','data','unzip_text_10M')  # Replace with your directory path\n",
    "directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "566fb8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_text(directory):\n",
    "    \"\"\"This function loads the dataset,provided in the zipped format and stores all the text files in list\n",
    "    each file has its contents stored as an item in list and at the end we concatenate all the sub-lists to create a flattened list and return it\"\"\"\n",
    "    directory = os.path.join('.','data','unzip_text_10M',str(directory))  # Replace with your directory path\n",
    "    print(f\"directory :{directory}\")\n",
    "    # List all files in the directory\n",
    "    files = [f for f in os.listdir(directory) if os.path.isfile(os.path.join(directory, f))]\n",
    "    print(f\"files:{files}\")\n",
    "    text_content = []\n",
    "    # Read each file\n",
    "    total_lines = 0\n",
    "    for filenum,filename in enumerate(files):\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            text = file.read()\n",
    "            text_content.append(text)\n",
    "            print(f\"the file:{filename} has been appeneded to the uber list and its length is {len(text_content)} \")\n",
    "            \n",
    "    \n",
    "    flattened_list = ''.join(text_content)\n",
    "    assert (len(flattened_list) == total_lines , f\"Expected {len(flattened_list)} to be equal to {total_lines}\" )\n",
    "    \n",
    "    return flattened_list\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed36ad3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "directory :./data/unzip_text_10M/train_10M\n",
      "files:['switchboard.train', 'simple_wiki.train', 'open_subtitles.train', 'gutenberg.train', 'childes.train', 'bnc_spoken.train']\n",
      "the file:switchboard.train has been appeneded to the uber list and its length is 1 \n",
      "the file:simple_wiki.train has been appeneded to the uber list and its length is 2 \n",
      "the file:open_subtitles.train has been appeneded to the uber list and its length is 3 \n",
      "the file:gutenberg.train has been appeneded to the uber list and its length is 4 \n",
      "the file:childes.train has been appeneded to the uber list and its length is 5 \n",
      "the file:bnc_spoken.train has been appeneded to the uber list and its length is 6 \n"
     ]
    }
   ],
   "source": [
    "#read the dataset and get a list\n",
    "train_list = read_text(\"train_10M\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03ddd705",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54215049"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "55274473",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1654\n"
     ]
    }
   ],
   "source": [
    "chunks = len(train_list)//(B*T)\n",
    "print(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d3f69e05",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (17191971 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "#tokeinze the training data\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\",return_tensors = \"pt\" , truncate = True, return_overflowing_tokens=True , padding = False,)\n",
    "enc_train = tokenizer(train_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "70be4209",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.1535097982657136"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comp_ratio = len(train_list)/len(enc_train['input_ids'])\n",
    "comp_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4c86468b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "directory :./data/unzip_text_10M/dev\n",
      "files:['switchboard.dev', 'simple_wiki.dev', 'open_subtitles.dev', 'gutenberg.dev', 'childes.dev', 'bnc_spoken.dev']\n",
      "the file:switchboard.dev has been appeneded to the uber list and its length is 1 \n",
      "the file:simple_wiki.dev has been appeneded to the uber list and its length is 2 \n",
      "the file:open_subtitles.dev has been appeneded to the uber list and its length is 3 \n",
      "the file:gutenberg.dev has been appeneded to the uber list and its length is 4 \n",
      "the file:childes.dev has been appeneded to the uber list and its length is 5 \n",
      "the file:bnc_spoken.dev has been appeneded to the uber list and its length is 6 \n"
     ]
    }
   ],
   "source": [
    "#read validation data and tokenize\n",
    "val_list = read_text(\"dev\")\n",
    "enc_val = tokenizer(val_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e19ae0",
   "metadata": {},
   "source": [
    "### we read the tokenize data and store the input_ids and attention mask as a single long list at run time we reshape the single [1,B*T] tensor into a batched tensor of dimension [B,T].\n",
    "This approach turns out to be more efficient as it removes the need for any extra tokens in the form of padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eb0453da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df_from_list(enc , B= B, T = T, val= False):\n",
    "    \"\"\"This function takes the tokenized list and reated a dataframe where each row contains the input_ids and attention_mask from the tokenizer.\n",
    "    each row of the dataframe is a list with items B*T\"\"\"\n",
    "    chunk_size = B*T\n",
    "    if val:\n",
    "        chunk_size = 2*B*T\n",
    "        \n",
    "    long_list_inp = enc['input_ids']\n",
    "    long_list_attention = enc['attention_mask']\n",
    "\n",
    "    # Step 3: Split the list into chunks and pad the last chunk if necessary\n",
    "    chunks_inp = [long_list_inp[i:i + chunk_size] for i in range(0, len(long_list_inp), chunk_size)]\n",
    "    chunks_att = [long_list_attention[i:i + chunk_size] for i in range(0, len(long_list_attention), chunk_size)]\n",
    "    df = pd.DataFrame({'input_ids': chunks_inp,'attention_mask':chunks_att})\n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9daea8ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the dataframe is = 525\n"
     ]
    }
   ],
   "source": [
    "df_train_temp = get_df_from_list(enc_train)\n",
    "# Display the DataFrame\n",
    "df_train_temp.head()\n",
    "print(f\"Length of the dataframe is = {len(df_train_temp)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b753fd65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the VALDATION dataframe is = 532\n"
     ]
    }
   ],
   "source": [
    "df_val_temp = get_df_from_list(enc_val)\n",
    "# Display the DataFrame\n",
    "df_val_temp.head()\n",
    "print(f\"Length of the VALDATION dataframe is = {len(df_val_temp)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a4ad5398",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_df(df,eos_char = tokenizer.eos_token_id,B = B, T = T,val = False):\n",
    "    \"\"\"The 2 functions below makes sure that each row of the dataframe is of equal length and if not it adds the eos token to make it B*T\"\"\"\n",
    "    if val:\n",
    "        B = 2*B\n",
    "    for ind,row in df.iterrows():\n",
    "        if len(row['input_ids']) != B*T :\n",
    "            print(f\"row = {ind} and input_id length = {len(row['input_ids'])}\")\n",
    "            print(f\"row = {ind} and attention length = {len(row['attention_mask'])}\")\n",
    "            pad_len = B*T - len(row['input_ids'])\n",
    "            print(f\"padding the row index {ind} with {pad_len} character\")\n",
    "            row['input_ids'] = row['input_ids']+ [eos_char] * pad_len\n",
    "            #attention mask should be padded to 0\n",
    "            row['attention_mask'] = row['attention_mask']+ [0] * pad_len\n",
    "            print(\"#### POST CONCAT####\")\n",
    "            print(f\"row = {ind} and input_id length = {len(row['input_ids'])}\")\n",
    "            print(f\"row = {ind} and attention length ={len(row['attention_mask'])}\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def verify_len(df,val = False, B= B):\n",
    "    row_ind = []\n",
    "    if val:\n",
    "        B = 2*B\n",
    "    for ind,row in df.iterrows():\n",
    "        if len(row['input_ids']) != B*T :\n",
    "            row_ind.append(ind)\n",
    "        else:\n",
    "            continue\n",
    "    if len(row_ind) !=0:\n",
    "        print(\"CONCATENATION Did not work\")\n",
    "    else:\n",
    "        print(\"CONCATENATION worked\")\n",
    "        \n",
    "            \n",
    "        \n",
    "    \n",
    "        \n",
    "        \n",
    "   \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d077b2c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row = 524 and input_id length = 21539\n",
      "row = 524 and attention length = 21539\n",
      "padding the row index 524 with 11229 character\n",
      "#### POST CONCAT####\n",
      "row = 524 and input_id length = 32768\n",
      "row = 524 and attention length =32768\n",
      "CONCATENATION worked\n"
     ]
    }
   ],
   "source": [
    "df_train  = pad_df(df_train_temp)\n",
    "verify_len(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "488063cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row = 531 and input_id length = 13588\n",
      "row = 531 and attention length = 13588\n",
      "padding the row index 531 with 19180 character\n",
      "#### POST CONCAT####\n",
      "row = 531 and input_id length = 32768\n",
      "row = 531 and attention length =32768\n",
      "CONCATENATION worked\n"
     ]
    }
   ],
   "source": [
    "df_val  = pad_df(df_val_temp)\n",
    "verify_len(df_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e60057ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_ids</th>\n",
       "      <th>attention_mask</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[32, 25, 197, 40, 1101, 1654, 484, 389, 13, 19...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[6510, 11, 14104, 290, 27913, 13, 198, 32, 25,...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[314, 1612, 11, 340, 338, 1611, 286, 43244, 11...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[318, 407, 922, 329, 262, 1200, 2035, 13, 198,...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[198, 32, 25, 197, 1870, 11, 21480, 11, 314, 4...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           input_ids  \\\n",
       "0  [32, 25, 197, 40, 1101, 1654, 484, 389, 13, 19...   \n",
       "1  [6510, 11, 14104, 290, 27913, 13, 198, 32, 25,...   \n",
       "2  [314, 1612, 11, 340, 338, 1611, 286, 43244, 11...   \n",
       "3  [318, 407, 922, 329, 262, 1200, 2035, 13, 198,...   \n",
       "4  [198, 32, 25, 197, 1870, 11, 21480, 11, 314, 4...   \n",
       "\n",
       "                                      attention_mask  \n",
       "0  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "1  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "2  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "3  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "4  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f53aa0",
   "metadata": {},
   "source": [
    "## Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5124433b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLoss(nn.Module):\n",
    "    def __init__(self, num_epochs):\n",
    "        super(CustomLoss, self).__init__()\n",
    "        #self.nll_loss = nn.CrossEntropyLoss()\n",
    "#        self.cosine_similarity = nn.CosineSimilarity(dim=2)\n",
    "        self.num_epochs = num_epochs\n",
    "        self.current_epoch = 0\n",
    "        self.nll_scale = 1.0  # Initialize with a default value\n",
    "\n",
    "    def forward(self, nll,cosine_sim):\n",
    "               \n",
    "        nll_normalized = nll / self.nll_scale         \n",
    "        # Calculate lambda value\n",
    "        lambda_val = self.get_lambda()\n",
    "        # Compute total loss\n",
    "        total_loss = nll_normalized + lambda_val * cosine_sim\n",
    "        #print(f\"inside CustomLoss->nll = {nll}|nll_scale = {self.nll_scale}|nll_normalized= {nll_normalized}|cosine_sim = {cosine_sim} | lambda val = {lambda_val}|total_loss = {total_loss}\")\n",
    "        return total_loss,lambda_val,self.nll_scale\n",
    "\n",
    "    def get_lambda(self):\n",
    "        # Exponential increase from 0 to 1\n",
    "        return 1 - math.exp(-5 * self.current_epoch / self.num_epochs)\n",
    "\n",
    "    def update_epoch(self, epoch):\n",
    "        self.current_epoch = epoch\n",
    "\n",
    "    def update_nll_scale(self, nll_value):\n",
    "        self.nll_scale = max(self.nll_scale, nll_value)\n",
    "    def update_epoch(self, epoch):\n",
    "        self.current_epoch = epoch\n",
    "    def get_nll_scale(self):\n",
    "        #print(self.nll_scale)\n",
    "        return self.nll_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a98d5431",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the tokenizer:\n",
    "model_name = \"gpt2\"\n",
    "if random_init_wts:\n",
    "    config = AutoConfig.from_pretrained(model_name, vocab_size = 50304)\n",
    "    # Initialize the model with random weights\n",
    "    model = AutoModelForCausalLM.from_config(config)\n",
    "else:\n",
    "    #model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "\n",
    "#model = torch.compile(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7d8d49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ac8e0759",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "525"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0836a878",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# device = \"cuda\"\n",
    "# inp = torch.tensor(df_train.iloc[5]['input_ids']).view(B,T).to(device)\n",
    "# att = torch.tensor(df_train.iloc[5]['attention_mask']).view(B,T).to(device)\n",
    "# lab = inp.clone()\n",
    "# # lab = F.pad(lab[:, 1:], (0, 1), value=-100)  # Shift labels to the left and pad with -100\n",
    "# # lab[:, -1] = -100\n",
    "# model.to(device)\n",
    "# model_out = model(input_ids = inp, attention_mask = att , labels = lab)\n",
    "# assert not torch.isnan(inp).any(), \"Input contains NaN\"\n",
    "# assert not torch.isinf(inp).any(), \"Input contains inf\"\n",
    "# model_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "769d2d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"Input shape: {inp.shape}\")\n",
    "# print(f\"Attention mask shape: {att.shape}\")\n",
    "# print(f\"Labels shape: {lab.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c8a7c4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0c799492",
   "metadata": {},
   "source": [
    "### Data loaders and Dataset for batched training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cb719f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataset_pyt_val(Dataset):\n",
    "    \"\"\"Dataset for validation data\"\"\"\n",
    "    def __init__(self, df, B = B, T = T ):\n",
    "        self.df = df\n",
    "        print(f\"Value of B {B}\")\n",
    "                                        \n",
    "    def __getitem__(self, idx):\n",
    "        #print(f\"inside loader...idx ->{idx}\")\n",
    "        input_id_temp = torch.tensor(self.df.iloc[idx]['input_ids'],dtype = torch.long)\n",
    "        att_mask = torch.tensor(self.df.iloc[idx]['attention_mask'],dtype = torch.long)\n",
    "        input_id =   input_id_temp.view(B,T)    \n",
    "        attention_mask = att_mask.view(B,T)\n",
    "        \n",
    "        \n",
    "        return input_id, attention_mask\n",
    "        \n",
    "    def __len__(self):\n",
    "        #return the length of the dataframe\n",
    "        return len(self.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "89d72e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataset_pyt_train(Dataset):\n",
    "    def __init__(self, df, B = B, T = T ):\n",
    "        self.df = df\n",
    "                                        \n",
    "    def __getitem__(self, idx):\n",
    "        #print(f\"************* idx for dataloader = {idx}\")\n",
    "        input_id_temp = torch.tensor(self.df.iloc[idx]['input_ids'],dtype = torch.long)\n",
    "        att_mask = torch.tensor(self.df.iloc[idx]['attention_mask'],dtype = torch.long)\n",
    "        input_id =   input_id_temp.view(B,T)    \n",
    "        attention_mask = att_mask.view(B,T)   \n",
    "        \n",
    "        return input_id, attention_mask\n",
    "        \n",
    "    def __len__(self):\n",
    "        #return the length of the dataframe\n",
    "        return len(self.df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d54ce9e",
   "metadata": {},
   "source": [
    "### Note - batch_size is 1 here because we reshape our input data in the shape [B,T] for a batched training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "456c32b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value of B 32\n"
     ]
    }
   ],
   "source": [
    "#train_dataset = dataset_pyt(filtered_df,tokenizer = tokenizer)\n",
    "train_dataset = dataset_pyt_train(df_train)\n",
    "val_dataset = dataset_pyt_val(df_val)\n",
    "#test_dataset = dataset_pyt(df_test,tokenizer = tokenizer)\n",
    "\n",
    "train_loader = DataLoader(train_dataset,batch_size = 1, shuffle = True , num_workers = 0, pin_memory = True)\n",
    "val_loader = DataLoader(val_dataset,batch_size = 1, shuffle = True , num_workers = 0, pin_memory = True)\n",
    "#test_loader = DataLoader(test_dataset,batch_size = batch_size, shuffle = False, collate_fn = custom_collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d13deb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2097def4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the train loader is 525\n",
      "Length of the val loader is 532\n",
      "num_tokens= 17203200\n"
     ]
    }
   ],
   "source": [
    "print(f\"Length of the train loader is {len(train_loader)}\")\n",
    "print(f\"Length of the val loader is {len(val_loader)}\")\n",
    "print(f\"num_tokens= {B*T*len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6bd2ae6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_file(log_message, model_name = model_name ,random_init_wts = random_init_wts ):\n",
    "    \"\"\"This function logs the training performance for future reference\"\"\"\n",
    "    current_datetime = datetime.datetime.now()\n",
    "    # Extract date and time components\n",
    "    current_date = str(current_datetime.date())\n",
    "    log_file = model_name +'_COS_SIM*lambda_'+'random_init_wts'+ '_'+str(random_init_wts)+'_' +current_date+'.log'\n",
    "    print(f\"*****LOGGING INFO IN {log_file}*********\")\n",
    "    filepath = os.path.join(\"model\",log_file)\n",
    "    logging.basicConfig(filename=filepath, \n",
    "                    filemode='a',  # Overwrite the log file each time\n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s', \n",
    "                    level=logging.DEBUG)\n",
    "    logger = logging.getLogger()\n",
    "    logger.info(log_message)\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8c6b29f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad\n",
    "\n",
    "def eval_model(val_loader, model, lambda_val ,nll_scale,epoch , device = device,tokenizer = tokenizer):\n",
    "    global global_val_loss\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    e = epoch+1\n",
    "    embedding_layer = model.transformer.wte\n",
    "    val_loss_accum = 0.0\n",
    "    print(f\"inside validation data for epoch {e}\")\n",
    "    for ind,(input_id,attention_mask) in enumerate(val_loader):\n",
    "        ids = input_id.to(device=device, non_blocking=True)\n",
    "        att_mask = attention_mask.to(device=device, non_blocking=True)\n",
    "        labels = ids.clone()\n",
    "        \n",
    "        with autocast(dtype = torch.bfloat16):\n",
    "            input_embeddings = embedding_layer(ids)\n",
    "            model_output = model(input_ids = ids ,attention_mask = att_mask, labels = labels)\n",
    "            logits = model_output.logits\n",
    "            predictions = torch.argmax(logits, dim=-1)\n",
    "            #print(f\"predictions = {predictions}\")\n",
    "            prediction_embeddings = embedding_layer(predictions)\n",
    "            cos_sim = F.cosine_similarity(torch.squeeze(prediction_embeddings,dim = 0), torch.squeeze(input_embeddings,dim = 0), dim=1)\n",
    "            cos_loss = 1- cos_sim.mean()\n",
    "            total_loss = cos_loss + model_output.loss\n",
    "        \n",
    "        val_loss_accum+=total_loss.detach().item()\n",
    "        del ids,att_mask,labels,input_embeddings,model_output,logits,predictions,prediction_embeddings,cos_sim,cos_loss\n",
    "    \n",
    "    #logging and saving if the validation loss has decreased.\n",
    "    if val_loss_accum < global_val_loss:\n",
    "        print(f\"Val loss has decreased -->reducing the global validation loss from {global_val_loss:.2f} to {val_loss_accum:.2f}\")\n",
    "        s1 = f\"Val loss has decreased -->reducing the global validation loss from {global_val_loss:.2f} to {val_loss_accum:.2f}\"\n",
    "        global_val_loss = val_loss_accum\n",
    "        print(f\" validation loss for epoch = {e} is {val_loss_accum:.4f}\")\n",
    "        s2 = f\" validation loss for epoch = {e} is {val_loss_accum:.4f}\"\n",
    "        print(f\" epoch= {e} :  val loss is {val_loss_accum:.4f} \")\n",
    "        s3 = f\" epoch= {e} :  val loss is {val_loss_accum:.4f} \"\n",
    "        #save the model\n",
    "        \n",
    "        # Get the current date and time\n",
    "        current_datetime = datetime.datetime.now()\n",
    "        # Extract date and time components\n",
    "        current_date = str(current_datetime.date())\n",
    "        current_time = str(current_datetime.time()).split('.')[0]\n",
    "        file_name = 'model'+ current_date+current_time+'.pth'\n",
    "        path = os.path.join(\"model\",file_name)\n",
    "        print(f\"saving the model {file_name}\")\n",
    "        s4 = f\"saving the model {file_name}\"\n",
    "        #torch.save(model.state_dict(), path)\n",
    "        model.save_pretrained(path)\n",
    "        tokenizer.save_pretrained(path)\n",
    "        log_message = s1+s2+s3+s4\n",
    "        write_file(log_message)\n",
    "        \n",
    "        #plot_confusion_matrix(y_val.cpu().numpy(), y_hat_val.cpu().numpy(), labels)\n",
    "    else:\n",
    "        print(f\"No improvement in validation loss-->epoch= {e} and global val loss is {global_val_loss:.2f}|current_Val loss = {val_loss_accum}\")\n",
    "    return val_loss_accum\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "239fd6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_loader,val_loader,model,num_epoch = 100,device = device,tokenizer = tokenizer):\n",
    "    global global_tr_loss\n",
    "    model.train()\n",
    "    device = device\n",
    "    lr_custom = 2e-6\n",
    "    print(f\"inside train model. Device = {device}\")\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.AdamW(params =  model.parameters(), lr= lr_custom,fused = True ,weight_decay = .1)\n",
    "    total_batch_size = 2**19\n",
    "    grad_accum_step = total_batch_size//(B*T)\n",
    "    \n",
    "    extra_train = .1*num_epoch\n",
    "    max_train_steps = int(num_epoch +extra_train )\n",
    "    import time\n",
    "    from transformers import get_linear_schedule_with_warmup\n",
    "    total_steps = len(train_loader) * num_epoch\n",
    "    scheduler_cos = transformers.get_cosine_schedule_with_warmup( optimizer= optimizer, num_warmup_steps =int(total_steps * 0.1) ,num_training_steps= total_steps )\n",
    "    embedding_layer = model.transformer.wte\n",
    "    criterion = CustomLoss(num_epoch)\n",
    "    epoch_train_log = []\n",
    "    epoch_val_log = []\n",
    "    for i in range (max_train_steps):\n",
    "        criterion.update_epoch(i)\n",
    "        epoch_start_time = time.time()\n",
    "        epoch_train_loss = 0.0\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        # we use 2 schedulers - the first LR scheduler uses a cosine decay for 100 epochs the second scheduler takes the last LR from cosine scheduler and then maintains that LR for the next 10 epochs\n",
    "        if i >= num_epoch:\n",
    "            optimizer_reduced_lr = torch.optim.AdamW(params =  model.parameters(), lr= current_lr ,fused = True , weight_decay=.1)\n",
    "            scheduler_constant = transformers.get_constant_schedule_with_warmup( optimizer = optimizer_reduced_lr ,num_warmup_steps = 0, last_epoch = -1 )\n",
    "        \n",
    "        lambda_val = None\n",
    "        nll_scale = None\n",
    "        epoch_nll = []\n",
    "        \n",
    "        for ind,(input_id,attention_mask) in enumerate(train_loader):\n",
    "            if ind == int(len(train_loader)/2):\n",
    "                batch_time = time.time()\n",
    "                duration = batch_time - epoch_start_time\n",
    "                print(f\"executing epoch:{i+1}, it took {duration/60} mins from beginning of epoch till batch#{ind}\")\n",
    "            \n",
    "            ids = input_id.to(device=device, non_blocking=True)\n",
    "            att_mask = attention_mask.to(device=device, non_blocking=True)\n",
    "            labels = ids.clone()\n",
    "            assert not torch.isnan(ids).any(), \"Input contains NaN values\"\n",
    "            assert not torch.isinf(ids).any(), \"Input contains infinite values\"\n",
    "            \n",
    "            with autocast(dtype = torch.bfloat16):\n",
    "                input_embeddings = embedding_layer(ids)\n",
    "                model_output = model(input_ids = ids ,attention_mask = att_mask, labels = labels)\n",
    "                logits = model_output.logits\n",
    "                predictions = torch.argmax(logits, dim=-1)\n",
    "                prediction_embeddings = embedding_layer(predictions)\n",
    "                #print(f\"model_output.logits = {model_output.logits}|model_output.loss = {model_output.loss}\")\n",
    "                cos_sim = F.cosine_similarity(torch.squeeze(prediction_embeddings,dim = 0), torch.squeeze(input_embeddings,dim = 0), dim=1)\n",
    "                cos_loss = 1- cos_sim.mean()\n",
    "                #print(f\"inside train_vale before calc custom loss|ind = {ind}|model_output.loss = {model_output.loss}|coss_loss = {cos_loss}\")\n",
    "                #print(f\"inside train_vale before calc custom loss|ind = {ind}|model_output.loss = {model_output.loss}|coss_loss = {cos_loss}|learning rate = {scheduler_cos.get_last_lr()[0]:.3e}\")\n",
    "                if np.isnan(model_output.loss.item()):\n",
    "                    print(\"f nan values encountered..\")\n",
    "                    decoded_texts = [tokenizer.decode(ids, skip_special_tokens=True) for ids in ids]\n",
    "                    print(f\"*********$$$$$$$$$ decoded_texts = {decoded_texts}*************\")\n",
    "                total_loss,lambda_val,nll_scale = criterion(nll = model_output.loss.detach().item() , cosine_sim = cos_loss)\n",
    "                #print(f\"Current value of nll max is {epoch_nll_max}| current mini batch loss is = {model_output.loss.item()}\")\n",
    "                epoch_nll.append(model_output.loss.detach())\n",
    "                #print(f\"batch index = {ind}|epoch_nll_max = {epoch_nll_max}\")\n",
    "\n",
    "\n",
    "            assert not np.isnan(model_output.loss.item()), \"NaN value found\"\n",
    "            #total_loss = cos_loss + model_output.loss.item()\n",
    "            \n",
    "            if ind == (len(train_loader) - 1):\n",
    "                lambda_val = lambda_val\n",
    "                nll_scale = nll_scale\n",
    "                \n",
    "            \n",
    "            total_loss.backward()\n",
    "            epoch_train_loss += total_loss.detach().item()\n",
    "            norm = torch.nn.utils.clip_grad_norm(model.parameters() , 1.0)\n",
    "            if i <= num_epoch:\n",
    "                optimizer.step()\n",
    "                scheduler_cos.step()\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "            else:\n",
    "                optimizer_reduced_lr.step()\n",
    "                optimizer_reduced_lr.zero_grad(set_to_none=True)\n",
    "                scheduler_constant.step()\n",
    "                \n",
    "                                \n",
    "            del ids,att_mask,labels,input_embeddings,model_output,logits,predictions,prediction_embeddings,cos_sim\n",
    "            \n",
    "        \n",
    "        \n",
    "        #batch processing complete \n",
    "        print(f\"batch processing complete , before updating nll_scale,updating the nll_scale from {criterion.get_nll_scale()} to {torch.mean(torch.tensor(epoch_nll))}|total_loss for batch= {total_loss}\")\n",
    "        mean_nll = torch.mean(torch.tensor(epoch_nll))\n",
    "        criterion.update_nll_scale(torch.mean(mean_nll).item())                           \n",
    "        #print(f\"batch processing complete , after updating nll_scale , value is {epoch_nll_max}\")\n",
    "        if i <= num_epoch:\n",
    "            current_lr = scheduler_cos.get_last_lr()[0]\n",
    "        #This code below compares the training loss during the current epoch with the global loss and if updates the global loss if there is an improvement.\n",
    "        # at every 4 epoch we evluate the model on the validation data or when \n",
    "        epoch_end_time = time.time()\n",
    "        epoch_durn = (epoch_end_time - epoch_start_time)\n",
    "        num_token = B*T*len(train_loader)\n",
    "        epoch_train_log.append(epoch_train_loss)\n",
    "        if (epoch_train_loss < global_tr_loss) :\n",
    "            improvement_percent = (abs(epoch_train_loss-global_tr_loss)/global_tr_loss)*100\n",
    "            print(f\"training loss has decreased---> reducing the global loss from {global_tr_loss:.2f} to {epoch_train_loss:.2f} | throughput = {int(num_token/epoch_durn)} tokens/second | norm = {norm:.4f} | learning rate = {current_lr:.5e}\")\n",
    "            global_tr_loss = epoch_train_loss\n",
    "            print(f\" epoch= {i+1} and  train loss is {epoch_train_loss:.2f}\")\n",
    "            if (i%4 == 0 or improvement_percent > 5):\n",
    "                val_loss= eval_model(val_loader, model, lambda_val ,nll_scale,epoch = i, device = device,tokenizer = tokenizer)\n",
    "                epoch_val_log.append(val_loss)\n",
    "            \n",
    "        else:\n",
    "            print(f\"No improvement in training loss..the global training loss is -->{global_tr_loss:.2f}|epoch train_loss = {epoch_train_loss:.2f} \")\n",
    "            #print(f\" epoch= {i+1} and mean train loss is {epoch_train_loss:.2f}\")\n",
    "        \n",
    "        \n",
    "    \n",
    "    return model,epoch_train_log,epoch_val_log\n",
    "        \n",
    "            \n",
    "            \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "869b1bff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inside train model. Device = cuda:0\n",
      "executing epoch:1, it took 0.8258710503578186 mins from beginning of epoch till batch#262\n",
      "batch processing complete , before updating nll_scale,updating the nll_scale from 1.0 to 3.4965267181396484|total_loss for batch= 3.8043603897094727\n",
      "training loss has decreased---> reducing the global loss from inf to 1835.68 | throughput = 173516 tokens/second | norm = 0.0000 | learning rate = 2.00000e-07\n",
      " epoch= 1 and  train loss is 1835.68\n",
      "inside validation data for epoch 1\n",
      "Val loss has decreased -->reducing the global validation loss from inf to 2253.45\n",
      " validation loss for epoch = 1 is 2253.4457\n",
      " epoch= 1 :  val loss is 2253.4457 \n",
      "saving the model model2024-07-1601:22:33.pth\n",
      "*****LOGGING INFO IN gpt2_COS_SIM*lambda_random_init_wts_False_2024-07-16.log*********\n",
      "executing epoch:2, it took 0.8133656501770019 mins from beginning of epoch till batch#262\n",
      "batch processing complete , before updating nll_scale,updating the nll_scale from 3.4965267181396484 to 3.3506088256835938|total_loss for batch= 1.1165951490402222\n",
      "training loss has decreased---> reducing the global loss from 1835.68 to 525.27 | throughput = 175760 tokens/second | norm = 0.0028 | learning rate = 4.00000e-07\n",
      " epoch= 2 and  train loss is 525.27\n",
      "inside validation data for epoch 2\n",
      "No improvement in validation loss-->epoch= 2 and global val loss is 2253.45|current_Val loss = 2254.253450155258\n",
      "executing epoch:3, it took 0.8143822431564331 mins from beginning of epoch till batch#262\n",
      "batch processing complete , before updating nll_scale,updating the nll_scale from 3.4965267181396484 to 3.3544766902923584|total_loss for batch= 1.0239653587341309\n",
      "No improvement in training loss..the global training loss is -->525.27|epoch train_loss = 546.90 \n",
      "executing epoch:4, it took 0.8142393827438354 mins from beginning of epoch till batch#262\n",
      "batch processing complete , before updating nll_scale,updating the nll_scale from 3.4965267181396484 to 3.359609365463257|total_loss for batch= 1.1275091171264648\n",
      "No improvement in training loss..the global training loss is -->525.27|epoch train_loss = 567.61 \n",
      "executing epoch:5, it took 0.8149467984835307 mins from beginning of epoch till batch#262\n",
      "batch processing complete , before updating nll_scale,updating the nll_scale from 3.4965267181396484 to 3.359708786010742|total_loss for batch= 1.1089214086532593\n",
      "No improvement in training loss..the global training loss is -->525.27|epoch train_loss = 586.48 \n",
      "executing epoch:6, it took 0.8147763967514038 mins from beginning of epoch till batch#262\n",
      "batch processing complete , before updating nll_scale,updating the nll_scale from 3.4965267181396484 to 3.358804941177368|total_loss for batch= 1.127514123916626\n",
      "No improvement in training loss..the global training loss is -->525.27|epoch train_loss = 604.15 \n",
      "executing epoch:7, it took 0.8146567622820536 mins from beginning of epoch till batch#262\n",
      "batch processing complete , before updating nll_scale,updating the nll_scale from 3.4965267181396484 to 3.3619682788848877|total_loss for batch= 1.041582703590393\n",
      "No improvement in training loss..the global training loss is -->525.27|epoch train_loss = 621.42 \n",
      "executing epoch:8, it took 0.8147763093312581 mins from beginning of epoch till batch#262\n",
      "batch processing complete , before updating nll_scale,updating the nll_scale from 3.4965267181396484 to 3.3716087341308594|total_loss for batch= 1.209566354751587\n",
      "No improvement in training loss..the global training loss is -->525.27|epoch train_loss = 638.67 \n",
      "executing epoch:9, it took 0.8141288995742798 mins from beginning of epoch till batch#262\n",
      "batch processing complete , before updating nll_scale,updating the nll_scale from 3.4965267181396484 to 3.3733015060424805|total_loss for batch= 1.233454704284668\n",
      "No improvement in training loss..the global training loss is -->525.27|epoch train_loss = 653.79 \n",
      "executing epoch:10, it took 0.8146433273951212 mins from beginning of epoch till batch#262\n",
      "batch processing complete , before updating nll_scale,updating the nll_scale from 3.4965267181396484 to 3.3791258335113525|total_loss for batch= 1.1943399906158447\n",
      "No improvement in training loss..the global training loss is -->525.27|epoch train_loss = 668.57 \n",
      "executing epoch:11, it took 0.8148467580477396 mins from beginning of epoch till batch#262\n",
      "batch processing complete , before updating nll_scale,updating the nll_scale from 3.4965267181396484 to 3.386387825012207|total_loss for batch= 1.304007887840271\n",
      "No improvement in training loss..the global training loss is -->525.27|epoch train_loss = 682.74 \n",
      "executing epoch:12, it took 0.8149461070696513 mins from beginning of epoch till batch#262\n",
      "batch processing complete , before updating nll_scale,updating the nll_scale from 3.4965267181396484 to 3.3941540718078613|total_loss for batch= 1.2765275239944458\n",
      "No improvement in training loss..the global training loss is -->525.27|epoch train_loss = 696.19 \n",
      "executing epoch:13, it took 0.8147846539815267 mins from beginning of epoch till batch#262\n",
      "batch processing complete , before updating nll_scale,updating the nll_scale from 3.4965267181396484 to 3.4023373126983643|total_loss for batch= 1.2597732543945312\n",
      "No improvement in training loss..the global training loss is -->525.27|epoch train_loss = 708.97 \n",
      "executing epoch:14, it took 0.8148835023244222 mins from beginning of epoch till batch#262\n",
      "batch processing complete , before updating nll_scale,updating the nll_scale from 3.4965267181396484 to 3.4065146446228027|total_loss for batch= 1.3747475147247314\n",
      "No improvement in training loss..the global training loss is -->525.27|epoch train_loss = 720.48 \n",
      "executing epoch:15, it took 0.8143964966138204 mins from beginning of epoch till batch#262\n",
      "batch processing complete , before updating nll_scale,updating the nll_scale from 3.4965267181396484 to 3.41469144821167|total_loss for batch= 1.3689544200897217\n",
      "No improvement in training loss..the global training loss is -->525.27|epoch train_loss = 731.96 \n",
      "executing epoch:16, it took 0.8143019040425619 mins from beginning of epoch till batch#262\n",
      "batch processing complete , before updating nll_scale,updating the nll_scale from 3.4965267181396484 to 3.4209492206573486|total_loss for batch= 1.4510246515274048\n",
      "No improvement in training loss..the global training loss is -->525.27|epoch train_loss = 742.53 \n",
      "executing epoch:17, it took 0.8145770351092021 mins from beginning of epoch till batch#262\n",
      "batch processing complete , before updating nll_scale,updating the nll_scale from 3.4965267181396484 to 3.428419351577759|total_loss for batch= 1.414874792098999\n",
      "No improvement in training loss..the global training loss is -->525.27|epoch train_loss = 752.68 \n",
      "executing epoch:18, it took 0.8143914182980855 mins from beginning of epoch till batch#262\n",
      "batch processing complete , before updating nll_scale,updating the nll_scale from 3.4965267181396484 to 3.4350087642669678|total_loss for batch= 1.4480756521224976\n",
      "No improvement in training loss..the global training loss is -->525.27|epoch train_loss = 762.12 \n",
      "executing epoch:19, it took 0.814587926864624 mins from beginning of epoch till batch#262\n",
      "batch processing complete , before updating nll_scale,updating the nll_scale from 3.4965267181396484 to 3.4406423568725586|total_loss for batch= 1.4232072830200195\n",
      "No improvement in training loss..the global training loss is -->525.27|epoch train_loss = 770.90 \n",
      "executing epoch:20, it took 0.8142381985982259 mins from beginning of epoch till batch#262\n",
      "batch processing complete , before updating nll_scale,updating the nll_scale from 3.4965267181396484 to 3.4475839138031006|total_loss for batch= 1.4022146463394165\n",
      "No improvement in training loss..the global training loss is -->525.27|epoch train_loss = 779.42 \n",
      "executing epoch:21, it took 0.8143816828727722 mins from beginning of epoch till batch#262\n",
      "batch processing complete , before updating nll_scale,updating the nll_scale from 3.4965267181396484 to 3.4571549892425537|total_loss for batch= 1.5817558765411377\n",
      "No improvement in training loss..the global training loss is -->525.27|epoch train_loss = 787.89 \n",
      "executing epoch:22, it took 0.8142561117808024 mins from beginning of epoch till batch#262\n",
      "batch processing complete , before updating nll_scale,updating the nll_scale from 3.4965267181396484 to 3.465057611465454|total_loss for batch= 1.5369691848754883\n",
      "No improvement in training loss..the global training loss is -->525.27|epoch train_loss = 795.66 \n",
      "executing epoch:23, it took 0.813915240764618 mins from beginning of epoch till batch#262\n",
      "batch processing complete , before updating nll_scale,updating the nll_scale from 3.4965267181396484 to 3.4719431400299072|total_loss for batch= 1.5266711711883545\n",
      "No improvement in training loss..the global training loss is -->525.27|epoch train_loss = 802.87 \n",
      "executing epoch:24, it took 0.8142944892247518 mins from beginning of epoch till batch#262\n",
      "batch processing complete , before updating nll_scale,updating the nll_scale from 3.4965267181396484 to 3.4798836708068848|total_loss for batch= 1.5700595378875732\n",
      "No improvement in training loss..the global training loss is -->525.27|epoch train_loss = 809.86 \n",
      "executing epoch:25, it took 0.8140084385871887 mins from beginning of epoch till batch#262\n",
      "batch processing complete , before updating nll_scale,updating the nll_scale from 3.4965267181396484 to 3.486928701400757|total_loss for batch= 1.517981767654419\n",
      "No improvement in training loss..the global training loss is -->525.27|epoch train_loss = 816.35 \n",
      "executing epoch:26, it took 0.8138110160827636 mins from beginning of epoch till batch#262\n",
      "batch processing complete , before updating nll_scale,updating the nll_scale from 3.4965267181396484 to 3.4957501888275146|total_loss for batch= 1.6154448986053467\n",
      "No improvement in training loss..the global training loss is -->525.27|epoch train_loss = 822.75 \n",
      "executing epoch:27, it took 0.8135000507036845 mins from beginning of epoch till batch#262\n",
      "batch processing complete , before updating nll_scale,updating the nll_scale from 3.4965267181396484 to 3.503936290740967|total_loss for batch= 1.5816893577575684\n",
      "No improvement in training loss..the global training loss is -->525.27|epoch train_loss = 828.72 \n",
      "executing epoch:28, it took 0.8136938691139222 mins from beginning of epoch till batch#262\n",
      "batch processing complete , before updating nll_scale,updating the nll_scale from 3.503936290740967 to 3.509282112121582|total_loss for batch= 1.548037052154541\n",
      "No improvement in training loss..the global training loss is -->525.27|epoch train_loss = 832.81 \n",
      "executing epoch:29, it took 0.8132040619850158 mins from beginning of epoch till batch#262\n",
      "batch processing complete , before updating nll_scale,updating the nll_scale from 3.509282112121582 to 3.516843318939209|total_loss for batch= 1.6889147758483887\n",
      "No improvement in training loss..the global training loss is -->525.27|epoch train_loss = 837.31 \n",
      "executing epoch:30, it took 0.8136284073193868 mins from beginning of epoch till batch#262\n",
      "batch processing complete , before updating nll_scale,updating the nll_scale from 3.516843318939209 to 3.524512767791748|total_loss for batch= 1.698390007019043\n",
      "No improvement in training loss..the global training loss is -->525.27|epoch train_loss = 841.15 \n",
      "executing epoch:31, it took 0.8133867303530375 mins from beginning of epoch till batch#262\n",
      "batch processing complete , before updating nll_scale,updating the nll_scale from 3.524512767791748 to 3.531020164489746|total_loss for batch= 1.5503764152526855\n",
      "No improvement in training loss..the global training loss is -->525.27|epoch train_loss = 844.58 \n",
      "executing epoch:32, it took 0.8140656232833863 mins from beginning of epoch till batch#262\n",
      "batch processing complete , before updating nll_scale,updating the nll_scale from 3.531020164489746 to 3.5381429195404053|total_loss for batch= 1.5078279972076416\n",
      "No improvement in training loss..the global training loss is -->525.27|epoch train_loss = 848.04 \n",
      "executing epoch:33, it took 0.813761305809021 mins from beginning of epoch till batch#262\n",
      "batch processing complete , before updating nll_scale,updating the nll_scale from 3.5381429195404053 to 3.544109582901001|total_loss for batch= 1.5644011497497559\n",
      "No improvement in training loss..the global training loss is -->525.27|epoch train_loss = 850.98 \n",
      "executing epoch:34, it took 0.8129801313082378 mins from beginning of epoch till batch#262\n",
      "batch processing complete , before updating nll_scale,updating the nll_scale from 3.544109582901001 to 3.5506720542907715|total_loss for batch= 1.5229984521865845\n",
      "No improvement in training loss..the global training loss is -->525.27|epoch train_loss = 853.99 \n",
      "executing epoch:35, it took 0.8136924187342326 mins from beginning of epoch till batch#262\n",
      "batch processing complete , before updating nll_scale,updating the nll_scale from 3.5506720542907715 to 3.556856870651245|total_loss for batch= 1.5947670936584473\n",
      "No improvement in training loss..the global training loss is -->525.27|epoch train_loss = 856.62 \n",
      "executing epoch:36, it took 0.8137392242749532 mins from beginning of epoch till batch#262\n",
      "batch processing complete , before updating nll_scale,updating the nll_scale from 3.556856870651245 to 3.5621178150177|total_loss for batch= 1.6687374114990234\n",
      "No improvement in training loss..the global training loss is -->525.27|epoch train_loss = 859.08 \n",
      "executing epoch:37, it took 0.813192896048228 mins from beginning of epoch till batch#262\n",
      "batch processing complete , before updating nll_scale,updating the nll_scale from 3.5621178150177 to 3.568451166152954|total_loss for batch= 1.542918086051941\n",
      "No improvement in training loss..the global training loss is -->525.27|epoch train_loss = 861.66 \n",
      "executing epoch:38, it took 0.8139145533243816 mins from beginning of epoch till batch#262\n",
      "batch processing complete , before updating nll_scale,updating the nll_scale from 3.568451166152954 to 3.5770509243011475|total_loss for batch= 1.6565532684326172\n",
      "No improvement in training loss..the global training loss is -->525.27|epoch train_loss = 864.25 \n",
      "executing epoch:39, it took 0.8136654734611511 mins from beginning of epoch till batch#262\n",
      "batch processing complete , before updating nll_scale,updating the nll_scale from 3.5770509243011475 to 3.585761547088623|total_loss for batch= 1.7069547176361084\n",
      "No improvement in training loss..the global training loss is -->525.27|epoch train_loss = 866.39 \n",
      "executing epoch:40, it took 0.8139264583587646 mins from beginning of epoch till batch#262\n",
      "batch processing complete , before updating nll_scale,updating the nll_scale from 3.585761547088623 to 3.590371608734131|total_loss for batch= 1.6735622882843018\n",
      "No improvement in training loss..the global training loss is -->525.27|epoch train_loss = 867.77 \n",
      "executing epoch:41, it took 0.8134222348531087 mins from beginning of epoch till batch#262\n",
      "batch processing complete , before updating nll_scale,updating the nll_scale from 3.590371608734131 to 3.5979156494140625|total_loss for batch= 1.6020087003707886\n",
      "No improvement in training loss..the global training loss is -->525.27|epoch train_loss = 870.02 \n",
      "executing epoch:42, it took 0.8133002320925394 mins from beginning of epoch till batch#262\n",
      "batch processing complete , before updating nll_scale,updating the nll_scale from 3.5979156494140625 to 3.6055777072906494|total_loss for batch= 1.7760941982269287\n",
      "No improvement in training loss..the global training loss is -->525.27|epoch train_loss = 871.75 \n",
      "executing epoch:43, it took 0.8129613002141317 mins from beginning of epoch till batch#262\n",
      "batch processing complete , before updating nll_scale,updating the nll_scale from 3.6055777072906494 to 3.6138410568237305|total_loss for batch= 1.6032252311706543\n",
      "No improvement in training loss..the global training loss is -->525.27|epoch train_loss = 873.45 \n",
      "executing epoch:44, it took 0.8128792524337769 mins from beginning of epoch till batch#262\n",
      "batch processing complete , before updating nll_scale,updating the nll_scale from 3.6138410568237305 to 3.621075391769409|total_loss for batch= 1.6851518154144287\n",
      "No improvement in training loss..the global training loss is -->525.27|epoch train_loss = 874.82 \n",
      "executing epoch:45, it took 0.8134645263353983 mins from beginning of epoch till batch#262\n",
      "batch processing complete , before updating nll_scale,updating the nll_scale from 3.621075391769409 to 3.6275994777679443|total_loss for batch= 1.5865402221679688\n",
      "No improvement in training loss..the global training loss is -->525.27|epoch train_loss = 876.09 \n",
      "executing epoch:46, it took 0.8136004169782003 mins from beginning of epoch till batch#262\n",
      "batch processing complete , before updating nll_scale,updating the nll_scale from 3.6275994777679443 to 3.637268543243408|total_loss for batch= 1.6090164184570312\n",
      "No improvement in training loss..the global training loss is -->525.27|epoch train_loss = 877.89 \n",
      "executing epoch:47, it took 0.8134751001993815 mins from beginning of epoch till batch#262\n",
      "batch processing complete , before updating nll_scale,updating the nll_scale from 3.637268543243408 to 3.6457273960113525|total_loss for batch= 1.7036247253417969\n",
      "No improvement in training loss..the global training loss is -->525.27|epoch train_loss = 878.86 \n",
      "executing epoch:48, it took 0.8131495555241902 mins from beginning of epoch till batch#262\n",
      "batch processing complete , before updating nll_scale,updating the nll_scale from 3.6457273960113525 to 3.6541738510131836|total_loss for batch= 1.5602242946624756\n",
      "No improvement in training loss..the global training loss is -->525.27|epoch train_loss = 879.96 \n",
      "executing epoch:49, it took 0.8129856864611308 mins from beginning of epoch till batch#262\n",
      "batch processing complete , before updating nll_scale,updating the nll_scale from 3.6541738510131836 to 3.660311460494995|total_loss for batch= 1.6162203550338745\n",
      "No improvement in training loss..the global training loss is -->525.27|epoch train_loss = 880.69 \n",
      "executing epoch:50, it took 0.8128313461939494 mins from beginning of epoch till batch#262\n",
      "batch processing complete , before updating nll_scale,updating the nll_scale from 3.660311460494995 to 3.6693687438964844|total_loss for batch= 1.6540066003799438\n",
      "No improvement in training loss..the global training loss is -->525.27|epoch train_loss = 882.16 \n",
      "executing epoch:51, it took 0.8124292254447937 mins from beginning of epoch till batch#262\n",
      "batch processing complete , before updating nll_scale,updating the nll_scale from 3.6693687438964844 to 3.676072597503662|total_loss for batch= 1.6235506534576416\n",
      "No improvement in training loss..the global training loss is -->525.27|epoch train_loss = 882.75 \n",
      "executing epoch:52, it took 0.8124791224797566 mins from beginning of epoch till batch#262\n",
      "batch processing complete , before updating nll_scale,updating the nll_scale from 3.676072597503662 to 3.6814281940460205|total_loss for batch= 1.6240589618682861\n",
      "No improvement in training loss..the global training loss is -->525.27|epoch train_loss = 883.43 \n",
      "executing epoch:53, it took 0.8132180770238241 mins from beginning of epoch till batch#262\n",
      "batch processing complete , before updating nll_scale,updating the nll_scale from 3.6814281940460205 to 3.6871957778930664|total_loss for batch= 1.7349177598953247\n",
      "No improvement in training loss..the global training loss is -->525.27|epoch train_loss = 884.27 \n",
      "executing epoch:54, it took 0.813011093934377 mins from beginning of epoch till batch#262\n",
      "batch processing complete , before updating nll_scale,updating the nll_scale from 3.6871957778930664 to 3.6946544647216797|total_loss for batch= 1.6568350791931152\n",
      "No improvement in training loss..the global training loss is -->525.27|epoch train_loss = 885.29 \n",
      "executing epoch:55, it took 0.8121708075205485 mins from beginning of epoch till batch#262\n",
      "batch processing complete , before updating nll_scale,updating the nll_scale from 3.6946544647216797 to 3.700775623321533|total_loss for batch= 1.7745206356048584\n",
      "No improvement in training loss..the global training loss is -->525.27|epoch train_loss = 885.82 \n",
      "executing epoch:56, it took 0.8125251889228821 mins from beginning of epoch till batch#262\n",
      "batch processing complete , before updating nll_scale,updating the nll_scale from 3.700775623321533 to 3.706974983215332|total_loss for batch= 1.646250605583191\n",
      "No improvement in training loss..the global training loss is -->525.27|epoch train_loss = 886.50 \n",
      "executing epoch:57, it took 0.8129918217658997 mins from beginning of epoch till batch#262\n",
      "batch processing complete , before updating nll_scale,updating the nll_scale from 3.706974983215332 to 3.715437412261963|total_loss for batch= 1.641345739364624\n",
      "No improvement in training loss..the global training loss is -->525.27|epoch train_loss = 887.53 \n",
      "executing epoch:58, it took 0.8132073124249776 mins from beginning of epoch till batch#262\n",
      "batch processing complete , before updating nll_scale,updating the nll_scale from 3.715437412261963 to 3.7217001914978027|total_loss for batch= 1.674485206604004\n",
      "No improvement in training loss..the global training loss is -->525.27|epoch train_loss = 887.80 \n",
      "executing epoch:59, it took 0.8128230969111124 mins from beginning of epoch till batch#262\n",
      "batch processing complete , before updating nll_scale,updating the nll_scale from 3.7217001914978027 to 3.7270331382751465|total_loss for batch= 1.704969882965088\n",
      "No improvement in training loss..the global training loss is -->525.27|epoch train_loss = 888.18 \n",
      "executing epoch:60, it took 0.8128688255945842 mins from beginning of epoch till batch#262\n",
      "batch processing complete , before updating nll_scale,updating the nll_scale from 3.7270331382751465 to 3.734400987625122|total_loss for batch= 1.6068832874298096\n",
      "No improvement in training loss..the global training loss is -->525.27|epoch train_loss = 888.96 \n",
      "executing epoch:61, it took 0.8130371888478597 mins from beginning of epoch till batch#262\n",
      "batch processing complete , before updating nll_scale,updating the nll_scale from 3.734400987625122 to 3.7424476146698|total_loss for batch= 1.8466880321502686\n",
      "No improvement in training loss..the global training loss is -->525.27|epoch train_loss = 889.61 \n",
      "executing epoch:62, it took 0.8126046061515808 mins from beginning of epoch till batch#262\n",
      "batch processing complete , before updating nll_scale,updating the nll_scale from 3.7424476146698 to 3.7527658939361572|total_loss for batch= 1.6382341384887695\n",
      "No improvement in training loss..the global training loss is -->525.27|epoch train_loss = 890.47 \n",
      "executing epoch:63, it took 0.8121774196624756 mins from beginning of epoch till batch#262\n",
      "batch processing complete , before updating nll_scale,updating the nll_scale from 3.7527658939361572 to 3.7640879154205322|total_loss for batch= 1.8975175619125366\n",
      "No improvement in training loss..the global training loss is -->525.27|epoch train_loss = 891.12 \n",
      "executing epoch:64, it took 0.8124435782432556 mins from beginning of epoch till batch#262\n",
      "batch processing complete , before updating nll_scale,updating the nll_scale from 3.7640879154205322 to 3.7711992263793945|total_loss for batch= 1.903026819229126\n",
      "No improvement in training loss..the global training loss is -->525.27|epoch train_loss = 890.96 \n",
      "executing epoch:65, it took 0.8125253319740295 mins from beginning of epoch till batch#262\n",
      "batch processing complete , before updating nll_scale,updating the nll_scale from 3.7711992263793945 to 3.777327537536621|total_loss for batch= 1.7175970077514648\n",
      "No improvement in training loss..the global training loss is -->525.27|epoch train_loss = 891.26 \n",
      "executing epoch:66, it took 0.8126214941342672 mins from beginning of epoch till batch#262\n",
      "batch processing complete , before updating nll_scale,updating the nll_scale from 3.777327537536621 to 3.780393362045288|total_loss for batch= 1.5802704095840454\n",
      "No improvement in training loss..the global training loss is -->525.27|epoch train_loss = 891.17 \n",
      "executing epoch:67, it took 0.8123660445213318 mins from beginning of epoch till batch#262\n",
      "batch processing complete , before updating nll_scale,updating the nll_scale from 3.780393362045288 to 3.779381275177002|total_loss for batch= 1.787795066833496\n",
      "No improvement in training loss..the global training loss is -->525.27|epoch train_loss = 890.85 \n",
      "executing epoch:68, it took 0.8125071009000142 mins from beginning of epoch till batch#262\n",
      "batch processing complete , before updating nll_scale,updating the nll_scale from 3.780393362045288 to 3.7789652347564697|total_loss for batch= 1.6053333282470703\n",
      "No improvement in training loss..the global training loss is -->525.27|epoch train_loss = 890.96 \n",
      "executing epoch:69, it took 0.812499729792277 mins from beginning of epoch till batch#262\n",
      "batch processing complete , before updating nll_scale,updating the nll_scale from 3.780393362045288 to 3.7837393283843994|total_loss for batch= 1.6895396709442139\n",
      "No improvement in training loss..the global training loss is -->525.27|epoch train_loss = 891.95 \n",
      "executing epoch:70, it took 0.8128501256306966 mins from beginning of epoch till batch#262\n",
      "batch processing complete , before updating nll_scale,updating the nll_scale from 3.7837393283843994 to 3.78836727142334|total_loss for batch= 1.671128749847412\n",
      "No improvement in training loss..the global training loss is -->525.27|epoch train_loss = 892.47 \n",
      "executing epoch:71, it took 0.8124658107757569 mins from beginning of epoch till batch#262\n",
      "batch processing complete , before updating nll_scale,updating the nll_scale from 3.78836727142334 to 3.792137622833252|total_loss for batch= 1.6894314289093018\n",
      "No improvement in training loss..the global training loss is -->525.27|epoch train_loss = 892.65 \n",
      "executing epoch:72, it took 0.8127738873163859 mins from beginning of epoch till batch#262\n",
      "batch processing complete , before updating nll_scale,updating the nll_scale from 3.792137622833252 to 3.7949860095977783|total_loss for batch= 1.9049087762832642\n",
      "No improvement in training loss..the global training loss is -->525.27|epoch train_loss = 892.79 \n",
      "executing epoch:73, it took 0.8126999338467916 mins from beginning of epoch till batch#262\n",
      "batch processing complete , before updating nll_scale,updating the nll_scale from 3.7949860095977783 to 3.798976182937622|total_loss for batch= 1.6244418621063232\n",
      "No improvement in training loss..the global training loss is -->525.27|epoch train_loss = 893.20 \n",
      "executing epoch:74, it took 0.8129374861717225 mins from beginning of epoch till batch#262\n",
      "batch processing complete , before updating nll_scale,updating the nll_scale from 3.798976182937622 to 3.802867889404297|total_loss for batch= 1.694785714149475\n",
      "No improvement in training loss..the global training loss is -->525.27|epoch train_loss = 893.45 \n",
      "executing epoch:75, it took 0.8125218351682028 mins from beginning of epoch till batch#262\n",
      "batch processing complete , before updating nll_scale,updating the nll_scale from 3.802867889404297 to 3.8061814308166504|total_loss for batch= 1.6399357318878174\n",
      "No improvement in training loss..the global training loss is -->525.27|epoch train_loss = 893.65 \n",
      "executing epoch:76, it took 0.8129629810651143 mins from beginning of epoch till batch#262\n",
      "batch processing complete , before updating nll_scale,updating the nll_scale from 3.8061814308166504 to 3.8109560012817383|total_loss for batch= 1.6244640350341797\n",
      "No improvement in training loss..the global training loss is -->525.27|epoch train_loss = 894.11 \n",
      "executing epoch:77, it took 0.8122522592544555 mins from beginning of epoch till batch#262\n",
      "batch processing complete , before updating nll_scale,updating the nll_scale from 3.8109560012817383 to 3.814380645751953|total_loss for batch= 1.648298978805542\n",
      "No improvement in training loss..the global training loss is -->525.27|epoch train_loss = 894.19 \n",
      "executing epoch:78, it took 0.8125309507052104 mins from beginning of epoch till batch#262\n",
      "batch processing complete , before updating nll_scale,updating the nll_scale from 3.814380645751953 to 3.8168842792510986|total_loss for batch= 1.620068073272705\n",
      "No improvement in training loss..the global training loss is -->525.27|epoch train_loss = 894.31 \n",
      "executing epoch:79, it took 0.8126144806543986 mins from beginning of epoch till batch#262\n",
      "batch processing complete , before updating nll_scale,updating the nll_scale from 3.8168842792510986 to 3.8178293704986572|total_loss for batch= 1.6585102081298828\n",
      "No improvement in training loss..the global training loss is -->525.27|epoch train_loss = 894.31 \n",
      "executing epoch:80, it took 0.8122815728187561 mins from beginning of epoch till batch#262\n",
      "batch processing complete , before updating nll_scale,updating the nll_scale from 3.8178293704986572 to 3.819070816040039|total_loss for batch= 1.654606580734253\n",
      "No improvement in training loss..the global training loss is -->525.27|epoch train_loss = 894.53 \n",
      "executing epoch:81, it took 0.8124674558639526 mins from beginning of epoch till batch#262\n",
      "batch processing complete , before updating nll_scale,updating the nll_scale from 3.819070816040039 to 3.821042776107788|total_loss for batch= 1.784276008605957\n",
      "No improvement in training loss..the global training loss is -->525.27|epoch train_loss = 894.86 \n",
      "executing epoch:82, it took 0.8125977516174316 mins from beginning of epoch till batch#262\n",
      "batch processing complete , before updating nll_scale,updating the nll_scale from 3.821042776107788 to 3.8227038383483887|total_loss for batch= 1.6972196102142334\n",
      "No improvement in training loss..the global training loss is -->525.27|epoch train_loss = 895.02 \n",
      "executing epoch:83, it took 0.812731941541036 mins from beginning of epoch till batch#262\n",
      "batch processing complete , before updating nll_scale,updating the nll_scale from 3.8227038383483887 to 3.82584547996521|total_loss for batch= 1.776308298110962\n",
      "No improvement in training loss..the global training loss is -->525.27|epoch train_loss = 895.45 \n",
      "executing epoch:84, it took 0.8125109076499939 mins from beginning of epoch till batch#262\n",
      "batch processing complete , before updating nll_scale,updating the nll_scale from 3.82584547996521 to 3.827547788619995|total_loss for batch= 1.675706386566162\n",
      "No improvement in training loss..the global training loss is -->525.27|epoch train_loss = 895.47 \n",
      "executing epoch:85, it took 0.8113332271575928 mins from beginning of epoch till batch#262\n",
      "batch processing complete , before updating nll_scale,updating the nll_scale from 3.827547788619995 to 3.8278872966766357|total_loss for batch= 1.827984094619751\n",
      "No improvement in training loss..the global training loss is -->525.27|epoch train_loss = 895.50 \n",
      "executing epoch:86, it took 0.8124362548192342 mins from beginning of epoch till batch#262\n",
      "batch processing complete , before updating nll_scale,updating the nll_scale from 3.8278872966766357 to 3.828571081161499|total_loss for batch= 1.8226330280303955\n",
      "No improvement in training loss..the global training loss is -->525.27|epoch train_loss = 895.74 \n",
      "executing epoch:87, it took 0.8124485731124877 mins from beginning of epoch till batch#262\n",
      "batch processing complete , before updating nll_scale,updating the nll_scale from 3.828571081161499 to 3.8288776874542236|total_loss for batch= 1.683971881866455\n",
      "No improvement in training loss..the global training loss is -->525.27|epoch train_loss = 895.87 \n",
      "executing epoch:88, it took 0.8118744929631551 mins from beginning of epoch till batch#262\n",
      "batch processing complete , before updating nll_scale,updating the nll_scale from 3.8288776874542236 to 3.8295018672943115|total_loss for batch= 1.6836614608764648\n",
      "No improvement in training loss..the global training loss is -->525.27|epoch train_loss = 896.10 \n",
      "executing epoch:89, it took 0.8124354124069214 mins from beginning of epoch till batch#262\n",
      "batch processing complete , before updating nll_scale,updating the nll_scale from 3.8295018672943115 to 3.8299612998962402|total_loss for batch= 1.7124743461608887\n",
      "No improvement in training loss..the global training loss is -->525.27|epoch train_loss = 896.24 \n",
      "executing epoch:90, it took 0.811920702457428 mins from beginning of epoch till batch#262\n",
      "batch processing complete , before updating nll_scale,updating the nll_scale from 3.8299612998962402 to 3.83062481880188|total_loss for batch= 1.7817153930664062\n",
      "No improvement in training loss..the global training loss is -->525.27|epoch train_loss = 896.45 \n",
      "executing epoch:91, it took 0.8126696586608887 mins from beginning of epoch till batch#262\n",
      "batch processing complete , before updating nll_scale,updating the nll_scale from 3.83062481880188 to 3.830761432647705|total_loss for batch= 1.6367290019989014\n",
      "No improvement in training loss..the global training loss is -->525.27|epoch train_loss = 896.54 \n",
      "executing epoch:92, it took 0.8126520872116089 mins from beginning of epoch till batch#262\n",
      "batch processing complete , before updating nll_scale,updating the nll_scale from 3.830761432647705 to 3.8307015895843506|total_loss for batch= 1.7227280139923096\n",
      "No improvement in training loss..the global training loss is -->525.27|epoch train_loss = 896.69 \n",
      "executing epoch:93, it took 0.812835963567098 mins from beginning of epoch till batch#262\n",
      "batch processing complete , before updating nll_scale,updating the nll_scale from 3.830761432647705 to 3.8307368755340576|total_loss for batch= 1.6546382904052734\n",
      "No improvement in training loss..the global training loss is -->525.27|epoch train_loss = 896.88 \n",
      "executing epoch:94, it took 0.8119758566220602 mins from beginning of epoch till batch#262\n",
      "batch processing complete , before updating nll_scale,updating the nll_scale from 3.830761432647705 to 3.831076145172119|total_loss for batch= 1.6905055046081543\n",
      "No improvement in training loss..the global training loss is -->525.27|epoch train_loss = 897.10 \n",
      "executing epoch:95, it took 0.8123536109924316 mins from beginning of epoch till batch#262\n",
      "batch processing complete , before updating nll_scale,updating the nll_scale from 3.831076145172119 to 3.8312222957611084|total_loss for batch= 1.728165626525879\n",
      "No improvement in training loss..the global training loss is -->525.27|epoch train_loss = 897.25 \n",
      "executing epoch:96, it took 0.8128952781359354 mins from beginning of epoch till batch#262\n",
      "batch processing complete , before updating nll_scale,updating the nll_scale from 3.8312222957611084 to 3.831183910369873|total_loss for batch= 1.6053886413574219\n",
      "No improvement in training loss..the global training loss is -->525.27|epoch train_loss = 897.38 \n",
      "executing epoch:97, it took 0.812258239587148 mins from beginning of epoch till batch#262\n",
      "batch processing complete , before updating nll_scale,updating the nll_scale from 3.8312222957611084 to 3.83133602142334|total_loss for batch= 1.7708239555358887\n",
      "No improvement in training loss..the global training loss is -->525.27|epoch train_loss = 897.56 \n",
      "executing epoch:98, it took 0.8126633246739705 mins from beginning of epoch till batch#262\n",
      "batch processing complete , before updating nll_scale,updating the nll_scale from 3.83133602142334 to 3.8312580585479736|total_loss for batch= 1.6407641172409058\n",
      "No improvement in training loss..the global training loss is -->525.27|epoch train_loss = 897.67 \n",
      "executing epoch:99, it took 0.8121203104654948 mins from beginning of epoch till batch#262\n",
      "batch processing complete , before updating nll_scale,updating the nll_scale from 3.83133602142334 to 3.8313043117523193|total_loss for batch= 1.8091270923614502\n",
      "No improvement in training loss..the global training loss is -->525.27|epoch train_loss = 897.83 \n",
      "executing epoch:100, it took 0.8121650179227193 mins from beginning of epoch till batch#262\n",
      "batch processing complete , before updating nll_scale,updating the nll_scale from 3.83133602142334 to 3.831268072128296|total_loss for batch= 1.6637911796569824\n",
      "No improvement in training loss..the global training loss is -->525.27|epoch train_loss = 897.95 \n",
      "executing epoch:101, it took 0.8118813713391622 mins from beginning of epoch till batch#262\n",
      "batch processing complete , before updating nll_scale,updating the nll_scale from 3.83133602142334 to 3.8312339782714844|total_loss for batch= 1.9148131608963013\n",
      "No improvement in training loss..the global training loss is -->525.27|epoch train_loss = 898.08 \n",
      "executing epoch:102, it took 0.8114736557006836 mins from beginning of epoch till batch#262\n",
      "batch processing complete , before updating nll_scale,updating the nll_scale from 3.83133602142334 to 3.831237554550171|total_loss for batch= 1.6862595081329346\n",
      "No improvement in training loss..the global training loss is -->525.27|epoch train_loss = 898.20 \n",
      "executing epoch:103, it took 0.8122957706451416 mins from beginning of epoch till batch#262\n",
      "batch processing complete , before updating nll_scale,updating the nll_scale from 3.83133602142334 to 3.8312246799468994|total_loss for batch= 1.6657239198684692\n",
      "No improvement in training loss..the global training loss is -->525.27|epoch train_loss = 898.32 \n",
      "executing epoch:104, it took 0.812600576877594 mins from beginning of epoch till batch#262\n",
      "batch processing complete , before updating nll_scale,updating the nll_scale from 3.83133602142334 to 3.8313376903533936|total_loss for batch= 1.7401714324951172\n",
      "No improvement in training loss..the global training loss is -->525.27|epoch train_loss = 898.44 \n",
      "executing epoch:105, it took 0.8124504804611206 mins from beginning of epoch till batch#262\n",
      "batch processing complete , before updating nll_scale,updating the nll_scale from 3.8313376903533936 to 3.831296682357788|total_loss for batch= 1.6783628463745117\n",
      "No improvement in training loss..the global training loss is -->525.27|epoch train_loss = 898.55 \n",
      "executing epoch:106, it took 0.8120309750239054 mins from beginning of epoch till batch#262\n",
      "batch processing complete , before updating nll_scale,updating the nll_scale from 3.8313376903533936 to 3.83121657371521|total_loss for batch= 1.7543588876724243\n",
      "No improvement in training loss..the global training loss is -->525.27|epoch train_loss = 898.64 \n",
      "executing epoch:107, it took 0.8126302480697631 mins from beginning of epoch till batch#262\n",
      "batch processing complete , before updating nll_scale,updating the nll_scale from 3.8313376903533936 to 3.8312065601348877|total_loss for batch= 1.643946647644043\n",
      "No improvement in training loss..the global training loss is -->525.27|epoch train_loss = 898.73 \n",
      "executing epoch:108, it took 0.8120399634043376 mins from beginning of epoch till batch#262\n",
      "batch processing complete , before updating nll_scale,updating the nll_scale from 3.8313376903533936 to 3.831270217895508|total_loss for batch= 1.6577731370925903\n",
      "No improvement in training loss..the global training loss is -->525.27|epoch train_loss = 898.83 \n",
      "executing epoch:109, it took 0.8125272432963053 mins from beginning of epoch till batch#262\n",
      "batch processing complete , before updating nll_scale,updating the nll_scale from 3.8313376903533936 to 3.831310987472534|total_loss for batch= 1.8209972381591797\n",
      "No improvement in training loss..the global training loss is -->525.27|epoch train_loss = 898.93 \n",
      "executing epoch:110, it took 0.8131091554959615 mins from beginning of epoch till batch#262\n",
      "batch processing complete , before updating nll_scale,updating the nll_scale from 3.8313376903533936 to 3.831252336502075|total_loss for batch= 1.8813912868499756\n",
      "No improvement in training loss..the global training loss is -->525.27|epoch train_loss = 899.00 \n"
     ]
    }
   ],
   "source": [
    "tr_model,epoch_train_log,epoch_val_log = train_model(train_loader, val_loader, model =  model,tokenizer = tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "37bf5c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "# Write the list to a JSON file\n",
    "with open(\"epoch_train_log_mean_nll.json\", \"w\") as file:\n",
    "    json.dump(epoch_train_log, file)\n",
    "\n",
    "with open(\"epoch_val_log_mean_nll.json\", \"w\") as file:\n",
    "    json.dump(epoch_val_log, file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462639aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc47281",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086f896d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a26a3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e6e594",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35bf46cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a10bd2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7555eab2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08d5888",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ba6f03",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
